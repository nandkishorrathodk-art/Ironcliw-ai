name: Unlock Integration E2E Testing

on:
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Test execution mode'
        required: false
        default: 'mock'
        type: choice
        options:
          - mock
          - integration
          - real
      test_duration:
        description: 'Maximum test duration (seconds)'
        required: false
        default: '600'
        type: number
      unlock_cycles:
        description: 'Number of unlock cycles to test'
        required: false
        default: '5'
        type: number
      stress_test:
        description: 'Run stress testing (rapid unlock cycles)'
        required: false
        default: false
        type: boolean
      keychain_test:
        description: 'Test keychain integration'
        required: false
        default: true
        type: boolean
      performance_baseline:
        description: 'Performance baseline check (ms)'
        required: false
        default: '2000'
        type: number

  # Auto-run on relevant changes
  push:
    branches: [main]
    paths:
      - 'backend/core/async_pipeline.py'
      - 'backend/macos_keychain_unlock.py'
      - 'backend/system_control/macos_controller.py'
      - 'backend/voice_unlock/secure_password_typer.py'
      - 'backend/voice_unlock/intelligent_voice_unlock_service.py'
      - 'backend/voice_unlock/objc/server/screen_lock_detector.py'
      - '.github/workflows/unlock-integration-e2e.yml'

  # Daily testing
  schedule:
    - cron: '0 4 * * *'  # 4 AM daily

  # PR testing (mock mode only)
  pull_request:
    paths:
      - 'backend/core/async_pipeline.py'
      - 'backend/macos_keychain_unlock.py'
      - 'backend/system_control/macos_controller.py'
      - 'backend/voice_unlock/**'

  # Can be called by other workflows
  workflow_call:
    inputs:
      test_mode:
        description: 'Test execution mode'
        required: false
        default: 'mock'
        type: string

env:
  PYTHON_VERSION_FILE: '.python-version'
  TEST_SCRIPT_PATH: '.github/workflows/scripts/unlock_integration_e2e_test.py'
  RESULTS_DIR: 'test-results/unlock-e2e'

jobs:
  setup-environment:
    name: Setup Test Environment
    runs-on: ubuntu-latest
    outputs:
      python-version: ${{ steps.detect-python.outputs.version }}
      test-mode: ${{ steps.determine-mode.outputs.mode }}
      can-test-real: ${{ steps.check-runner.outputs.can-test-real }}
      test-matrix: ${{ steps.setup-matrix.outputs.matrix }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v6

      - name: Detect Python Version
        id: detect-python
        run: |
          if [ -f "${{ env.PYTHON_VERSION_FILE }}" ]; then
            VERSION=$(cat "${{ env.PYTHON_VERSION_FILE }}" | tr -d '[:space:]')
            echo "version=${VERSION}" >> $GITHUB_OUTPUT
            echo "üìå Detected Python version: ${VERSION}"
          else
            echo "version=3.10" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è No .python-version file, using default: 3.10"
          fi

      - name: Determine Test Mode
        id: determine-mode
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            MODE="mock"
            echo "üîí PR detected - forcing mock mode for safety"
          elif [ "${{ github.event_name }}" = "schedule" ]; then
            MODE="integration"
            echo "‚è∞ Scheduled run - using integration mode"
          else
            MODE="${{ inputs.test_mode || 'mock' }}"
            echo "üéØ Using requested mode: ${MODE}"
          fi
          echo "mode=${MODE}" >> $GITHUB_OUTPUT

      - name: Check Runner Capabilities
        id: check-runner
        run: |
          # Real tests only on self-hosted macOS runners
          if [ "${{ runner.os }}" = "macOS" ] && [ "${{ runner.name }}" != "GitHub Actions" ]; then
            echo "can-test-real=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Self-hosted macOS runner - real tests available"
          else
            echo "can-test-real=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è GitHub-hosted runner - real tests disabled"
          fi

      - name: Setup Test Matrix
        id: setup-matrix
        run: |
          # Dynamically create test matrix based on mode
          MODE="${{ steps.determine-mode.outputs.mode }}"

          if [ "$MODE" = "mock" ]; then
            MATRIX='{"test-suite": ["keychain-retrieval", "unlock-logic", "secure-password-typer", "intelligent-voice-service", "screen-detector-integration", "adaptive-timing", "memory-security", "fallback-mechanisms", "error-handling", "performance", "security-checks"], "python-version": ["${{ steps.detect-python.outputs.version }}"]}'
          elif [ "$MODE" = "integration" ]; then
            MATRIX='{"test-suite": ["keychain-retrieval", "unlock-logic", "secure-password-typer", "intelligent-voice-service", "screen-detector-integration", "adaptive-timing", "memory-security", "fallback-mechanisms", "error-handling", "performance", "security-checks"], "python-version": ["${{ steps.detect-python.outputs.version }}"]}'
          else
            MATRIX='{"test-suite": ["full-e2e"], "python-version": ["${{ steps.detect-python.outputs.version }}"]}'
          fi

          echo "matrix=${MATRIX}" >> $GITHUB_OUTPUT
          echo "üìã Test matrix: ${MATRIX}"

  test-mock-mode:
    name: Mock Tests - ${{ matrix.test-suite }}
    runs-on: ubuntu-latest
    needs: setup-environment
    if: needs.setup-environment.outputs.test-mode == 'mock'
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.setup-environment.outputs.test-matrix) }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v6

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-timeout pytest-mock aiohttp asyncio-pool aiodns aiofiles
          echo "‚úÖ Dependencies installed"

      - name: Verify Test Script Exists
        run: |
          if [ -f "${{ env.TEST_SCRIPT_PATH }}" ]; then
            chmod +x "${{ env.TEST_SCRIPT_PATH }}"
            echo "‚úÖ Test script ready: ${{ env.TEST_SCRIPT_PATH }}"
          else
            echo "‚ùå Test script not found: ${{ env.TEST_SCRIPT_PATH }}"
            exit 1
          fi


      - name: Run Mock Tests
        env:
          TEST_MODE: mock
          TEST_DURATION: ${{ inputs.test_duration || '600' }}
          UNLOCK_CYCLES: ${{ inputs.unlock_cycles || '5' }}
          STRESS_TEST: ${{ inputs.stress_test || 'false' }}
          KEYCHAIN_TEST: ${{ inputs.keychain_test || 'true' }}
          PERFORMANCE_BASELINE: ${{ inputs.performance_baseline || '2000' }}
          TEST_SUITE: ${{ matrix.test-suite }}
          RESULTS_DIR: ${{ env.RESULTS_DIR }}
          MAX_CONCURRENT: 5
        run: |
          echo "üöÄ Running async mock tests (concurrent: 5)..."
          python3 "${{ env.TEST_SCRIPT_PATH }}"

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: test-results-mock-${{ matrix.test-suite }}
          path: ${{ env.RESULTS_DIR }}
          retention-days: 30

  test-integration-mode:
    name: Integration Tests - macOS
    runs-on: macos-latest
    needs: setup-environment
    if: needs.setup-environment.outputs.test-mode == 'integration'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v6

      - name: Setup Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ needs.setup-environment.outputs.python-version }}
          cache: 'pip'

      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-timeout aiohttp

          # Install backend dependencies
          if [ -f backend/requirements.txt ]; then
            pip install -r backend/requirements.txt
          fi

          echo "‚úÖ Dependencies installed"

      - name: Create Test Script
        run: |
          # Same script as mock mode (already created above)
          mkdir -p $(dirname "${{ env.TEST_SCRIPT_PATH }}")
          # Script content would be same as above

      - name: Run Integration Tests
        env:
          TEST_MODE: integration
          TEST_DURATION: ${{ inputs.test_duration || '600' }}
          UNLOCK_CYCLES: ${{ inputs.unlock_cycles || '5' }}
          STRESS_TEST: ${{ inputs.stress_test || 'false' }}
          KEYCHAIN_TEST: ${{ inputs.keychain_test || 'true' }}
          PERFORMANCE_BASELINE: ${{ inputs.performance_baseline || '2000' }}
          RESULTS_DIR: ${{ env.RESULTS_DIR }}
          MAX_CONCURRENT: 3
        run: |
          echo "üü° Running async integration tests on macOS (concurrent: 3)..."
          python3 "${{ env.TEST_SCRIPT_PATH }}" || true

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: test-results-integration
          path: ${{ env.RESULTS_DIR }}
          retention-days: 30

  test-real-mode:
    name: Real E2E Tests - Self-Hosted
    runs-on: self-hosted
    needs: setup-environment
    if: |
      needs.setup-environment.outputs.test-mode == 'real' &&
      needs.setup-environment.outputs.can-test-real == 'true'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v6

      - name: Check Prerequisites
        run: |
          echo "üîê Checking keychain access..."
          if security find-generic-password -s "com.jarvis.voiceunlock" -a "unlock_token" -w > /dev/null 2>&1; then
            echo "‚úÖ Keychain entry found"
          else
            echo "‚ùå Keychain entry not found"
            echo "Please run: ./backend/voice_unlock/enable_screen_unlock.sh"
            exit 1
          fi

      - name: Setup Python
        run: |
          # Use system Python or specific version
          python3 --version

      - name: Install Dependencies
        run: |
          pip3 install --upgrade pip
          pip3 install pytest pytest-asyncio aiohttp

          if [ -f backend/requirements.txt ]; then
            pip3 install -r backend/requirements.txt
          fi

      - name: Run Real E2E Tests
        env:
          TEST_MODE: real
          TEST_DURATION: ${{ inputs.test_duration || '600' }}
          UNLOCK_CYCLES: ${{ inputs.unlock_cycles || '3' }}
          RESULTS_DIR: ${{ env.RESULTS_DIR }}
          MAX_CONCURRENT: 1
        run: |
          echo "üî¥ Running REAL unlock tests (sequential for safety)..."
          echo "‚ö†Ô∏è  This will actually unlock your screen!"
          python3 "${{ env.TEST_SCRIPT_PATH }}"

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: test-results-real
          path: ${{ env.RESULTS_DIR }}
          retention-days: 90

  report-summary:
    name: Generate Test Summary
    runs-on: ubuntu-latest
    needs: [setup-environment, test-mock-mode]
    if: always()
    steps:
      - name: Download All Results
        uses: actions/download-artifact@v7
        with:
          path: all-results

      - name: Generate Summary
        run: |
          echo "# üîì Unlock Integration E2E Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Mode:** ${{ needs.setup-environment.outputs.test-mode }}" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Parse results
          TOTAL_PASSED=0
          TOTAL_FAILED=0

          for report in all-results/*/report-*.json; do
            if [ -f "$report" ]; then
              PASSED=$(jq -r '.summary.passed' "$report")
              FAILED=$(jq -r '.summary.failed' "$report")
              TOTAL_PASSED=$((TOTAL_PASSED + PASSED))
              TOTAL_FAILED=$((TOTAL_FAILED + FAILED))
            fi
          done

          TOTAL=$((TOTAL_PASSED + TOTAL_FAILED))
          SUCCESS_RATE=$(echo "scale=1; $TOTAL_PASSED * 100 / $TOTAL" | bc)

          echo "## Results" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Passed: $TOTAL_PASSED" >> $GITHUB_STEP_SUMMARY
          echo "- ‚ùå Failed: $TOTAL_FAILED" >> $GITHUB_STEP_SUMMARY
          echo "- üìà Success Rate: ${SUCCESS_RATE}%" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ $TOTAL_FAILED -gt 0 ]; then
            echo "## ‚ö†Ô∏è Failed Tests" >> $GITHUB_STEP_SUMMARY
            for report in all-results/*/report-*.json; do
              if [ -f "$report" ]; then
                jq -r '.tests[] | select(.success == false) | "- ‚ùå \(.name): \(.message)"' "$report" >> $GITHUB_STEP_SUMMARY
              fi
            done
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "üìä Full reports available in workflow artifacts" >> $GITHUB_STEP_SUMMARY

      - name: Check Test Status
        run: |
          # Fail workflow if any tests failed
          for report in all-results/*/report-*.json; do
            if [ -f "$report" ]; then
              FAILED=$(jq -r '.summary.failed' "$report")
              if [ "$FAILED" -gt 0 ]; then
                echo "‚ùå Tests failed"
                exit 1
              fi
            fi
          done

          echo "‚úÖ All tests passed"
