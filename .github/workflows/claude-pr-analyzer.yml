name: Claude AI PR Analyzer

on:
  workflow_dispatch:
    inputs:
      pr_number:
        description: 'PR number to analyze'
        required: true
        type: number
  # Automatic triggers disabled to save API costs
  # Uncomment below to re-enable automatic analysis:
  # pull_request:
  #   types: [opened, synchronize, reopened]
  # pull_request_review_comment:
  #   types: [created]
  # issue_comment:
  #   types: [created]

permissions:
  contents: write
  pull-requests: write
  issues: write

jobs:
  analyze-pr:
    name: AI-Powered PR Analysis
    runs-on: ubuntu-latest
    # Concurrency control - only one Claude analysis at a time to prevent rate limiting
    concurrency:
      group: claude-pr-analysis
      cancel-in-progress: false
    if: |
      (github.event_name == 'pull_request' &&
       github.event.pull_request.changed_files < 50) ||
      (github.event_name == 'issue_comment' &&
       contains(github.event.comment.body, '@claude') &&
       github.event.issue.pull_request)

    steps:
      - name: Checkout PR
        uses: actions/checkout@v6
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          fetch-depth: 0

      - name: Get PR Number
        id: pr_number
        uses: actions/github-script@v8
        with:
          script: |
            const pr = context.payload.pull_request ||
                      (await github.rest.pulls.get({
                        owner: context.repo.owner,
                        repo: context.repo.repo,
                        pull_number: context.issue.number
                      })).data;

            return pr.number;

      - name: Rate Limit Protection
        run: |
          echo "ðŸ• Applying rate limit protection to prevent API throttling..."
          echo "â³ Waiting 75 seconds to ensure rate limit window is clear..."
          echo "   (Anthropic API: 8,000 output tokens/minute limit)"
          sleep 75
          echo "âœ… Rate limit window clear - proceeding with analysis"

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: '3.10'

      - name: Install Dependencies
        run: |
          pip install anthropic pygithub gitpython

      - name: Run Claude AI Analysis
        id: ai_analysis
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          PR_NUMBER: ${{ steps.pr_number.outputs.result }}
        run: |
          python3 << 'PYTHON_SCRIPT'
          import anthropic
          import os
          import json
          import sys
          import re
          import time
          import asyncio
          from pathlib import Path
          from typing import List, Dict, Optional, Tuple
          from dataclasses import dataclass
          from github import Github, GithubException
          import concurrent.futures

          # Initialize clients
          claude_client = anthropic.Anthropic(api_key=os.environ['ANTHROPIC_API_KEY'])
          github_client = Github(os.environ['GITHUB_TOKEN'])

          # Get repository and PR
          repo = github_client.get_repo(os.environ['GITHUB_REPOSITORY'])
          pr_number = int(os.environ['PR_NUMBER'])
          pr = repo.get_pull(pr_number)

          print(f"ðŸ” Analyzing PR #{pr_number}: {pr.title}")
          print(f"ðŸ“Š Initial stats: {pr.changed_files} files, +{pr.additions}/-{pr.deletions} lines")

          # File filtering patterns - skip these
          SKIP_PATTERNS = [
              r'\.lock$',           # Lock files
              r'\.min\.(js|css)$',  # Minified files
              r'^dist/',            # Distribution builds
              r'^build/',           # Build artifacts
              r'node_modules/',     # Dependencies
              r'__pycache__/',      # Python cache
              r'\.pyc$',            # Compiled Python
              r'^venv/',            # Virtual environments
              r'\.(png|jpg|jpeg|gif|svg|ico)$',  # Images
              r'\.(pdf|zip|tar|gz)$',             # Binaries
          ]

          # Documentation file patterns - analyze differently
          DOC_PATTERNS = [
              r'\.md$',
              r'^docs/',
              r'README',
              r'CHANGELOG',
              r'LICENSE',
          ]

          def should_skip_file(filename):
              """Check if file should be skipped entirely."""
              for pattern in SKIP_PATTERNS:
                  if re.search(pattern, filename, re.IGNORECASE):
                      return True
              return False

          def is_doc_file(filename):
              """Check if file is documentation."""
              for pattern in DOC_PATTERNS:
                  if re.search(pattern, filename, re.IGNORECASE):
                      return True
              return False

          def estimate_tokens(text):
              """Rough token estimation (1 token â‰ˆ 4 characters)."""
              return len(text) // 4

          def truncate_content(content, max_tokens=2000):
              """Intelligently truncate content while preserving structure."""
              if estimate_tokens(content) <= max_tokens:
                  return content, False

              lines = content.split('\n')
              if len(lines) <= 100:
                  # Small file, just truncate
                  max_chars = max_tokens * 4
                  return content[:max_chars] + '\n... [truncated]', True

              # For large files, keep beginning and end
              keep_lines = 50
              truncated = '\n'.join([
                  *lines[:keep_lines],
                  f'\n... [{len(lines) - (keep_lines * 2)} lines truncated] ...\n',
                  *lines[-keep_lines:]
              ])
              return truncated, True

          def truncate_patch(patch, max_lines=30):
              """Truncate diff patch to show most relevant changes."""
              if not patch:
                  return patch, False

              lines = patch.split('\n')
              if len(lines) <= max_lines:
                  return patch, False

              # Keep first and last portions of patch
              keep = max_lines // 2
              truncated = '\n'.join([
                  *lines[:keep],
                  f'... [{len(lines) - (keep * 2)} lines truncated] ...',
                  *lines[-keep:]
              ])
              return truncated, True

          @dataclass
          class FileAnalysis:
              """Structured file analysis data."""
              path: str
              status: str
              additions: int
              deletions: int
              patch: str
              content_preview: str
              was_truncated: bool
              tokens: int
              file_type: str  # 'code', 'doc', 'skipped'

          def process_file(file_obj) -> Optional[FileAnalysis]:
              """Process a single file with intelligent filtering."""
              file_path = file_obj.filename

              # Skip filtered files
              if should_skip_file(file_path):
                  return FileAnalysis(
                      path=file_path, status='skipped', additions=0, deletions=0,
                      patch='', content_preview='', was_truncated=False,
                      tokens=0, file_type='skipped'
                  )

              # Handle documentation files
              if is_doc_file(file_path):
                  return FileAnalysis(
                      path=file_path, status=file_obj.status,
                      additions=file_obj.additions, deletions=file_obj.deletions,
                      patch='', content_preview='', was_truncated=False,
                      tokens=0, file_type='doc'
                  )

              # Process code files
              try:
                  if Path(file_path).exists():
                      content = Path(file_path).read_text(encoding='utf-8', errors='ignore')
                  else:
                      content = "[File deleted]"
              except Exception as e:
                  content = f"[Error: {str(e)[:100]}]"

              # Truncate intelligently
              content_truncated, content_was_truncated = truncate_content(content, max_tokens=2000)

              # Get patch with size limit
              try:
                  patch = file_obj.patch if hasattr(file_obj, 'patch') and file_obj.patch else ''
              except:
                  patch = ''

              patch_truncated, patch_was_truncated = truncate_patch(patch, max_lines=30)

              file_tokens = estimate_tokens(content_truncated) + estimate_tokens(patch_truncated)

              return FileAnalysis(
                  path=file_path,
                  status=file_obj.status,
                  additions=file_obj.additions,
                  deletions=file_obj.deletions,
                  patch=patch_truncated,
                  content_preview=content_truncated,
                  was_truncated=content_was_truncated or patch_was_truncated,
                  tokens=file_tokens,
                  file_type='code'
              )

          # Fetch and process files with parallel execution
          print(f"ðŸ”„ Fetching PR files...")
          pr_files = list(pr.get_files())
          print(f"ðŸ“Š Processing {len(pr_files)} files in parallel...")

          # Process files in parallel for speed
          processed_files = []
          with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
              future_to_file = {executor.submit(process_file, f): f for f in pr_files}
              for future in concurrent.futures.as_completed(future_to_file):
                  try:
                      result = future.result()
                      if result:
                          processed_files.append(result)
                  except Exception as e:
                      file_obj = future_to_file[future]
                      print(f"âš ï¸  Error processing {file_obj.filename}: {e}")

          # Categorize files
          code_files = []
          doc_files = []
          skipped_files = []
          total_tokens = 0
          MAX_TOTAL_TOKENS = 40000

          for file in processed_files:
              if file.file_type == 'skipped':
                  skipped_files.append(file.path)
                  print(f"â­ï¸  Skipped: {file.path}")
              elif file.file_type == 'doc':
                  doc_files.append(file)
                  print(f"ðŸ“ Doc: {file.path}")
              elif file.file_type == 'code':
                  # Check token budget
                  if total_tokens + file.tokens > MAX_TOTAL_TOKENS:
                      print(f"âš ï¸  Token budget exceeded at {file.path}")
                      skipped_files.append(f"{file.path} (token limit)")
                      continue

                  code_files.append(file)
                  total_tokens += file.tokens
                  print(f"âœ… Analyzed: {file.path} (~{file.tokens} tokens)")

          # Calculate totals
          total_additions = sum(f.additions for f in processed_files)
          total_deletions = sum(f.deletions for f in processed_files)

          print(f"\nðŸ“Š Processing Summary:")
          print(f"  - Total files in PR: {len(pr_files)}")
          print(f"  - Code files analyzed: {len(code_files)}")
          print(f"  - Documentation files: {len(doc_files)}")
          print(f"  - Skipped files: {len(skipped_files)}")
          print(f"  - Total tokens: ~{total_tokens:,}")
          print(f"  - Lines: +{total_additions:,}/-{total_deletions:,}")

          # Prepare context for Claude
          pr_body = pr.body or "No description provided"
          context = f"""# Pull Request Analysis Request

          ## PR Information
          - **Title:** {pr.title}
          - **Author:** {pr.user.login}
          - **Base Branch:** {pr.base.ref}
          - **Head Branch:** {pr.head.ref}
          - **Description:** {pr_body[:500]}{'...' if len(pr_body) > 500 else ''}

          ## Changes Summary
          - **Total Files Changed:** {len(processed_files)}
          - **Code Files Analyzed:** {len(code_files)}
          - **Documentation Files:** {len(doc_files)} (summarized below)
          - **Skipped Files:** {len(skipped_files)}
          - **Lines Added:** {total_additions}
          - **Lines Deleted:** {total_deletions}

          """

          # Add documentation summary if present
          if doc_files:
              context += "\n## Documentation Changes\n"
              context += f"This PR includes {len(doc_files)} documentation file(s):\n"
              for doc in doc_files[:5]:  # Limit to 5
                  context += f"- {doc.path}: +{doc.additions} -{doc.deletions} lines\n"
              if len(doc_files) > 5:
                  context += f"- ... and {len(doc_files) - 5} more documentation files\n"
              context += "\n**Note:** Focus analysis on code changes; documentation appears comprehensive.\n"

          # Add code file details
          if code_files:
              context += "\n## Code Files Changed\n"
              for file in code_files[:15]:  # Limit to 15 files for analysis
                  context += f"""
          ### {file.path}
          - **Status:** {file.status}
          - **Changes:** +{file.additions} -{file.deletions}
          {'- **Note:** Content truncated for analysis' if file.was_truncated else ''}

          **Diff:**
          ```diff
          {file.patch if file.patch else 'No diff available'}
          ```

          **Current Content:**
          ```
          {file.content_preview}
          ```
          """

              if len(code_files) > 15:
                  context += f"\n... and {len(code_files) - 15} more code files not shown due to size limits.\n"

          # Add skipped files note
          if skipped_files:
              context += f"\n## Files Not Analyzed\n"
              context += f"Skipped {len(skipped_files)} file(s): {', '.join(skipped_files[:10])}"
              if len(skipped_files) > 10:
                  context += f" and {len(skipped_files) - 10} more"
              context += "\n"

          # Call Claude for analysis
          print("ðŸ¤– Calling Claude AI for analysis...")

          # Adjust analysis based on PR type
          is_doc_heavy = len(doc_files) > len(code_files) and len(doc_files) > 3

          if is_doc_heavy:
              system_prompt = """You are an expert documentation reviewer analyzing a documentation-heavy pull request.

          This PR is primarily documentation changes. Focus your analysis on:
          - **Documentation Quality:** Clarity, completeness, accuracy
          - **Structure:** Organization, navigation, consistency
          - **Code Examples:** Correctness of any code snippets in docs
          - **Accessibility:** Readability for target audience

          Provide a concise review with:
          1. **Overall Assessment** (score 1-10)
          2. **Documentation Quality** (clarity, completeness, accuracy)
          3. **Suggestions** (improvements, corrections, additions)
          4. **Merge Decision** (ready/needs-work)

          Keep response concise since this is primarily documentation.
          Format in clear Markdown with sections and emojis."""
          else:
              system_prompt = """You are an expert code reviewer and software architect analyzing a pull request for the JARVIS AI Agent project.

          Your analysis should be:
          - **Comprehensive:** Cover code quality, architecture, security, performance, and best practices
          - **Actionable:** Provide specific suggestions with code examples
          - **Constructive:** Focus on improvements while acknowledging good work
          - **Context-aware:** Understand this is an AI agent system with hybrid architecture
          - **Security-focused:** Identify any security vulnerabilities or credential exposure
          - **Performance-conscious:** Highlight performance issues or optimizations

          Analyze the PR and provide:
          1. **Overall Assessment** (score 1-10 and summary)
          2. **Code Quality Analysis** (style, readability, maintainability)
          3. **Architecture Impact** (how it fits with existing system)
          4. **Security Review** (vulnerabilities, best practices)
          5. **Performance Analysis** (bottlenecks, optimizations)
          6. **Test Coverage** (adequacy of tests)
          7. **Specific Issues Found** (with severity: critical/major/minor)
          8. **Recommendations** (prioritized action items)
          9. **Merge Decision** (ready/needs-work/blocking-issues)

          Format your response in clear Markdown with sections, code blocks, and emojis."""

          # Call Claude API with exponential backoff retry for transient errors
          # Reduced max_tokens to 4000 to stay well under 8000/minute rate limit
          max_tokens_limit = 4000 if is_doc_heavy else 3000
          max_retries = 4  # Increased retries for rate limit handling
          retry_delay = 10  # Longer initial delay for rate limits

          for attempt in range(max_retries):
              try:
                  message = claude_client.messages.create(
                      model="claude-sonnet-4-20250514",
                      max_tokens=max_tokens_limit,
                      temperature=0,
                      system=system_prompt,
                      messages=[
                          {
                              "role": "user",
                              "content": context
                          }
                      ]
                  )
                  break  # Success, exit retry loop
              except Exception as e:
                  error_type = type(e).__name__
                  # Handle rate limits and transient errors with longer backoff
                  if attempt < max_retries - 1 and ('overloaded' in str(e).lower() or 'rate_limit' in str(e).lower() or '529' in str(e) or '429' in str(e)):
                      # For rate limits, use 60s+ delays to clear the minute window
                      if 'rate_limit' in str(e).lower() or '429' in str(e):
                          wait_time = 60 + (retry_delay * (2 ** attempt))
                      else:
                          wait_time = retry_delay * (2 ** attempt)
                      print(f"âš ï¸  API temporarily unavailable ({error_type}), retrying in {wait_time}s... (attempt {attempt + 1}/{max_retries})")
                      time.sleep(wait_time)
                  else:
                      # Final attempt failed or non-retryable error
                      print(f"âŒ API call failed: {e}")
                      raise

          analysis = message.content[0].text

          # Save analysis to file
          with open('/tmp/claude_analysis.md', 'w') as f:
              f.write(analysis)

          # Extract merge decision
          merge_ready = "ready" in analysis.lower() and "merge decision" in analysis.lower()

          # Calculate token usage
          input_tokens = message.usage.input_tokens
          output_tokens = message.usage.output_tokens
          total_tokens = input_tokens + output_tokens
          estimated_cost = (input_tokens * 0.003 / 1000) + (output_tokens * 0.015 / 1000)

          # Output for GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"merge_ready={str(merge_ready).lower()}\n")
              f.write(f"analysis_length={len(analysis)}\n")
              f.write(f"code_files={len(code_files)}\n")
              f.write(f"doc_files={len(doc_files)}\n")
              f.write(f"skipped_files={len(skipped_files)}\n")
              f.write(f"input_tokens={input_tokens}\n")
              f.write(f"output_tokens={output_tokens}\n")
              f.write(f"total_tokens={total_tokens}\n")
              f.write(f"estimated_cost={estimated_cost:.4f}\n")

          print("âœ… Claude AI analysis complete!")
          print(f"ðŸ“Š Statistics:")
          print(f"  - Analysis length: {len(analysis)} characters")
          print(f"  - Code files analyzed: {len(code_files)}")
          print(f"  - Documentation files: {len(doc_files)}")
          print(f"  - Skipped files: {len(skipped_files)}")
          print(f"  - Input tokens: {input_tokens:,}")
          print(f"  - Output tokens: {output_tokens:,}")
          print(f"  - Total tokens: {total_tokens:,}")
          print(f"  - Estimated cost: ${estimated_cost:.4f}")
          print(f"  - Merge ready: {merge_ready}")

          PYTHON_SCRIPT

      - name: Post AI Analysis as Comment
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const analysis = fs.readFileSync('/tmp/claude_analysis.md', 'utf8');

            const pr = context.payload.pull_request ||
                      (await github.rest.pulls.get({
                        owner: context.repo.owner,
                        repo: context.repo.repo,
                        pull_number: context.issue.number
                      })).data;

            // Find existing Claude comment
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: pr.number
            });

            const claudeComment = comments.data.find(c =>
              c.user.type === 'Bot' &&
              c.body.includes('ðŸ¤– Claude AI Code Review')
            );

            const commentBody = `## ðŸ¤– Claude AI Code Review

            ${analysis}

            ---
            <sub>Powered by Claude Sonnet 4 | Generated at ${new Date().toUTCString()}</sub>
            <sub>Mention @claude in comments to re-trigger analysis</sub>`;

            if (claudeComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: claudeComment.id,
                body: commentBody
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: pr.number,
                body: commentBody
              });
            }

      - name: Add Labels Based on AI Analysis
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const analysis = fs.readFileSync('/tmp/claude_analysis.md', 'utf8').toLowerCase();

            const pr = context.payload.pull_request ||
                      (await github.rest.pulls.get({
                        owner: context.repo.owner,
                        repo: context.repo.repo,
                        pull_number: context.issue.number
                      })).data;

            const labels = [];

            // Intelligent label detection
            if (analysis.includes('security') || analysis.includes('vulnerability')) {
              labels.push('security');
            }
            if (analysis.includes('performance') || analysis.includes('optimization')) {
              labels.push('performance');
            }
            if (analysis.includes('breaking') || analysis.includes('breaking change')) {
              labels.push('breaking-change');
            }
            if (analysis.includes('needs work') || analysis.includes('blocking')) {
              labels.push('needs-work');
            }
            if (analysis.includes('ready') && analysis.includes('merge decision')) {
              labels.push('ready-to-merge');
            }
            if (analysis.includes('test') && (analysis.includes('missing') || analysis.includes('inadequate'))) {
              labels.push('needs-tests');
            }
            if (analysis.includes('documentation') || analysis.includes('docs')) {
              labels.push('documentation');
            }

            if (labels.length > 0) {
              await github.rest.issues.addLabels({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: pr.number,
                labels: labels
              });
            }

      - name: Create Review with AI Feedback
        if: steps.ai_analysis.outputs.merge_ready == 'false'
        uses: actions/github-script@v8
        with:
          script: |
            const pr = context.payload.pull_request;
            if (!pr) return;

            await github.rest.pulls.createReview({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: pr.number,
              event: 'REQUEST_CHANGES',
              body: 'ðŸ¤– Claude AI has identified issues that need to be addressed before merging. Please review the detailed analysis above.'
            });

      - name: Summary
        run: |
          echo "## ðŸ¤– Claude AI Analysis Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“Š Analysis Metrics" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Code Files Analyzed | ${{ steps.ai_analysis.outputs.code_files }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Documentation Files | ${{ steps.ai_analysis.outputs.doc_files }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Files Skipped | ${{ steps.ai_analysis.outputs.skipped_files }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Analysis Length | ${{ steps.ai_analysis.outputs.analysis_length }} chars |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ’° Token Usage & Cost" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Input Tokens | ${{ steps.ai_analysis.outputs.input_tokens }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Output Tokens | ${{ steps.ai_analysis.outputs.output_tokens }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Total Tokens | ${{ steps.ai_analysis.outputs.total_tokens }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Estimated Cost | \$${{ steps.ai_analysis.outputs.estimated_cost }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### âœ… Merge Decision" >> $GITHUB_STEP_SUMMARY
          echo "**Merge Ready:** ${{ steps.ai_analysis.outputs.merge_ready }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Full analysis posted as PR comment." >> $GITHUB_STEP_SUMMARY
