#!/usr/bin/env python3
"""
JARVIS Supervisor Entry Point - Production Grade v5.0 (Living OS Edition)
===========================================================================

Advanced, robust, async, parallel, intelligent, and dynamic supervisor entry point.
This is the SINGLE COMMAND needed to run JARVIS - it handles everything.

v5.0 Living OS Features:
- ğŸ”¥ DEV MODE: Hot reload / live reload - edit code and see changes instantly
- ğŸ”„ Zero-Touch autonomous self-updating without human intervention
- ğŸ›¡ï¸ Dead Man's Switch for post-update stability verification
- ğŸ™ï¸ Unified voice coordination (narrator + announcer working together)
- ğŸ“‹ Prime Directives (immutable safety constraints)
- ğŸ§  AGI OS integration for intelligent decision making

v7.0 JARVIS-Prime Integration:
- ğŸ§  Tier-0 Local Brain: GGUF model inference via llama-cpp-python
- ğŸ³ Docker/Cloud Run: Serverless deployment to Google Cloud Run
- ğŸ”¬ Reactor-Core: Auto-deployment of trained models from reactor-core
- ğŸ’° Cost-Effective: Free local inference, reduces cloud API costs

Dev Mode (Hot Reload):
    When you run `python3 run_supervisor.py`, it:
    1. Starts JARVIS normally
    2. Watches your code for changes (.py, .yaml files)
    3. Automatically restarts JARVIS when you save changes
    4. You never have to manually restart while developing!
    
    This is like "nodemon" for Python - write code, save, see changes.

Core Features:
- Parallel process discovery and termination with cascade strategy
- Async resource validation (memory, disk, ports, network)
- Dynamic configuration with environment variable overrides
- Connection pooling and intelligent health pre-checks
- Parallel component initialization with dependency resolution
- Graceful shutdown orchestration with cleanup tasks
- Performance metrics and timing analysis
- Circuit breaker for repeated failures
- Adaptive startup based on system resources

Architecture:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  SupervisorBootstrapper (this file)                         â”‚
    â”‚  â”œâ”€â”€ ParallelProcessCleaner (async process termination)     â”‚
    â”‚  â”œâ”€â”€ IntelligentResourceOrchestrator (pre-flight checks)    â”‚
    â”‚  â”œâ”€â”€ DynamicConfigLoader (env + yaml + defaults)            â”‚
    â”‚  â”œâ”€â”€ AGIOSBridge (intelligent decision propagation)         â”‚
    â”‚  â”œâ”€â”€ HotReloadWatcher (file change detection) [DEV MODE]    â”‚
    â”‚  â””â”€â”€ IntelligentStartupOrchestrator (phased initialization) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  JARVISSupervisor (core/supervisor/jarvis_supervisor.py)    â”‚
    â”‚  â”œâ”€â”€ ZeroTouchEngine (autonomous updates)                   â”‚
    â”‚  â”œâ”€â”€ DeadManSwitch (stability verification)                 â”‚
    â”‚  â”œâ”€â”€ UpdateEngine (staging, validation, classification)     â”‚
    â”‚  â”œâ”€â”€ RollbackManager (version history, snapshots)           â”‚
    â”‚  â”œâ”€â”€ UnifiedStartupVoiceCoordinator (narrator + announcer)  â”‚
    â”‚  â””â”€â”€ SupervisorNarrator v5.0 (intelligent voice feedback)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Usage:
    # Run supervisor (recommended way to start JARVIS)
    # This is ALL you need - it handles everything including hot reload!
    python run_supervisor.py

    # Disable dev mode / hot reload (production)
    JARVIS_DEV_MODE=false python run_supervisor.py

    # With debug logging
    JARVIS_SUPERVISOR_LOG_LEVEL=DEBUG python run_supervisor.py

    # Disable voice narration
    STARTUP_NARRATOR_VOICE=false python run_supervisor.py

    # Skip resource validation (faster startup)
    SKIP_RESOURCE_CHECK=true python run_supervisor.py

    # Enable Zero-Touch autonomous updates
    JARVIS_ZERO_TOUCH_ENABLED=true python run_supervisor.py

    # Configure Dead Man's Switch probation period
    JARVIS_DMS_PROBATION_SECONDS=60 python run_supervisor.py

    # Configure hot reload check interval (default: 10s)
    JARVIS_RELOAD_CHECK_INTERVAL=5 python run_supervisor.py

    # Configure startup grace period before hot reload activates (default: 120s)
    JARVIS_RELOAD_GRACE_PERIOD=60 python run_supervisor.py

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # JARVIS-Prime Configuration (v7.0)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Disable JARVIS-Prime (use cloud APIs only)
    JARVIS_PRIME_ENABLED=false python run_supervisor.py

    # Use Docker container for JARVIS-Prime
    JARVIS_PRIME_USE_DOCKER=true python run_supervisor.py

    # Use Cloud Run for JARVIS-Prime
    JARVIS_PRIME_USE_CLOUD_RUN=true \
    JARVIS_PRIME_CLOUD_RUN_URL=https://jarvis-prime-xxx.run.app \
    python run_supervisor.py

    # Custom model path
    JARVIS_PRIME_MODELS_DIR=/path/to/models python run_supervisor.py

    # Enable Reactor-Core auto-deployment
    REACTOR_CORE_ENABLED=true \
    REACTOR_CORE_OUTPUT=/path/to/reactor-core/output \
    python run_supervisor.py

Author: JARVIS System
Version: 7.0.0
"""
from __future__ import annotations

# =============================================================================
# v122.0: CRITICAL - EARLY SIGNAL PROTECTION FOR CLI COMMANDS
# =============================================================================
# When running --restart, the supervisor sends signals that can kill the client
# process DURING Python startup (before main() runs). This protection MUST
# happen at module level, before ANY other imports, to survive the signal storm.
#
# Exit code 144 = 128 + 16 (killed by signal 16) was happening because signals
# arrived during import phase when Python signal handlers weren't yet installed.
# =============================================================================
import sys as _early_sys
import signal as _early_signal
import os as _early_os

# Check if this is a CLI command that needs signal protection
_cli_flags = ('--restart', '--shutdown', '--status', '--cleanup')
_is_cli_mode = any(flag in _early_sys.argv for flag in _cli_flags)

if _is_cli_mode:
    # FIRST: Ignore ALL signals to protect this process
    for _sig in (
        _early_signal.SIGINT,   # 2 - Ctrl+C
        _early_signal.SIGTERM,  # 15 - Termination
        _early_signal.SIGHUP,   # 1 - Hangup
        _early_signal.SIGURG,   # 16 - Urgent data (exit 144!)
        _early_signal.SIGPIPE,  # 13 - Broken pipe
        _early_signal.SIGALRM,  # 14 - Alarm
        _early_signal.SIGUSR1,  # 30 - User signal 1
        _early_signal.SIGUSR2,  # 31 - User signal 2
    ):
        try:
            _early_signal.signal(_sig, _early_signal.SIG_IGN)
        except (OSError, ValueError):
            pass  # Some signals can't be ignored

    # v122.4: For --restart, the parent process will be killed by signals regardless
    # of what we do. So instead: launch detached child and EXIT IMMEDIATELY.
    # The detached child does the actual restart work.
    if '--restart' in _early_sys.argv and not _early_os.environ.get('_JARVIS_RESTART_REEXEC'):
        import subprocess as _sp
        import tempfile as _tmp

        # Create result file path
        _result_path = f"/tmp/jarvis_restart_{_early_os.getpid()}.result"

        # Write standalone restart script
        _script_content = f'''#!/usr/bin/env python3
import os, sys, signal, subprocess, time

# Full signal immunity
for s in range(1, 32):
    try:
        if s not in (9, 17):
            signal.signal(s, signal.SIG_IGN)
    except: pass

# New session
try: os.setsid()
except: pass

# Run the actual restart
env = dict(os.environ)
env["_JARVIS_RESTART_REEXEC"] = "1"
result = subprocess.run(
    [{_early_sys.executable!r}] + {_early_sys.argv!r},
    cwd={_early_os.getcwd()!r},
    capture_output=True,
    env=env,
)

# Write result
with open({_result_path!r}, "w") as f:
    f.write(str(result.returncode) + "\\n")
    f.write(result.stdout.decode())
    f.write(result.stderr.decode())
'''
        _fd, _script_path = _tmp.mkstemp(suffix='.py', prefix='jarvis_restart_')
        _early_os.write(_fd, _script_content.encode())
        _early_os.close(_fd)
        _early_os.chmod(_script_path, 0o755)

        # Launch completely detached (double-fork daemon pattern)
        _proc = _sp.Popen(
            [_early_sys.executable, _script_path],
            start_new_session=True,
            stdin=_sp.DEVNULL,
            stdout=_sp.DEVNULL,
            stderr=_sp.DEVNULL,
        )

        # Print message and exit IMMEDIATELY - don't wait for child
        _early_sys.stdout.write(f"\n")
        _early_sys.stdout.write(f"{'='*60}\n")
        _early_sys.stdout.write(f"  JARVIS Supervisor Restart Initiated\n")
        _early_sys.stdout.write(f"{'='*60}\n")
        _early_sys.stdout.write(f"  The restart is running in background.\n")
        _early_sys.stdout.write(f"  \n")
        _early_sys.stdout.write(f"  To check status:  python3 run_supervisor.py --status\n")
        _early_sys.stdout.write(f"  Results file:     {_result_path}\n")
        _early_sys.stdout.write(f"{'='*60}\n")
        _early_sys.stdout.flush()
        _early_os._exit(0)  # Use _exit to avoid any cleanup that might hang
        del _sp, _tmp

    # Try to create own process group for additional isolation
    try:
        _early_os.setpgrp()
    except (OSError, PermissionError):
        pass  # May fail if already session leader

    # Debug marker for verification
    _early_os.environ['_JARVIS_CLI_PROTECTED'] = '1'

# Clean up early imports (will be re-imported properly below)
del _early_sys, _early_signal, _early_os, _cli_flags, _is_cli_mode

# =============================================================================
# CRITICAL: VENV AUTO-ACTIVATION (MUST BE FIRST - BEFORE ANY IMPORTS)
# =============================================================================
# This ensures we use the venv Python with correct packages, avoiding the
# "cannot import name 'get_hashable_key' from partially initialized module"
# error that occurs when user-level numba conflicts with venv numba.
#
# If running with system Python and venv exists, re-exec with venv Python.
# This MUST happen before ANY imports to prevent loading wrong packages.
# =============================================================================
import os as _os
import sys as _sys
from pathlib import Path as _Path

def _ensure_venv_python():
    """
    Ensure we're running with the venv Python.
    If not, re-execute the script with the venv Python.

    The key is checking if venv site-packages is in sys.path, NOT comparing
    executable paths (since venv Python often symlinks to system Python).
    """
    # Skip if explicitly disabled (for debugging)
    if _os.environ.get('JARVIS_SKIP_VENV_CHECK') == '1':
        return

    # Skip if already re-executed (prevent infinite loop)
    if _os.environ.get('_JARVIS_VENV_REEXEC') == '1':
        return

    # Find project root and venv
    script_dir = _Path(__file__).parent.resolve()
    venv_python = script_dir / "venv" / "bin" / "python3"
    if not venv_python.exists():
        venv_python = script_dir / "venv" / "bin" / "python"

    if not venv_python.exists():
        # No venv found, continue with current Python
        return

    # KEY CHECK: Is the venv's site-packages in sys.path?
    # This is the definitive test - venv Python adds its site-packages to path
    venv_site_packages = str(script_dir / "venv" / "lib")
    venv_in_path = any(venv_site_packages in p for p in _sys.path)

    if venv_in_path:
        # Running with venv Python - all good
        return

    # Check if we're actually running from the venv's bin directory
    # (handles case where venv is symlinked but executable path matches)
    current_exe = _Path(_sys.executable)
    if str(script_dir / "venv" / "bin") in str(current_exe):
        # Running from venv bin directory - should be fine
        return

    # NOT running with venv - need to re-exec
    print(f"[JARVIS] Detected system Python without venv packages")
    print(f"[JARVIS] Current: {_sys.executable}")
    print(f"[JARVIS] Switching to: {venv_python}")

    # Set marker to prevent infinite re-exec
    _os.environ['_JARVIS_VENV_REEXEC'] = '1'

    # Set PYTHONPATH to include project directories
    pythonpath = _os.pathsep.join([
        str(script_dir),
        str(script_dir / "backend"),
        _os.environ.get('PYTHONPATH', '')
    ])
    _os.environ['PYTHONPATH'] = pythonpath

    # Re-execute with venv Python
    # This replaces the current process with the venv Python running the same script
    _os.execv(str(venv_python), [str(venv_python)] + _sys.argv)

# Execute venv check immediately
_ensure_venv_python()

# Clean up the temporary imports (they'll be re-imported properly below)
del _os, _sys, _Path, _ensure_venv_python

# =============================================================================
# v119.2b: FAST EARLY-EXIT FOR RUNNING SUPERVISOR
# =============================================================================
# This check runs BEFORE heavy imports (PyTorch, transformers, GCP libs).
# If supervisor is already running and healthy, we can exit immediately
# without loading 2GB+ of ML libraries. Makes `python3 run_supervisor.py`
# instant when supervisor is already running.
# =============================================================================
def _fast_supervisor_check():
    """
    v119.2b: Ultra-fast check for running supervisor before heavy imports.

    Uses only standard library - no external dependencies.
    Returns True if we handled the request and should exit.
    """
    import os as _os
    import sys as _sys
    import socket as _socket
    import json as _json
    from pathlib import Path as _Path

    # Only run fast path if no action flags passed
    # These flags require full initialization
    action_flags = ['--restart', '--shutdown', '--takeover', '--force',
                    '--status', '--cleanup', '--task', '--mode', '--help', '-h']
    if any(flag in _sys.argv for flag in action_flags):
        return False  # Need full initialization

    # Check if IPC socket exists
    sock_path = _Path.home() / ".jarvis" / "locks" / "supervisor.sock"
    if not sock_path.exists():
        return False  # No supervisor running

    # Try to connect to supervisor with retry
    data = b''
    max_retries = 2
    sock_timeout = 8.0  # Longer timeout - health check can take a few seconds

    for attempt in range(max_retries):
        try:
            sock = _socket.socket(_socket.AF_UNIX, _socket.SOCK_STREAM)
            sock.settimeout(sock_timeout)
            sock.connect(str(sock_path))

            # Send health command
            msg = _json.dumps({'command': 'health'}) + '\n'
            sock.sendall(msg.encode())

            # Receive response
            while True:
                try:
                    chunk = sock.recv(4096)
                    if not chunk:
                        break
                    data += chunk
                    if b'\n' in data:
                        break
                except _socket.timeout:
                    break  # Partial data timeout

            sock.close()

            if data:
                break  # Got data, exit retry loop

        except (_socket.timeout, ConnectionRefusedError, FileNotFoundError):
            if attempt < max_retries - 1:
                import time as _time
                _time.sleep(0.5)  # Brief pause before retry
                continue
            return False  # All retries failed
        except Exception:
            return False  # Unexpected error

    if not data:
        return False  # No data received

    # Parse response
    try:
        result = _json.loads(data.decode().strip())
    except (_json.JSONDecodeError, UnicodeDecodeError):
        return False  # Invalid response

    if not result.get('success'):
        return False  # Health check failed

    health_data = result.get('result', {})
    health_level = health_data.get('health_level', 'UNKNOWN')

    # Only fast-exit if supervisor is healthy
    if health_level not in ('FULLY_READY', 'HTTP_HEALTHY', 'IPC_RESPONSIVE'):
        return False  # Unhealthy - need full init for auto-recovery

    # v119.2b: Show concise success message and exit
    pid = health_data.get('pid', 'unknown')
    uptime = health_data.get('uptime_seconds', 0)
    uptime_str = f"{int(uptime // 60)}m {int(uptime % 60)}s" if uptime > 60 else f"{int(uptime)}s"

    print(f"\n{'='*70}")
    print(f"âœ… JARVIS Supervisor (PID {pid}) is running and healthy")
    print(f"{'='*70}")
    print(f"   Health:  {health_level}")
    print(f"   Uptime:  {uptime_str}")

    # Show cross-repo status if available
    checks = health_data.get('checks', {})
    if 'http_health' in checks:
        http_details = checks['http_health'].get('details', {}).get('response', {})
        if http_details.get('trinity_enabled'):
            print(f"   Trinity: Enabled")
        if http_details.get('ready_for_inference'):
            print(f"   J-Prime: Ready for inference")

    print(f"")
    print(f"   No action needed - supervisor is ready to use.")
    print(f"")
    print(f"   Commands:  --restart | --shutdown | --status")
    print(f"{'='*70}\n")

    return True  # Handled - should exit

# Run fast check before heavy imports
if _fast_supervisor_check():
    import sys as _sys
    _sys.exit(0)

del _fast_supervisor_check

# =============================================================================
# CRITICAL: PYTHON 3.9 COMPATIBILITY PATCH - MUST BE BEFORE ANY IMPORTS!
# =============================================================================
# This MUST happen BEFORE any module that imports google-api-core or other
# packages that use importlib.metadata.packages_distributions() which was
# added in Python 3.10. Without this patch, Python 3.9 users see:
#   "module 'importlib.metadata' has no attribute 'packages_distributions'"
# =============================================================================
import sys as _sys
if _sys.version_info < (3, 10):
    try:
        from importlib import metadata as _metadata
        if not hasattr(_metadata, 'packages_distributions'):
            def _packages_distributions_fallback():
                """Minimal fallback for packages_distributions on Python 3.9."""
                try:
                    import importlib_metadata as _backport
                    if hasattr(_backport, 'packages_distributions'):
                        return _backport.packages_distributions()
                except ImportError:
                    pass
                return {}
            _metadata.packages_distributions = _packages_distributions_fallback
    except Exception:
        pass  # Silently ignore - this is a best-effort compatibility fix
del _sys

# =============================================================================
# v93.0: PYTORCH/TRANSFORMERS COMPATIBILITY SHIM - MUST BE BEFORE ANY TORCH IMPORTS!
# =============================================================================
# Fix for: AttributeError: module 'torch.utils._pytree' has no attribute 'register_pytree_node'
# Root cause: transformers 4.57+ expects public register_pytree_node but PyTorch 2.1.x
# only exposes _register_pytree_node (private). This shim creates a wrapper that:
# 1. Maps the private API to the public API name
# 2. Filters out unsupported kwargs (like 'serialized_type_name' from transformers 4.57+)
# =============================================================================
def _apply_early_pytorch_compat():
    """Apply PyTorch compatibility shim before any transformers imports."""
    import os as _os
    import sys as _sys

    # Skip if torch not installed
    try:
        import torch.utils._pytree as _pytree
    except ImportError:
        return False

    # Check if register_pytree_node already exists
    if hasattr(_pytree, 'register_pytree_node'):
        return False  # No shim needed

    # Create wrapper that filters unsupported kwargs
    if hasattr(_pytree, '_register_pytree_node'):
        _original_register = _pytree._register_pytree_node

        # Get supported kwargs from the original function signature
        import inspect as _inspect
        try:
            _sig = _inspect.signature(_original_register)
            _supported_kwargs = {
                name for name, param in _sig.parameters.items()
                if param.kind in (_inspect.Parameter.KEYWORD_ONLY, _inspect.Parameter.VAR_KEYWORD)
            }
        except (ValueError, TypeError):
            # Fallback: known supported kwargs for PyTorch 2.1.x
            _supported_kwargs = {'to_dumpable_context', 'from_dumpable_context'}

        def _compat_register_pytree_node(
            typ,
            flatten_fn,
            unflatten_fn,
            *,
            serialized_type_name=None,  # transformers 4.57+ passes this
            to_dumpable_context=None,
            from_dumpable_context=None,
            **extra_kwargs  # Catch any other future kwargs
        ):
            """
            v93.0: Compatibility wrapper for register_pytree_node.

            Filters out unsupported kwargs (like serialized_type_name) that
            transformers 4.57+ passes but PyTorch 2.1.x doesn't support.
            """
            # Build kwargs dict with only supported parameters
            kwargs = {}
            if to_dumpable_context is not None:
                kwargs['to_dumpable_context'] = to_dumpable_context
            if from_dumpable_context is not None:
                kwargs['from_dumpable_context'] = from_dumpable_context

            # Call original function with filtered kwargs
            try:
                return _original_register(typ, flatten_fn, unflatten_fn, **kwargs)
            except TypeError as e:
                # Last resort: try without any kwargs
                if 'unexpected keyword argument' in str(e):
                    return _original_register(typ, flatten_fn, unflatten_fn)
                raise

        _pytree.register_pytree_node = _compat_register_pytree_node
        if _os.environ.get("JARVIS_DEBUG"):
            print("[v93.0] âœ“ Applied pytree compatibility wrapper (filters unsupported kwargs)", file=_sys.stderr)
        return True

    # Create no-op fallback to prevent crashes
    def _noop_register(cls, flatten_fn, unflatten_fn, **kwargs):
        """No-op pytree registration for compatibility."""
        pass  # Silently ignore - prevents import errors

    _pytree.register_pytree_node = _noop_register
    if _os.environ.get("JARVIS_DEBUG"):
        print("[v93.0] âš  Applied no-op pytree shim (limited functionality)", file=_sys.stderr)
    return True

_apply_early_pytorch_compat()
del _apply_early_pytorch_compat

# =============================================================================
# v93.1: TRANSFORMERS SECURITY CHECK BYPASS (CVE-2025-32434)
# =============================================================================
# transformers 4.57+ requires PyTorch 2.6+ due to a torch.load vulnerability.
# This bypass is ONLY for loading trusted HuggingFace models (SpeechBrain, etc.)
# The actual vulnerability is in loading untrusted pickle files - HuggingFace
# models are cryptographically signed and verified.
#
# Options (in order of preference):
# 1. Upgrade PyTorch to 2.6+ (recommended): pip install torch>=2.6
# 2. Use safetensors format (no bypass needed)
# 3. This bypass (for development/trusted environments only)
# =============================================================================
def _apply_transformers_security_bypass():
    """
    Bypass the torch.load security check for trusted HuggingFace models.

    WARNING: Only use this in trusted environments where you're loading
    models from verified sources (HuggingFace Hub).
    """
    import os as _os
    import sys as _sys

    # Check if bypass is explicitly disabled
    if _os.environ.get("JARVIS_STRICT_TORCH_SECURITY") == "1":
        return False

    try:
        # First apply pytree shim if not already applied
        import torch.utils._pytree as _pytree
        if not hasattr(_pytree, 'register_pytree_node') and hasattr(_pytree, '_register_pytree_node'):
            _original = _pytree._register_pytree_node
            def _compat(typ, flatten_fn, unflatten_fn, *, serialized_type_name=None,
                       to_dumpable_context=None, from_dumpable_context=None, **kw):
                kwargs = {}
                if to_dumpable_context is not None: kwargs['to_dumpable_context'] = to_dumpable_context
                if from_dumpable_context is not None: kwargs['from_dumpable_context'] = from_dumpable_context
                return _original(typ, flatten_fn, unflatten_fn, **kwargs)
            _pytree.register_pytree_node = _compat

        # Check PyTorch version
        import torch
        torch_version = tuple(int(x) for x in torch.__version__.split('.')[:2])
        if torch_version >= (2, 6):
            return False  # No bypass needed

        # Import transformers utils (this will work now with pytree shim)
        import transformers.utils.import_utils as _import_utils

        # Check if the security check exists
        if not hasattr(_import_utils, 'check_torch_load_is_safe'):
            return False

        # Store original for potential restoration
        _original_check = _import_utils.check_torch_load_is_safe

        def _bypassed_check():
            """
            Bypassed security check for trusted HuggingFace model loading.

            WARNING: This bypasses CVE-2025-32434 protection. Only use when:
            - Loading models from trusted sources (HuggingFace Hub)
            - Models are cryptographically verified
            - You understand the security implications
            """
            pass  # No-op - allow torch.load for trusted sources

        # Apply bypass
        _import_utils.check_torch_load_is_safe = _bypassed_check

        # Also patch in modeling_utils if already imported
        try:
            import transformers.modeling_utils as _modeling_utils
            if hasattr(_modeling_utils, 'check_torch_load_is_safe'):
                _modeling_utils.check_torch_load_is_safe = _bypassed_check
        except ImportError:
            pass

        if _os.environ.get("JARVIS_DEBUG"):
            print(f"[v93.1] âš  Bypassed transformers CVE-2025-32434 check (PyTorch {torch.__version__} < 2.6)", file=_sys.stderr)
            print(f"[v93.1]   Recommendation: pip install torch>=2.6 for full security", file=_sys.stderr)

        return True

    except ImportError:
        return False
    except Exception as e:
        if _os.environ.get("JARVIS_DEBUG"):
            print(f"[v93.1] Failed to apply transformers security bypass: {e}", file=_sys.stderr)
        return False

_apply_transformers_security_bypass()
del _apply_transformers_security_bypass

# =============================================================================
# SYSTEM RESOURCE OPTIMIZATION (v1.0)
# =============================================================================
# Critical for high-concurrency async operations.
# Handles auto-maximization of ulimits/file descriptors.
# =============================================================================
try:
    from backend.core.system_optimization import get_system_optimizer
    _optimizer = get_system_optimizer()
    _optimization_stats = _optimizer.optimize()
except Exception as e:
    print(f"âš ï¸  System optimization warning: {e}")

# =============================================================================
# NORMAL IMPORTS START HERE
# =============================================================================
import asyncio
import json
import logging
import os
import platform
import signal
import sys
import threading
import time
import warnings
from concurrent.futures import ThreadPoolExecutor
from contextlib import suppress
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Set, Tuple

# v109.3: Safe file descriptor management to prevent EXC_GUARD crashes
try:
    from backend.core.safe_fd import safe_close
except ImportError:
    # Fallback if module not available
    safe_close = lambda fd, **kwargs: os.close(fd) if fd >= 0 else None  # noqa: E731

# =============================================================================
# v111.4: ENVIRONMENT LOADING - Load .env files including GCP hybrid cloud config
# =============================================================================
# This enables the hybrid cloud architecture to offload heavy model inference
# to GCP Cloud Run or Spot VMs instead of loading locally (which can cause
# memory pressure and process death on resource-constrained local systems).
# =============================================================================
def _load_environment_files():
    """
    Load environment variables from .env files in priority order.

    v111.4: Added .env.gcp loading for hybrid cloud architecture.
    This enables JARVIS_PREFER_CLOUD_RUN, JARVIS_SPOT_VM_ENABLED, etc.
    which are critical for offloading ML workloads to GCP.
    """
    try:
        from dotenv import load_dotenv
    except ImportError:
        # dotenv not available - skip
        return

    project_root = Path(__file__).parent.resolve()

    # Load order (later files override earlier):
    # 1. Root .env (base configuration)
    # 2. backend/.env (backend-specific)
    # 3. .env.gcp (GCP hybrid cloud - enables cloud offloading)

    env_files = [
        project_root / ".env",
        project_root / "backend" / ".env",
        project_root / ".env.gcp",  # v111.4: Critical for hybrid cloud
    ]

    loaded_files = []
    for env_file in env_files:
        if env_file.exists():
            load_dotenv(env_file, override=True)
            loaded_files.append(env_file.name)

    # Log which files were loaded (only if GCP config was found)
    if ".env.gcp" in loaded_files:
        print(f"[v111.4] âœ… Loaded GCP hybrid cloud config - cloud offloading enabled")
        # Verify key settings
        if os.environ.get("JARVIS_PREFER_CLOUD_RUN", "").lower() == "true":
            print(f"[v111.4]    â†’ JARVIS_PREFER_CLOUD_RUN=true (ML inference â†’ GCP Cloud Run)")
        if os.environ.get("JARVIS_SPOT_VM_ENABLED", "").lower() == "true":
            print(f"[v111.4]    â†’ JARVIS_SPOT_VM_ENABLED=true (heavy workloads â†’ Spot VMs)")

# Execute environment loading immediately
_load_environment_files()

# v110.0: Singleton enforcement to prevent duplicate supervisors
try:
    from backend.core.supervisor_singleton import (
        acquire_supervisor_lock,
        release_supervisor_lock,
        start_supervisor_heartbeat,
        is_supervisor_running,
    )
    _SINGLETON_AVAILABLE = True
except ImportError:
    _SINGLETON_AVAILABLE = False
    def acquire_supervisor_lock(entry_point: str) -> bool:
        return True  # Fallback: always allow
    def release_supervisor_lock() -> None:
        pass
    async def start_supervisor_heartbeat() -> None:
        pass
    def is_supervisor_running():
        return False, None

# v12.0: Intelligent Docker Daemon Manager with Self-Healing
try:
    from backend.infrastructure.docker_daemon_manager import (
        DockerDaemonManager,
        get_docker_manager,
    )
    _DOCKER_MANAGER_AVAILABLE = True
except ImportError:
    _DOCKER_MANAGER_AVAILABLE = False
    DockerDaemonManager = None  # type: ignore
    async def get_docker_manager():
        return None

# =============================================================================
# v93.12: SUPPRESS SPEECHBRAIN AND TORCHAUDIO DEPRECATION WARNINGS
# =============================================================================
# SpeechBrain 1.0 deprecated speechbrain.pretrained in favor of speechbrain.inference
# This filter prevents noisy deprecation warnings during startup while we maintain
# backwards compatibility with older SpeechBrain versions.
# =============================================================================
warnings.filterwarnings(
    "ignore",
    message=".*speechbrain\.pretrained.*deprecated.*",
    category=UserWarning
)
warnings.filterwarnings(
    "ignore",
    message=".*torchaudio.*_backend.*deprecated.*",
    category=UserWarning
)
warnings.filterwarnings(
    "ignore",
    message=".*list_audio_backends.*deprecated.*",
    category=UserWarning
)
# v93.14: Suppress "Wav2Vec2Model is frozen" informational warning
# This is expected behavior - the model is frozen for inference
warnings.filterwarnings(
    "ignore",
    message=".*Wav2Vec2Model is frozen.*",
    category=UserWarning
)
warnings.filterwarnings(
    "ignore",
    message=".*model is frozen.*",
    category=UserWarning
)


# v95.0: Custom logging filter for benign ML framework messages
# v95.1: Extended to cover DEBUG messages and more patterns
class BenignWarningFilter(logging.Filter):
    """
    Advanced filter to suppress known benign warnings from ML frameworks.

    These warnings are informational and not actual problems:
    - "Wav2Vec2Model is frozen" = Model frozen for inference (correct behavior)
    - "Some weights not initialized" = Expected for fine-tuned models
    - "You should probably TRAIN" = Irrelevant for inference-only usage
    - "Registered checkpoint" = DEBUG noise from SpeechBrain
    """

    _SUPPRESSED_PATTERNS = [
        'wav2vec2model is frozen',
        'model is frozen',
        'weights were not initialized',
        'you should probably train',
        'some weights of the model checkpoint',
        'initializing bert',
        'initializing wav2vec',
        # SpeechBrain DEBUG noise (v95.1)
        'registered checkpoint',
        'checkpoint save hook',
        'checkpoint load hook',
        # Google API version warnings (v95.1)
        'non-supported python version',
        'google will not post any further updates',
        # Optional dependencies (v95.1)
        'gspread not available',
        'redis not available',
        'install with: pip install',
    ]

    def filter(self, record: logging.LogRecord) -> bool:
        """Return False to suppress the record, True to allow it."""
        msg_lower = record.getMessage().lower()
        for pattern in self._SUPPRESSED_PATTERNS:
            if pattern in msg_lower:
                return False
        return True


# v95.0: Install filter on SpeechBrain and HuggingFace loggers
_benign_filter = BenignWarningFilter()

# Also pre-configure SpeechBrain loggers to WARNING level BEFORE import
# This prevents DEBUG messages during model loading
# v93.14: Added huggingface_transformers for "frozen model" warnings
# v95.0: Added filter and more comprehensive logger coverage
for _sb_logger_name in [
    "speechbrain",
    "speechbrain.utils.checkpoints",
    "speechbrain.utils.quirks",
    "speechbrain.utils.torch_audio_backend",
    "speechbrain.lobes.models.huggingface_transformers",
    "speechbrain.lobes.models.huggingface_transformers.huggingface",
    "transformers",
    "transformers.modeling_utils",
]:
    _sb_logger = logging.getLogger(_sb_logger_name)
    _sb_logger.setLevel(logging.ERROR)  # v93.14: Changed to ERROR to suppress frozen warnings
    _sb_logger.addFilter(_benign_filter)  # v95.0: Also add filter for extra coverage

# =============================================================================
# HYPER-RUNTIME ENGINE v9.0: Rust-First Async Architecture
# =============================================================================
# Intelligent runtime selection that maximizes async performance:
#   Level 3 (HYPER):    Granian (Rust/Tokio) - 3-5x faster than uvicorn
#   Level 2 (FAST):     uvloop (C/libuv)     - 2-4x faster than asyncio
#   Level 1 (STANDARD): asyncio              - Python standard library
#
# The system auto-detects the best available runtime and activates it.
# For HTTP servers, Granian replaces the entire Python async stack with Rust.
# =============================================================================

# Add backend to path first (needed for hyper_runtime import)
backend_path = Path(__file__).parent / "backend"
if str(backend_path) not in sys.path:
    sys.path.insert(0, str(backend_path))

# Initialize hyper-runtime (auto-detects and activates best engine)
_HYPER_RUNTIME_LEVEL = 1  # Default to standard
_HYPER_RUNTIME_NAME = "asyncio"
try:
    from core.hyper_runtime import (
        get_runtime_engine,
        activate_runtime,
        RuntimeLevel,
    )
    _runtime_engine = activate_runtime()
    _HYPER_RUNTIME_LEVEL = _runtime_engine.level.value
    _HYPER_RUNTIME_NAME = _runtime_engine.name
except ImportError:
    # Fallback to uvloop if hyper_runtime not available
    if sys.platform != "win32":
        try:
            import uvloop
            asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
            _HYPER_RUNTIME_LEVEL = 2
            _HYPER_RUNTIME_NAME = "uvloop"
        except ImportError:
            pass
except Exception:
    pass  # Fall back to standard asyncio

# =============================================================================
# v90.0: SYSTEM PRIMITIVES - Iron-Clad Production Layer
# =============================================================================
# Import the production-grade system primitives that fix critical issues:
# - SafeProcess: Subprocess lifecycle with guaranteed cleanup
# - AtomicStateWriter: Corruption-proof file writes
# - PortManager: Distributed port locking
# - TrueHeartbeat: PID-validated health monitoring
# =============================================================================
_SYSTEM_PRIMITIVES_AVAILABLE = False
_ADVANCED_PRIMITIVES_AVAILABLE = False
try:
    from core.system_primitives import (
        # Core primitives (v90.0)
        SafeProcess,
        AtomicStateWriter,
        PortManager,
        TrueHeartbeat,
        FileLock,
        AdaptiveWaiter,
        ResourceGuard,
        EnhancedExceptionHandler,
        ProcessState,
        HeartbeatStatus,
        SystemConfig as SystemPrimitivesConfig,
    )
    _SYSTEM_PRIMITIVES_AVAILABLE = True

    # Advanced features (v91.0) - Optional but powerful
    try:
        from core.system_primitives import (
            ProcessHealthPredictor,
            SelfHealingOrchestrator,
            ResourceQuotaManager,
            DistributedStateCoordinator,
            GracefulDegradationManager,
            DegradationLevel,
            RemediationAction,
            ResourceUsage,
        )
        _ADVANCED_PRIMITIVES_AVAILABLE = True
        print("[INFO] Advanced system primitives v91.0 loaded (ML prediction, self-healing, coordination)")
    except ImportError as e:
        print(f"[INFO] Advanced primitives not available (optional): {e}")
        ProcessHealthPredictor = None
        SelfHealingOrchestrator = None
        ResourceQuotaManager = None
        DistributedStateCoordinator = None
        GracefulDegradationManager = None
        DegradationLevel = None
        RemediationAction = None
        ResourceUsage = None

except ImportError as e:
    print(f"[WARNING] System primitives not available: {e}")
    # Provide stubs for backwards compatibility
    SafeProcess = None
    AtomicStateWriter = None
    PortManager = None
    TrueHeartbeat = None
    FileLock = None
    AdaptiveWaiter = None
    ProcessHealthPredictor = None
    SelfHealingOrchestrator = None
    ResourceQuotaManager = None
    DistributedStateCoordinator = None
    GracefulDegradationManager = None
    DegradationLevel = None
    RemediationAction = None
    ResourceUsage = None


# =============================================================================
# v16.0: OPTIONAL DEPENDENCY MANAGER - Centralized ML Dependency Checking
# =============================================================================
# Provides thread-safe, cached checking of optional dependencies (PyTorch,
# CoreML, etc.) with consolidated logging to avoid warning spam.
# =============================================================================
_OPTIONAL_DEPS_AVAILABLE = False
try:
    from backend.core.optional_dependencies import (
        get_dependency_manager,
        log_dependency_status,
        is_torch_available,
        is_coreml_available,
        get_ml_capabilities,
    )
    _OPTIONAL_DEPS_AVAILABLE = True
except ImportError as e:
    print(f"[INFO] Optional dependency manager not available: {e}")
    # Provide stubs
    def get_dependency_manager():
        return None
    def log_dependency_status():
        pass
    def is_torch_available():
        return False
    def is_coreml_available():
        return False
    def get_ml_capabilities():
        return {"ml_available": False, "gpu_available": False}


# v91.0: Lazy psutil import for process health monitoring
_psutil_cache = None


def _get_psutil():
    """
    v91.0: Lazy import psutil for process monitoring.

    Avoids import overhead on startup while allowing ML-based health monitoring.

    Returns:
        psutil module or None if not available
    """
    global _psutil_cache
    if _psutil_cache is None:
        try:
            import psutil
            _psutil_cache = psutil
        except ImportError:
            _psutil_cache = False  # Mark as unavailable
    return _psutil_cache if _psutil_cache is not False else None


def _safe_read_file(path: Path, default: str = "") -> str:
    """
    v92.0: Robust file reading that handles "Bad file descriptor" and other I/O errors.

    The pathlib.read_text() method can fail with errno 9 (EBADF) when:
    - File descriptors are exhausted or recycled
    - Race conditions with file operations
    - System file descriptor limits are stressed (e.g., after setrlimit)

    This function uses explicit file opening with proper error handling.

    Args:
        path: Path object to read
        default: Default value to return on error

    Returns:
        File contents as string, or default on error
    """
    import errno

    if not isinstance(path, Path):
        path = Path(path)

    try:
        # Check existence first
        if not path.exists():
            return default
    except OSError:
        return default

    try:
        # Use explicit file open instead of path.read_text()
        with open(str(path), 'r', encoding='utf-8') as f:
            return f.read()
    except (OSError, IOError) as e:
        # Handle specific error codes gracefully
        if hasattr(e, 'errno') and e.errno in (
            errno.EBADF,    # Bad file descriptor
            errno.ENOENT,   # File not found
            errno.EACCES,   # Permission denied
            errno.EIO,      # I/O error
            errno.ESTALE,   # Stale file handle
        ):
            return default
        # Log and return default for unexpected errors
        return default
    except Exception:
        return default


def _safe_read_json(path: Path, default: dict = None) -> dict:
    """
    v92.0: Robust JSON file reading with error handling.

    Args:
        path: Path to JSON file
        default: Default value to return on error (None becomes {})

    Returns:
        Parsed JSON data or default on error
    """
    import json

    if default is None:
        default = {}

    content = _safe_read_file(path, default="")
    if not content:
        return default

    try:
        return json.loads(content)
    except (json.JSONDecodeError, ValueError):
        return default


# v10.6: Structured Logging System with Real-Time Monitoring
try:
    from core.logging import (
        configure_structured_logging,
        get_structured_logger,
        get_global_logging_stats,
        LoggingConfig,
        get_log_monitor,
        stop_global_monitor,
        LogMonitorConfig,
    )
    STRUCTURED_LOGGING_AVAILABLE = True
except ImportError:
    STRUCTURED_LOGGING_AVAILABLE = False
    print("[WARNING] Structured logging not available - using basic logging")

# =============================================================================
# CRITICAL: Python 3.9 Compatibility - MUST run before any Google API imports
# =============================================================================
# This patches importlib.metadata.packages_distributions() which google-api-core needs
try:
    from utils.python39_compat import ensure_python39_compatibility
    ensure_python39_compatibility()
except ImportError:
    # Fallback: manually patch if the module isn't available
    import importlib.metadata as metadata
    if not hasattr(metadata, 'packages_distributions'):
        def packages_distributions():
            return {}
        metadata.packages_distributions = packages_distributions

# =============================================================================
# v111.0: UNIFIED SIGNAL HANDLER - Graceful Shutdown Orchestration
# =============================================================================
# Provides escalating shutdown behavior for SIGINT (Ctrl+C) and SIGTERM:
#   - 1st signal: Graceful shutdown (waits for cleanup)
#   - 2nd signal: Faster shutdown (shorter timeouts)
#   - 3rd signal: Immediate exit (os._exit)
#
# This ensures the monolith shuts down cleanly without signal handler conflicts
# between the supervisor and Uvicorn.
# =============================================================================


class UnifiedSignalHandler:
    """
    v111.0: Unified signal handling for the monolith.

    Handles SIGINT (Ctrl+C) and SIGTERM gracefully, ensuring
    all components shut down in the correct order.

    Signal escalation:
    - 1st signal: Graceful shutdown (waits for cleanup)
    - 2nd signal: Faster shutdown (shorter timeouts)
    - 3rd signal: Immediate exit (sys.exit)

    Thread-safe: Uses threading.Lock for signal counting since signals
    can arrive from any thread context.
    """

    def __init__(self):
        self._shutdown_event: Optional[asyncio.Event] = None
        self._shutdown_requested = False
        self._shutdown_count = 0
        self._lock = threading.Lock()
        self._shutdown_reason: Optional[str] = None
        self._loop: Optional[asyncio.AbstractEventLoop] = None
        self._installed = False

    def _get_event(self) -> asyncio.Event:
        """Lazily create shutdown event (needs running event loop)."""
        if self._shutdown_event is None:
            self._shutdown_event = asyncio.Event()
        return self._shutdown_event

    def install(self, loop: asyncio.AbstractEventLoop) -> None:
        """
        Install signal handlers on the event loop.

        Args:
            loop: The running asyncio event loop
        """
        if self._installed:
            return  # Avoid duplicate registration

        self._loop = loop

        for sig in (signal.SIGINT, signal.SIGTERM):
            try:
                # Unix: Use async-safe loop.add_signal_handler
                loop.add_signal_handler(
                    sig,
                    lambda s=sig: self._schedule_signal_handling(s)
                )
            except NotImplementedError:
                # Windows doesn't support add_signal_handler
                signal.signal(sig, lambda s, f, sig=sig: self._sync_handle_signal(sig))
            except Exception as e:
                # Log but don't fail - signal handling is best-effort
                print(f"[v111.0] Warning: Could not install handler for {sig.name}: {e}")

        self._installed = True
        print("[v111.0] Unified signal handlers installed (SIGINT, SIGTERM)")

    def _schedule_signal_handling(self, sig: signal.Signals) -> None:
        """
        Schedule async signal handling from sync context.

        This is called by loop.add_signal_handler which runs in sync context.
        We use create_task to handle the signal asynchronously.
        """
        if self._loop is not None and self._loop.is_running():
            self._loop.create_task(self._handle_signal(sig))
        else:
            # Fallback to sync handling if loop not available
            self._sync_handle_signal(sig.value)

    def _sync_handle_signal(self, sig: int) -> None:
        """
        Synchronous signal handler (for Windows compatibility and fallback).

        This handles signals when async handling is not possible.
        """
        with self._lock:
            self._shutdown_count += 1
            count = self._shutdown_count
            self._shutdown_requested = True

            try:
                sig_name = signal.Signals(sig).name
            except (ValueError, AttributeError):
                sig_name = f"signal_{sig}"

            self._shutdown_reason = sig_name

            if count == 1:
                print(f"\n[v111.0] Received {sig_name} - initiating graceful shutdown...")
            elif count == 2:
                print(f"[v111.0] Received second {sig_name} - forcing faster shutdown...")
            else:
                print(f"[v111.0] Received third {sig_name} - forcing immediate exit!")
                os._exit(128 + sig)

            # Try to set the shutdown event if available
            if self._shutdown_event is not None:
                try:
                    if self._loop is not None and self._loop.is_running():
                        self._loop.call_soon_threadsafe(self._shutdown_event.set)
                    else:
                        # Direct set as fallback
                        self._shutdown_event.set()
                except Exception:
                    pass  # Best effort

    async def _handle_signal(self, sig: signal.Signals) -> None:
        """
        Handle incoming signal asynchronously.

        Provides escalating shutdown behavior based on signal count.
        """
        with self._lock:
            self._shutdown_count += 1
            count = self._shutdown_count

        sig_name = sig.name
        self._shutdown_reason = sig_name
        self._shutdown_requested = True

        if count == 1:
            print(f"\n[v111.0] Received {sig_name} - initiating graceful shutdown...")
            self._get_event().set()
        elif count == 2:
            print(f"[v111.0] Received second {sig_name} - forcing faster shutdown...")
            self._get_event().set()
        else:
            print(f"[v111.0] Received third {sig_name} - forcing immediate exit!")
            os._exit(128 + sig.value)

    async def wait_for_shutdown(self) -> None:
        """Wait for shutdown signal."""
        await self._get_event().wait()

    @property
    def shutdown_requested(self) -> bool:
        """Check if shutdown was requested."""
        return self._shutdown_requested

    @property
    def shutdown_count(self) -> int:
        """Number of shutdown signals received."""
        return self._shutdown_count

    @property
    def shutdown_reason(self) -> Optional[str]:
        """Reason for shutdown (signal name)."""
        return self._shutdown_reason

    @property
    def is_fast_shutdown(self) -> bool:
        """Check if we're in fast shutdown mode (2+ signals received)."""
        return self._shutdown_count >= 2

    def reset(self) -> None:
        """Reset the signal handler state (for testing or restart scenarios)."""
        with self._lock:
            self._shutdown_requested = False
            self._shutdown_count = 0
            self._shutdown_reason = None
            if self._shutdown_event is not None:
                self._shutdown_event.clear()


# v111.0: Global signal handler instance
_unified_signal_handler: Optional[UnifiedSignalHandler] = None


def get_unified_signal_handler() -> UnifiedSignalHandler:
    """
    Get or create the unified signal handler singleton.

    Returns:
        The global UnifiedSignalHandler instance
    """
    global _unified_signal_handler
    if _unified_signal_handler is None:
        _unified_signal_handler = UnifiedSignalHandler()
    return _unified_signal_handler


# =============================================================================
# EARLY SHUTDOWN HOOK REGISTRATION
# =============================================================================
# Register shutdown hook as early as possible to ensure GCP VMs are cleaned up
# even if JARVIS crashes during startup. The hook handles its own idempotency.
# =============================================================================
try:
    from backend.scripts.shutdown_hook import register_handlers as _register_shutdown_handlers
    _register_shutdown_handlers()
except ImportError:
    pass  # Will be registered later by SupervisorBootstrapper

# =============================================================================
# SUPERVISOR RESTART MANAGER v101.0 - Cross-Repo Process Supervision
# =============================================================================

@dataclass
class SupervisorManagedProcess:
    """
    Metadata for a supervisor-managed cross-repo process.

    These are processes like JARVIS-Prime and Reactor-Core that run
    in sibling repositories and need cross-repo restart management.
    """
    name: str
    process: Optional[asyncio.subprocess.Process]
    restart_func: Callable[[], Any]  # Async function to restart the process
    restart_count: int = 0
    last_restart: float = 0.0
    max_restarts: int = 3
    port: Optional[int] = None
    exit_code: Optional[int] = None
    enabled: bool = True


class SupervisorRestartManager:
    """
    Cross-repo process restart manager for supervisor-level services.

    Manages automatic restart of:
    - JARVIS-Prime (local inference server)
    - Reactor-Core (training/ML services)

    Features:
    - Named process tracking (not index-based)
    - Exponential backoff: 1s â†’ 2s â†’ 4s â†’ max configurable
    - Per-process restart tracking
    - Maximum restart limit with alerting
    - Async-safe with proper locking
    - Environment variable configuration

    Environment Variables:
        JARVIS_SUPERVISOR_MAX_RESTARTS: Maximum restart attempts (default: 3)
        JARVIS_SUPERVISOR_MAX_BACKOFF: Maximum backoff delay in seconds (default: 60.0)
        JARVIS_SUPERVISOR_RESTART_COOLDOWN: Stability period before count reset (default: 600.0)
    """

    def __init__(self, logger: Optional[logging.Logger] = None):
        """Initialize the supervisor restart manager."""
        self.processes: Dict[str, SupervisorManagedProcess] = {}
        self._lock = asyncio.Lock()
        self._shutdown_requested = False
        self._logger = logger or logging.getLogger("SupervisorRestartManager")

        # Environment-driven configuration
        self.max_restarts = int(os.getenv("JARVIS_SUPERVISOR_MAX_RESTARTS", "3"))
        self.max_backoff = float(os.getenv("JARVIS_SUPERVISOR_MAX_BACKOFF", "60.0"))
        self.restart_cooldown = float(os.getenv("JARVIS_SUPERVISOR_RESTART_COOLDOWN", "600.0"))
        self.base_backoff = float(os.getenv("JARVIS_SUPERVISOR_BASE_BACKOFF", "2.0"))

    def register(
        self,
        name: str,
        process: Optional[asyncio.subprocess.Process],
        restart_func: Callable[[], Any],
        port: Optional[int] = None,
        enabled: bool = True,
    ) -> None:
        """
        Register a cross-repo process for monitoring and automatic restart.

        Args:
            name: Human-readable identifier (e.g., "jarvis-prime", "reactor-core")
            process: The asyncio subprocess object (can be None if not yet started)
            restart_func: Async function to restart the process
            port: Port the process listens on (for logging)
            enabled: Whether this process should be monitored
        """
        self.processes[name] = SupervisorManagedProcess(
            name=name,
            process=process,
            restart_func=restart_func,
            restart_count=0,
            last_restart=0.0,
            max_restarts=self.max_restarts,
            port=port,
            enabled=enabled,
        )
        if process:
            self._logger.info(
                f"[v101] Registered cross-repo process '{name}' (PID: {process.pid})"
                + (f" on port {port}" if port else "")
            )

    def update_process(self, name: str, process: asyncio.subprocess.Process) -> None:
        """Update the process reference for a registered service."""
        if name in self.processes:
            self.processes[name].process = process
            self._logger.debug(f"[v101] Updated process reference for '{name}' (PID: {process.pid})")

    def request_shutdown(self) -> None:
        """Signal that shutdown is requested - stop all restart attempts."""
        self._shutdown_requested = True
        self._logger.info("[v101] Supervisor shutdown requested - restart manager disabled")

    def reset_shutdown(self) -> None:
        """Reset shutdown flag - allow restarts again."""
        self._shutdown_requested = False

    async def check_and_restart_all(self) -> List[str]:
        """
        Check all cross-repo processes and restart any that have unexpectedly exited.

        Returns:
            List of process names that were restarted
        """
        if self._shutdown_requested:
            return []

        restarted = []

        async with self._lock:
            for name, managed in list(self.processes.items()):
                if not managed.enabled or managed.process is None:
                    continue

                proc = managed.process

                # Check if process has exited
                if proc.returncode is not None:
                    managed.exit_code = proc.returncode

                    # Normal exit or controlled shutdown - don't restart
                    if proc.returncode in (0, -2, -15):
                        self._logger.debug(
                            f"[v101] {name} exited normally (code: {proc.returncode})"
                        )
                        continue

                    # Unexpected exit - attempt restart
                    success = await self._handle_unexpected_exit(name, managed)
                    if success:
                        restarted.append(name)

        return restarted

    async def _handle_unexpected_exit(
        self, name: str, managed: SupervisorManagedProcess
    ) -> bool:
        """
        Handle an unexpected cross-repo process exit with exponential backoff restart.

        Args:
            name: Process name
            managed: SupervisorManagedProcess metadata

        Returns:
            True if restart was successful, False otherwise
        """
        current_time = time.time()

        # Check if we've exceeded restart limit
        if managed.restart_count >= managed.max_restarts:
            self._logger.error(
                f"[v101] âŒ {name} exceeded supervisor restart limit ({managed.max_restarts}). "
                f"Last exit code: {managed.exit_code}. Manual intervention required."
            )
            return False

        # Apply cooldown - reset count if stable for a while
        if current_time - managed.last_restart > self.restart_cooldown:
            if managed.restart_count > 0:
                self._logger.info(
                    f"[v101] {name} was stable for {self.restart_cooldown}s - "
                    f"resetting restart count from {managed.restart_count} to 0"
                )
            managed.restart_count = 0

        # Calculate exponential backoff
        backoff = min(
            self.base_backoff * (2 ** managed.restart_count),
            self.max_backoff
        )

        managed.restart_count += 1
        managed.last_restart = current_time

        self._logger.warning(
            f"[v101] ğŸ”„ Supervisor restarting '{name}' in {backoff:.1f}s "
            f"(attempt {managed.restart_count}/{managed.max_restarts}, "
            f"exit code: {managed.exit_code})"
        )

        # Wait with backoff
        await asyncio.sleep(backoff)

        # Check if shutdown was requested during backoff
        if self._shutdown_requested:
            self._logger.info(f"[v101] Shutdown requested - aborting restart of '{name}'")
            return False

        # Reset global shutdown flag BEFORE restarting
        try:
            from backend.core.resilience.graceful_shutdown import reset_global_shutdown
            reset_global_shutdown()
            self._logger.debug(f"[v101] Global shutdown flag reset for '{name}' restart")
        except ImportError:
            pass
        except Exception as e:
            self._logger.debug(f"[v101] Could not reset global shutdown: {e}")

        # Attempt restart
        try:
            await managed.restart_func()
            self._logger.info(
                f"[v101] âœ… {name} restart initiated successfully"
            )
            return True
        except Exception as e:
            self._logger.error(f"[v101] âŒ Failed to restart '{name}': {e}")
            return False

    def get_status(self) -> Dict[str, Dict[str, Any]]:
        """Get status of all supervised cross-repo processes."""
        status = {}
        for name, managed in self.processes.items():
            proc = managed.process
            status[name] = {
                "pid": proc.pid if proc else None,
                "running": proc.returncode is None if proc else False,
                "exit_code": managed.exit_code,
                "restart_count": managed.restart_count,
                "last_restart": managed.last_restart,
                "port": managed.port,
                "enabled": managed.enabled,
            }
        return status


# =============================================================================
# Configuration - Dynamic with Environment Overrides
# =============================================================================

@dataclass
class BootstrapConfig:
    """Dynamic configuration for supervisor bootstrap."""
    
    # Logging
    log_level: str = field(default_factory=lambda: os.getenv("JARVIS_SUPERVISOR_LOG_LEVEL", "INFO"))
    log_format: str = "%(asctime)s | %(levelname)s | %(name)s | %(message)s"
    
    # Voice narration
    voice_enabled: bool = field(default_factory=lambda: os.getenv("STARTUP_NARRATOR_VOICE", "true").lower() == "true")
    voice_name: str = field(default_factory=lambda: os.getenv("STARTUP_NARRATOR_VOICE_NAME", "Daniel"))
    voice_rate: int = field(default_factory=lambda: int(os.getenv("STARTUP_NARRATOR_RATE", "190")))
    
    # Resource validation
    skip_resource_check: bool = field(default_factory=lambda: os.getenv("SKIP_RESOURCE_CHECK", "false").lower() == "true")
    min_memory_gb: float = field(default_factory=lambda: float(os.getenv("MIN_MEMORY_GB", "2.0")))
    min_disk_gb: float = field(default_factory=lambda: float(os.getenv("MIN_DISK_GB", "1.0")))
    
    # Process cleanup
    cleanup_timeout_sigint: float = field(default_factory=lambda: float(os.getenv("CLEANUP_TIMEOUT_SIGINT", "10.0")))
    cleanup_timeout_sigterm: float = field(default_factory=lambda: float(os.getenv("CLEANUP_TIMEOUT_SIGTERM", "5.0")))
    cleanup_timeout_sigkill: float = field(default_factory=lambda: float(os.getenv("CLEANUP_TIMEOUT_SIGKILL", "2.0")))
    max_parallel_cleanups: int = field(default_factory=lambda: int(os.getenv("MAX_PARALLEL_CLEANUPS", "5")))
    
    # Ports to validate
    required_ports: List[int] = field(default_factory=lambda: [
        int(os.getenv("BACKEND_PORT", "8010")),
        int(os.getenv("FRONTEND_PORT", "3000")),
        int(os.getenv("LOADING_SERVER_PORT", "3001")),
    ])
    
    # Patterns to identify JARVIS processes
    # v97.0: Added run_supervisor.py to detect and kill existing supervisor processes
    # that may be holding fcntl locks. This prevents 30-second ownership timeouts.
    jarvis_patterns: List[str] = field(default_factory=lambda: [
        "run_supervisor.py",  # v97.0: CRITICAL - must be first to detect old supervisors
        "start_system.py",
        "main.py",
        "jarvis",
    ])
    
    # PID file locations
    pid_files: List[Path] = field(default_factory=lambda: [
        Path("/tmp/jarvis_master.pid"),
        Path("/tmp/jarvis.pid"),
        Path("/tmp/jarvis_supervisor.pid"),
    ])
    
    # =========================================================================
    # v3.0: Zero-Touch Autonomous Update Settings
    # =========================================================================
    # Default: ENABLED - JARVIS should be a living, self-updating system
    # Disable with: JARVIS_ZERO_TOUCH_ENABLED=false
    zero_touch_enabled: bool = field(default_factory=lambda: os.getenv("JARVIS_ZERO_TOUCH_ENABLED", "true").lower() == "true")
    zero_touch_require_idle: bool = field(default_factory=lambda: os.getenv("JARVIS_ZERO_TOUCH_REQUIRE_IDLE", "true").lower() == "true")
    zero_touch_check_busy: bool = field(default_factory=lambda: os.getenv("JARVIS_ZERO_TOUCH_CHECK_BUSY", "true").lower() == "true")
    zero_touch_auto_security: bool = field(default_factory=lambda: os.getenv("JARVIS_ZERO_TOUCH_AUTO_SECURITY", "true").lower() == "true")
    zero_touch_auto_critical: bool = field(default_factory=lambda: os.getenv("JARVIS_ZERO_TOUCH_AUTO_CRITICAL", "true").lower() == "true")
    zero_touch_auto_minor: bool = field(default_factory=lambda: os.getenv("JARVIS_ZERO_TOUCH_AUTO_MINOR", "true").lower() == "true")
    zero_touch_auto_major: bool = field(default_factory=lambda: os.getenv("JARVIS_ZERO_TOUCH_AUTO_MAJOR", "false").lower() == "true")
    
    # Dead Man's Switch settings
    dms_enabled: bool = field(default_factory=lambda: os.getenv("JARVIS_DMS_ENABLED", "true").lower() == "true")
    dms_probation_seconds: int = field(default_factory=lambda: int(os.getenv("JARVIS_DMS_PROBATION_SECONDS", "30")))
    dms_max_failures: int = field(default_factory=lambda: int(os.getenv("JARVIS_DMS_MAX_FAILURES", "3")))

    # =========================================================================
    # v80.0: Global Startup Timeout and Deep Health Verification
    # v86.0: Adaptive timeout based on cold vs warm start
    # =========================================================================
    # Prevents infinite hangs during startup with a global timeout.
    # Deep health verification ensures components are actually functional.
    # =========================================================================
    global_startup_timeout: float = field(default_factory=lambda: float(os.getenv("SUPERVISOR_STARTUP_TIMEOUT", "600.0")))  # v86.0: Increased to 600s for cold starts
    deep_health_enabled: bool = field(default_factory=lambda: os.getenv("DEEP_HEALTH_ENABLED", "true").lower() == "true")
    deep_health_timeout: float = field(default_factory=lambda: float(os.getenv("DEEP_HEALTH_TIMEOUT", "10.0")))
    advanced_primitives_enabled: bool = field(default_factory=lambda: os.getenv("ADVANCED_PRIMITIVES_ENABLED", "true").lower() == "true")
    
    # AGI OS integration
    agi_os_enabled: bool = field(default_factory=lambda: os.getenv("JARVIS_AGI_OS_ENABLED", "true").lower() == "true")
    agi_os_approval_for_updates: bool = field(default_factory=lambda: os.getenv("JARVIS_AGI_OS_APPROVAL_UPDATES", "false").lower() == "true")

    # =========================================================================
    # JARVIS-Prime Tier-0 Brain Integration
    # =========================================================================
    # Local brain for fast, cost-effective inference without cloud API calls.
    # Can run locally (subprocess) or in Docker/Cloud Run.
    # =========================================================================
    jarvis_prime_enabled: bool = field(default_factory=lambda: os.getenv("JARVIS_PRIME_ENABLED", "true").lower() == "true")
    jarvis_prime_auto_start: bool = field(default_factory=lambda: os.getenv("JARVIS_PRIME_AUTO_START", "true").lower() == "true")
    jarvis_prime_host: str = field(default_factory=lambda: os.getenv("JARVIS_PRIME_HOST", "127.0.0.1"))
    jarvis_prime_port: int = field(default_factory=lambda: int(os.getenv("JARVIS_PRIME_PORT", "8000")))  # v93.15: Standardized to 8000
    jarvis_prime_repo_path: Path = field(default_factory=lambda: Path(os.getenv(
        "JARVIS_PRIME_PATH",
        str(Path.home() / "Documents" / "repos" / "jarvis-prime")
    )))
    jarvis_prime_models_dir: str = field(default_factory=lambda: os.getenv("JARVIS_PRIME_MODELS_DIR", "models"))
    jarvis_prime_startup_timeout: float = field(default_factory=lambda: float(os.getenv("JARVIS_PRIME_STARTUP_TIMEOUT", "60.0")))

    # Docker/Cloud Run mode
    jarvis_prime_use_docker: bool = field(default_factory=lambda: os.getenv("JARVIS_PRIME_USE_DOCKER", "false").lower() == "true")
    jarvis_prime_docker_image: str = field(default_factory=lambda: os.getenv("JARVIS_PRIME_DOCKER_IMAGE", "jarvis-prime:latest"))
    jarvis_prime_use_cloud_run: bool = field(default_factory=lambda: os.getenv("JARVIS_PRIME_USE_CLOUD_RUN", "false").lower() == "true")
    jarvis_prime_cloud_run_url: str = field(default_factory=lambda: os.getenv("JARVIS_PRIME_CLOUD_RUN_URL", ""))

    # =========================================================================
    # v9.4: Intelligent Model Manager (Auto-download & reactor-core deployment)
    # =========================================================================
    # Ensures JARVIS-Prime always has a model to load:
    # - Auto-downloads base models if missing (TinyLlama for testing, Mistral for prod)
    # - Integrates with reactor-core for trained model auto-deployment
    # - Hot-swap models without restart via versioned registry
    # - Memory-aware model selection based on available RAM
    # =========================================================================
    model_manager_enabled: bool = field(default_factory=lambda: os.getenv("MODEL_MANAGER_ENABLED", "true").lower() == "true")
    model_manager_auto_download: bool = field(default_factory=lambda: os.getenv("MODEL_AUTO_DOWNLOAD", "true").lower() == "true")
    model_manager_default_model: str = field(default_factory=lambda: os.getenv("MODEL_DEFAULT", "tinyllama-chat"))
    model_manager_prod_model: str = field(default_factory=lambda: os.getenv("MODEL_PRODUCTION", "mistral-7b-instruct"))
    model_manager_memory_threshold_gb: float = field(default_factory=lambda: float(os.getenv("MODEL_MEMORY_THRESHOLD", "8.0")))
    model_manager_auto_select: bool = field(default_factory=lambda: os.getenv("MODEL_AUTO_SELECT", "true").lower() == "true")
    model_manager_hot_swap_enabled: bool = field(default_factory=lambda: os.getenv("MODEL_HOT_SWAP", "true").lower() == "true")
    model_manager_reactor_core_watch: bool = field(default_factory=lambda: os.getenv("MODEL_REACTOR_WATCH", "true").lower() == "true")

    # =========================================================================
    # v9.4: Enhanced Neural Mesh (Full production agent activation)
    # =========================================================================
    # Activates the full Neural Mesh system with 60+ agents:
    # - Production agents: Memory, Coordinator, HealthMonitor, etc.
    # - JARVIS Bridge for auto-discovery of all JARVIS systems
    # - Crew multi-agent collaboration system
    # - Shared knowledge graph with ChromaDB + NetworkX
    # =========================================================================
    neural_mesh_production: bool = field(default_factory=lambda: os.getenv("NEURAL_MESH_PRODUCTION", "true").lower() == "true")
    neural_mesh_agents_enabled: bool = field(default_factory=lambda: os.getenv("NEURAL_MESH_AGENTS", "true").lower() == "true")
    neural_mesh_knowledge_graph: bool = field(default_factory=lambda: os.getenv("NEURAL_MESH_KNOWLEDGE", "true").lower() == "true")
    neural_mesh_crew_enabled: bool = field(default_factory=lambda: os.getenv("NEURAL_MESH_CREW", "true").lower() == "true")
    neural_mesh_jarvis_bridge: bool = field(default_factory=lambda: os.getenv("NEURAL_MESH_JARVIS_BRIDGE", "true").lower() == "true")
    neural_mesh_health_interval: float = field(default_factory=lambda: float(os.getenv("NEURAL_MESH_HEALTH_INTERVAL", "30.0")))

    # =========================================================================
    # Reactor-Core Integration (Auto-deployment of trained models)
    # =========================================================================
    reactor_core_enabled: bool = field(default_factory=lambda: os.getenv("REACTOR_CORE_ENABLED", "true").lower() == "true")
    reactor_core_watch_dir: Optional[str] = field(default_factory=lambda: os.getenv("REACTOR_CORE_OUTPUT"))
    reactor_core_auto_deploy: bool = field(default_factory=lambda: os.getenv("REACTOR_CORE_AUTO_DEPLOY", "true").lower() == "true")

    # =========================================================================
    # Data Flywheel Integration (Self-improving learning loop)
    # =========================================================================
    data_flywheel_enabled: bool = field(default_factory=lambda: os.getenv("DATA_FLYWHEEL_ENABLED", "true").lower() == "true")
    data_flywheel_auto_collect: bool = field(default_factory=lambda: os.getenv("DATA_FLYWHEEL_AUTO_COLLECT", "true").lower() == "true")
    data_flywheel_auto_train: bool = field(default_factory=lambda: os.getenv("DATA_FLYWHEEL_AUTO_TRAIN", "true").lower() == "true")
    data_flywheel_training_schedule: str = field(default_factory=lambda: os.getenv("DATA_FLYWHEEL_TRAINING_SCHEDULE", "03:00"))  # 3 AM daily
    data_flywheel_min_experiences: int = field(default_factory=lambda: int(os.getenv("DATA_FLYWHEEL_MIN_EXPERIENCES", "100")))
    data_flywheel_cooldown_hours: int = field(default_factory=lambda: int(os.getenv("DATA_FLYWHEEL_COOLDOWN_HOURS", "24")))

    # =========================================================================
    # v9.3: Intelligent Learning Goals (Auto-discovery from interactions)
    # =========================================================================
    # Automatic discovery of learning topics from JARVIS interactions:
    # - Failed responses (JARVIS couldn't help)
    # - User corrections (JARVIS was wrong)
    # - Unknown technical terms
    # - Frequently asked topics
    # - Trending technologies mentioned
    # =========================================================================
    learning_goals_enabled: bool = field(default_factory=lambda: os.getenv("LEARNING_GOALS_ENABLED", "true").lower() == "true")
    learning_goals_auto_discover: bool = field(default_factory=lambda: os.getenv("LEARNING_GOALS_AUTO_DISCOVER", "true").lower() == "true")
    learning_goals_max_topics: int = field(default_factory=lambda: int(os.getenv("LEARNING_GOALS_MAX_TOPICS", "50")))

    # Discovery triggers and intervals
    learning_goals_discovery_interval_hours: float = field(default_factory=lambda: float(os.getenv("LEARNING_DISCOVERY_INTERVAL", "2")))
    learning_goals_min_mentions: int = field(default_factory=lambda: int(os.getenv("LEARNING_MIN_MENTIONS", "2")))
    learning_goals_min_confidence: float = field(default_factory=lambda: float(os.getenv("LEARNING_MIN_CONFIDENCE", "0.5")))
    learning_goals_lookback_days: int = field(default_factory=lambda: int(os.getenv("LEARNING_LOOKBACK_DAYS", "30")))

    # Safe Scout integration for auto-scraping discovered topics
    learning_goals_auto_scrape: bool = field(default_factory=lambda: os.getenv("LEARNING_AUTO_SCRAPE", "true").lower() == "true")
    learning_goals_scrape_concurrency: int = field(default_factory=lambda: int(os.getenv("LEARNING_SCRAPE_CONCURRENCY", "3")))
    learning_goals_max_pages_per_topic: int = field(default_factory=lambda: int(os.getenv("LEARNING_MAX_PAGES", "10")))

    # Source weights for priority calculation (0.0-1.0)
    learning_goals_weight_corrections: float = field(default_factory=lambda: float(os.getenv("LEARNING_WEIGHT_CORRECTIONS", "1.0")))
    learning_goals_weight_failures: float = field(default_factory=lambda: float(os.getenv("LEARNING_WEIGHT_FAILURES", "0.9")))
    learning_goals_weight_questions: float = field(default_factory=lambda: float(os.getenv("LEARNING_WEIGHT_QUESTIONS", "0.7")))
    learning_goals_weight_trending: float = field(default_factory=lambda: float(os.getenv("LEARNING_WEIGHT_TRENDING", "0.5")))

    # =========================================================================
    # v9.0: Intelligence Systems (UAE/SAI/Neural Mesh/MAS)
    # =========================================================================
    # UAE (Unified Awareness Engine) - Screen awareness and computer vision
    uae_enabled: bool = field(default_factory=lambda: os.getenv("UAE_ENABLED", "true").lower() == "true")
    uae_chain_of_thought: bool = field(default_factory=lambda: os.getenv("UAE_CHAIN_OF_THOUGHT", "true").lower() == "true")

    # SAI (Situational Awareness Intelligence) - Window/app tracking
    sai_enabled: bool = field(default_factory=lambda: os.getenv("SAI_ENABLED", "true").lower() == "true")
    sai_yabai_bridge: bool = field(default_factory=lambda: os.getenv("SAI_YABAI_BRIDGE", "true").lower() == "true")

    # Neural Mesh - Distributed intelligence coordination
    neural_mesh_enabled: bool = field(default_factory=lambda: os.getenv("NEURAL_MESH_ENABLED", "true").lower() == "true")
    neural_mesh_sync_interval: float = field(default_factory=lambda: float(os.getenv("NEURAL_MESH_SYNC_INTERVAL", "5.0")))

    # MAS (Multi-Agent System) - Coordinated agent execution
    mas_enabled: bool = field(default_factory=lambda: os.getenv("MAS_ENABLED", "true").lower() == "true")
    mas_max_concurrent_agents: int = field(default_factory=lambda: int(os.getenv("MAS_MAX_CONCURRENT_AGENTS", "5")))

    # CAI (Collective AI Intelligence) - Cross-system insight aggregation
    cai_enabled: bool = field(default_factory=lambda: os.getenv("CAI_ENABLED", "true").lower() == "true")

    # =========================================================================
    # v9.0: Continuous Background Web Scraping
    # =========================================================================
    continuous_scraping_enabled: bool = field(default_factory=lambda: os.getenv("CONTINUOUS_SCRAPING_ENABLED", "true").lower() == "true")
    continuous_scraping_interval_hours: float = field(default_factory=lambda: float(os.getenv("CONTINUOUS_SCRAPING_INTERVAL_HOURS", "4")))
    continuous_scraping_max_pages: int = field(default_factory=lambda: int(os.getenv("CONTINUOUS_SCRAPING_MAX_PAGES", "50")))
    continuous_scraping_topics: str = field(default_factory=lambda: os.getenv("CONTINUOUS_SCRAPING_TOPICS", ""))

    # =========================================================================
    # v9.0: Reactor-Core Integration (Training Pipeline)
    # =========================================================================
    reactor_core_integration_enabled: bool = field(default_factory=lambda: os.getenv("REACTOR_CORE_ENABLED", "true").lower() == "true")
    reactor_core_repo_path: str = field(default_factory=lambda: os.getenv("REACTOR_CORE_PATH", str(Path.home() / "Documents" / "repos" / "reactor-core")))

    # =========================================================================
    # v9.2: Intelligent Training Scheduler (Reactor-Core Pipeline Orchestration)
    # =========================================================================
    # Automatic training runs triggered by multiple intelligent conditions:
    # - Time-based: Cron schedule (default: 3 AM daily)
    # - Data-threshold: When enough new experiences accumulate
    # - Quality-degradation: When model performance drops below threshold
    # - Manual: Via API or voice command
    # =========================================================================
    training_scheduler_enabled: bool = field(default_factory=lambda: os.getenv("TRAINING_SCHEDULER_ENABLED", "true").lower() == "true")

    # Time-based scheduling (cron expression)
    training_cron_schedule: str = field(default_factory=lambda: os.getenv("TRAINING_CRON_SCHEDULE", "0 3 * * *"))  # 3 AM daily
    training_timezone: str = field(default_factory=lambda: os.getenv("TRAINING_TIMEZONE", "America/Chicago"))

    # Data-threshold trigger
    training_data_threshold_enabled: bool = field(default_factory=lambda: os.getenv("TRAINING_DATA_THRESHOLD_ENABLED", "true").lower() == "true")
    training_min_new_experiences: int = field(default_factory=lambda: int(os.getenv("TRAINING_MIN_NEW_EXPERIENCES", "100")))
    training_data_check_interval_hours: float = field(default_factory=lambda: float(os.getenv("TRAINING_DATA_CHECK_INTERVAL", "4")))

    # Quality-degradation trigger
    training_quality_trigger_enabled: bool = field(default_factory=lambda: os.getenv("TRAINING_QUALITY_TRIGGER_ENABLED", "true").lower() == "true")
    training_quality_threshold: float = field(default_factory=lambda: float(os.getenv("TRAINING_QUALITY_THRESHOLD", "0.7")))
    training_quality_check_interval_hours: float = field(default_factory=lambda: float(os.getenv("TRAINING_QUALITY_CHECK_INTERVAL", "6")))

    # Pipeline configuration
    training_base_model: str = field(default_factory=lambda: os.getenv("TRAINING_BASE_MODEL", "meta-llama/Llama-3.2-3B"))
    training_lora_rank: int = field(default_factory=lambda: int(os.getenv("TRAINING_LORA_RANK", "64")))
    training_epochs: int = field(default_factory=lambda: int(os.getenv("TRAINING_EPOCHS", "3")))
    training_quantization_method: str = field(default_factory=lambda: os.getenv("TRAINING_QUANTIZATION", "q4_k_m"))
    training_eval_threshold: float = field(default_factory=lambda: float(os.getenv("TRAINING_EVAL_THRESHOLD", "0.7")))
    training_skip_gatekeeper: bool = field(default_factory=lambda: os.getenv("TRAINING_SKIP_GATEKEEPER", "false").lower() == "true")

    # Retry and cooldown
    training_max_retries: int = field(default_factory=lambda: int(os.getenv("TRAINING_MAX_RETRIES", "3")))
    training_retry_delay_minutes: int = field(default_factory=lambda: int(os.getenv("TRAINING_RETRY_DELAY", "30")))
    training_cooldown_hours: int = field(default_factory=lambda: int(os.getenv("TRAINING_COOLDOWN_HOURS", "24")))

    # Auto-deployment after training
    training_auto_deploy_to_prime: bool = field(default_factory=lambda: os.getenv("TRAINING_AUTO_DEPLOY_PRIME", "true").lower() == "true")
    training_auto_upload_to_gcs: bool = field(default_factory=lambda: os.getenv("TRAINING_AUTO_UPLOAD_GCS", "true").lower() == "true")

    # =========================================================================
    # v9.5: Infrastructure Orchestrator (On-Demand GCP Resources)
    # =========================================================================
    # Fixes the root issue: GCP resources staying deployed when JARVIS is off.
    # When enabled, infrastructure is provisioned on startup and destroyed on shutdown.
    # =========================================================================
    infra_on_demand_enabled: bool = field(default_factory=lambda: os.getenv("JARVIS_INFRA_ON_DEMAND", "true").lower() == "true")
    infra_auto_destroy_on_shutdown: bool = field(default_factory=lambda: os.getenv("JARVIS_INFRA_AUTO_DESTROY", "true").lower() == "true")
    infra_terraform_timeout_seconds: int = field(default_factory=lambda: int(os.getenv("JARVIS_TERRAFORM_TIMEOUT", "300")))
    infra_memory_threshold_gb: float = field(default_factory=lambda: float(os.getenv("JARVIS_INFRA_MEMORY_THRESHOLD_GB", "4.0")))
    infra_daily_cost_limit_usd: float = field(default_factory=lambda: float(os.getenv("JARVIS_DAILY_COST_LIMIT", "1.0")))


# =============================================================================
# v100.0: ULTRA-ROBUST TRINITY LAUNCH CONFIGURATION
# =============================================================================
# 100% environment-driven configuration for Trinity component launch.
# Zero hardcoding - everything configurable at runtime.
# =============================================================================

@dataclass
class TrinityLaunchConfig:
    """
    Ultra-robust configuration for Trinity component launch v100.0.

    ALL values are environment-driven with sensible defaults.
    Zero hardcoding - everything configurable at runtime.

    Features:
    - Dynamic repo discovery (multiple search strategies)
    - Robust venv detection (handles all Python environments)
    - Adaptive timeouts (based on system performance)
    - Retry logic with exponential backoff and jitter
    - Graceful degradation (continue if optional components fail)
    - Distributed tracing (W3C trace context)
    - Process lifecycle management (proper cleanup)
    - Circuit breakers (prevent cascade failures)
    """

    # =========================================================================
    # Core Trinity Settings
    # =========================================================================
    trinity_enabled: bool = field(default_factory=lambda: os.getenv("TRINITY_ENABLED", "true").lower() == "true")
    trinity_auto_launch: bool = field(default_factory=lambda: os.getenv("TRINITY_AUTO_LAUNCH", "true").lower() == "true")
    trinity_instance_id: str = field(default_factory=lambda: os.getenv("TRINITY_INSTANCE_ID", ""))

    # =========================================================================
    # Repo Discovery Settings
    # =========================================================================
    # Primary paths (check first)
    jprime_repo_path: Optional[Path] = field(default_factory=lambda: Path(os.getenv(
        "JARVIS_PRIME_PATH",
        str(Path.home() / "Documents" / "repos" / "jarvis-prime")
    )) if os.getenv("JARVIS_PRIME_PATH") or (Path.home() / "Documents" / "repos" / "jarvis-prime").exists() else None)

    reactor_core_repo_path: Optional[Path] = field(default_factory=lambda: Path(os.getenv(
        "REACTOR_CORE_PATH",
        str(Path.home() / "Documents" / "repos" / "reactor-core")
    )) if os.getenv("REACTOR_CORE_PATH") or (Path.home() / "Documents" / "repos" / "reactor-core").exists() else None)

    # Secondary search locations (fallback)
    repo_search_paths: List[Path] = field(default_factory=lambda: [
        Path(p) for p in os.getenv("TRINITY_REPO_SEARCH_PATHS", "").split(":") if p
    ] or [
        Path.home() / "Documents" / "repos",
        Path.home() / "repos",
        Path.home() / "code",
        Path.home() / "projects",
        Path.home() / "dev",
        Path.cwd().parent,  # Sibling directories
    ])

    # Repo identification patterns (for dynamic discovery)
    jprime_identifiers: List[str] = field(default_factory=lambda:
        os.getenv("TRINITY_JPRIME_IDENTIFIERS", "jarvis-prime,jarvis_prime,j-prime,jprime").split(",")
    )
    reactor_core_identifiers: List[str] = field(default_factory=lambda:
        os.getenv("TRINITY_REACTOR_IDENTIFIERS", "reactor-core,reactor_core,reactorcore").split(",")
    )

    # =========================================================================
    # Python Environment Detection
    # =========================================================================
    # Venv detection methods (in order of preference)
    venv_detection_order: List[str] = field(default_factory=lambda:
        os.getenv("TRINITY_VENV_DETECTION_ORDER", "venv,env,.venv,.env,virtualenv").split(",")
    )

    # Additional Python paths to check
    python_executable_names: List[str] = field(default_factory=lambda:
        os.getenv("TRINITY_PYTHON_NAMES", "python3,python,python3.11,python3.10,python3.9").split(",")
    )

    # Whether to use system Python as fallback
    fallback_to_system_python: bool = field(default_factory=lambda:
        os.getenv("TRINITY_FALLBACK_SYSTEM_PYTHON", "true").lower() == "true"
    )

    # =========================================================================
    # Launch Script Detection
    # =========================================================================
    # J-Prime launch scripts (in order of preference)
    jprime_launch_scripts: List[str] = field(default_factory=lambda:
        os.getenv("TRINITY_JPRIME_SCRIPTS",
            "jarvis_prime/server.py,run_server.py,jarvis_prime/core/trinity_bridge.py,main.py"
        ).split(",")
    )

    # Reactor-Core launch scripts (in order of preference)
    reactor_core_launch_scripts: List[str] = field(default_factory=lambda:
        os.getenv("TRINITY_REACTOR_SCRIPTS",
            "reactor_core/orchestration/trinity_orchestrator.py,run_orchestrator.py,main.py"
        ).split(",")
    )

    # =========================================================================
    # Timeout Configuration (Adaptive)
    # =========================================================================
    launch_timeout_sec: float = field(default_factory=lambda: float(os.getenv("TRINITY_LAUNCH_TIMEOUT", "120.0")))
    registration_timeout_sec: float = field(default_factory=lambda: float(os.getenv("TRINITY_REGISTRATION_TIMEOUT", "30.0")))
    health_check_timeout_sec: float = field(default_factory=lambda: float(os.getenv("TRINITY_HEALTH_CHECK_TIMEOUT", "10.0")))
    shutdown_timeout_sec: float = field(default_factory=lambda: float(os.getenv("TRINITY_SHUTDOWN_TIMEOUT", "30.0")))

    # Adaptive timeout multiplier (based on system load)
    adaptive_timeout_enabled: bool = field(default_factory=lambda:
        os.getenv("TRINITY_ADAPTIVE_TIMEOUT", "true").lower() == "true"
    )
    adaptive_timeout_max_multiplier: float = field(default_factory=lambda:
        float(os.getenv("TRINITY_ADAPTIVE_TIMEOUT_MAX", "3.0"))
    )

    # =========================================================================
    # Heartbeat Configuration
    # =========================================================================
    heartbeat_dir: Path = field(default_factory=lambda:
        Path(os.getenv("TRINITY_HEARTBEAT_DIR", str(Path.home() / ".jarvis" / "trinity" / "components")))
    )
    heartbeat_max_age_sec: float = field(default_factory=lambda: float(os.getenv("TRINITY_HEARTBEAT_MAX_AGE", "30.0")))
    heartbeat_check_interval_sec: float = field(default_factory=lambda: float(os.getenv("TRINITY_HEARTBEAT_INTERVAL", "5.0")))

    # =========================================================================
    # Retry Configuration
    # =========================================================================
    max_retries: int = field(default_factory=lambda: int(os.getenv("TRINITY_MAX_RETRIES", "3")))
    retry_base_delay_sec: float = field(default_factory=lambda: float(os.getenv("TRINITY_RETRY_BASE_DELAY", "1.0")))
    retry_max_delay_sec: float = field(default_factory=lambda: float(os.getenv("TRINITY_RETRY_MAX_DELAY", "30.0")))
    retry_exponential_base: float = field(default_factory=lambda: float(os.getenv("TRINITY_RETRY_EXPONENTIAL_BASE", "2.0")))
    retry_jitter_factor: float = field(default_factory=lambda: float(os.getenv("TRINITY_RETRY_JITTER", "0.1")))

    # =========================================================================
    # Circuit Breaker Configuration
    # =========================================================================
    circuit_breaker_enabled: bool = field(default_factory=lambda:
        os.getenv("TRINITY_CIRCUIT_BREAKER_ENABLED", "true").lower() == "true"
    )
    circuit_breaker_failure_threshold: int = field(default_factory=lambda:
        int(os.getenv("TRINITY_CIRCUIT_FAILURE_THRESHOLD", "5"))
    )
    circuit_breaker_timeout_sec: float = field(default_factory=lambda:
        float(os.getenv("TRINITY_CIRCUIT_TIMEOUT", "60.0"))
    )
    circuit_breaker_half_open_max_calls: int = field(default_factory=lambda:
        int(os.getenv("TRINITY_CIRCUIT_HALF_OPEN_CALLS", "3"))
    )

    # =========================================================================
    # Process Management
    # =========================================================================
    log_dir: Path = field(default_factory=lambda:
        Path(os.getenv("TRINITY_LOG_DIR", str(Path.home() / ".jarvis" / "logs" / "services")))
    )
    detach_processes: bool = field(default_factory=lambda:
        os.getenv("TRINITY_DETACH_PROCESSES", "true").lower() == "true"
    )

    # Graceful shutdown escalation (SIGTERM â†’ SIGKILL)
    sigterm_timeout_sec: float = field(default_factory=lambda: float(os.getenv("TRINITY_SIGTERM_TIMEOUT", "5.0")))
    sigkill_timeout_sec: float = field(default_factory=lambda: float(os.getenv("TRINITY_SIGKILL_TIMEOUT", "2.0")))

    # =========================================================================
    # Port Configuration
    # =========================================================================
    jprime_ports: List[int] = field(default_factory=lambda:
        [int(p) for p in os.getenv("TRINITY_JPRIME_PORTS", "8000").split(",")]  # v93.15: Standardized to 8000
    )
    reactor_core_ports: List[int] = field(default_factory=lambda:
        [int(p) for p in os.getenv("TRINITY_REACTOR_PORTS", "8090").split(",")]
    )

    # Dynamic port allocation (fallback if primary ports busy)
    dynamic_port_enabled: bool = field(default_factory=lambda:
        os.getenv("TRINITY_DYNAMIC_PORTS", "true").lower() == "true"
    )
    dynamic_port_range_start: int = field(default_factory=lambda:
        int(os.getenv("TRINITY_DYNAMIC_PORT_START", "8100"))
    )
    dynamic_port_range_end: int = field(default_factory=lambda:
        int(os.getenv("TRINITY_DYNAMIC_PORT_END", "8199"))
    )

    # =========================================================================
    # Graceful Degradation
    # =========================================================================
    jprime_optional: bool = field(default_factory=lambda:
        os.getenv("TRINITY_JPRIME_OPTIONAL", "true").lower() == "true"
    )
    reactor_core_optional: bool = field(default_factory=lambda:
        os.getenv("TRINITY_REACTOR_OPTIONAL", "true").lower() == "true"
    )
    continue_on_partial_failure: bool = field(default_factory=lambda:
        os.getenv("TRINITY_CONTINUE_ON_PARTIAL", "true").lower() == "true"
    )

    # =========================================================================
    # Distributed Tracing
    # =========================================================================
    tracing_enabled: bool = field(default_factory=lambda:
        os.getenv("TRINITY_TRACING_ENABLED", "true").lower() == "true"
    )
    trace_sample_rate: float = field(default_factory=lambda:
        float(os.getenv("TRINITY_TRACE_SAMPLE_RATE", "1.0"))
    )

    # =========================================================================
    # Health Monitoring
    # =========================================================================
    health_monitor_enabled: bool = field(default_factory=lambda:
        os.getenv("TRINITY_HEALTH_MONITOR_ENABLED", "true").lower() == "true"
    )
    health_monitor_interval_sec: float = field(default_factory=lambda:
        float(os.getenv("TRINITY_HEALTH_MONITOR_INTERVAL", "10.0"))
    )
    auto_restart_on_crash: bool = field(default_factory=lambda:
        os.getenv("TRINITY_AUTO_RESTART", "true").lower() == "true"
    )
    max_auto_restarts: int = field(default_factory=lambda:
        int(os.getenv("TRINITY_MAX_RESTARTS", "3"))
    )
    restart_cooldown_sec: float = field(default_factory=lambda:
        float(os.getenv("TRINITY_RESTART_COOLDOWN", "60.0"))
    )
    health_check_http_enabled: bool = field(default_factory=lambda:
        os.getenv("TRINITY_HEALTH_HTTP_ENABLED", "true").lower() == "true"
    )

    # =========================================================================
    # v100.0: Zombie Process Management
    # =========================================================================
    zombie_kill_timeout_sec: float = field(default_factory=lambda:
        float(os.getenv("TRINITY_ZOMBIE_KILL_TIMEOUT", "5.0"))
    )
    port_release_wait_sec: float = field(default_factory=lambda:
        float(os.getenv("TRINITY_PORT_RELEASE_WAIT", "0.5"))
    )
    trinity_process_patterns: List[str] = field(default_factory=lambda:
        os.getenv("TRINITY_PROCESS_PATTERNS",
            "jarvis,jprime,j_prime,jarvis_prime,reactor,trinity,python,uvicorn"
        ).split(",")
    )

    # =========================================================================
    # v93.3: Heartbeat Thresholds with Startup Awareness
    # =========================================================================
    heartbeat_stale_threshold_sec: float = field(default_factory=lambda:
        float(os.getenv("TRINITY_HEARTBEAT_STALE_THRESHOLD", "60.0"))  # v93.3: Increased from 30s to 60s
    )
    # v93.3: Startup grace period - extended stale threshold during initial startup
    startup_grace_period_sec: float = field(default_factory=lambda:
        float(os.getenv("TRINITY_STARTUP_GRACE_PERIOD", "120.0"))  # 2 minutes grace
    )
    # v93.3: Multiplier for stale threshold during startup
    startup_stale_multiplier: float = field(default_factory=lambda:
        float(os.getenv("TRINITY_STARTUP_STALE_MULTIPLIER", "5.0"))  # 5x during startup
    )

    # =========================================================================
    # v100.0: Log Rotation
    # =========================================================================
    log_rotation_size_bytes: int = field(default_factory=lambda:
        int(os.getenv("TRINITY_LOG_ROTATION_SIZE", str(10 * 1024 * 1024)))  # 10 MB default
    )
    log_level: str = field(default_factory=lambda:
        os.getenv("TRINITY_LOG_LEVEL", "INFO")
    )

    # =========================================================================
    # v100.0: API Ports
    # =========================================================================
    jarvis_api_port: int = field(default_factory=lambda:
        int(os.getenv("JARVIS_API_PORT", "8080"))
    )

    # =========================================================================
    # Environment Variables to Pass
    # =========================================================================
    env_passthrough: List[str] = field(default_factory=lambda:
        os.getenv("TRINITY_ENV_PASSTHROUGH",
            "PYTHONPATH,TRINITY_ENABLED,ANTHROPIC_API_KEY,GOOGLE_APPLICATION_CREDENTIALS"
        ).split(",")
    )

    # Additional environment variables to set
    env_overrides: Dict[str, str] = field(default_factory=lambda: {
        k.replace("TRINITY_ENV_", ""): v
        for k, v in os.environ.items()
        if k.startswith("TRINITY_ENV_")
    })

    def __post_init__(self):
        """Validate and create necessary directories."""
        # Ensure heartbeat directory exists
        self.heartbeat_dir.mkdir(parents=True, exist_ok=True)

        # Ensure log directory exists
        self.log_dir.mkdir(parents=True, exist_ok=True)

        # Generate instance ID if not provided
        if not self.trinity_instance_id:
            import uuid
            self.trinity_instance_id = f"trinity_{uuid.uuid4().hex[:8]}"


# =============================================================================
# v100.0: DYNAMIC REPO DISCOVERY SYSTEM
# =============================================================================

class DynamicRepoDiscovery:
    """
    Intelligent repo discovery system that finds Trinity repos dynamically.

    Discovery strategies (in order):
    1. Environment variables (JARVIS_PRIME_PATH, REACTOR_CORE_PATH)
    2. User config file (~/.jarvis/repos.json)
    3. Common repo locations (~/Documents/repos, ~/repos, ~/code, etc.)
    4. Git remote scanning (looks for known repo URLs)
    5. Parent/sibling directory scanning
    6. Recursive search (limited depth)
    """

    def __init__(self, config: TrinityLaunchConfig):
        self.config = config
        self._discovery_cache: Dict[str, Optional[Path]] = {}
        self._logger = logging.getLogger("TrinityRepoDiscovery")

    async def discover_jprime(self) -> Optional[Path]:
        """Discover J-Prime repository path."""
        if "jprime" in self._discovery_cache:
            return self._discovery_cache["jprime"]

        # Strategy 1: Environment variable / config
        if self.config.jprime_repo_path and self.config.jprime_repo_path.exists():
            self._discovery_cache["jprime"] = self.config.jprime_repo_path
            return self.config.jprime_repo_path

        # Strategy 2: User config file
        config_path = Path.home() / ".jarvis" / "repos.json"
        if config_path.exists():
            try:
                import json
                with open(config_path) as f:
                    repos = json.load(f)
                if "jarvis_prime" in repos:
                    path = Path(repos["jarvis_prime"])
                    if path.exists():
                        self._discovery_cache["jprime"] = path
                        return path
            except Exception:
                pass

        # Strategy 3: Search common locations
        for search_path in self.config.repo_search_paths:
            if not search_path.exists():
                continue
            for identifier in self.config.jprime_identifiers:
                candidate = search_path / identifier
                if candidate.exists() and self._is_jprime_repo(candidate):
                    self._discovery_cache["jprime"] = candidate
                    self._logger.info(f"Discovered J-Prime at: {candidate}")
                    return candidate

        # Strategy 4: Git remote scanning
        found = await self._scan_for_git_remote("jarvis-prime", self.config.repo_search_paths)
        if found:
            self._discovery_cache["jprime"] = found
            return found

        self._discovery_cache["jprime"] = None
        return None

    async def discover_reactor_core(self) -> Optional[Path]:
        """Discover Reactor-Core repository path."""
        if "reactor_core" in self._discovery_cache:
            return self._discovery_cache["reactor_core"]

        # Strategy 1: Environment variable / config
        if self.config.reactor_core_repo_path and self.config.reactor_core_repo_path.exists():
            self._discovery_cache["reactor_core"] = self.config.reactor_core_repo_path
            return self.config.reactor_core_repo_path

        # Strategy 2: User config file
        config_path = Path.home() / ".jarvis" / "repos.json"
        if config_path.exists():
            try:
                import json
                with open(config_path) as f:
                    repos = json.load(f)
                if "reactor_core" in repos:
                    path = Path(repos["reactor_core"])
                    if path.exists():
                        self._discovery_cache["reactor_core"] = path
                        return path
            except Exception:
                pass

        # Strategy 3: Search common locations
        for search_path in self.config.repo_search_paths:
            if not search_path.exists():
                continue
            for identifier in self.config.reactor_core_identifiers:
                candidate = search_path / identifier
                if candidate.exists() and self._is_reactor_core_repo(candidate):
                    self._discovery_cache["reactor_core"] = candidate
                    self._logger.info(f"Discovered Reactor-Core at: {candidate}")
                    return candidate

        # Strategy 4: Git remote scanning
        found = await self._scan_for_git_remote("reactor-core", self.config.repo_search_paths)
        if found:
            self._discovery_cache["reactor_core"] = found
            return found

        self._discovery_cache["reactor_core"] = None
        return None

    def _is_jprime_repo(self, path: Path) -> bool:
        """Verify this is the J-Prime repo by checking for signature files."""
        signature_files = [
            path / "jarvis_prime" / "server.py",
            path / "jarvis_prime" / "__init__.py",
            path / "run_server.py",
        ]
        return any(f.exists() for f in signature_files)

    def _is_reactor_core_repo(self, path: Path) -> bool:
        """Verify this is the Reactor-Core repo by checking for signature files."""
        signature_files = [
            path / "reactor_core" / "orchestration" / "trinity_orchestrator.py",
            path / "reactor_core" / "__init__.py",
            path / "run_orchestrator.py",
        ]
        return any(f.exists() for f in signature_files)

    async def _scan_for_git_remote(self, repo_name: str, search_paths: List[Path]) -> Optional[Path]:
        """Scan for repos by checking git remote URLs."""
        import subprocess

        for search_path in search_paths:
            if not search_path.exists():
                continue

            try:
                # List directories
                for entry in search_path.iterdir():
                    if not entry.is_dir():
                        continue
                    git_dir = entry / ".git"
                    if not git_dir.exists():
                        continue

                    # Check git remote
                    try:
                        result = subprocess.run(
                            ["git", "-C", str(entry), "remote", "-v"],
                            capture_output=True, text=True, timeout=5
                        )
                        if repo_name in result.stdout.lower():
                            return entry
                    except (subprocess.TimeoutExpired, FileNotFoundError):
                        continue
            except PermissionError:
                continue

        return None


# =============================================================================
# v100.0: ROBUST VENV DETECTOR
# =============================================================================

class RobustVenvDetector:
    """
    Robust Python virtual environment detector.

    Handles:
    - Standard venv (venv, env, .venv, .env)
    - Virtualenvwrapper (~/.virtualenvs)
    - Conda environments
    - Poetry environments
    - Pipenv environments
    - pyenv
    - System Python fallback
    """

    def __init__(self, config: TrinityLaunchConfig):
        self.config = config
        self._logger = logging.getLogger("TrinityVenvDetector")

    def find_python(self, repo_path: Path) -> str:
        """Find the best Python executable for a repo."""
        # Strategy 1: Check standard venv locations
        for venv_name in self.config.venv_detection_order:
            venv_path = repo_path / venv_name
            python = self._find_python_in_venv(venv_path)
            if python:
                self._logger.debug(f"Found Python in {venv_name}: {python}")
                return python

        # Strategy 2: Check .python-version (pyenv)
        pyenv_file = repo_path / ".python-version"
        if pyenv_file.exists():
            try:
                # v92.0: Use safe file reading to avoid "Bad file descriptor" errors
                version = _safe_read_file(pyenv_file, default="").strip()
                if not version:
                    version = None  # Will skip pyenv if file is empty/unreadable
                pyenv_python = Path.home() / ".pyenv" / "versions" / (version or "") / "bin" / "python"
                if pyenv_python.exists():
                    self._logger.debug(f"Found pyenv Python: {pyenv_python}")
                    return str(pyenv_python)
            except Exception:
                pass

        # Strategy 3: Check poetry.lock (poetry environment)
        if (repo_path / "poetry.lock").exists():
            poetry_python = self._find_poetry_python(repo_path)
            if poetry_python:
                self._logger.debug(f"Found poetry Python: {poetry_python}")
                return poetry_python

        # Strategy 4: Check Pipfile.lock (pipenv environment)
        if (repo_path / "Pipfile.lock").exists():
            pipenv_python = self._find_pipenv_python(repo_path)
            if pipenv_python:
                self._logger.debug(f"Found pipenv Python: {pipenv_python}")
                return pipenv_python

        # Strategy 5: Fallback to system Python
        if self.config.fallback_to_system_python:
            for name in self.config.python_executable_names:
                import shutil
                python = shutil.which(name)
                if python:
                    self._logger.debug(f"Using system Python: {python}")
                    return python

        # Last resort: use current interpreter
        self._logger.warning(f"No Python found for {repo_path}, using current interpreter")
        return sys.executable

    def _find_python_in_venv(self, venv_path: Path) -> Optional[str]:
        """Find Python executable in a venv directory."""
        if not venv_path.exists():
            return None

        # Unix-like systems
        for name in self.config.python_executable_names:
            python_path = venv_path / "bin" / name
            if python_path.exists():
                return str(python_path)

        # Windows
        for name in self.config.python_executable_names:
            python_path = venv_path / "Scripts" / f"{name}.exe"
            if python_path.exists():
                return str(python_path)

        return None

    def _find_poetry_python(self, repo_path: Path) -> Optional[str]:
        """Find Python from poetry environment."""
        import subprocess
        try:
            result = subprocess.run(
                ["poetry", "env", "info", "-p"],
                cwd=str(repo_path),
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                venv_path = Path(result.stdout.strip())
                return self._find_python_in_venv(venv_path)
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        return None

    def _find_pipenv_python(self, repo_path: Path) -> Optional[str]:
        """Find Python from pipenv environment."""
        import subprocess
        try:
            result = subprocess.run(
                ["pipenv", "--venv"],
                cwd=str(repo_path),
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                venv_path = Path(result.stdout.strip())
                return self._find_python_in_venv(venv_path)
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        return None

    def get_python_executable(self, repo_path: Path) -> str:
        """Alias for find_python - used by v100 launch methods."""
        return self.find_python(repo_path)


# =============================================================================
# v100.0: W3C DISTRIBUTED TRACING FOR TRINITY
# =============================================================================

@dataclass
class TrinityTraceContext:
    """W3C Trace Context for Trinity distributed tracing."""
    trace_id: str = field(default_factory=lambda: uuid.uuid4().hex if 'uuid' in dir() else os.urandom(16).hex())
    span_id: str = field(default_factory=lambda: (uuid.uuid4().hex[:16] if 'uuid' in dir() else os.urandom(8).hex()))
    parent_span_id: Optional[str] = None
    component: str = "supervisor"
    operation: str = "trinity_launch"
    sampled: bool = True
    start_time: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def create_child_span(self, component: str, operation: str) -> "TrinityTraceContext":
        """Create a child span from this context."""
        return TrinityTraceContext(
            trace_id=self.trace_id,
            span_id=uuid.uuid4().hex[:16] if 'uuid' in dir() else os.urandom(8).hex(),
            parent_span_id=self.span_id,
            component=component,
            operation=operation,
            sampled=self.sampled,
            start_time=time.time()
        )

    def to_traceparent(self) -> str:
        """Convert to W3C traceparent header format."""
        flags = "01" if self.sampled else "00"
        return f"00-{self.trace_id}-{self.span_id}-{flags}"

    def to_env_vars(self) -> Dict[str, str]:
        """Convert to environment variables for subprocess."""
        return {
            "TRINITY_TRACE_ID": self.trace_id,
            "TRINITY_SPAN_ID": self.span_id,
            "TRINITY_PARENT_SPAN_ID": self.parent_span_id or "",
            "TRINITY_TRACE_SAMPLED": "1" if self.sampled else "0",
            "TRACEPARENT": self.to_traceparent(),
        }

    def duration_ms(self) -> float:
        """Get span duration in milliseconds."""
        return (time.time() - self.start_time) * 1000


# Import uuid if not already imported
import uuid


# =============================================================================
# v100.0: TRINITY CIRCUIT BREAKER
# =============================================================================

class TrinityCircuitBreakerState(Enum):
    """Circuit breaker states."""
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"


class TrinityCircuitBreaker:
    """
    Circuit breaker for Trinity component launch with PERSISTENT STATE.
    Prevents cascade failures by stopping launch attempts after repeated failures.

    v100.1: Added state persistence across restarts to prevent infinite retry loops.
    State is saved to ~/.jarvis/state/circuit_breakers/ and loaded on init.
    """

    def __init__(self, name: str, config: TrinityLaunchConfig):
        self.name = name
        self.config = config
        self.half_open_calls = 0
        self._logger = logging.getLogger(f"TrinityCircuitBreaker.{name}")

        # v100.1: Persistent state file
        self._state_dir = Path.home() / ".jarvis" / "state" / "circuit_breakers"
        self._state_file = self._state_dir / f"{name}.json"

        # Load persisted state or initialize fresh
        loaded_state = self._load_state()
        self.state = loaded_state.get("state", TrinityCircuitBreakerState.CLOSED)
        if isinstance(self.state, str):
            self.state = TrinityCircuitBreakerState(self.state)
        self.failure_count = loaded_state.get("failure_count", 0)
        self.success_count = loaded_state.get("success_count", 0)
        self.last_failure_time = loaded_state.get("last_failure_time")
        self.last_state_change = loaded_state.get("last_state_change", time.time())
        self.total_failures = loaded_state.get("total_failures", 0)
        self.total_successes = loaded_state.get("total_successes", 0)

        # Check if OPEN state has timed out
        if self.state == TrinityCircuitBreakerState.OPEN and self.last_failure_time:
            elapsed = time.time() - self.last_failure_time
            if elapsed > self.config.circuit_breaker_timeout_sec:
                self._transition_to(TrinityCircuitBreakerState.HALF_OPEN)
                self._logger.info(f"[{name}] OPEN â†’ HALF_OPEN (timeout elapsed during restart)")

    def _load_state(self) -> Dict[str, Any]:
        """Load circuit breaker state from disk."""
        if not self._state_file.exists():
            return {}
        try:
            with open(self._state_file) as f:
                return json.load(f)
        except Exception as e:
            self._logger.warning(f"Failed to load circuit breaker state: {e}")
            return {}

    def _save_state(self) -> None:
        """Persist circuit breaker state to disk (v100.1)."""
        try:
            self._state_dir.mkdir(parents=True, exist_ok=True)
            state_data = {
                "state": self.state.value if isinstance(self.state, TrinityCircuitBreakerState) else self.state,
                "failure_count": self.failure_count,
                "success_count": self.success_count,
                "last_failure_time": self.last_failure_time,
                "last_state_change": self.last_state_change,
                "total_failures": self.total_failures,
                "total_successes": self.total_successes,
                "updated_at": time.time(),
            }
            with open(self._state_file, "w") as f:
                json.dump(state_data, f, indent=2)
        except Exception as e:
            self._logger.warning(f"Failed to save circuit breaker state: {e}")

    def can_execute(self) -> bool:
        """Check if execution is allowed."""
        if self.state == TrinityCircuitBreakerState.CLOSED:
            return True

        if self.state == TrinityCircuitBreakerState.OPEN:
            # Check if timeout has elapsed
            if self.last_failure_time and (time.time() - self.last_failure_time) > self.config.circuit_breaker_timeout_sec:
                self._transition_to(TrinityCircuitBreakerState.HALF_OPEN)
                return True
            return False

        if self.state == TrinityCircuitBreakerState.HALF_OPEN:
            return self.half_open_calls < self.config.circuit_breaker_half_open_max_calls

        return False

    def record_success(self) -> None:
        """Record a successful execution."""
        self.success_count += 1
        self.total_successes += 1
        self.failure_count = max(0, self.failure_count - 1)

        if self.state == TrinityCircuitBreakerState.HALF_OPEN:
            self._transition_to(TrinityCircuitBreakerState.CLOSED)
        else:
            self._save_state()  # v100.1: Persist state

    def record_failure(self) -> None:
        """Record a failed execution."""
        self.failure_count += 1
        self.total_failures += 1
        self.last_failure_time = time.time()

        if self.state == TrinityCircuitBreakerState.HALF_OPEN:
            self._transition_to(TrinityCircuitBreakerState.OPEN)
        elif self.failure_count >= self.config.circuit_breaker_failure_threshold:
            self._transition_to(TrinityCircuitBreakerState.OPEN)
        else:
            self._save_state()  # v100.1: Persist state

    def _transition_to(self, new_state: TrinityCircuitBreakerState) -> None:
        """Transition to a new state."""
        old_state = self.state
        self.state = new_state
        self.last_state_change = time.time()

        if new_state == TrinityCircuitBreakerState.HALF_OPEN:
            self.half_open_calls = 0
        elif new_state == TrinityCircuitBreakerState.CLOSED:
            self.failure_count = 0  # Reset on recovery

        self._save_state()  # v100.1: Persist on every state transition
        self._logger.info(f"[{self.name}] {old_state.value} â†’ {new_state.value}")

    def get_status(self) -> Dict[str, Any]:
        """Get circuit breaker status."""
        return {
            "name": self.name,
            "state": self.state.value,
            "failure_count": self.failure_count,
            "success_count": self.success_count,
            "last_state_change": self.last_state_change,
        }


# =============================================================================
# v100.0: RETRY WITH EXPONENTIAL BACKOFF
# =============================================================================

class RetryWithBackoff:
    """
    Retry logic with exponential backoff and jitter.

    Features:
    - Exponential backoff (delay doubles each retry)
    - Jitter (random variation to prevent thundering herd)
    - Max delay cap
    - Configurable retry conditions
    """

    def __init__(self, config: TrinityLaunchConfig):
        self.config = config
        self._logger = logging.getLogger("TrinityRetry")

    def calculate_delay(self, attempt: int) -> float:
        """Calculate delay for a given attempt number."""
        base_delay = self.config.retry_base_delay_sec
        max_delay = self.config.retry_max_delay_sec
        exp_base = self.config.retry_exponential_base
        jitter_factor = self.config.retry_jitter_factor

        # Exponential backoff
        delay = base_delay * (exp_base ** attempt)

        # Cap at max delay
        delay = min(delay, max_delay)

        # Add jitter (Â±jitter_factor)
        import random
        jitter = delay * jitter_factor * (2 * random.random() - 1)
        delay += jitter

        return max(0, delay)

    async def execute(
        self,
        operation: Callable[[], Any],
        operation_name: str = "operation",
        max_retries: Optional[int] = None,
        should_retry: Optional[Callable[[Exception], bool]] = None,
    ) -> Tuple[bool, Any, List[Exception]]:
        """
        Execute an operation with retry logic.

        Returns:
            Tuple of (success, result, exceptions)
        """
        max_attempts = (max_retries or self.config.max_retries) + 1
        exceptions: List[Exception] = []

        for attempt in range(max_attempts):
            try:
                # v100.1: Fix for lambdas that return coroutines
                # asyncio.iscoroutinefunction returns False for lambdas even when
                # they return coroutines, so we must also check the result
                if asyncio.iscoroutinefunction(operation):
                    result = await operation()
                else:
                    result = operation()
                    # If the result is a coroutine (e.g., from a lambda), await it
                    if asyncio.iscoroutine(result):
                        result = await result

                if attempt > 0:
                    self._logger.info(f"[{operation_name}] Succeeded on attempt {attempt + 1}")

                return True, result, exceptions

            except Exception as e:
                exceptions.append(e)
                self._logger.warning(f"[{operation_name}] Attempt {attempt + 1} failed: {e}")

                # Check if we should retry
                if should_retry and not should_retry(e):
                    self._logger.info(f"[{operation_name}] Not retrying due to exception type")
                    break

                # Check if we have retries left
                if attempt < max_attempts - 1:
                    delay = self.calculate_delay(attempt)
                    self._logger.info(f"[{operation_name}] Retrying in {delay:.2f}s...")
                    await asyncio.sleep(delay)

        return False, None, exceptions


# =============================================================================
# v95.0: SIMPLE ASYNC RETRY UTILITY
# Standalone retry function for critical operations (HTTP, subprocess, etc.)
# =============================================================================

async def async_retry(
    operation: Callable[[], Any],
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 30.0,
    exponential_base: float = 2.0,
    operation_name: str = "operation",
    retryable_exceptions: Optional[Tuple[type, ...]] = None,
    logger: Optional[logging.Logger] = None,
) -> Any:
    """
    v95.0: Simple async retry utility for critical operations.

    Unlike RetryWithBackoff (tied to TrinityLaunchConfig), this is a standalone
    function that can be used anywhere for HTTP requests, subprocess ops, etc.

    Args:
        operation: Async callable to execute (can be lambda returning coroutine)
        max_retries: Maximum number of retry attempts (default: 3)
        base_delay: Initial delay in seconds (default: 1.0)
        max_delay: Maximum delay cap (default: 30.0)
        exponential_base: Multiplier for exponential backoff (default: 2.0)
        operation_name: Name for logging (default: "operation")
        retryable_exceptions: Tuple of exception types to retry on (default: all)
        logger: Optional logger instance

    Returns:
        The result of the operation

    Raises:
        The last exception if all retries fail

    Example:
        result = await async_retry(
            lambda: session.get(url),
            max_retries=3,
            operation_name="health_check",
            retryable_exceptions=(aiohttp.ClientError, asyncio.TimeoutError),
        )
    """
    _logger = logger or logging.getLogger("AsyncRetry")
    last_exception: Optional[Exception] = None
    max_attempts = max_retries + 1

    for attempt in range(max_attempts):
        try:
            # Handle both async functions and lambdas returning coroutines
            if asyncio.iscoroutinefunction(operation):
                result = await operation()
            else:
                result = operation()
                if asyncio.iscoroutine(result):
                    result = await result

            if attempt > 0:
                _logger.info(f"[{operation_name}] Succeeded on attempt {attempt + 1}")

            return result

        except Exception as e:
            last_exception = e

            # Check if this exception type should be retried
            if retryable_exceptions and not isinstance(e, retryable_exceptions):
                _logger.debug(f"[{operation_name}] Non-retryable exception: {type(e).__name__}")
                raise

            _logger.warning(f"[{operation_name}] Attempt {attempt + 1}/{max_attempts} failed: {e}")

            # Check if we have retries left
            if attempt < max_attempts - 1:
                # Calculate delay with exponential backoff and jitter
                import random
                delay = min(base_delay * (exponential_base ** attempt), max_delay)
                jitter = delay * 0.1 * (2 * random.random() - 1)  # Â±10% jitter
                delay = max(0, delay + jitter)

                _logger.debug(f"[{operation_name}] Retrying in {delay:.2f}s...")
                await asyncio.sleep(delay)

    # All retries exhausted
    _logger.error(f"[{operation_name}] All {max_attempts} attempts failed")
    if last_exception:
        raise last_exception
    raise RuntimeError(f"{operation_name} failed after {max_attempts} attempts")


# Global Trinity config (lazy initialization)
_trinity_config: Optional[TrinityLaunchConfig] = None


def get_trinity_config() -> TrinityLaunchConfig:
    """Get the global Trinity configuration."""
    global _trinity_config
    if _trinity_config is None:
        _trinity_config = TrinityLaunchConfig()
    return _trinity_config


class StartupPhase(Enum):
    """Phases of supervisor startup."""
    INIT = "init"
    CLEANUP = "cleanup"
    VALIDATION = "validation"
    SUPERVISOR_INIT = "supervisor_init"
    JARVIS_START = "jarvis_start"
    COMPLETE = "complete"
    FAILED = "failed"


# =============================================================================
# Logging Setup - Advanced with Performance Tracking
# =============================================================================

class PerformanceLogger:
    """Track and log performance metrics during startup."""
    
    def __init__(self):
        self._timings: Dict[str, float] = {}
        self._start_times: Dict[str, float] = {}
        self._start_time = time.perf_counter()
    
    def start(self, operation: str) -> None:
        """Start timing an operation."""
        self._start_times[operation] = time.perf_counter()
    
    def end(self, operation: str) -> float:
        """End timing an operation and return duration."""
        if operation in self._start_times:
            duration = time.perf_counter() - self._start_times[operation]
            self._timings[operation] = duration
            del self._start_times[operation]
            return duration
        return 0.0
    
    def get_total_time(self) -> float:
        """Get total time since logger creation."""
        return time.perf_counter() - self._start_time
    
    def get_summary(self) -> Dict[str, float]:
        """Get all recorded timings."""
        return {
            **self._timings,
            "total": self.get_total_time()
        }


def setup_logging(config: BootstrapConfig) -> logging.Logger:
    """
    Configure advanced structured logging for the supervisor (v10.6).

    v10.6 Features:
    - JSON formatted logs for easy parsing and analysis
    - Async file writing (non-blocking)
    - Automatic log rotation (prevents huge files)
    - Context enrichment (session IDs, tracing, stack traces)
    - Intelligent error aggregation and pattern detection
    - Performance metrics tracking
    - Real-time error analysis

    Logs are written to:
    - Console: Structured JSON to stdout
    - File: ~/.jarvis/logs/supervisor.bootstrap.jsonl (all logs)
    - Errors: ~/.jarvis/logs/supervisor.bootstrap_errors.jsonl (errors only)

    Features:
    - Configurable log level via environment
    - Reduced noise from libraries
    - Performance-friendly async format
    """
    if STRUCTURED_LOGGING_AVAILABLE:
        # Configure structured logging system
        logging_config = LoggingConfig.from_env()
        logging_config.default_level = config.log_level.upper()
        logging_config.console_level = config.log_level.upper()

        configure_structured_logging(logging_config)

        # Get structured logger
        structured_logger = get_structured_logger("supervisor.bootstrap")

        # Reduce noise from libraries (still using basic logging for third-party libs)
        noisy_loggers = [
            "urllib3", "asyncio", "aiohttp", "httpx",
            "httpcore", "charset_normalizer", "google", "grpc",
            # v93.12: Suppress SpeechBrain verbose debug logging
            "speechbrain",
            "speechbrain.utils.checkpoints",
            "speechbrain.utils.quirks",
            "speechbrain.utils.torch_audio_backend",
            "speechbrain.lobes.models.huggingface_transformers.huggingface",
        ]
        for logger_name in noisy_loggers:
            logging.getLogger(logger_name).setLevel(logging.WARNING)

        # Log that structured logging is active
        structured_logger.info(
            "Structured logging system initialized",
            log_dir=str(logging_config.log_dir),
            max_file_size_mb=logging_config.max_bytes / (1024 * 1024),
            backup_count=logging_config.backup_count,
            error_aggregation=logging_config.enable_error_aggregation,
            performance_tracking=logging_config.enable_performance_tracking,
        )

        return structured_logger
    else:
        # Fallback to basic logging
        logging.basicConfig(
            level=getattr(logging, config.log_level.upper()),
            format=config.log_format,
            datefmt="%Y-%m-%d %H:%M:%S",
        )

        # Reduce noise from libraries
        noisy_loggers = [
            "urllib3", "asyncio", "aiohttp", "httpx",
            "httpcore", "charset_normalizer",
            # v93.12: Suppress SpeechBrain verbose debug logging
            "speechbrain",
            "speechbrain.utils.checkpoints",
            "speechbrain.utils.quirks",
            "speechbrain.utils.torch_audio_backend",
            "speechbrain.lobes.models.huggingface_transformers.huggingface",
        ]
        for logger_name in noisy_loggers:
            logging.getLogger(logger_name).setLevel(logging.WARNING)

        return logging.getLogger("supervisor.bootstrap")


# =============================================================================
# Voice Narration - Unified Orchestrator Integration (v2.0)
# =============================================================================

class AsyncVoiceNarrator:
    """
    Async voice narrator that delegates to UnifiedVoiceOrchestrator.

    v2.0 CHANGE: Instead of spawning direct `say` processes, this now
    delegates to the unified orchestrator to ensure only ONE voice
    speaks at a time across the entire JARVIS system.

    Features:
    - Non-blocking voice output
    - Queue management via unified orchestrator
    - Platform-aware (macOS only)
    - Graceful fallback on errors
    - PREVENTS multiple voices during startup
    """

    def __init__(self, config: BootstrapConfig):
        self.config = config
        self.enabled = config.voice_enabled and platform.system() == "Darwin"
        self._orchestrator = None
        self._started = False

    async def _ensure_orchestrator(self):
        """Ensure the unified orchestrator is initialized and started."""
        if self._orchestrator is None:
            try:
                # CRITICAL: Use the SAME import path as startup_narrator.py
                # to ensure we get the SAME singleton instance.
                # 
                # startup_narrator.py uses: from .unified_voice_orchestrator import ...
                # which resolves to: core.supervisor.unified_voice_orchestrator
                # 
                # If we use "backend.core.supervisor..." here, Python treats it as
                # a DIFFERENT module, creating a SEPARATE singleton!
                from core.supervisor.unified_voice_orchestrator import (
                    get_voice_orchestrator,
                    VoicePriority,
                    VoiceSource,
                )
                self._orchestrator = get_voice_orchestrator()
                self._VoicePriority = VoicePriority
                self._VoiceSource = VoiceSource
            except ImportError as e:
                # Fallback if orchestrator not available
                logging.getLogger(__name__).debug(f"Voice orchestrator not available: {e}")
                self.enabled = False
                return

        if not self._started and self._orchestrator:
            await self._orchestrator.start()
            self._started = True

    async def speak(self, text: str, wait: bool = True, priority: bool = False) -> None:
        """
        Speak text through unified orchestrator.

        Args:
            text: Text to speak
            wait: Whether to wait for speech to complete
            priority: If True, use CRITICAL priority (interrupts current)
        """
        if not self.enabled:
            return

        try:
            await self._ensure_orchestrator()

            if self._orchestrator is None:
                return

            # Map priority to VoicePriority
            voice_priority = (
                self._VoicePriority.CRITICAL if priority
                else self._VoicePriority.MEDIUM
            )

            await self._orchestrator.speak(
                text=text,
                priority=voice_priority,
                source=self._VoiceSource.SYSTEM,
                wait=wait,
            )
        except Exception as e:
            logging.getLogger(__name__).debug(f"Voice error: {e}")


# =============================================================================
# Parallel Process Cleaner - Advanced Termination
# =============================================================================

@dataclass
class ProcessInfo:
    """Information about a discovered process."""
    pid: int
    cmdline: str
    age_seconds: float
    memory_mb: float = 0.0
    source: str = "scan"  # "pid_file" or "scan"


class ParallelProcessCleaner:
    """
    Intelligent parallel process cleaner with cascade termination.
    
    Features:
    - Parallel process discovery using ThreadPoolExecutor
    - Async termination with SIGINT â†’ SIGTERM â†’ SIGKILL cascade
    - Semaphore-controlled parallelism
    - Detailed progress reporting
    - PID file cleanup
    """
    
    def __init__(self, config: BootstrapConfig, logger: logging.Logger, narrator: AsyncVoiceNarrator):
        self.config = config
        self.logger = logger
        self.narrator = narrator
        self._my_pid = os.getpid()
        self._my_parent = os.getppid()
    
    async def discover_and_cleanup(self) -> Tuple[int, List[ProcessInfo]]:
        """
        Discover and cleanup existing JARVIS instances.

        v97.0: Now includes lock holder cleanup as Phase 0 to prevent
        30-second ownership acquisition timeouts.

        Returns:
            Tuple of (terminated_count, discovered_processes)
        """
        perf = PerformanceLogger()

        # v97.0: Phase 0 - Clean up any processes holding ownership locks
        # This MUST run before discovery to prevent 30-second timeouts
        perf.start("lock_cleanup")
        lock_holders_killed = await self.cleanup_stale_lock_holders()
        perf.end("lock_cleanup")
        if lock_holders_killed > 0:
            self.logger.info(f"[v97.0] Killed {lock_holders_killed} stale lock holder(s)")
            await asyncio.sleep(0.5)  # Allow OS to release resources

        # Phase 1: Parallel discovery
        perf.start("discovery")
        discovered = await self._parallel_discover()
        perf.end("discovery")
        
        if not discovered:
            return 0, []

        # v117.0: Filter out PRESERVED services from GlobalProcessRegistry
        # These services were preserved during os.execv() restart and should NOT be killed
        try:
            from backend.core.supervisor_singleton import GlobalProcessRegistry
            preserved_pids = set(GlobalProcessRegistry.get_all().keys())
            if preserved_pids:
                original_count = len(discovered)
                # Filter out preserved services
                preserved_removed = []
                for pid in list(discovered.keys()):
                    if pid in preserved_pids:
                        info = discovered.pop(pid)
                        preserved_removed.append((pid, info.command[:50]))
                if preserved_removed:
                    self.logger.info(
                        f"[v117.0] Preserved {len(preserved_removed)} service(s) from cleanup: "
                        f"{[(p, c) for p, c in preserved_removed]}"
                    )
                    # Reduced message for narrator
                    if not discovered:
                        self.logger.info("[v117.0] All discovered processes are preserved - skipping cleanup")
                        return 0, list(discovered.values())
        except ImportError:
            pass
        except Exception as e:
            self.logger.debug(f"[v117.0] GlobalProcessRegistry check failed: {e}")

        if not discovered:
            return 0, []

        # Announce cleanup
        await self.narrator.speak("Cleaning up previous session.", wait=False)

        # Phase 2: Parallel termination with semaphore
        perf.start("termination")
        terminated = await self._parallel_terminate(discovered)
        perf.end("termination")
        
        # Phase 3: PID file cleanup
        await self._cleanup_pid_files()
        
        self.logger.debug(f"Cleanup performance: {perf.get_summary()}")
        
        # Send log to loading page if available
        try:
            from loading_server import get_progress_reporter
            reporter = get_progress_reporter()
            if terminated > 0:
                await reporter.log(
                    "Supervisor",
                    f"Cleaned up {terminated} existing instance(s)",
                    "success"
                )
        except Exception:
            pass
        
        return terminated, list(discovered.values())
    
    async def _parallel_discover(self) -> Dict[int, ProcessInfo]:
        """Discover processes in parallel using ThreadPoolExecutor."""
        discovered: Dict[int, ProcessInfo] = {}
        
        # Run in thread pool for psutil operations (they can block)
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=4) as executor:
            # Task 1: Check PID files
            pid_file_task = loop.run_in_executor(
                executor, self._discover_from_pid_files
            )
            
            # Task 2: Scan process list
            process_scan_task = loop.run_in_executor(
                executor, self._discover_from_process_list
            )

            # Task 3: Scan ports (new)
            port_scan_task = loop.run_in_executor(
                executor, self._discover_from_ports
            )
            
            # Wait for all
            pid_file_procs, scanned_procs, port_procs = await asyncio.gather(
                pid_file_task, process_scan_task, port_scan_task
            )
        
        # Merge results (PID files take precedence, then ports, then scan)
        discovered.update(scanned_procs)
        discovered.update(port_procs)
        discovered.update(pid_file_procs)
        
        return discovered
    
    def _discover_from_pid_files(self) -> Dict[int, ProcessInfo]:
        """
        Discover processes from PID files (runs in thread).

        v2.0 FIX: Uses explicit file handling to avoid "Bad file descriptor" errors.
        The pathlib.read_text() method can fail with errno 9 when:
        - File descriptors are exhausted or recycled
        - Race conditions with file operations
        - System file descriptor limits are stressed
        """
        try:
            import psutil
        except ImportError:
            return {}

        discovered = {}

        for pid_file in self.config.pid_files:
            # v2.0: Check existence more carefully
            try:
                if not pid_file.exists():
                    continue
            except OSError:
                # Can't even check existence - skip this file
                continue

            pid = None
            try:
                # v2.0: Use explicit file open with proper context management
                # This avoids the "Bad file descriptor" issue with read_text()
                try:
                    with open(str(pid_file), 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                    pid = int(content) if content else None
                except (OSError, IOError) as e:
                    # Handle bad file descriptor, permission errors, etc.
                    import errno
                    if e.errno in (errno.EBADF, errno.ENOENT, errno.EACCES, errno.EIO):
                        # Bad fd, file gone, no access, or I/O error - skip
                        continue
                    # Re-raise unexpected OSError
                    raise

                if pid is None:
                    continue

                if not psutil.pid_exists(pid) or pid in (self._my_pid, self._my_parent):
                    self._safe_unlink(pid_file)
                    continue

                proc = psutil.Process(pid)
                cmdline = " ".join(proc.cmdline()).lower()

                if any(p in cmdline for p in self.config.jarvis_patterns):
                    discovered[pid] = ProcessInfo(
                        pid=pid,
                        cmdline=cmdline[:100],
                        age_seconds=time.time() - proc.create_time(),
                        memory_mb=proc.memory_info().rss / (1024 * 1024),
                        source="pid_file"
                    )
            except (ValueError, psutil.NoSuchProcess, psutil.AccessDenied, ProcessLookupError):
                self._safe_unlink(pid_file)
            except Exception as e:
                # Log unexpected errors but don't crash
                self.logger.debug(f"[ProcessCleaner] Error reading PID file {pid_file}: {e}")
                continue

        return discovered

    def _safe_unlink(self, path: Path) -> bool:
        """Safely unlink a file with proper error handling."""
        try:
            path.unlink(missing_ok=True)
            return True
        except (OSError, IOError, PermissionError):
            return False
    
    def _discover_from_process_list(self) -> Dict[int, ProcessInfo]:
        """Scan process list for JARVIS processes (runs in thread)."""
        try:
            import psutil
        except ImportError:
            return {}
        
        discovered = {}
        
        for proc in psutil.process_iter(['pid', 'cmdline', 'create_time', 'memory_info']):
            try:
                pid = proc.info['pid']
                if pid in (self._my_pid, self._my_parent):
                    continue
                
                cmdline = " ".join(proc.info.get('cmdline') or []).lower()
                if any(p in cmdline for p in self.config.jarvis_patterns):
                    mem_info = proc.info.get('memory_info')
                    discovered[pid] = ProcessInfo(
                        pid=pid,
                        cmdline=cmdline[:100],
                        age_seconds=time.time() - proc.info['create_time'],
                        memory_mb=mem_info.rss / (1024 * 1024) if mem_info else 0,
                        source="scan"
                    )
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass
        
        return discovered

    def _discover_from_ports(self) -> Dict[int, ProcessInfo]:
        """Discover processes holding critical ports."""
        try:
            import psutil
        except ImportError:
            return {}

        discovered = {}
        critical_ports = self.config.required_ports

        try:
            for conn in psutil.net_connections(kind='inet'):
                if conn.laddr.port in critical_ports:
                    try:
                        pid = conn.pid
                        if not pid or pid in (self._my_pid, self._my_parent):
                            continue
                            
                        # Don't rediscover if we already found it, but verify it exists
                        if pid in discovered:
                            continue

                        proc = psutil.Process(pid)
                        cmdline = " ".join(proc.cmdline()).lower()
                        mem_info = proc.memory_info()
                        
                        discovered[pid] = ProcessInfo(
                            pid=pid,
                            cmdline=cmdline[:100],
                            age_seconds=time.time() - proc.create_time(),
                            memory_mb=mem_info.rss / (1024 * 1024),
                            source=f"port_{conn.laddr.port}"
                        )
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        pass
        except (psutil.AccessDenied, PermissionError):
            # macOS might require root for net_connections on other users' procs
            pass
            
        return discovered
    
    async def _parallel_terminate(self, processes: Dict[int, ProcessInfo]) -> int:
        """Terminate processes in parallel with semaphore control."""
        import psutil
        
        # Limit parallelism
        semaphore = asyncio.Semaphore(self.config.max_parallel_cleanups)
        
        async def terminate_one(pid: int, info: ProcessInfo) -> bool:
            async with semaphore:
                return await self._terminate_process(pid, info)
        
        # Create termination tasks
        tasks = [
            asyncio.create_task(terminate_one(pid, info))
            for pid, info in processes.items()
        ]
        
        # Wait for all with gather
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # v95.0: Count successes and log exceptions for observability
        terminated = 0
        for i, result in enumerate(results):
            if result is True:
                terminated += 1
            elif isinstance(result, Exception):
                # Log exceptions from individual termination tasks
                self.logger.debug(f"Process termination exception: {result}")

        return terminated
    
    async def _terminate_process(self, pid: int, info: ProcessInfo) -> bool:
        """
        v79.0: Terminate a single process with cascade strategy and race-safe PID validation.

        RACE CONDITION FIX: Previous implementation had race between checking process
        existence and sending signals. Now validates PID before each signal.

        Strategy: SIGINT â†’ wait â†’ SIGTERM â†’ wait â†’ SIGKILL
        """
        try:
            import psutil

            # v79.0: Helper to safely send signal with PID validation
            def _safe_kill(target_pid: int, sig: int) -> bool:
                """Send signal with race-safe PID validation."""
                try:
                    # Re-validate process exists before each signal
                    proc = psutil.Process(target_pid)

                    # Extra validation: check process matches expected info
                    if info and info.cmdline:
                        try:
                            current_cmdline = " ".join(proc.cmdline())
                            # If cmdline completely changed, it's a different process
                            if info.cmdline and info.cmdline[0] not in current_cmdline:
                                self.logger.debug(
                                    f"PID {target_pid} recycled: expected '{info.cmdline[0]}', "
                                    f"got '{current_cmdline[:50]}'"
                                )
                                return True  # Don't kill, wrong process
                        except (psutil.NoSuchProcess, psutil.AccessDenied):
                            pass

                    os.kill(target_pid, sig)
                    return True
                except (psutil.NoSuchProcess, ProcessLookupError, OSError) as e:
                    if isinstance(e, OSError) and e.errno == 3:  # ESRCH: No such process
                        return True  # Process already gone
                    return True  # Consider success if process doesn't exist

            # Phase 1: SIGINT (graceful)
            if _safe_kill(pid, signal.SIGINT):
                try:
                    proc = psutil.Process(pid)
                    await asyncio.sleep(0.1)  # Brief pause
                    proc.wait(timeout=self.config.cleanup_timeout_sigint)
                    return True
                except (psutil.NoSuchProcess, psutil.TimeoutExpired):
                    pass

            # Phase 2: SIGTERM
            if _safe_kill(pid, signal.SIGTERM):
                try:
                    proc = psutil.Process(pid)
                    proc.wait(timeout=self.config.cleanup_timeout_sigterm)
                    return True
                except (psutil.NoSuchProcess, psutil.TimeoutExpired):
                    pass

            # Phase 3: SIGKILL (force)
            if _safe_kill(pid, signal.SIGKILL):
                try:
                    proc = psutil.Process(pid)
                    proc.wait(timeout=self.config.cleanup_timeout_sigkill)
                except (psutil.NoSuchProcess, psutil.TimeoutExpired):
                    pass
            return True

        except (psutil.NoSuchProcess, ProcessLookupError):
            return True  # Already gone
        except Exception as e:
            self.logger.debug(f"Failed to terminate {pid}: {e}")
            return False
    
    async def _cleanup_pid_files(self) -> None:
        """Clean up stale PID files."""
        for pid_file in self.config.pid_files:
            try:
                pid_file.unlink(missing_ok=True)
            except Exception:
                pass

    async def cleanup_stale_lock_holders(self) -> int:
        """
        v97.0: Clean up processes holding fcntl locks on ownership files.

        This is a critical safeguard to prevent 30-second ownership timeouts.
        Uses lsof to detect which process holds the lock file, then kills
        it if it's an orphaned supervisor from a previous session.

        Returns:
            Number of lock holders killed
        """
        import subprocess

        lock_file = Path.home() / ".jarvis" / "state" / "locks" / "jarvis.lock"
        killed_count = 0

        if not lock_file.exists():
            return 0

        try:
            # Use lsof to find what process has the lock file open
            result = subprocess.run(
                ["lsof", str(lock_file)],
                capture_output=True,
                text=True,
                timeout=5
            )

            if result.returncode != 0:
                # No process has the file open
                return 0

            # Parse lsof output to find PIDs
            # Format: COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME
            for line in result.stdout.strip().split('\n')[1:]:  # Skip header
                parts = line.split()
                if len(parts) < 2:
                    continue

                try:
                    holder_pid = int(parts[1])
                except ValueError:
                    continue

                # Skip if it's us or our parent
                if holder_pid in (self._my_pid, self._my_parent):
                    continue

                # Check if this is a supervisor process
                try:
                    import psutil
                    proc = psutil.Process(holder_pid)
                    cmdline = " ".join(proc.cmdline())

                    # Only kill if it's a supervisor process
                    if "run_supervisor.py" in cmdline or "start_system.py" in cmdline:
                        self.logger.warning(
                            f"[v97.0] Found stale lock holder PID {holder_pid} "
                            f"({cmdline[:60]}...), killing..."
                        )

                        # Kill the process holding the lock
                        try:
                            os.kill(holder_pid, signal.SIGTERM)
                            await asyncio.sleep(0.5)

                            # Force kill if still alive
                            if psutil.pid_exists(holder_pid):
                                os.kill(holder_pid, signal.SIGKILL)
                                await asyncio.sleep(0.2)

                            killed_count += 1
                            self.logger.info(f"[v97.0] Killed stale lock holder PID {holder_pid}")

                        except (ProcessLookupError, OSError):
                            # Process already dead
                            killed_count += 1

                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass

            # Also clean up the lock file itself if we killed holders
            if killed_count > 0:
                await asyncio.sleep(0.3)  # Wait for OS to release lock
                try:
                    if lock_file.exists():
                        lock_file.unlink()
                        self.logger.debug(f"[v97.0] Removed stale lock file: {lock_file}")
                except Exception as e:
                    self.logger.debug(f"[v97.0] Could not remove lock file: {e}")

        except subprocess.TimeoutExpired:
            self.logger.debug("[v97.0] lsof timed out")
        except FileNotFoundError:
            # lsof not available
            self.logger.debug("[v97.0] lsof not available")
        except Exception as e:
            self.logger.debug(f"[v97.0] Lock holder cleanup error: {e}")

        return killed_count


# =============================================================================
# v109.7: COMPREHENSIVE ZOMBIE CLEANUP SYSTEM
# =============================================================================
# Ultra-robust, async, parallel cleanup that integrates:
# - Cross-repo coordination (JARVIS, J-Prime, Reactor-Core)
# - Zombie detection (orphaned, stuck, unresponsive processes)
# - Memory-aware cleanup (IntelligentMemoryController integration)
# - Port-based Trinity service cleanup
# - Registry-based orphan detection
# =============================================================================

@dataclass
class ZombieProcessInfo:
    """Extended process info with zombie detection metadata."""
    pid: int
    cmdline: str
    age_seconds: float
    memory_mb: float = 0.0
    cpu_percent: float = 0.0
    status: str = "unknown"
    is_orphaned: bool = False
    is_zombie_like: bool = False
    stale_connection_count: int = 0
    repo_origin: str = "unknown"  # jarvis, jarvis-prime, reactor-core
    detection_source: str = "scan"  # scan, port, registry, pid_file


class ComprehensiveZombieCleanup:
    """
    v109.7: Comprehensive Zombie Cleanup System for JARVIS Ecosystem.

    This system provides ultra-robust cleanup across all three repos:
    - JARVIS (main AI agent) - port 8010
    - JARVIS-Prime (J-Prime Mind) - port 8000
    - Reactor-Core (Nerves) - port 8090

    Features:
    - Async parallel discovery across multiple detection sources
    - Zombie detection via responsiveness heuristics (orphaned, stuck, stale connections)
    - Cross-repo registry integration for coordinated cleanup
    - Memory-aware cleanup with IntelligentMemoryController
    - Port-based Trinity service detection
    - Graceful termination with cascade (SIGINT â†’ SIGTERM â†’ SIGKILL)
    - Circuit breaker pattern to prevent cleanup storms
    - File descriptor safe operations

    This runs BEFORE Trinity launch to ensure a clean environment.
    """

    # Trinity ports by service
    TRINITY_PORTS = {
        "jarvis-body": [8010],  # Main JARVIS backend
        "jarvis-prime": [8000],  # J-Prime orchestrator
        "reactor-core": [8090],  # Reactor-Core API
    }

    # Process patterns by repo
    REPO_PATTERNS = {
        "jarvis": ["run_supervisor.py", "start_system.py", "jarvis", "uvicorn.*8010"],
        "jarvis-prime": ["trinity_orchestrator.*jarvis-prime", "jarvis.prime", "uvicorn.*8000"],
        "reactor-core": ["trinity_orchestrator.*reactor-core", "reactor.core", "uvicorn.*8090"],
    }

    def __init__(
        self,
        config: "BootstrapConfig",
        logger: logging.Logger,
        enable_cross_repo: bool = True,
        enable_memory_aware: bool = True,
        enable_circuit_breaker: bool = True,
    ):
        self.config = config
        self.logger = logger
        self._my_pid = os.getpid()
        self._my_parent = os.getppid()
        self._enable_cross_repo = enable_cross_repo
        self._enable_memory_aware = enable_memory_aware
        self._enable_circuit_breaker = enable_circuit_breaker

        # Circuit breaker state
        self._cleanup_attempts = 0
        self._cleanup_failures = 0
        self._circuit_open = False
        self._circuit_open_until = 0.0
        self._max_failures_before_open = 3
        self._circuit_cooldown = 30.0

        # Stats
        self._stats = {
            "zombies_detected": 0,
            "zombies_killed": 0,
            "ports_freed": 0,
            "orphans_cleaned": 0,
            "cross_repo_cleaned": 0,
        }

    async def run_comprehensive_cleanup(self) -> Dict[str, Any]:
        """
        v109.7: Run comprehensive zombie cleanup before Trinity launch.

        This is the main entry point that coordinates all cleanup phases:
        1. Circuit breaker check
        2. Cross-repo orphan cleanup (registry-based)
        3. Zombie process detection (multi-source)
        4. Parallel termination
        5. Port verification
        6. Memory cleanup (if enabled)

        Returns:
            Dict with cleanup results and statistics
        """
        results = {
            "success": True,
            "phases_completed": [],
            "zombies_found": 0,
            "zombies_killed": 0,
            "ports_freed": [],
            "errors": [],
            "duration_ms": 0,
        }

        start_time = time.time()

        try:
            # Phase 0: Circuit breaker check
            if self._enable_circuit_breaker and self._is_circuit_open():
                results["success"] = False
                results["errors"].append("Circuit breaker open - cleanup skipped")
                self.logger.warning("[v109.7] Zombie cleanup skipped - circuit breaker open")
                return results

            self._cleanup_attempts += 1
            self.logger.info("[v109.7] ğŸ§¹ Starting comprehensive zombie cleanup...")

            # Phase 1: Cross-repo orphan cleanup
            if self._enable_cross_repo:
                try:
                    orphans_cleaned = await self._cleanup_cross_repo_orphans()
                    self._stats["orphans_cleaned"] += orphans_cleaned
                    results["phases_completed"].append("cross_repo_orphans")
                    if orphans_cleaned > 0:
                        self.logger.info(f"[v109.7] Cleaned {orphans_cleaned} cross-repo orphans")
                except Exception as e:
                    self.logger.debug(f"[v109.7] Cross-repo cleanup error (non-fatal): {e}")

            # Phase 2: Parallel zombie discovery
            zombies = await self._parallel_zombie_discovery()
            results["zombies_found"] = len(zombies)
            self._stats["zombies_detected"] += len(zombies)
            results["phases_completed"].append("zombie_discovery")

            if zombies:
                self.logger.info(f"[v109.7] Found {len(zombies)} zombie process(es)")

                # Phase 3: Parallel termination
                killed = await self._parallel_zombie_termination(zombies)
                results["zombies_killed"] = killed
                self._stats["zombies_killed"] += killed
                results["phases_completed"].append("zombie_termination")

                # Phase 4: Port verification and cleanup
                await asyncio.sleep(0.3)  # Brief pause for port release
                ports_freed = await self._verify_and_free_ports()
                results["ports_freed"] = ports_freed
                self._stats["ports_freed"] += len(ports_freed)
                results["phases_completed"].append("port_verification")

            # Phase 5: Memory cleanup (if enabled and needed)
            if self._enable_memory_aware:
                try:
                    memory_result = await self._memory_aware_cleanup()
                    if memory_result.get("actions_taken"):
                        results["phases_completed"].append("memory_cleanup")
                except Exception as e:
                    self.logger.debug(f"[v109.7] Memory cleanup error (non-fatal): {e}")

            results["success"] = True
            self._cleanup_failures = 0  # Reset on success

        except Exception as e:
            results["success"] = False
            results["errors"].append(str(e))
            self._cleanup_failures += 1

            # Open circuit if too many failures
            if self._cleanup_failures >= self._max_failures_before_open:
                self._open_circuit()

            self.logger.error(f"[v109.7] Comprehensive cleanup failed: {e}")

        results["duration_ms"] = int((time.time() - start_time) * 1000)
        self.logger.info(
            f"[v109.7] âœ… Cleanup complete: "
            f"{results['zombies_killed']}/{results['zombies_found']} zombies killed, "
            f"{len(results['ports_freed'])} ports freed in {results['duration_ms']}ms"
        )

        return results

    async def _cleanup_cross_repo_orphans(self) -> int:
        """
        Phase 1: Clean up orphaned resources from the cross-repo registry.

        Uses the CrossRepoCleanupCoordinator to find resources registered
        by processes that are no longer running.
        """
        try:
            from backend.core.cross_repo_cleanup import (
                get_cleanup_coordinator,
                cleanup_orphaned,
            )

            # Get orphaned resources count
            coordinator = get_cleanup_coordinator()
            orphaned = coordinator.registry.get_orphaned_resources()

            if not orphaned:
                return 0

            self.logger.info(f"[v109.7] Found {len(orphaned)} orphaned cross-repo resources")

            # Clean them up
            cleaned = cleanup_orphaned()
            self._stats["cross_repo_cleaned"] += cleaned

            return cleaned

        except ImportError:
            self.logger.debug("[v109.7] cross_repo_cleanup not available")
            return 0
        except Exception as e:
            self.logger.debug(f"[v109.7] Cross-repo orphan cleanup error: {e}")
            return 0

    async def _parallel_zombie_discovery(self) -> Dict[int, ZombieProcessInfo]:
        """
        Phase 2: Parallel zombie discovery using multiple detection sources.

        Detection sources:
        1. Port scanning (Trinity ports)
        2. Process pattern matching (repo-specific patterns)
        3. Zombie heuristics (orphaned, stuck, stale connections)
        4. Cross-repo registry
        """
        discovered: Dict[int, ZombieProcessInfo] = {}

        try:
            import psutil
        except ImportError:
            return discovered

        loop = asyncio.get_event_loop()

        with ThreadPoolExecutor(max_workers=4) as executor:
            # Task 1: Trinity port scanning
            port_task = loop.run_in_executor(
                executor, self._discover_from_trinity_ports
            )

            # Task 2: Process pattern scanning
            pattern_task = loop.run_in_executor(
                executor, self._discover_from_patterns
            )

            # Task 3: Zombie heuristic detection
            zombie_task = loop.run_in_executor(
                executor, self._discover_zombies_by_heuristics
            )

            # Wait for all
            port_procs, pattern_procs, zombie_procs = await asyncio.gather(
                port_task, pattern_task, zombie_task,
                return_exceptions=True
            )

        # Merge results (zombie heuristics take precedence)
        if isinstance(pattern_procs, dict):
            discovered.update(pattern_procs)
        if isinstance(port_procs, dict):
            discovered.update(port_procs)
        if isinstance(zombie_procs, dict):
            discovered.update(zombie_procs)

        # Filter out ourselves and our parent
        discovered = {
            pid: info for pid, info in discovered.items()
            if pid not in (self._my_pid, self._my_parent)
        }

        return discovered

    def _discover_from_trinity_ports(self) -> Dict[int, ZombieProcessInfo]:
        """Discover processes holding Trinity ports."""
        try:
            import psutil
        except ImportError:
            return {}

        discovered = {}

        # Flatten all Trinity ports
        all_ports = []
        port_to_repo = {}
        for repo, ports in self.TRINITY_PORTS.items():
            for port in ports:
                all_ports.append(port)
                port_to_repo[port] = repo

        try:
            for conn in psutil.net_connections(kind='inet'):
                if conn.laddr.port in all_ports and conn.pid:
                    pid = conn.pid
                    if pid in (self._my_pid, self._my_parent):
                        continue
                    if pid in discovered:
                        continue

                    try:
                        proc = psutil.Process(pid)
                        cmdline = " ".join(proc.cmdline())
                        mem_info = proc.memory_info()

                        discovered[pid] = ZombieProcessInfo(
                            pid=pid,
                            cmdline=cmdline[:200],
                            age_seconds=time.time() - proc.create_time(),
                            memory_mb=mem_info.rss / (1024 * 1024),
                            cpu_percent=proc.cpu_percent(interval=0.05),
                            status=proc.status(),
                            repo_origin=port_to_repo.get(conn.laddr.port, "unknown"),
                            detection_source=f"port_{conn.laddr.port}",
                        )
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        pass
        except (psutil.AccessDenied, PermissionError):
            pass

        return discovered

    def _discover_from_patterns(self) -> Dict[int, ZombieProcessInfo]:
        """Discover processes matching repo patterns."""
        try:
            import psutil
        except ImportError:
            return {}

        discovered = {}

        for proc in psutil.process_iter(['pid', 'cmdline', 'create_time', 'memory_info', 'status']):
            try:
                pid = proc.info['pid']
                if pid in (self._my_pid, self._my_parent):
                    continue

                cmdline = " ".join(proc.info.get('cmdline') or [])
                if not cmdline:
                    continue

                cmdline_lower = cmdline.lower()

                # Check against repo patterns
                for repo, patterns in self.REPO_PATTERNS.items():
                    import re
                    for pattern in patterns:
                        if re.search(pattern, cmdline_lower):
                            mem_info = proc.info.get('memory_info')
                            discovered[pid] = ZombieProcessInfo(
                                pid=pid,
                                cmdline=cmdline[:200],
                                age_seconds=time.time() - proc.info['create_time'],
                                memory_mb=mem_info.rss / (1024 * 1024) if mem_info else 0,
                                status=proc.info.get('status', 'unknown'),
                                repo_origin=repo,
                                detection_source="pattern_scan",
                            )
                            break
                    if pid in discovered:
                        break

            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass

        return discovered

    def _discover_zombies_by_heuristics(self) -> Dict[int, ZombieProcessInfo]:
        """
        Discover zombie-like processes using heuristics.

        A process is zombie-like if:
        - Orphaned (PPID=1) AND sleeping AND has stale connections
        - OR has many stale connections (>5) and <0.1% CPU
        - OR is in zombie/dead state
        """
        try:
            import psutil
        except ImportError:
            return {}

        discovered = {}

        for proc in psutil.process_iter(['pid', 'ppid', 'cmdline', 'create_time', 'status', 'connections']):
            try:
                pid = proc.info['pid']
                if pid in (self._my_pid, self._my_parent):
                    continue

                cmdline = " ".join(proc.info.get('cmdline') or [])
                cmdline_lower = cmdline.lower()

                # Only check JARVIS-related processes
                is_jarvis_related = any(
                    pattern in cmdline_lower
                    for patterns in self.REPO_PATTERNS.values()
                    for pattern in patterns
                )

                if not is_jarvis_related:
                    continue

                # Get process details
                ppid = proc.info.get('ppid', 0)
                status = proc.info.get('status', '')
                is_orphaned = ppid == 1
                is_sleeping = status in ('sleeping', 'idle')
                is_zombie_state = status in ('zombie', 'dead')

                # Count stale connections
                stale_count = 0
                try:
                    connections = proc.connections(kind='inet')
                    for conn in connections:
                        if conn.status in ('CLOSE_WAIT', 'TIME_WAIT', 'FIN_WAIT1', 'FIN_WAIT2'):
                            stale_count += 1
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass

                # Get CPU percent
                try:
                    cpu_percent = proc.cpu_percent(interval=0.05)
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    cpu_percent = 0.0

                # Apply zombie heuristics
                is_zombie_like = (
                    is_zombie_state or
                    (is_orphaned and is_sleeping and stale_count > 0) or
                    (stale_count > 5 and cpu_percent < 0.1)
                )

                if is_zombie_like:
                    mem_info = proc.memory_info()
                    discovered[pid] = ZombieProcessInfo(
                        pid=pid,
                        cmdline=cmdline[:200],
                        age_seconds=time.time() - proc.info['create_time'],
                        memory_mb=mem_info.rss / (1024 * 1024),
                        cpu_percent=cpu_percent,
                        status=status,
                        is_orphaned=is_orphaned,
                        is_zombie_like=True,
                        stale_connection_count=stale_count,
                        detection_source="zombie_heuristic",
                    )

            except (psutil.NoSuchProcess, psutil.AccessDenied):
                pass

        return discovered

    async def _parallel_zombie_termination(
        self, zombies: Dict[int, ZombieProcessInfo]
    ) -> int:
        """
        Phase 3: Terminate zombies in parallel with semaphore control.

        Uses cascade strategy: SIGINT â†’ SIGTERM â†’ SIGKILL
        """
        if not zombies:
            return 0

        semaphore = asyncio.Semaphore(self.config.max_parallel_cleanups)

        async def terminate_one(pid: int, info: ZombieProcessInfo) -> bool:
            async with semaphore:
                return await self._terminate_zombie(pid, info)

        tasks = [
            asyncio.create_task(terminate_one(pid, info))
            for pid, info in zombies.items()
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        terminated = sum(1 for r in results if r is True)
        return terminated

    async def _terminate_zombie(
        self, pid: int, info: ZombieProcessInfo
    ) -> bool:
        """Terminate a single zombie with cascade strategy."""
        try:
            import psutil

            self.logger.info(
                f"[v109.7] Killing zombie PID {pid} "
                f"(repo={info.repo_origin}, source={info.detection_source})"
            )

            # Phase 1: SIGINT (graceful)
            try:
                os.kill(pid, signal.SIGINT)
                await asyncio.sleep(0.5)
                if not psutil.pid_exists(pid):
                    return True
            except (ProcessLookupError, OSError):
                return True

            # Phase 2: SIGTERM
            try:
                os.kill(pid, signal.SIGTERM)
                await asyncio.sleep(1.0)
                if not psutil.pid_exists(pid):
                    return True
            except (ProcessLookupError, OSError):
                return True

            # Phase 3: SIGKILL (force)
            try:
                os.kill(pid, signal.SIGKILL)
                await asyncio.sleep(0.3)
            except (ProcessLookupError, OSError):
                pass

            return True

        except Exception as e:
            self.logger.debug(f"[v109.7] Failed to terminate zombie {pid}: {e}")
            return False

    async def _verify_and_free_ports(self) -> List[int]:
        """
        Phase 4: Verify Trinity ports are free, force-free if needed.
        """
        freed_ports = []

        try:
            import psutil
        except ImportError:
            return freed_ports

        # Check all Trinity ports
        all_ports = []
        for ports in self.TRINITY_PORTS.values():
            all_ports.extend(ports)

        for port in all_ports:
            try:
                for conn in psutil.net_connections(kind='inet'):
                    if conn.laddr.port == port and conn.pid:
                        pid = conn.pid
                        if pid in (self._my_pid, self._my_parent):
                            continue

                        self.logger.warning(
                            f"[v109.7] Port {port} still held by PID {pid}, force-freeing..."
                        )

                        try:
                            os.kill(pid, signal.SIGKILL)
                            freed_ports.append(port)
                            await asyncio.sleep(0.2)
                        except (ProcessLookupError, OSError):
                            pass
            except (psutil.AccessDenied, PermissionError):
                pass

        return freed_ports

    async def _memory_aware_cleanup(self) -> Dict[str, Any]:
        """
        Phase 5: Memory-aware cleanup using IntelligentMemoryController.
        """
        result = {"actions_taken": []}

        try:
            import psutil

            # Check memory pressure
            memory = psutil.virtual_memory()
            available_gb = memory.available / (1024**3)

            if available_gb < 2.0:
                self.logger.info(
                    f"[v109.7] Low memory ({available_gb:.1f}GB available), "
                    "triggering additional cleanup..."
                )

                # Try to use IntelligentMemoryController
                try:
                    from backend.process_cleanup_manager import emergency_cleanup
                    cleanup_result = emergency_cleanup(force=True)
                    result["actions_taken"].append("emergency_cleanup")
                    result["cleanup_result"] = cleanup_result
                except ImportError:
                    pass

                # Force garbage collection
                import gc
                gc.collect()
                result["actions_taken"].append("gc_collect")

        except Exception as e:
            self.logger.debug(f"[v109.7] Memory cleanup error: {e}")

        return result

    def _is_circuit_open(self) -> bool:
        """Check if circuit breaker is open."""
        if not self._circuit_open:
            return False

        if time.time() > self._circuit_open_until:
            # Circuit cooldown expired, close it
            self._circuit_open = False
            self._cleanup_failures = 0
            self.logger.info("[v109.7] Circuit breaker closed after cooldown")
            return False

        return True

    def _open_circuit(self) -> None:
        """Open the circuit breaker."""
        self._circuit_open = True
        self._circuit_open_until = time.time() + self._circuit_cooldown
        self.logger.warning(
            f"[v109.7] Circuit breaker OPEN - cleanup disabled for {self._circuit_cooldown}s"
        )

    def get_stats(self) -> Dict[str, Any]:
        """Get cleanup statistics."""
        return {
            **self._stats,
            "cleanup_attempts": self._cleanup_attempts,
            "cleanup_failures": self._cleanup_failures,
            "circuit_open": self._circuit_open,
        }


# =============================================================================
# System Resource Validator - Pre-flight Checks
# =============================================================================

@dataclass
class ResourceStatus:
    """
    Enhanced status of system resources with intelligent analysis.

    Includes not just resource metrics but also:
    - Recommendations for optimization
    - Actions taken automatically
    - Startup mode decision
    - Cloud activation status
    - ARM64 SIMD availability
    """
    memory_available_gb: float
    memory_total_gb: float
    disk_available_gb: float
    ports_available: List[int]
    ports_in_use: List[int]
    cpu_count: int
    load_average: Optional[Tuple[float, float, float]] = None
    warnings: List[str] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)

    # New intelligent fields
    recommendations: List[str] = field(default_factory=list)
    actions_taken: List[str] = field(default_factory=list)
    startup_mode: Optional[str] = None  # local_full, cloud_first, cloud_only
    cloud_activated: bool = False
    arm64_simd_available: bool = False
    memory_pressure: float = 0.0  # 0-100%

    @property
    def is_healthy(self) -> bool:
        return len(self.errors) == 0

    @property
    def is_cloud_mode(self) -> bool:
        return self.startup_mode in ("cloud_first", "cloud_only")


class IntelligentResourceOrchestrator:
    """
    Intelligent Resource Orchestrator for JARVIS Startup.

    This is a comprehensive, async, parallel, intelligent, and dynamic resource
    management system that integrates:

    1. MemoryAwareStartup - Intelligent cloud offloading decisions
    2. IntelligentMemoryOptimizer - Active memory optimization
    3. HybridRouter - Resource-aware request routing
    4. GCP Hybrid Cloud - Automatic cloud activation when needed

    Features:
    - Parallel resource checks with intelligent analysis
    - Automatic memory optimization when constrained
    - Dynamic startup mode selection (LOCAL_FULL, CLOUD_FIRST, CLOUD_ONLY)
    - Intelligent port conflict resolution
    - Cost-aware cloud activation recommendations
    - ARM64 SIMD optimization detection
    - Real-time resource monitoring

    Architecture:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         IntelligentResourceOrchestrator                      â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚MemoryAwareStartupâ”‚  â”‚MemoryOptimizer â”‚  â”‚ HybridRouter â”‚ â”‚
        â”‚  â”‚  (Cloud Decision)â”‚  â”‚ (Active Optim) â”‚  â”‚  (Routing)   â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚           â”‚                    â”‚                   â”‚         â”‚
        â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
        â”‚                          â†“                                   â”‚
        â”‚              Unified Resource Decision                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """

    # Thresholds (configurable via environment)
    CLOUD_THRESHOLD_GB = float(os.getenv("JARVIS_CLOUD_THRESHOLD_GB", "6.0"))
    CRITICAL_THRESHOLD_GB = float(os.getenv("JARVIS_CRITICAL_THRESHOLD_GB", "2.0"))
    OPTIMIZE_THRESHOLD_GB = float(os.getenv("JARVIS_OPTIMIZE_THRESHOLD_GB", "4.0"))

    def __init__(self, config: BootstrapConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger

        # Lazy-loaded components
        self._memory_aware_startup = None
        self._memory_optimizer = None
        self._hybrid_router = None

        # State
        self._startup_mode = None
        self._optimization_performed = False
        self._cloud_activated = False
        self._arm64_available = self._check_arm64_simd()

    def _check_arm64_simd(self) -> bool:
        """Check if ARM64 SIMD optimizations are available."""
        try:
            asm_path = Path(__file__).parent / "backend" / "core" / "arm64_simd_asm.s"
            return asm_path.exists() and platform.machine() == "arm64"
        except Exception:
            return False

    async def _get_memory_aware_startup(self):
        """Lazy load MemoryAwareStartup."""
        if self._memory_aware_startup is None:
            try:
                from core.memory_aware_startup import MemoryAwareStartup
                self._memory_aware_startup = MemoryAwareStartup()
            except ImportError as e:
                self.logger.debug(f"MemoryAwareStartup not available: {e}")
        return self._memory_aware_startup

    async def _get_memory_optimizer(self):
        """Lazy load IntelligentMemoryOptimizer."""
        if self._memory_optimizer is None:
            try:
                from memory.intelligent_memory_optimizer import IntelligentMemoryOptimizer
                self._memory_optimizer = IntelligentMemoryOptimizer()
            except ImportError as e:
                self.logger.debug(f"IntelligentMemoryOptimizer not available: {e}")
        return self._memory_optimizer

    async def validate_and_optimize(self) -> ResourceStatus:
        """
        Validate system resources AND take intelligent action.

        This goes beyond just checking - it actively optimizes and
        makes decisions about startup mode and cloud activation.

        Returns:
            ResourceStatus with enhanced recommendations and actions taken
        """
        if self.config.skip_resource_check:
            self.logger.debug("Resource check skipped via config")
            return ResourceStatus(
                memory_available_gb=0,
                memory_total_gb=0,
                disk_available_gb=0,
                ports_available=[],
                ports_in_use=[],
                cpu_count=os.cpu_count() or 1,
            )

        # Phase 1: Parallel resource checks
        memory_task = asyncio.create_task(self._check_memory_detailed())
        disk_task = asyncio.create_task(self._check_disk())
        ports_task = asyncio.create_task(self._check_ports_intelligent())
        cpu_task = asyncio.create_task(self._check_cpu())

        memory_result, disk_result, ports_result, cpu_result = await asyncio.gather(
            memory_task, disk_task, ports_task, cpu_task
        )

        # Phase 2: Intelligent analysis and action
        warnings = []
        errors = []
        actions_taken = []
        recommendations = []

        available_gb = memory_result["available_gb"]
        total_gb = memory_result["total_gb"]
        memory_pressure = memory_result["pressure"]

        # === INTELLIGENT MEMORY HANDLING ===
        if available_gb < self.CRITICAL_THRESHOLD_GB:
            # Critical memory - attempt aggressive optimization
            self.logger.warning(f"âš ï¸  CRITICAL: Only {available_gb:.1f}GB available!")

            optimizer = await self._get_memory_optimizer()
            if optimizer:
                self.logger.info("ğŸ§¹ Attempting emergency memory optimization...")
                success, report = await optimizer.optimize_for_langchain(aggressive=True)
                if success:
                    freed_mb = report.get("memory_freed_mb", 0)
                    actions_taken.append(f"Emergency optimization freed {freed_mb:.0f}MB")
                    # Re-check memory after optimization
                    memory_result = await self._check_memory_detailed()
                    available_gb = memory_result["available_gb"]

            if available_gb < self.CRITICAL_THRESHOLD_GB:
                errors.append(f"Critical memory: {available_gb:.1f}GB (need {self.CRITICAL_THRESHOLD_GB}GB)")
                recommendations.append("ğŸ”´ Consider closing applications or using GCP cloud mode")

        elif available_gb < self.CLOUD_THRESHOLD_GB:
            # Low memory - recommend cloud mode
            warnings.append(f"Low memory: {available_gb:.1f}GB available")

            # Determine startup mode
            startup_manager = await self._get_memory_aware_startup()
            if startup_manager:
                decision = await startup_manager.determine_startup_mode()
                self._startup_mode = decision.mode.value

                if decision.use_cloud_ml:
                    recommendations.append(f"â˜ï¸  Cloud-First Mode: GCP will handle ML processing")
                    recommendations.append(f"ğŸ’° Estimated cost: ~$0.029/hour (Spot VM)")
                    recommendations.append(f"ğŸš€ Voice unlock will be instant (cloud-powered)")

                    # Offer to activate cloud
                    if decision.gcp_vm_required:
                        recommendations.append("âœ¨ GCP Spot VM will be activated automatically")
                        self._cloud_activated = True
                else:
                    recommendations.append(f"ğŸ  Local Mode: {decision.reason}")
            else:
                recommendations.append("ğŸ’¡ Tip: Close Chrome tabs or IDEs to free memory")

        elif available_gb < self.OPTIMIZE_THRESHOLD_GB:
            # Moderate memory - try light optimization
            optimizer = await self._get_memory_optimizer()
            if optimizer:
                suggestions = await optimizer.get_optimization_suggestions()
                if suggestions:
                    recommendations.extend([f"ğŸ’¡ {s}" for s in suggestions[:3]])

        else:
            # Plenty of memory - full local mode
            recommendations.append(f"âœ… Sufficient memory ({available_gb:.1f}GB) - Full local mode")
            self._startup_mode = "local_full"

            if self._arm64_available:
                recommendations.append("âš¡ ARM64 SIMD optimizations available (40-50x faster ML)")

        # === INTELLIGENT PORT HANDLING ===
        ports_available, ports_in_use, port_actions = ports_result
        if port_actions:
            actions_taken.extend(port_actions)
        if ports_in_use:
            warnings.append(f"Ports in use: {ports_in_use} (JARVIS processes found - will be recycled)")

        # === DISK VALIDATION ===
        if disk_result < self.config.min_disk_gb:
            errors.append(f"Insufficient disk: {disk_result:.1f}GB available")
        elif disk_result < self.config.min_disk_gb * 2:
            warnings.append(f"Low disk: {disk_result:.1f}GB available")

        # === CPU ANALYSIS ===
        cpu_count, load_avg = cpu_result
        if load_avg and load_avg[0] > cpu_count * 0.8:
            warnings.append(f"High CPU load: {load_avg[0]:.1f} (cores: {cpu_count})")
            recommendations.append("ğŸ’¡ Consider cloud offloading for CPU-intensive tasks")

        return ResourceStatus(
            memory_available_gb=available_gb,
            memory_total_gb=total_gb,
            disk_available_gb=disk_result,
            ports_available=ports_available,
            ports_in_use=ports_in_use,
            cpu_count=cpu_count,
            load_average=load_avg,
            warnings=warnings,
            errors=errors,
            recommendations=recommendations,
            actions_taken=actions_taken,
            startup_mode=self._startup_mode,
            cloud_activated=self._cloud_activated,
            arm64_simd_available=self._arm64_available,
            memory_pressure=memory_pressure,
        )

    async def _check_memory_detailed(self) -> Dict[str, Any]:
        """Get detailed memory analysis."""
        try:
            import psutil
            mem = psutil.virtual_memory()

            # Calculate memory pressure
            pressure = (mem.used / mem.total) * 100 if mem.total > 0 else 0

            return {
                "available_gb": mem.available / (1024**3),
                "total_gb": mem.total / (1024**3),
                "used_gb": mem.used / (1024**3),
                "pressure": pressure,
                "percent_used": mem.percent,
            }
        except Exception:
            return {
                "available_gb": 0.0,
                "total_gb": 0.0,
                "used_gb": 0.0,
                "pressure": 100.0,
                "percent_used": 100.0,
            }

    async def _check_disk(self) -> float:
        """Check available disk space."""
        try:
            import shutil
            total, used, free = shutil.disk_usage("/")
            return free / (1024**3)
        except Exception:
            return 0.0

    async def _check_ports_intelligent(self) -> Tuple[List[int], List[int], List[str]]:
        """
        Intelligently check and handle port conflicts.

        If a port is in use by a JARVIS process, it will be marked for recycling.
        """
        import socket

        available = []
        in_use = []
        actions = []

        for port in self.config.required_ports:
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.settimeout(0.5)
                result = sock.connect_ex(('localhost', port))
                sock.close()

                if result == 0:
                    in_use.append(port)
                    # Check if it's a JARVIS process (will be recycled by cleanup)
                    is_jarvis = await self._is_jarvis_port(port)
                    if is_jarvis:
                        actions.append(f"Port {port}: JARVIS process detected (will recycle)")
                else:
                    available.append(port)
            except Exception:
                available.append(port)

        return available, in_use, actions

    async def _is_jarvis_port(self, port: int) -> bool:
        """Check if a port is being used by a JARVIS process."""
        try:
            import psutil
            for conn in psutil.net_connections(kind='inet'):
                if conn.laddr.port == port and conn.status == 'LISTEN':
                    try:
                        proc = psutil.Process(conn.pid)
                        cmdline = " ".join(proc.cmdline()).lower()
                        return any(p in cmdline for p in ["jarvis", "main.py", "start_system"])
                    except Exception:
                        pass
        except Exception:
            pass
        return False

    async def _check_cpu(self) -> Tuple[int, Optional[Tuple[float, float, float]]]:
        """Check CPU info."""
        cpu_count = os.cpu_count() or 1
        load_avg = None

        try:
            if hasattr(os, 'getloadavg'):
                load_avg = os.getloadavg()
        except Exception:
            pass

        return cpu_count, load_avg

    def get_startup_mode(self) -> Optional[str]:
        """Get the determined startup mode."""
        return self._startup_mode

    def is_cloud_activated(self) -> bool:
        """Check if cloud mode was activated."""
        return self._cloud_activated


# Backwards compatibility alias
SystemResourceValidator = IntelligentResourceOrchestrator


# =============================================================================
# Banner and UI - Enhanced Visual Feedback
# =============================================================================

class TerminalUI:
    """Enhanced terminal UI with colors and formatting."""

    # ANSI color codes - Complete palette
    CYAN = "\033[36m"
    GREEN = "\033[32m"
    YELLOW = "\033[33m"
    RED = "\033[31m"
    GRAY = "\033[90m"
    MAGENTA = "\033[35m"
    BLUE = "\033[34m"
    WHITE = "\033[37m"
    BOLD = "\033[1m"
    DIM = "\033[2m"
    UNDERLINE = "\033[4m"
    RESET = "\033[0m"

    # Background colors
    BG_RED = "\033[41m"
    BG_GREEN = "\033[42m"
    BG_YELLOW = "\033[43m"
    BG_BLUE = "\033[44m"
    BG_MAGENTA = "\033[45m"
    BG_CYAN = "\033[46m"
    
    @classmethod
    def print_banner(cls) -> None:
        """Print an engaging startup banner."""
        print()
        print(f"{cls.CYAN}{'=' * 65}{cls.RESET}")
        print(f"{cls.CYAN}{' ' * 10}âš¡ JARVIS LIFECYCLE SUPERVISOR v3.0 âš¡{' ' * 10}{cls.RESET}")
        print(f"{cls.CYAN}{' ' * 18}Zero-Touch Edition{' ' * 18}{cls.RESET}")
        print(f"{cls.CYAN}{'=' * 65}{cls.RESET}")
        print()
        print(f"  {cls.YELLOW}ğŸ¤– Self-Updating â€¢ Self-Healing â€¢ Autonomous â€¢ AGI-Powered{cls.RESET}")
        print()
        print(f"  {cls.GRAY}The Living OS - Manages updates, restarts, and rollbacks")
        print(f"  while keeping JARVIS online and responsive.{cls.RESET}")
        print()
        print(f"  {cls.GRAY}Zero-Touch: Autonomous updates with Dead Man's Switch{cls.RESET}")
        print()
        print(f"{cls.CYAN}{'-' * 65}{cls.RESET}")
        print()

    @classmethod
    def print_phase(cls, number: int, total: int, message: str) -> None:
        """Print a phase indicator."""
        print(f"  {cls.GRAY}[{number}/{total}] {message}...{cls.RESET}")
    
    @classmethod
    def print_success(cls, message: str) -> None:
        """Print a success message."""
        print(f"  {cls.GREEN}âœ“{cls.RESET} {message}")
    
    @classmethod
    def print_info(cls, key: str, value: str, highlight: bool = False) -> None:
        """Print an info line."""
        value_str = f"{cls.BOLD}{value}{cls.RESET}" if highlight else value
        print(f"  {cls.GREEN}â—{cls.RESET} {key}: {value_str}")
    
    @classmethod
    def print_warning(cls, message: str) -> None:
        """Print a warning message."""
        print(f"  {cls.YELLOW}âš {cls.RESET} {message}")
    
    @classmethod
    def print_error(cls, message: str) -> None:
        """Print an error message."""
        print(f"  {cls.RED}âœ—{cls.RESET} {message}")
    
    @classmethod
    def print_divider(cls) -> None:
        """Print a divider line."""
        print()
        print(f"{cls.CYAN}{'-' * 65}{cls.RESET}")
        print()

    @classmethod
    def print_step(cls, message: str) -> None:
        """Print a step/progress indicator for multi-step processes."""
        print(f"  {cls.CYAN}â–¶{cls.RESET} {message}")

    @classmethod
    def print_process_list(cls, processes: List[ProcessInfo]) -> None:
        """Print discovered processes."""
        print(f"  {cls.YELLOW}â—{cls.RESET} Found {len(processes)} existing instance(s):")
        for proc in processes:
            age_min = proc.age_seconds / 60
            print(f"    â””â”€ PID {proc.pid} ({age_min:.1f} min, {proc.memory_mb:.0f}MB)")
        print()


# =============================================================================
# Hot Reload Watcher - Intelligent Polyglot File Change Detection v5.0
# =============================================================================

class FileTypeCategory(Enum):
    """Categories of file types for intelligent restart decisions."""
    BACKEND_CODE = "backend_code"       # Python, Rust - requires backend restart
    FRONTEND_CODE = "frontend_code"     # JS, JSX, TS, TSX, CSS, HTML - may need frontend rebuild
    NATIVE_CODE = "native_code"         # Swift, Rust - may need recompilation
    CONFIG = "config"                   # YAML, TOML, JSON - configuration changes
    SCRIPT = "script"                   # Shell scripts - utility scripts
    DOCS = "docs"                       # Markdown, text - documentation (usually no restart)
    BUILD = "build"                     # Cargo.toml, package.json - build configs
    UNKNOWN = "unknown"


@dataclass
class FileTypeInfo:
    """Information about a file type."""
    extension: str
    category: FileTypeCategory
    requires_restart: bool
    restart_target: str  # "backend", "frontend", "native", "all", "none"
    description: str


class IntelligentFileTypeRegistry:
    """
    Dynamically discovers and categorizes file types in the codebase.
    
    Instead of hardcoding patterns, this registry:
    1. Scans the codebase to discover all file types
    2. Categorizes them intelligently
    3. Determines restart requirements for each type
    """
    
    # Known file type mappings (extensible, not exhaustive)
    KNOWN_TYPES: Dict[str, FileTypeInfo] = {
        # Backend code (requires backend restart)
        ".py": FileTypeInfo(".py", FileTypeCategory.BACKEND_CODE, True, "backend", "Python"),
        ".pyx": FileTypeInfo(".pyx", FileTypeCategory.BACKEND_CODE, True, "backend", "Cython"),
        ".pyi": FileTypeInfo(".pyi", FileTypeCategory.BACKEND_CODE, False, "none", "Python type stubs"),
        
        # Rust (native extensions - may need rebuild)
        ".rs": FileTypeInfo(".rs", FileTypeCategory.NATIVE_CODE, True, "backend", "Rust"),
        
        # Swift (native macOS code - may need rebuild)
        ".swift": FileTypeInfo(".swift", FileTypeCategory.NATIVE_CODE, True, "backend", "Swift"),
        
        # Frontend code
        ".js": FileTypeInfo(".js", FileTypeCategory.FRONTEND_CODE, True, "frontend", "JavaScript"),
        ".jsx": FileTypeInfo(".jsx", FileTypeCategory.FRONTEND_CODE, True, "frontend", "React JSX"),
        ".ts": FileTypeInfo(".ts", FileTypeCategory.FRONTEND_CODE, True, "frontend", "TypeScript"),
        ".tsx": FileTypeInfo(".tsx", FileTypeCategory.FRONTEND_CODE, True, "frontend", "React TSX"),
        ".css": FileTypeInfo(".css", FileTypeCategory.FRONTEND_CODE, True, "frontend", "CSS"),
        ".scss": FileTypeInfo(".scss", FileTypeCategory.FRONTEND_CODE, True, "frontend", "SCSS"),
        ".less": FileTypeInfo(".less", FileTypeCategory.FRONTEND_CODE, True, "frontend", "LESS"),
        ".html": FileTypeInfo(".html", FileTypeCategory.FRONTEND_CODE, True, "frontend", "HTML"),
        
        # Configuration files
        ".yaml": FileTypeInfo(".yaml", FileTypeCategory.CONFIG, True, "backend", "YAML config"),
        ".yml": FileTypeInfo(".yml", FileTypeCategory.CONFIG, True, "backend", "YAML config"),
        ".toml": FileTypeInfo(".toml", FileTypeCategory.BUILD, True, "backend", "TOML config"),
        ".json": FileTypeInfo(".json", FileTypeCategory.CONFIG, False, "none", "JSON config"),  # Usually runtime
        ".env": FileTypeInfo(".env", FileTypeCategory.CONFIG, True, "all", "Environment"),
        ".ini": FileTypeInfo(".ini", FileTypeCategory.CONFIG, True, "backend", "INI config"),
        
        # Shell scripts
        ".sh": FileTypeInfo(".sh", FileTypeCategory.SCRIPT, False, "none", "Shell script"),
        ".bash": FileTypeInfo(".bash", FileTypeCategory.SCRIPT, False, "none", "Bash script"),
        ".zsh": FileTypeInfo(".zsh", FileTypeCategory.SCRIPT, False, "none", "Zsh script"),
        
        # Build files (require full rebuild)
        "Cargo.toml": FileTypeInfo("Cargo.toml", FileTypeCategory.BUILD, True, "all", "Rust build"),
        "package.json": FileTypeInfo("package.json", FileTypeCategory.BUILD, True, "frontend", "NPM package"),
        "requirements.txt": FileTypeInfo("requirements.txt", FileTypeCategory.BUILD, True, "all", "Python deps"),
        "pyproject.toml": FileTypeInfo("pyproject.toml", FileTypeCategory.BUILD, True, "all", "Python project"),
        
        # Documentation (no restart needed)
        ".md": FileTypeInfo(".md", FileTypeCategory.DOCS, False, "none", "Markdown"),
        ".txt": FileTypeInfo(".txt", FileTypeCategory.DOCS, False, "none", "Text"),
        ".rst": FileTypeInfo(".rst", FileTypeCategory.DOCS, False, "none", "RST docs"),
        
        # SQL (may need migration)
        ".sql": FileTypeInfo(".sql", FileTypeCategory.CONFIG, False, "none", "SQL"),
    }
    
    def __init__(self, repo_root: Path, logger: logging.Logger):
        self.repo_root = repo_root
        self.logger = logger
        self._discovered_extensions: Set[str] = set()
        self._file_counts: Dict[str, int] = {}
    
    def discover_file_types(self) -> Dict[str, int]:
        """
        Dynamically discover all file types in the codebase.
        Returns a dict of extension -> count.
        """
        self._discovered_extensions.clear()
        self._file_counts.clear()
        
        exclude_dirs = {
            '.git', '__pycache__', 'node_modules', 'venv', 'env',
            '.venv', 'build', 'dist', 'target', '.cursor', '.idea',
            '.vscode', 'coverage', '.pytest_cache', '.mypy_cache',
        }
        
        for root, dirs, files in os.walk(self.repo_root):
            # Skip excluded directories
            dirs[:] = [d for d in dirs if d not in exclude_dirs and not d.startswith('.')]
            
            for file in files:
                if file.startswith('.'):
                    continue
                
                # Get extension
                if '.' in file:
                    ext = '.' + file.rsplit('.', 1)[-1].lower()
                else:
                    ext = ''
                
                if ext:
                    self._discovered_extensions.add(ext)
                    self._file_counts[ext] = self._file_counts.get(ext, 0) + 1
        
        return self._file_counts
    
    def get_file_info(self, file_path: str) -> FileTypeInfo:
        """Get info about a file type."""
        path = Path(file_path)
        filename = path.name
        
        # Check exact filename match first (e.g., "Cargo.toml")
        if filename in self.KNOWN_TYPES:
            return self.KNOWN_TYPES[filename]
        
        # Check extension
        ext = path.suffix.lower()
        if ext in self.KNOWN_TYPES:
            return self.KNOWN_TYPES[ext]
        
        # Unknown type - return safe default
        return FileTypeInfo(ext, FileTypeCategory.UNKNOWN, False, "none", f"Unknown ({ext})")
    
    def get_watch_patterns(self) -> List[str]:
        """
        Dynamically generate watch patterns based on discovered file types.
        Only includes types that require restart.
        """
        patterns = []
        
        # Discover file types if not already done
        if not self._discovered_extensions:
            self.discover_file_types()
        
        # Add patterns for known restart-requiring types
        for ext in self._discovered_extensions:
            if ext in self.KNOWN_TYPES:
                info = self.KNOWN_TYPES[ext]
                if info.requires_restart:
                    patterns.append(f"**/*{ext}")
            else:
                # For unknown types, be conservative - don't watch by default
                pass
        
        # Always include important config files
        patterns.extend([
            "**/Cargo.toml",
            "**/package.json",
            "**/requirements.txt",
            "**/pyproject.toml",
        ])
        
        return list(set(patterns))  # Deduplicate
    
    def categorize_changes(self, changed_files: List[str]) -> Dict[str, List[str]]:
        """
        Categorize changed files by restart target.
        Returns dict of target -> list of files.
        """
        categorized: Dict[str, List[str]] = {
            "backend": [],
            "frontend": [],
            "native": [],
            "all": [],
            "none": [],
        }
        
        for file_path in changed_files:
            info = self.get_file_info(file_path)
            categorized[info.restart_target].append(file_path)
        
        return categorized
    
    def get_summary(self) -> str:
        """Get a summary of discovered file types."""
        if not self._file_counts:
            self.discover_file_types()
        
        # Sort by count
        sorted_types = sorted(self._file_counts.items(), key=lambda x: -x[1])
        
        lines = ["File types in codebase:"]
        for ext, count in sorted_types[:15]:  # Top 15
            info = self.KNOWN_TYPES.get(ext, None)
            if info:
                restart = "ğŸ”„" if info.requires_restart else "ğŸ“"
                lines.append(f"  {restart} {ext}: {count} files ({info.description})")
            else:
                lines.append(f"  â“ {ext}: {count} files")
        
        if len(sorted_types) > 15:
            lines.append(f"  ... and {len(sorted_types) - 15} more types")
        
        return "\n".join(lines)


class HotReloadWatcher:
    """
    v5.0: Intelligent polyglot hot reload watcher.
    
    Features:
    - Dynamic file type discovery (no hardcoding!)
    - Category-based restart decisions (backend vs frontend)
    - Parallel file hash calculation
    - Smart debouncing and cooldown
    - Frontend rebuild support (npm run build)
    - React dev server detection (skip if HMR is active)
    """
    
    def __init__(self, config: BootstrapConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.repo_root = Path(__file__).parent
        self.frontend_dir = self.repo_root / "frontend"
        self.backend_dir = self.repo_root / "backend"
        
        # Configuration from environment
        self.enabled = os.getenv("JARVIS_DEV_MODE", "true").lower() == "true"
        self.grace_period = int(os.getenv("JARVIS_RELOAD_GRACE_PERIOD", "120"))
        self.check_interval = int(os.getenv("JARVIS_RELOAD_CHECK_INTERVAL", "10"))
        self.cooldown_seconds = int(os.getenv("JARVIS_RELOAD_COOLDOWN", "10"))
        self.verbose = os.getenv("JARVIS_RELOAD_VERBOSE", "false").lower() == "true"
        
        # Frontend-specific config
        self.frontend_auto_rebuild = os.getenv("JARVIS_FRONTEND_AUTO_REBUILD", "true").lower() == "true"
        self.frontend_dev_server_port = int(os.getenv("JARVIS_FRONTEND_DEV_PORT", "3000"))
        
        # Intelligent file type registry
        self._type_registry = IntelligentFileTypeRegistry(self.repo_root, logger)
        
        # Exclude patterns (directories and file patterns to skip)
        self.exclude_dirs = {
            '.git', '__pycache__', 'node_modules', 'venv', 'env',
            '.venv', 'build', 'dist', 'target', '.cursor', '.idea',
            '.vscode', 'coverage', '.pytest_cache', '.mypy_cache',
            'logs', 'cache', '.jarvis_cache', 'htmlcov',
        }
        self.exclude_patterns = [
            "*.pyc", "*.pyo", "*.log", "*.tmp", "*.bak",
            "*.swp", "*.swo", "*~", ".DS_Store",
        ]
        
        # State
        self._start_time = time.time()
        self._file_hashes: Dict[str, str] = {}
        self._last_restart_time = 0.0
        self._last_frontend_rebuild_time = 0.0
        self._grace_period_ended = False
        self._monitor_task: Optional[asyncio.Task] = None
        self._restart_callback: Optional[Callable] = None
        self._frontend_callback: Optional[Callable] = None
        self._pending_changes: List[str] = []
        self._pending_frontend_changes: List[str] = []
        self._debounce_task: Optional[asyncio.Task] = None
        self._frontend_debounce_task: Optional[asyncio.Task] = None
        self._react_dev_server_running: Optional[bool] = None
    
    def set_restart_callback(self, callback: Callable) -> None:
        """Set the callback to invoke when a backend restart is needed."""
        self._restart_callback = callback
    
    def set_frontend_callback(self, callback: Callable) -> None:
        """Set the callback to invoke when a frontend rebuild is needed."""
        self._frontend_callback = callback
    
    async def _is_react_dev_server_running(self) -> bool:
        """
        Check if React dev server is running.
        If it is, we don't need to trigger rebuilds - React HMR handles it.
        """
        if self._react_dev_server_running is not None:
            return self._react_dev_server_running
        
        import socket
        
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(1)
            result = sock.connect_ex(('localhost', self.frontend_dev_server_port))
            sock.close()
            
            self._react_dev_server_running = (result == 0)
            
            if self._react_dev_server_running:
                self.logger.info(f"ğŸŒ React dev server detected on port {self.frontend_dev_server_port} - HMR active")
            else:
                self.logger.info("ğŸ“¦ React dev server not running - will trigger rebuilds on frontend changes")
            
            return self._react_dev_server_running
        except Exception:
            self._react_dev_server_running = False
            return False
    
    async def _rebuild_frontend(self, changed_files: List[str]) -> bool:
        """
        Trigger frontend rebuild (npm run build).
        Only runs if React dev server is NOT running.
        """
        if await self._is_react_dev_server_running():
            self.logger.info("   ğŸ”„ React HMR will handle these changes automatically")
            return True
        
        if not self.frontend_auto_rebuild:
            self.logger.info("   âš ï¸ Frontend auto-rebuild disabled (JARVIS_FRONTEND_AUTO_REBUILD=false)")
            return False
        
        self.logger.info("   ğŸ”¨ Triggering frontend rebuild...")
        
        process = None
        try:
            # Run npm run build in frontend directory
            process = await asyncio.create_subprocess_exec(
                "npm", "run", "build",
                cwd=str(self.frontend_dir),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env={**os.environ, "CI": "true"}  # Prevent interactive prompts
            )

            stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=120)

            if process.returncode == 0:
                self.logger.info("   âœ… Frontend rebuild completed successfully")
                return True
            else:
                self.logger.error(f"   âŒ Frontend rebuild failed: {stderr.decode()[:200]}")
                return False

        except asyncio.TimeoutError:
            self.logger.error("   âŒ Frontend rebuild timed out (120s)")
            # Clean up zombie process on timeout
            if process is not None:
                try:
                    process.kill()
                    await asyncio.wait_for(process.wait(), timeout=5.0)
                except Exception:
                    pass  # Best effort cleanup
            return False
        except Exception as e:
            self.logger.error(f"   âŒ Frontend rebuild error: {e}")
            # Clean up zombie process on error
            if process is not None:
                try:
                    process.kill()
                    await asyncio.wait_for(process.wait(), timeout=5.0)
                except Exception:
                    pass  # Best effort cleanup
            return False
    
    def _should_watch_file(self, file_path: Path) -> bool:
        """Determine if a file should be watched."""
        # Check if in excluded directory
        for part in file_path.parts:
            if part in self.exclude_dirs or part.startswith('.'):
                return False
        
        # Check exclude patterns
        from fnmatch import fnmatch
        for pattern in self.exclude_patterns:
            if fnmatch(file_path.name, pattern):
                return False
        
        # Check if file type requires restart
        info = self._type_registry.get_file_info(str(file_path))
        return info.requires_restart
    
    def _calculate_file_hashes_parallel(self) -> Dict[str, str]:
        """Calculate file hashes in parallel for speed."""
        import hashlib
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        def hash_file(file_path: Path) -> Tuple[str, Optional[str]]:
            try:
                with open(file_path, 'rb') as f:
                    return str(file_path.relative_to(self.repo_root)), hashlib.md5(f.read()).hexdigest()
            except Exception:
                return str(file_path), None
        
        files_to_hash = []
        
        # Walk directories and find watchable files
        for root, dirs, files in os.walk(self.repo_root):
            # Filter out excluded directories
            dirs[:] = [d for d in dirs if d not in self.exclude_dirs and not d.startswith('.')]
            
            root_path = Path(root)
            for file in files:
                file_path = root_path / file
                if self._should_watch_file(file_path):
                    files_to_hash.append(file_path)
        
        # Calculate hashes in parallel
        hashes = {}
        with ThreadPoolExecutor(max_workers=os.cpu_count() or 4) as executor:
            futures = {executor.submit(hash_file, fp): fp for fp in files_to_hash}
            for future in as_completed(futures):
                rel_path, file_hash = future.result()
                if file_hash:
                    hashes[rel_path] = file_hash
        
        return hashes
    
    def _detect_changes(self) -> Tuple[bool, List[str], Dict[str, List[str]]]:
        """
        Detect which files have changed.
        Returns: (has_changes, changed_files, categorized_changes)
        """
        current = self._calculate_file_hashes_parallel()
        changed = []
        
        for path, hash_val in current.items():
            if path not in self._file_hashes or self._file_hashes[path] != hash_val:
                changed.append(path)
        
        # Check for deleted files
        for path in self._file_hashes:
            if path not in current:
                changed.append(f"[DELETED] {path}")
        
        self._file_hashes = current
        
        # Categorize changes
        categorized = self._type_registry.categorize_changes(changed)
        
        return len(changed) > 0, changed, categorized
    
    def _is_in_grace_period(self) -> bool:
        """Check if we're still in the startup grace period."""
        elapsed = time.time() - self._start_time
        in_grace = elapsed < self.grace_period
        
        if not in_grace and not self._grace_period_ended:
            self._grace_period_ended = True
            self.logger.info(f"â° Hot reload grace period ended after {elapsed:.0f}s - now active")
        
        return in_grace
    
    def _is_in_cooldown(self) -> bool:
        """Check if we're in cooldown from a recent restart."""
        return (time.time() - self._last_restart_time) < self.cooldown_seconds
    
    async def start(self) -> None:
        """Start the hot reload watcher."""
        if not self.enabled:
            self.logger.info("ğŸ”¥ Hot reload disabled (JARVIS_DEV_MODE=false)")
            return
        
        # Discover and log file types
        self._type_registry.discover_file_types()
        
        if self.verbose:
            self.logger.info(self._type_registry.get_summary())
        
        # Initialize file hashes
        self._file_hashes = self._calculate_file_hashes_parallel()
        
        # Count files by category
        backend_count = 0
        frontend_count = 0
        for file_path in self._file_hashes:
            info = self._type_registry.get_file_info(file_path)
            if info.restart_target == "backend" or info.restart_target == "native":
                backend_count += 1
            elif info.restart_target == "frontend":
                frontend_count += 1
        
        # Log summary
        watch_patterns = self._type_registry.get_watch_patterns()
        file_types = sorted(set(p.split('*')[-1] for p in watch_patterns if '*' in p))
        
        self.logger.info(f"ğŸ”¥ Hot reload watching {len(self._file_hashes)} files")
        self.logger.info(f"   ğŸ Backend/Native: {backend_count} files")
        self.logger.info(f"   âš›ï¸  Frontend: {frontend_count} files")
        self.logger.info(f"   File types: {', '.join(file_types)}")
        self.logger.info(f"   Grace period: {self.grace_period}s, Check interval: {self.check_interval}s")
        
        # Start monitor task
        self._monitor_task = asyncio.create_task(self._monitor_loop())
    
    async def stop(self) -> None:
        """Stop the hot reload watcher."""
        if self._monitor_task:
            self._monitor_task.cancel()
            try:
                await self._monitor_task
            except asyncio.CancelledError:
                pass
        
        if self._debounce_task:
            self._debounce_task.cancel()
        
        if self._frontend_debounce_task:
            self._frontend_debounce_task.cancel()
    
    async def _debounced_restart(self, delay: float = 0.5) -> None:
        """Debounce rapid backend file changes into a single restart."""
        await asyncio.sleep(delay)
        
        if self._pending_changes and self._restart_callback:
            changes = self._pending_changes.copy()
            self._pending_changes.clear()
            
            self._last_restart_time = time.time()
            await self._restart_callback(changes)
    
    async def _debounced_frontend_rebuild(self, delay: float = 1.0) -> None:
        """Debounce rapid frontend file changes into a single rebuild."""
        await asyncio.sleep(delay)
        
        if self._pending_frontend_changes:
            changes = self._pending_frontend_changes.copy()
            self._pending_frontend_changes.clear()
            
            self._last_frontend_rebuild_time = time.time()
            await self._rebuild_frontend(changes)
    
    async def _monitor_loop(self) -> None:
        """Main monitoring loop."""
        # Check React dev server status on first run
        await self._is_react_dev_server_running()
        
        while True:
            try:
                await asyncio.sleep(self.check_interval)
                
                # Skip during grace period
                if self._is_in_grace_period():
                    continue
                
                # Check for changes
                has_changes, changed_files, categorized = self._detect_changes()
                
                if has_changes:
                    # Log changes by category
                    self.logger.info(f"ğŸ”¥ Detected {len(changed_files)} file change(s):")
                    
                    for target, files in categorized.items():
                        if files and target != "none":
                            icon = {
                                "backend": "ğŸ",
                                "frontend": "âš›ï¸",
                                "native": "ğŸ¦€",
                                "all": "ğŸŒ",
                            }.get(target, "ğŸ“")
                            self.logger.info(f"   {icon} {target.upper()}: {len(files)} file(s)")
                            if self.verbose:
                                for f in files[:3]:
                                    self.logger.info(f"     â””â”€ {f}")
                                if len(files) > 3:
                                    self.logger.info(f"     â””â”€ ... and {len(files) - 3} more")
                    
                    # Separate backend and frontend changes
                    backend_changes = categorized.get("backend", []) + categorized.get("native", []) + categorized.get("all", [])
                    frontend_changes = categorized.get("frontend", []) + categorized.get("all", [])
                    
                    # Handle backend changes
                    if backend_changes:
                        if self._is_in_cooldown():
                            remaining = self.cooldown_seconds - (time.time() - self._last_restart_time)
                            self.logger.info(f"   â³ Backend cooldown ({remaining:.0f}s remaining), deferring")
                            self._pending_changes.extend(backend_changes)
                        else:
                            self._pending_changes.extend(backend_changes)
                            if self._debounce_task:
                                self._debounce_task.cancel()
                            self._debounce_task = asyncio.create_task(self._debounced_restart())
                    
                    # Handle frontend changes
                    if frontend_changes:
                        # Check frontend cooldown
                        frontend_cooldown = (time.time() - self._last_frontend_rebuild_time) < self.cooldown_seconds
                        
                        if frontend_cooldown:
                            remaining = self.cooldown_seconds - (time.time() - self._last_frontend_rebuild_time)
                            self.logger.info(f"   â³ Frontend cooldown ({remaining:.0f}s remaining), deferring")
                            self._pending_frontend_changes.extend(frontend_changes)
                        else:
                            self._pending_frontend_changes.extend(frontend_changes)
                            if self._frontend_debounce_task:
                                self._frontend_debounce_task.cancel()
                            self._frontend_debounce_task = asyncio.create_task(self._debounced_frontend_rebuild())
                    
                    # Log if only docs changed
                    if not backend_changes and not frontend_changes:
                        self.logger.info("   ğŸ“ Changes don't require restart (docs only)")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Hot reload monitor error: {e}")
                await asyncio.sleep(self.check_interval)


# =============================================================================
# Main Orchestrator - Intelligent Startup Sequence
# =============================================================================

class SupervisorBootstrapper:
    """
    Intelligent orchestrator for supervisor startup.

    Features:
    - Phased startup with dependency resolution
    - Parallel operations where possible
    - Graceful error handling and recovery
    - Performance tracking and reporting
    - Two-tier agentic security (v1.0)
    - Watchdog safety supervision
    """

    def __init__(self):
        self.config = BootstrapConfig()
        self.logger = setup_logging(self.config)
        self.narrator = AsyncVoiceNarrator(self.config)
        self.perf = PerformanceLogger()
        self.phase = StartupPhase.INIT
        self._shutdown_event = asyncio.Event()
        self._loading_server_process: Optional[asyncio.subprocess.Process] = None  # Track for cleanup

        # v5.0: Hot Reload Watcher for dev mode
        self._hot_reload = HotReloadWatcher(self.config, self.logger)
        self._hot_reload.set_restart_callback(self._on_hot_reload_triggered)
        self._supervisor: Optional[Any] = None  # Reference to running supervisor for restart

        # v6.0: Agentic Watchdog and Tiered Router for Two-Tier Security
        self._watchdog = None
        self._tiered_router = None
        self._agentic_runner = None  # v6.0: Unified Agentic Task Runner
        self._vbia_adapter = None    # v6.0: Tiered VBIA Adapter
        self._watchdog_enabled = os.getenv("JARVIS_WATCHDOG_ENABLED", "true").lower() == "true"
        self._tiered_routing_enabled = os.getenv("JARVIS_TIERED_ROUTING", "true").lower() == "true"
        self._agentic_runner_enabled = os.getenv("JARVIS_AGENTIC_RUNNER", "true").lower() == "true"

        # v7.0: JARVIS-Prime Tier-0 Brain Integration
        self._jarvis_prime_orchestrator = None
        self._jarvis_prime_client = None
        self._jarvis_prime_process: Optional[asyncio.subprocess.Process] = None
        self._reactor_core_watcher = None

        # v101.0: Cross-Repo Process Restart Manager
        self._supervisor_restart_manager = SupervisorRestartManager(logger=self.logger)

        # v8.0: Data Flywheel (Self-Improving Learning Loop)
        self._data_flywheel = None
        self._learning_goals_manager = None
        self._training_scheduler_task = None
        self._experience_collection_task = None  # v9.1: Background experience collection

        # v9.2: Intelligent Training Orchestrator (Reactor-Core Pipeline)
        self._training_orchestrator = None
        self._training_orchestrator_task = None
        self._data_threshold_monitor_task = None
        self._quality_monitor_task = None
        self._last_training_run = None  # Timestamp of last successful training

        # v9.3: Intelligent Learning Goals Discovery (Auto-topic extraction)
        self._learning_goals_discovery = None
        self._learning_goals_discovery_task = None
        self._discovery_queue_processor_task = None
        self._safe_scout_orchestrator = None
        self._topic_queue = None
        self._last_discovery_run = None  # Timestamp of last discovery sweep
        self._discovery_stats = {
            "total_discovered": 0,
            "topics_scraped": 0,
            "topics_queued": 0,
            "failed_extractions": 0,
            "last_sources": {},  # Track discovery by source type
        }

        # v9.4: Intelligent Model Manager (Auto-download & reactor-core deployment)
        self._model_manager = None
        self._model_watcher_task = None
        self._current_model_info = {
            "name": None,
            "path": None,
            "size_mb": 0,
            "loaded": False,
            "source": None,  # "downloaded", "reactor_core", "existing"
        }

        # v90.0: System Primitives Integration (Iron-Clad Production Layer)
        self._port_manager = None
        self._jprime_safe_process = None
        self._reactor_safe_process = None
        self._adaptive_waiter = None

        # Initialize port manager if available
        if _SYSTEM_PRIMITIVES_AVAILABLE and PortManager is not None:
            try:
                self._port_manager = PortManager()
                self._adaptive_waiter = AdaptiveWaiter()
                self.logger.info("[v90] System primitives initialized (SafeProcess, PortManager, TrueHeartbeat)")
            except Exception as e:
                self.logger.warning(f"[v90] Could not initialize port manager: {e}")

        # v3.0: Advanced Multi-LLM Integration (integration.py components)
        self._trinity_coordinator = None  # Cross-repo health and coordination
        self._model_selector = None       # Intelligent model selection
        self._multi_model_orchestrator = None  # Complex task decomposition

        # v91.0: Advanced System Primitives (ML Prediction, Self-Healing, Coordination)
        self._health_predictor = None
        self._self_healing_orchestrator = None
        self._resource_quota_manager = None
        self._distributed_state_coordinator = None
        self._graceful_degradation_manager = None
        self._health_prediction_task = None
        self._self_healing_task = None
        self._resource_monitoring_task = None
        self._degradation_monitoring_task = None

        # Initialize advanced primitives if available
        if _ADVANCED_PRIMITIVES_AVAILABLE:
            try:
                # Resource quota manager (ulimit protection)
                self._resource_quota_manager = ResourceQuotaManager()

                # Process health predictor (ML-based failure prediction)
                self._health_predictor = ProcessHealthPredictor(
                    window_size=int(os.getenv("JARVIS_HEALTH_WINDOW_SIZE", "100")),
                    ewma_alpha=float(os.getenv("JARVIS_HEALTH_EWMA_ALPHA", "0.3")),
                    anomaly_threshold=float(os.getenv("JARVIS_HEALTH_ANOMALY_THRESHOLD", "2.5")),
                )

                # Self-healing orchestrator (automatic remediation)
                self._self_healing_orchestrator = SelfHealingOrchestrator(
                    health_predictor=self._health_predictor,
                    max_remediation_attempts=int(os.getenv("JARVIS_MAX_HEAL_ATTEMPTS", "3")),
                    cooldown_seconds=float(os.getenv("JARVIS_HEAL_COOLDOWN", "60.0")),
                )

                # Distributed state coordinator (cross-repo sync)
                self._distributed_state_coordinator = DistributedStateCoordinator(
                    component_name="JARVIS-Supervisor",
                    state_dir=Path(os.getenv(
                        "JARVIS_STATE_DIR",
                        str(Path.home() / ".jarvis" / "state")
                    )),
                )

                # Graceful degradation manager (resource-aware feature flags)
                self._graceful_degradation_manager = GracefulDegradationManager(
                    resource_manager=self._resource_quota_manager,
                )

                # Register features for graceful degradation
                self._register_degradation_features()

                self.logger.info(
                    "[v91] Advanced primitives initialized: "
                    "ProcessHealthPredictor, SelfHealingOrchestrator, "
                    "ResourceQuotaManager, DistributedStateCoordinator, GracefulDegradationManager"
                )
            except Exception as e:
                self.logger.warning(f"[v91] Could not initialize advanced primitives: {e}")
                import traceback
                self.logger.debug(f"[v91] Traceback: {traceback.format_exc()}")

        # v16.0: Log optional dependency status (consolidated, cached)
        # This provides a single summary log instead of scattered warnings
        if _OPTIONAL_DEPS_AVAILABLE:
            try:
                log_dependency_status()
                # Also store capabilities for later reference
                self._ml_capabilities = get_ml_capabilities()
                if self._ml_capabilities.get("ml_available"):
                    gpu_type = (
                        "MPS" if self._ml_capabilities.get("mps_available") else
                        "CUDA" if self._ml_capabilities.get("cuda_available") else
                        "CPU"
                    )
                    self.logger.info(f"[v16.0] ML Runtime: {gpu_type}")
            except Exception as e:
                self.logger.debug(f"[v16.0] Dependency status logging skipped: {e}")
                self._ml_capabilities = {"ml_available": False, "gpu_available": False}
        else:
            self._ml_capabilities = {"ml_available": False, "gpu_available": False}

        self._model_download_in_progress = False

        # v9.4: Enhanced Neural Mesh (Production agent system)
        self._neural_mesh_coordinator = None
        self._neural_mesh_bridge = None
        self._neural_mesh_agents = {}
        self._neural_mesh_health_task = None
        self._neural_mesh_stats = {
            "agents_registered": 0,
            "messages_sent": 0,
            "knowledge_entries": 0,
            "workflows_completed": 0,
        }

        # v9.5: Infrastructure Orchestrator (On-Demand GCP Resources)
        self._infra_orchestrator = None
        self._infra_orchestrator_enabled = self.config.infra_on_demand_enabled

        # v12.0: Docker Daemon Manager (Intelligent Self-Healing)
        self._docker_manager = None
        self._docker_manager_initialized = False

        # v10.0: Reactor-Core API Server (Training Pipeline)
        self._reactor_core_process = None

        # v11.0: PROJECT TRINITY - Unified Cognitive Architecture
        self._trinity_initialized = False
        self._trinity_instance_id: Optional[str] = None
        self._trinity_enabled = os.getenv("TRINITY_ENABLED", "true").lower() == "true"

        # v101.0: UnifiedTrinityConnector (Claude Code-like behaviors)
        self._unified_trinity_connector = None
        self._reactor_core_enabled = os.getenv("JARVIS_REACTOR_CORE_ENABLED", "true").lower() == "true"
        self._reactor_core_port = int(os.getenv("REACTOR_CORE_PORT", "8090"))

        # v72.0: Trinity Component Auto-Launch (One-Command Startup)
        # These track subprocesses for J-Prime and Reactor-Core launched by this supervisor
        self._jprime_orchestrator_process: Optional[asyncio.subprocess.Process] = None
        self._reactor_core_orchestrator_process: Optional[asyncio.subprocess.Process] = None
        self._trinity_auto_launch_enabled = os.getenv("TRINITY_AUTO_LAUNCH", "true").lower() == "true"

        # v95.0: Direct Subprocess Health Monitoring (complements heartbeat-based monitoring)
        # - Directly monitors process handles for early crash detection
        # - Faster detection than heartbeat timeout (immediate vs 15s)
        # - Triggers restart callbacks when subprocess dies unexpectedly
        self._subprocess_health_task: Optional[asyncio.Task] = None
        self._subprocess_health_interval = float(os.getenv("SUBPROCESS_HEALTH_INTERVAL", "5.0"))
        self._subprocess_health_enabled = os.getenv("SUBPROCESS_HEALTH_ENABLED", "true").lower() == "true"

        # v75.0: Trinity Health Monitor for crash detection & auto-recovery
        self._trinity_health_monitor = None
        self._jprime_repo_path = Path(os.getenv(
            "JARVIS_PRIME_PATH",
            str(Path.home() / "Documents" / "repos" / "jarvis-prime")
        ))
        self._reactor_core_repo_path = Path(os.getenv(
            "REACTOR_CORE_PATH",
            str(Path.home() / "Documents" / "repos" / "reactor-core")
        ))

        # v10.3: Unified Progress Hub (Cross-component progress synchronization)
        self._progress_hub = None

        # v10.6: Real-Time Log Monitor (Intelligent health monitoring with voice alerts)
        self._log_monitor = None
        self._log_monitor_enabled = (
            STRUCTURED_LOGGING_AVAILABLE and
            os.getenv("JARVIS_LOG_MONITOR_ENABLED", "true").lower() == "true"
        )

        # v79.1: Trinity Voice Coordination (Cross-repo voice announcements)
        self._trinity_voice_coordinator = None
        self._trinity_voice_enabled = os.getenv("TRINITY_VOICE_ENABLED", "true").lower() == "true"
        self._voice_announcer = None

        # v88.0: Trinity Knowledge Indexer (Web scraping â†’ Vector DB â†’ RAG)
        self._trinity_knowledge_indexer = None
        self._trinity_knowledge_indexer_enabled = os.getenv("TRINITY_KNOWLEDGE_INDEXER_ENABLED", "true").lower() == "true"

        # v80.0: Advanced Cross-Repo Loading System
        # - CrossRepoHealthMonitor: Circuit breakers, adaptive intervals, trend analysis
        # - TrinityStartupCoordinator: Parallel startup, dependency resolution, progress broadcasting
        self._cross_repo_health_monitor = None
        self._trinity_startup_coordinator = None
        self._cross_repo_health_task = None
        self._v80_enabled = os.getenv("JARVIS_V80_CROSS_REPO", "true").lower() == "true"

        # v81.0: TrinityIntegrator - Unified Cross-Repo Integration
        # - Orphan process detection and cleanup
        # - Resilient IPC with circuit breakers
        # - Coordinated phased shutdown
        # - Port allocation with fallback
        self._trinity_integrator = None
        self._trinity_integrator_enabled = os.getenv("TRINITY_INTEGRATOR_ENABLED", "true").lower() == "true"

        # v100.0: Trinity Core Systems (Advanced Cross-Repo Infrastructure)
        # - TrinityEventBus: Unified pub/sub with priority queues, persistence, replay
        # - TrinityKnowledgeGraph: Shared knowledge storage with semantic search
        # - TrinityTrainingPipeline: End-to-end training (JARVIS â†’ Reactor â†’ Prime)
        # - TrinityMonitoring: Unified observability with distributed tracing, alerting
        self._trinity_event_bus = None
        self._trinity_knowledge_graph = None
        self._trinity_training_pipeline = None
        self._trinity_monitoring = None
        self._trinity_core_systems_enabled = os.getenv("TRINITY_CORE_SYSTEMS_ENABLED", "true").lower() == "true"
        self._trinity_core_systems_task = None

        # v93.16: Enterprise-Grade Heartbeat System
        # - Continuous heartbeat updates for jarvis_body.json
        # - Service registry integration
        # - Adaptive intervals based on system load
        # - Cross-repo heartbeat synchronization
        self._trinity_heartbeat_task: Optional[asyncio.Task] = None
        self._trinity_heartbeat_interval = float(os.getenv("JARVIS_HEARTBEAT_INTERVAL", "15.0"))
        self._heartbeat_adaptive_enabled = os.getenv("JARVIS_HEARTBEAT_ADAPTIVE", "true").lower() == "true"
        self._last_heartbeat_metrics: Dict[str, Any] = {}

        # v111.0: In-Process Backend Management (Unified Monolith Mode)
        # - Backend runs in supervisor's event loop (no subprocess spawning)
        # - Enables shared memory and coordinated shutdown
        # - Uvicorn server with disabled signal handlers (supervisor manages signals)
        self._backend_server: Optional[Any] = None  # uvicorn.Server instance
        self._backend_task: Optional[asyncio.Task] = None  # Background task running serve()
        self._inprocess_heartbeat_task: Optional[asyncio.Task] = None  # v111.2: Heartbeat writer
        self._in_process_mode: bool = os.getenv("JARVIS_IN_PROCESS_MODE", "true").lower() == "true"
        self._backend_port: int = int(os.getenv("BACKEND_PORT", "8010"))

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v113.0: LOADING SERVER AND FRONTEND MANAGEMENT
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # These are the MISSING PIECES that caused ERR_CONNECTION_REFUSED:
        # - Loading server (port 3001): Shows startup progress during initialization
        # - Frontend (port 3000): React app served after backend is ready
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self._loading_server_process: Optional[asyncio.subprocess.Process] = None
        self._frontend_process: Optional[asyncio.subprocess.Process] = None
        self._loading_server_port: int = int(os.getenv("JARVIS_LOADING_PORT", "3001"))
        self._frontend_port: int = int(os.getenv("JARVIS_FRONTEND_PORT", "3000"))
        self._jarvis_repo: Path = Path(os.getenv(
            "JARVIS_REPO",
            str(Path.home() / "Documents" / "repos" / "JARVIS-AI-Agent")
        ))
        self._frontend_startup_task: Optional[asyncio.Task] = None

        # v100.0: AGI Orchestrator (Unified Cognitive Architecture)
        # - MetaCognitiveEngine: Self-aware reasoning and introspection
        # - MultiModalPerceptionFusion: Vision + voice + text integration
        # - ContinuousImprovementEngine: Self-improving learning loop
        # - EmotionalIntelligenceModule: Empathetic response system
        self._agi_orchestrator = None
        self._agi_orchestrator_enabled = os.getenv("AGI_ORCHESTRATOR_ENABLED", "true").lower() == "true"

        # v100.0: Unified Model Serving (Prime + Claude Fallback)
        # - PrimeLocalClient: Local GGUF model inference
        # - PrimeCloudRunClient: Cloud Run Prime deployment
        # - ClaudeClient: Claude API fallback
        # - CircuitBreaker: Failure detection and automatic failover
        # - ModelRouter: Task-based routing with fallback chains
        self._unified_model_serving = None
        self._model_serving_enabled = os.getenv("UNIFIED_MODEL_SERVING_ENABLED", "true").lower() == "true"

        # v100.0: Unified Agent Registry (Redis-backed Distributed Registry)
        # - Service discovery with capability-based routing
        # - Redis-backed state for multi-instance coordination
        # - Pub/sub for real-time agent status updates
        # - Circuit breaker for failing agents
        # - Load balancing with health-aware routing
        self._unified_agent_registry = None
        self._agent_registry_enabled = os.getenv("UNIFIED_AGENT_REGISTRY_ENABLED", "true").lower() == "true"

        # v100.0: Distributed State Manager (Transactional State Coordination)
        # - Transactional state updates with atomicity guarantees
        # - Redis-backed distributed state with local fallback
        # - Leader election for coordination tasks
        # - State snapshots and recovery
        # - Pub/sub for state change notifications
        self._distributed_state_manager = None
        self._state_manager_enabled = os.getenv("DISTRIBUTED_STATE_MANAGER_ENABLED", "true").lower() == "true"

        # v100.0: Continuous Learning Orchestrator (Unified Learning Pipeline)
        # - Experience aggregation from all JARVIS components
        # - Intelligent training job scheduling
        # - A/B testing with automatic promotion/rollback
        # - Model performance tracking and validation
        # - Cross-repo experience forwarding to Reactor Core
        self._continuous_learning_orchestrator = None
        self._continuous_learning_enabled = os.getenv("CONTINUOUS_LEARNING_ENABLED", "true").lower() == "true"

        # v100.0: Neural Mesh Registry Bridge (Cross-System Sync)
        # - Bidirectional sync between UnifiedAgentRegistry and Neural Mesh
        # - Data model translation (AgentInfo formats)
        # - Unified capability queries across both systems
        # - Cross-system event propagation
        self._neural_mesh_bridge = None
        self._neural_mesh_bridge_enabled = os.getenv("NEURAL_MESH_BRIDGE_ENABLED", "true").lower() == "true"

        # v100.0: Learning State Connector (State-Learning Integration)
        # - Training job state persistence across restarts
        # - A/B test state synchronization
        # - Experience buffer coordination
        # - Cross-instance training coordination via leader election
        self._learning_state_connector = None
        self._learning_state_connector_enabled = os.getenv("LEARNING_STATE_CONNECTOR_ENABLED", "true").lower() == "true"

        # v100.0: Cross-Repo Experience Forwarder (JARVIS â†” Reactor Core)
        # - Forwards learning experiences to Reactor Core
        # - Batch forwarding with retry and backoff
        # - File-based fallback when event bus unavailable
        # - Distributed model training coordination
        self._experience_forwarder = None
        self._experience_forwarder_enabled = os.getenv("CROSS_REPO_EXPERIENCE_FORWARDING", "true").lower() == "true"

        # v101.0: Trinity Bridge Adapter (MODEL_READY Event Forwarding)
        # - Watches Reactor Core event directories for MODEL_READY events
        # - Forwards events to TrinityEventBus for hot-swap processing
        # - Closes the Trinity Loop: Training â†’ MODEL_READY â†’ Hot-Swap
        # - FileWatchGuard for robust file watching with recovery
        # - Circuit breaker for event bus dispatch failures
        self._trinity_bridge_adapter = None
        self._trinity_bridge_adapter_enabled = os.getenv("TRINITY_BRIDGE_ADAPTER_ENABLED", "true").lower() == "true"

        # v101.0: Cross-Repo Neural Mesh Bridge (External Agent Integration)
        # - Registers JARVIS Prime and Reactor Core as Neural Mesh agents
        # - Health monitoring via heartbeat files
        # - Task routing with capability matching
        # - Circuit breaker protection for external operations
        self._cross_repo_neural_mesh = None
        self._cross_repo_neural_mesh_enabled = os.getenv("CROSS_REPO_NEURAL_MESH_ENABLED", "true").lower() == "true"

        # v101.0: Cross-Repo Cost Sync (Unified Budget Tracking)
        # - Redis-backed real-time cost synchronization
        # - Atomic budget checks across all repos
        # - Auto-reconnecting Redis client with fallback
        # - Prevents budget overruns from concurrent requests
        self._cross_repo_cost_sync = None
        self._cross_repo_cost_sync_enabled = os.getenv("CROSS_REPO_COST_SYNC_ENABLED", "true").lower() == "true"

        # v101.0: GCP Hybrid Prime Router (Memory-Triggered VM Provisioning)
        # - Intelligent routing between local/GCP/cloud tiers
        # - Memory pressure-triggered VM provisioning
        # - Redis distributed locking for cross-repo VM creation
        # - Failure classification with intelligent retry
        self._gcp_hybrid_router = None
        self._gcp_hybrid_router_enabled = os.getenv("GCP_HYBRID_ROUTER_ENABLED", "true").lower() == "true"

        # v102.0: Trinity Integration Coordinator (Advanced Cross-Repo Orchestration)
        # - Causal event delivery with vector clocks
        # - Distributed locking for model hot-swap
        # - Health monitoring with auto-recovery
        # - Experience validation and schema enforcement
        # - Directory lifecycle management
        self._trinity_integration_coordinator = None
        self._trinity_integration_coordinator_enabled = os.getenv("TRINITY_INTEGRATION_COORDINATOR_ENABLED", "true").lower() == "true"

        # v104.0: Trinity IPC Hub (All 10 Communication Channels)
        # - Direct Body â†’ Reactor Command Channel
        # - Reactor â†’ Body Status Push Channel
        # - Prime â†’ Reactor Feedback Channel
        # - Body â†’ Reactor Training Data Pipeline
        # - Bidirectional Model Metadata Exchange
        # - Cross-Repo Query Interface
        # - Real-Time Event Streaming
        # - Cross-Repo RPC Layer
        # - Multi-Cast Event Broadcasting (Pub/Sub)
        # - Reliable Message Queue with ACK
        self._trinity_ipc_hub = None
        self._trinity_bridge_v4 = None
        self._trinity_ipc_hub_enabled = os.getenv("TRINITY_IPC_HUB_ENABLED", "true").lower() == "true"

        # v105.0: Trinity State Manager (Distributed State Management)
        # Addresses all 8 state management gaps:
        # - Gap 1: Unified State Coordinator (single source of truth)
        # - Gap 2: Distributed State Synchronization (CRDT-based)
        # - Gap 3: State Versioning (history and rollback)
        # - Gap 4: State Conflict Resolution (vector clocks)
        # - Gap 5: State Snapshot & Restore
        # - Gap 6: State Partitioning (by namespace)
        # - Gap 7: State Compression (LZ4/zlib)
        # - Gap 8: State Access Control (RBAC)
        self._trinity_state_manager = None
        self._trinity_state_manager_enabled = os.getenv("TRINITY_STATE_MANAGER_ENABLED", "true").lower() == "true"

        # v106.0: Trinity Observability System (Distributed Monitoring)
        # Enterprise-grade observability with ALL 10 gaps addressed:
        # - Gap 1: Distributed Tracing (W3C Trace Context)
        # - Gap 2: Cross-Repo Metrics (Prometheus-compatible)
        # - Gap 3: Centralized Logging (structured JSON)
        # - Gap 4: Performance Profiling (flame graphs)
        # - Gap 5: Error Aggregation (Sentry-style)
        # - Gap 6: Health Dashboard (unified view)
        # - Gap 7: Alert System (deduplication)
        # - Gap 8: Dependency Graph (Mermaid/GraphViz)
        # - Gap 9: Request Flow (bottleneck detection)
        # - Gap 10: Resource Monitoring (CPU/Memory/Disk/Network)
        self._trinity_observability = None
        self._trinity_observability_enabled = os.getenv("TRINITY_OBSERVABILITY_ENABLED", "true").lower() == "true"

        # v102.0: Reactor Core Bridge (Training Pipeline Integration)
        # - MODEL_READY event publishing from Reactor Core
        # - Experience batch receiving and validation
        # - Training pipeline lifecycle hooks
        self._reactor_core_bridge = None
        self._reactor_core_bridge_enabled = os.getenv("REACTOR_CORE_BRIDGE_ENABLED", "true").lower() == "true"
        self._training_health_task: Optional[asyncio.Task] = None

        # v103.0: Trinity Orchestration Engine (God Process)
        # - Distributed consensus with Raft-inspired leader election
        # - Predictive auto-scaling with Holt-Winters forecasting
        # - Graceful degradation with fallback modes
        # - Dead letter queue for failed event recovery
        # - Resource governance with memory limits
        self._trinity_orchestration_engine = None
        self._trinity_orchestration_engine_enabled = os.getenv("TRINITY_ORCHESTRATION_ENGINE_ENABLED", "true").lower() == "true"
        self._orchestration_status_task: Optional[asyncio.Task] = None

        # v104.0: Ouroboros Self-Improvement Engine
        # - Autonomous code evolution using local LLM (JARVIS Prime)
        # - Genetic algorithm for multi-path improvement
        # - AST-based code analysis and semantic diff
        # - Test-driven validation with mutation testing
        # - Git-based rollback protection
        self._ouroboros_engine = None
        self._ouroboros_enabled = os.getenv("OUROBOROS_ENABLED", "true").lower() == "true"
        self._ouroboros_auto_improve = os.getenv("OUROBOROS_AUTO_IMPROVE", "false").lower() == "true"
        self._ouroboros_advanced = None  # v105.0: Advanced Orchestrator
        self._ouroboros_cross_repo = None  # v105.0: Cross-Repo Integration
        self._brain_orchestrator = None  # v106.0: LLM Infrastructure Manager
        self._native_self_improvement = None  # v107.0: Native Self-Improvement (Motor Function)
        self._neural_mesh = None  # v107.0: Cross-Repo Neural Mesh
        self._ouroboros_ui_controller = None  # v107.0: UI Integration
        self._ouroboros_trinity = None  # v107.0: Trinity Integration (Unified Layer)

        # v85.0: Unified State Coordination - Atomic locks with process cookies
        # - Prevents race conditions between run_supervisor.py and start_system.py
        # - Uses fcntl locks with TTL-based expiration
        # - Process cookies prevent PID reuse issues
        self._state_coordinator: Optional[Any] = None  # UnifiedStateCoordinator
        self._ownership_acquired: bool = False
        self._heartbeat_task: Optional[asyncio.Task] = None

        # v108.0: Enterprise DI Container (Dependency Injection)
        # - Type-safe service registration and resolution
        # - Topological sorting via Kahn's algorithm for init order
        # - Cycle detection via Tarjan's algorithm
        # - Parallel group computation for concurrent startup
        # - Lazy async lock initialization (safe before event loop)
        # - Cross-repo coordination with circuit breakers
        # - Transactional initialization with rollback
        self._di_container = None
        self._intelligence_services_initialized = False

        # v108.0: Intelligence Layer Services (managed via DI container)
        # These are now resolved from the DI container for proper dependency management
        self._collaboration_engine = None
        self._collab_coordinator = None
        self._code_ownership_engine = None
        self._ownership_coordinator = None
        self._review_workflow_engine = None
        self._review_coordinator = None
        self._lsp_server = None
        self._ide_integration_engine = None
        self._ide_coordinator = None

        # CRITICAL: Set CI=true to prevent npm start from hanging interactively
        # if port 3000 is taken. This ensures we fail fast or handle it automatically.
        os.environ["CI"] = "true"

        self._setup_signal_handlers()

    async def _safe_phase_init(
        self,
        phase_name: str,
        init_coro,
        timeout_seconds: float = 30.0,
        critical: bool = False,
    ) -> bool:
        """
        v107.0: Safe phase initialization with timeout and error handling.

        CRITICAL FIX: This prevents any single initialization phase from blocking
        the entire startup flow indefinitely. Each phase gets a timeout and proper
        error handling so the startup can continue even if non-critical phases fail.

        Args:
            phase_name: Human-readable name for logging (e.g., "PHASE 13: Neural Mesh Bridge")
            init_coro: The async coroutine to execute
            timeout_seconds: Maximum time to wait (default: 30s)
            critical: If True, failure will be logged as error; otherwise warning

        Returns:
            True if initialization succeeded, False otherwise

        Features:
        - Timeout protection prevents indefinite blocking
        - Error isolation ensures one phase can't crash startup
        - Clear logging for debugging startup issues
        - Graceful degradation - startup continues on non-critical failures
        - Async-safe with proper cancellation handling
        """
        try:
            self.logger.info(f"[v107.0] Starting {phase_name} (timeout: {timeout_seconds}s)...")
            await asyncio.wait_for(init_coro, timeout=timeout_seconds)
            self.logger.info(f"[v107.0] âœ… {phase_name} completed")
            return True
        except asyncio.TimeoutError:
            msg = f"[v107.0] â±ï¸ {phase_name} timed out after {timeout_seconds}s - skipping"
            if critical:
                self.logger.error(msg)
            else:
                self.logger.warning(msg)
            print(f"  {TerminalUI.YELLOW}âš ï¸ {phase_name}: Timed out (continuing){TerminalUI.RESET}")
            return False
        except asyncio.CancelledError:
            self.logger.warning(f"[v107.0] âŒ {phase_name} cancelled")
            raise  # Re-raise cancellation
        except Exception as e:
            msg = f"[v107.0] âŒ {phase_name} failed: {e}"
            if critical:
                self.logger.error(msg)
            else:
                self.logger.warning(msg)
            print(f"  {TerminalUI.YELLOW}âš ï¸ {phase_name}: Failed ({e}){TerminalUI.RESET}")
            return False

    async def _run_fast_startup(self) -> int:
        """
        Fast startup mode - minimal overhead, instant JARVIS boot.

        Skips:
        - Resource validation and optimization
        - Loading page browser display
        - Voice narration
        - Heavy initialization phases

        Performs:
        - Quick port cleanup (8010 only)
        - PYTHONPATH configuration
        - Direct main.py startup via supervisor

        Returns:
            Exit code (0 for success, non-zero for failure)
        """
        import subprocess
        import signal
        from pathlib import Path

        print(f"\n{TerminalUI.CYAN}{'â•' * 60}{TerminalUI.RESET}")
        print(f"{TerminalUI.CYAN}âš¡ JARVIS FAST STARTUP MODE{TerminalUI.RESET}")
        print(f"{TerminalUI.CYAN}{'â•' * 60}{TerminalUI.RESET}\n")

        self.logger.info("âš¡ Fast startup mode - minimal initialization")

        # Step 1: Quick port cleanup (8010 only)
        self.perf.start("fast_cleanup")
        ports_to_clean = [8010]

        for port in ports_to_clean:
            try:
                result = subprocess.run(
                    ["lsof", "-ti", f":{port}"],
                    capture_output=True, text=True, timeout=5
                )
                if result.stdout.strip():
                    pids = result.stdout.strip().split('\n')
                    for pid in pids:
                        if pid:
                            try:
                                os.kill(int(pid), signal.SIGTERM)
                                self.logger.info(f"âš¡ Killed process {pid} on port {port}")
                            except (ProcessLookupError, ValueError):
                                pass
                    # Brief wait for port release
                    await asyncio.sleep(0.3)
            except subprocess.TimeoutExpired:
                pass
            except Exception as e:
                self.logger.debug(f"Port cleanup warning: {e}")

        self.perf.end("fast_cleanup")
        print(f"  {TerminalUI.GREEN}âœ“ Port cleanup complete{TerminalUI.RESET}")

        # Step 2: Configure environment
        project_root = Path(__file__).parent.resolve()

        # Set PYTHONPATH for proper imports
        pythonpath_parts = [
            str(project_root),
            str(project_root / "backend"),
        ]
        existing_pythonpath = os.environ.get("PYTHONPATH", "")
        if existing_pythonpath:
            pythonpath_parts.append(existing_pythonpath)
        os.environ["PYTHONPATH"] = os.pathsep.join(pythonpath_parts)

        # Signal fast mode to supervisor
        os.environ["JARVIS_FAST_STARTUP"] = "true"
        os.environ["JARVIS_CLEANUP_DONE"] = "1"
        os.environ["JARVIS_CLEANUP_TIMESTAMP"] = str(time.time())

        self.logger.info(f"ğŸ“ PYTHONPATH: {os.environ['PYTHONPATH']}")
        print(f"  {TerminalUI.GREEN}âœ“ Environment configured{TerminalUI.RESET}")

        # Step 3: Import and run supervisor
        self.perf.start("fast_supervisor")
        try:
            # Dynamic import to avoid circular dependencies
            from backend.core.supervisor.jarvis_supervisor import JARVISSupervisor

            print(f"\n{TerminalUI.YELLOW}âš¡ Starting JARVIS Core...{TerminalUI.RESET}")

            supervisor = JARVISSupervisor()

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v111.2: Start backend in-process (Unified Monolith Mode) - FAST PATH
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Fast startup mode must also use in-process backend for consistency.
            # This ensures the unified monolith architecture works in all modes.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self._in_process_mode:
                backend_started = await self._start_backend_in_process()
                if not backend_started:
                    self.logger.error("[v111.2] Backend failed to start in-process mode (fast path)")
                    TerminalUI.print_error("[v111.2] Backend failed to start - aborting")
                    return 1

            print(f"\n{TerminalUI.GREEN}{'â•' * 60}{TerminalUI.RESET}")
            print(f"{TerminalUI.GREEN}âš¡ JARVIS FAST MODE STARTING{TerminalUI.RESET}")
            print(f"{TerminalUI.GREEN}   Backend: http://localhost:{self._backend_port}{TerminalUI.RESET}")
            print(f"{TerminalUI.GREEN}   Mode: {'In-Process' if self._in_process_mode else 'Subprocess'}{TerminalUI.RESET}")
            print(f"{TerminalUI.GREEN}{'â•' * 60}{TerminalUI.RESET}\n")

            try:
                # Run supervisor (blocks until shutdown)
                await supervisor.run()
            finally:
                # v111.2: Stop in-process backend on fast path exit
                if self._in_process_mode and self._backend_server:
                    await self._stop_backend_in_process()

            self.perf.end("fast_supervisor")
            return 0

        except ImportError as e:
            self.logger.error(f"Failed to import supervisor: {e}")
            print(f"  {TerminalUI.RED}âœ— Import error: {e}{TerminalUI.RESET}")

            # Fallback: direct subprocess execution
            print(f"\n{TerminalUI.YELLOW}âš¡ Falling back to direct execution...{TerminalUI.RESET}")

            python_exe = sys.executable
            cmd = [python_exe, "-B", "-m", "backend.main"]

            env = os.environ.copy()

            try:
                process = subprocess.Popen(
                    cmd,
                    cwd=str(project_root),
                    env=env,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=1
                )

                print(f"\n{TerminalUI.GREEN}âš¡ JARVIS started (PID: {process.pid}){TerminalUI.RESET}")
                print(f"{TerminalUI.GREEN}   Backend: http://localhost:8010{TerminalUI.RESET}\n")

                # Stream output
                try:
                    for line in process.stdout:
                        print(line, end='')
                except KeyboardInterrupt:
                    self.logger.info("Shutdown requested...")
                    process.terminate()
                    try:
                        process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        self.logger.warning("Process didn't terminate gracefully, forcing kill...")
                        process.kill()
                        try:
                            process.wait(timeout=3)
                        except subprocess.TimeoutExpired:
                            self.logger.error("Process couldn't be killed - may be zombie")

                return process.returncode or 0

            except Exception as sub_e:
                self.logger.error(f"Fallback execution failed: {sub_e}")
                return 1

        except Exception as e:
            self.logger.error(f"Fast startup failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Startup failed: {e}{TerminalUI.RESET}")
            return 1

    async def run(self) -> int:
        """
        Run the complete bootstrap sequence.

        Supports fast startup mode (JARVIS_FAST_STARTUP=true) which skips
        resource validation, loading page, and other non-essential initialization
        for instant JARVIS startup.

        v80.0: Wrapped with global timeout to prevent infinite hangs.

        Returns:
            Exit code (0 for success, non-zero for failure)
        """
        # Check for fast startup mode
        fast_startup = os.environ.get("JARVIS_FAST_STARTUP", "").lower() in ("1", "true", "yes")

        if fast_startup:
            return await self._run_fast_startup()

        # v80.0: Wrap entire startup in global timeout
        # v5.3: Properly handle CancelledError (not subclass of Exception in Python 3.8+)
        try:
            return await asyncio.wait_for(
                self._run_with_deep_health(),
                timeout=self.config.global_startup_timeout,
            )
        except asyncio.TimeoutError:
            self.logger.error(
                f"ğŸš¨ GLOBAL STARTUP TIMEOUT after {self.config.global_startup_timeout}s - "
                "forcing emergency shutdown"
            )
            await self._emergency_shutdown()
            return 1
        except asyncio.CancelledError:
            # v5.3: CancelledError is BaseException, not Exception - handle explicitly
            self.logger.warning("ğŸ›‘ Startup cancelled (SIGINT/SIGTERM received)")
            try:
                await self._emergency_shutdown()
            except asyncio.CancelledError:
                pass  # Already shutting down
            return 130  # Standard exit code for SIGINT
        except Exception as e:
            self.logger.error(f"ğŸš¨ Startup failed with exception: {e}")
            await self._emergency_shutdown()
            return 1

    async def _run_v87_preflight_checks(self) -> bool:
        """
        v87.0/v88.2: Ultra-Advanced Pre-flight Resource Checks.

        Features:
        - Overall sequence timeout protection (prevents indefinite hangs)
        - Per-check adaptive timeouts (adjusts based on system load)
        - Parallel execution of independent checks (30%+ faster)
        - Comprehensive error isolation (no cascading failures)
        - Graceful degradation (startup continues even if checks fail)

        Verifies system resources are adequate before acquiring ownership:
        - Network partition detection (for NFS-mounted state directories)
        - Filesystem writability
        - Disk space availability
        - Clock skew detection
        - File descriptor availability
        - Process group isolation

        Returns:
            True if all checks pass, False if critical checks fail
        """
        enable_preflight = os.environ.get(
            "JARVIS_V87_PREFLIGHT", "true"
        ).lower() in ("1", "true", "yes")

        if not enable_preflight:
            self.logger.info("[v87.0] Pre-flight checks disabled via JARVIS_V87_PREFLIGHT=false")
            return True

        self.logger.info("[v87.0] Running pre-flight resource checks...")
        TerminalUI.print_step("[v87.0] Pre-flight checks")

        # v88.2: Overall sequence timeout protection
        overall_timeout = float(os.environ.get("JARVIS_V87_PREFLIGHT_TIMEOUT", "30.0"))
        try:
            return await asyncio.wait_for(
                self._execute_preflight_checks_internal(),
                timeout=overall_timeout
            )
        except asyncio.TimeoutError:
            self.logger.warning(
                f"[v87.0] Pre-flight checks timed out after {overall_timeout}s. "
                "Continuing with startup (some checks may be skipped)."
            )
            TerminalUI.print_warning(f"[v87.0] Pre-flight timeout after {overall_timeout}s - continuing anyway")
            return True  # Don't block startup on timeout
        except Exception as e:
            self.logger.warning(f"[v87.0] Pre-flight sequence error: {e}", exc_info=True)
            TerminalUI.print_warning("[v87.0] Pre-flight checks encountered errors - continuing anyway")
            return True  # Don't block startup on errors

    async def _get_adaptive_check_timeout(self, base_timeout: float) -> float:
        """
        v88.2: Calculate adaptive timeout based on system load.

        Increases timeout when system is under heavy load to prevent
        false timeouts during legitimate slow operations.

        Args:
            base_timeout: Base timeout in seconds

        Returns:
            Adjusted timeout (potentially higher if system is loaded)
        """
        try:
            import psutil

            # Quick CPU and memory check (non-blocking)
            cpu_percent = psutil.cpu_percent(interval=0.05)
            memory = psutil.virtual_memory()

            # Calculate load multiplier
            if cpu_percent > 90 or memory.percent > 95:
                multiplier = 2.0  # Heavy load - double timeout
            elif cpu_percent > 75 or memory.percent > 85:
                multiplier = 1.5  # Moderate load - 50% more time
            elif cpu_percent > 50 or memory.percent > 70:
                multiplier = 1.25  # Light load - 25% more time
            else:
                multiplier = 1.0  # Normal

            adjusted = base_timeout * multiplier
            if multiplier > 1.0:
                self.logger.debug(
                    f"[v88.2] Adaptive timeout: {base_timeout}s â†’ {adjusted}s "
                    f"(CPU: {cpu_percent}%, MEM: {memory.percent}%)"
                )
            return adjusted

        except ImportError:
            return base_timeout  # psutil not available
        except Exception:
            return base_timeout  # Any error, use base timeout

    async def _execute_preflight_checks_internal(self) -> bool:
        """
        v88.2: Internal preflight check implementation with parallel execution.
        v100.1: Integrated TrinityBootstrapValidator for comprehensive pre-flight validation.

        Runs independent checks in parallel for 30%+ faster startup,
        with individual timeout protection for each check.
        """
        warnings: List[str] = []
        critical_failures: List[str] = []
        coord = None

        # v100.1: Run TrinityBootstrapValidator first (comprehensive validation)
        trinity_validation_enabled = os.environ.get(
            "TRINITY_BOOTSTRAP_VALIDATION", "true"
        ).lower() in ("1", "true", "yes")

        if trinity_validation_enabled:
            try:
                from backend.core.trinity_bootstrap_validator import (
                    TrinityBootstrapValidator,
                    ValidationSeverity,
                )
                validator = TrinityBootstrapValidator()
                validation_result = await validator.validate_all()

                # Log validation results
                if validation_result.critical_issues:
                    for issue in validation_result.critical_issues:
                        critical_failures.append(f"[v100.1] {issue.message}")
                        self.logger.error(f"[v100.1 CRITICAL] {issue}")
                        if issue.fix_suggestion:
                            self.logger.error(f"    Fix: {issue.fix_suggestion}")

                if validation_result.errors:
                    for issue in validation_result.errors:
                        warnings.append(f"[v100.1] {issue.message}")
                        self.logger.warning(f"[v100.1 ERROR] {issue}")

                if validation_result.warnings:
                    for issue in validation_result.warnings:
                        self.logger.info(f"[v100.1 WARNING] {issue}")

                # Report validation summary
                self.logger.info(
                    f"[v100.1] Validation complete: {len(validation_result.issues)} issues "
                    f"({len(validation_result.critical_issues)} critical, "
                    f"{len(validation_result.errors)} errors, "
                    f"{len(validation_result.warnings)} warnings) "
                    f"in {validation_result.duration_ms:.0f}ms"
                )

                if not validation_result.passed:
                    TerminalUI.print_error("[v100.1] Bootstrap validation FAILED - critical issues found")
                    # Don't block startup for now, let existing checks run too
                else:
                    TerminalUI.print_success("[v100.1] Bootstrap validation passed")

            except ImportError as e:
                self.logger.debug(f"[v100.1] TrinityBootstrapValidator not available: {e}")
            except Exception as e:
                self.logger.warning(f"[v100.1] Validation error (non-blocking): {e}")

        # v88.2: Get adaptive base timeout
        base_check_timeout = float(os.environ.get("JARVIS_V87_CHECK_TIMEOUT", "5.0"))
        check_timeout = await self._get_adaptive_check_timeout(base_check_timeout)

        try:
            # v88.1: Import with isolated error handling
            try:
                from backend.core.trinity_integrator import TrinityAdvancedCoordinator
            except ImportError as import_err:
                self.logger.info(
                    f"[v87.0] TrinityAdvancedCoordinator not available: {import_err}. "
                    "Skipping advanced pre-flight checks (this is OK)."
                )
                TerminalUI.print_success("[v87.0] Pre-flight checks skipped (basic mode)")
                return True

            # v88.1: Create coordinator with timeout protection
            try:
                coord = TrinityAdvancedCoordinator()
                init_timeout = float(os.environ.get("JARVIS_V87_INIT_TIMEOUT", "10.0"))
                await asyncio.wait_for(coord.initialize(), timeout=init_timeout)
            except asyncio.TimeoutError:
                warnings.append(f"Coordinator initialization timed out after {init_timeout}s")
                self.logger.warning(f"[v87.0] Coordinator init timeout ({init_timeout}s) - using basic checks")
                coord = None
            except Exception as coord_err:
                warnings.append(f"Coordinator initialization failed: {coord_err}")
                self.logger.warning(f"[v87.0] Coordinator init failed (non-critical): {coord_err}")
                coord = None

            # v88.2: Run checks if coordinator available
            if coord is not None:
                state_dir = Path(os.environ.get(
                    "TRINITY_STATE_DIR",
                    str(Path.home() / ".jarvis" / "trinity")
                ))

                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # v88.2: PARALLEL EXECUTION OF INDEPENDENT CHECKS
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # Run independent checks in parallel for faster startup.
                # Checks that don't depend on each other run simultaneously.
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                async def check_network() -> Tuple[str, bool, Optional[str], Optional[str]]:
                    """Network partition check."""
                    try:
                        is_partitioned, reason = await asyncio.wait_for(
                            coord.check_network_partition(),
                            timeout=check_timeout
                        )
                        if is_partitioned:
                            return ("network", False, f"Network partition: {reason}", None)
                        return ("network", True, None, f"Network OK: {reason}")
                    except asyncio.TimeoutError:
                        return ("network", True, None, f"Network check timeout ({check_timeout}s)")
                    except Exception as e:
                        return ("network", True, None, f"Network check error: {e}")

                async def check_filesystem() -> Tuple[str, bool, Optional[str], Optional[str]]:
                    """Filesystem writability check."""
                    try:
                        fs_ok, fs_reason = await asyncio.wait_for(
                            coord.check_filesystem_writable(state_dir),
                            timeout=check_timeout
                        )
                        if not fs_ok:
                            return ("filesystem", False, f"Filesystem: {fs_reason}", None)
                        return ("filesystem", True, None, f"Filesystem OK: {fs_reason}")
                    except asyncio.TimeoutError:
                        return ("filesystem", True, None, f"Filesystem check timeout ({check_timeout}s)")
                    except Exception as e:
                        return ("filesystem", True, None, f"Filesystem check error: {e}")

                async def check_disk() -> Tuple[str, bool, Optional[str], Optional[str]]:
                    """Disk space check."""
                    try:
                        disk_ok, disk_metrics = await asyncio.wait_for(
                            coord.check_disk_space(state_dir),
                            timeout=check_timeout
                        )
                        if not disk_ok:
                            disk_warnings = disk_metrics.get("warnings", ["Low disk space"])
                            return ("disk", True, None, "; ".join(disk_warnings))
                        free_pct = disk_metrics.get("free_percent", 0)
                        return ("disk", True, None, f"Disk OK: {free_pct:.1f}% free")
                    except asyncio.TimeoutError:
                        return ("disk", True, None, f"Disk check timeout ({check_timeout}s)")
                    except Exception as e:
                        return ("disk", True, None, f"Disk check error: {e}")

                async def check_clock() -> Tuple[str, bool, Optional[str], Optional[str]]:
                    """Clock skew detection."""
                    try:
                        has_skew, skew_seconds = await asyncio.wait_for(
                            coord.check_clock_skew(),
                            timeout=check_timeout
                        )
                        if has_skew:
                            return ("clock", True, None, f"Clock skew: {skew_seconds:.2f}s")
                        return ("clock", True, None, f"Clock OK: skew {skew_seconds:.2f}s")
                    except asyncio.TimeoutError:
                        return ("clock", True, None, f"Clock check timeout ({check_timeout}s)")
                    except Exception as e:
                        return ("clock", True, None, f"Clock check error: {e}")

                async def check_fd() -> Tuple[str, bool, Optional[str], Optional[str]]:
                    """File descriptor check."""
                    try:
                        fd_ok, fd_metrics = await asyncio.wait_for(
                            coord.check_file_descriptors(),
                            timeout=check_timeout
                        )
                        fd_count = fd_metrics.get("current", 0)
                        fd_limit = fd_metrics.get("soft_limit", 0)
                        pct_used = fd_metrics.get("percent_used", 0)
                        if not fd_ok:
                            fd_warnings = fd_metrics.get("warnings", [])
                            return ("fd", True, None, f"FD issues: {fd_count}/{fd_limit}")
                        return ("fd", True, None, f"FD OK: {fd_count}/{fd_limit} ({pct_used:.1f}% used)")
                    except asyncio.TimeoutError:
                        return ("fd", True, None, f"FD check timeout ({check_timeout}s)")
                    except Exception as e:
                        return ("fd", True, None, f"FD check error: {e}")

                async def check_process_isolation() -> Tuple[str, bool, Optional[str], Optional[str]]:
                    """Process group isolation check."""
                    try:
                        from backend.core.trinity_integrator import UnifiedStateCoordinator
                        temp_coord = UnifiedStateCoordinator()
                        isolated, isolation_reason = await asyncio.wait_for(
                            temp_coord.verify_process_isolation(),
                            timeout=check_timeout
                        )
                        if not isolated:
                            return ("isolation", True, None, f"Process isolation: {isolation_reason}")
                        return ("isolation", True, None, f"Process isolation OK: {isolation_reason}")
                    except asyncio.TimeoutError:
                        return ("isolation", True, None, f"Isolation check timeout ({check_timeout}s)")
                    except Exception as e:
                        return ("isolation", True, None, f"Isolation check error: {e}")

                # v88.2: Run all checks in parallel
                self.logger.debug("[v88.2] Running preflight checks in parallel...")
                start_time = time.time()

                check_tasks = [
                    asyncio.create_task(check_network()),
                    asyncio.create_task(check_filesystem()),
                    asyncio.create_task(check_disk()),
                    asyncio.create_task(check_clock()),
                    asyncio.create_task(check_fd()),
                    asyncio.create_task(check_process_isolation()),
                ]

                # Wait for all checks with overall timeout already applied by caller
                results = await asyncio.gather(*check_tasks, return_exceptions=True)

                elapsed = (time.time() - start_time) * 1000
                self.logger.info(f"[v88.2] Parallel preflight checks completed in {elapsed:.1f}ms")

                # Process results
                for result in results:
                    if isinstance(result, Exception):
                        warnings.append(f"Check exception: {result}")
                        continue

                    check_name, passed, critical_msg, warning_msg = result

                    if not passed and critical_msg:
                        critical_failures.append(critical_msg)
                        self.logger.error(f"[v87.0] âŒ {critical_msg}")
                    elif warning_msg:
                        if "error" in warning_msg.lower() or "timeout" in warning_msg.lower():
                            warnings.append(warning_msg)
                            self.logger.warning(f"[v87.0] âš  {warning_msg}")
                        else:
                            self.logger.debug(f"[v87.0] âœ“ {warning_msg}")

                # v88.1: Cleanup coordinator with timeout protection
                try:
                    await asyncio.wait_for(coord.shutdown(), timeout=5.0)
                except asyncio.TimeoutError:
                    self.logger.debug("[v87.0] Coordinator shutdown timeout (ignored)")
                except Exception as shutdown_err:
                    self.logger.debug(f"[v87.0] Coordinator shutdown error (ignored): {shutdown_err}")

        except Exception as e:
            # v88.1: Catch-all for any unexpected errors - don't fail startup
            self.logger.warning(f"[v87.0] Pre-flight check error (continuing anyway): {e}", exc_info=True)
            warnings.append(f"Pre-flight checks encountered errors: {e}")
            # Cleanup coordinator if it was created
            if coord is not None:
                try:
                    await asyncio.wait_for(coord.shutdown(), timeout=2.0)
                except Exception:
                    pass

        # Report results
        if critical_failures:
            self.logger.error(f"[v87.0] âŒ {len(critical_failures)} CRITICAL failures:")
            for failure in critical_failures:
                self.logger.error(f"  - {failure}")
                TerminalUI.print_error(f"[v87.0] {failure}")
            return False

        if warnings:
            self.logger.warning(f"[v87.0] âš  {len(warnings)} warnings (continuing):")
            for warning in warnings:
                self.logger.warning(f"  - {warning}")
            TerminalUI.print_warning(f"[v87.0] {len(warnings)} warnings - check logs")

        self.logger.info("[v87.0] âœ… Pre-flight checks passed")
        TerminalUI.print_success("[v87.0] Pre-flight checks passed")
        return True

    async def _execute_protected_v88(
        self,
        component: str,
        operation: Callable[[], Awaitable[Any]],
        timeout: float = 60.0,
        fallback_on_failure: bool = True,
    ) -> Tuple[bool, Any, Dict[str, Any]]:
        """
        v88.0: Execute operation with ultra coordinator protection.

        Applies:
        - Adaptive circuit breaker with ML-based prediction
        - Backpressure handling with AIMD rate limiting
        - W3C distributed tracing
        - Timeout enforcement

        Args:
            component: Component name (jprime, reactor, voice, etc.)
            operation: Async operation to execute
            timeout: Operation timeout in seconds
            fallback_on_failure: If True, execute directly on protection failure

        Returns:
            (success, result, metadata)
        """
        # Try with ultra coordinator protection
        if hasattr(self, "_ultra_coordinator") and self._ultra_coordinator:
            try:
                success, result, metadata = await self._ultra_coordinator.execute_with_protection(
                    component, operation, timeout
                )
                if success:
                    return success, result, metadata
                elif not fallback_on_failure:
                    return success, result, metadata
                # Fall through to direct execution
                self.logger.warning(
                    f"[v88.0] Protected execution failed for {component}: {metadata.get('reason', 'unknown')}"
                )
            except Exception as e:
                self.logger.warning(f"[v88.0] Ultra coordinator error: {e}")

        # Fallback: direct execution without protection
        if fallback_on_failure:
            try:
                result = await asyncio.wait_for(operation(), timeout=timeout)
                return True, result, {"fallback": True, "component": component}
            except asyncio.TimeoutError:
                return False, None, {"timeout": True, "fallback": True, "component": component}
            except Exception as e:
                return False, None, {"error": str(e), "fallback": True, "component": component}

        return False, None, {"error": "Protection disabled, no fallback", "component": component}

    async def _emergency_shutdown(self) -> None:
        """v80.0: Emergency shutdown when startup times out."""
        self.logger.warning("ğŸš¨ Emergency shutdown initiated")
        try:
            # v101.0: Notify supervisor restart manager to stop restart attempts
            self._supervisor_restart_manager.request_shutdown()

            # v85.0: Release ownership and stop heartbeat FIRST
            await self._release_v85_ownership()

            # v111.0: Stop in-process backend first (fast shutdown)
            if self._in_process_mode and self._backend_server:
                try:
                    await self._stop_backend_in_process(timeout=5.0)  # Shorter timeout for emergency
                except Exception:
                    pass  # Don't block emergency shutdown

            # Kill all Trinity components
            await self._shutdown_trinity_components()

            # Kill loading server
            if self._loading_server_process:
                try:
                    self._loading_server_process.terminate()
                except Exception:
                    pass

            # v113.0: Kill frontend
            if self._frontend_process:
                try:
                    self._frontend_process.terminate()
                except Exception:
                    pass

            # v113.0: Cancel frontend startup task
            if self._frontend_startup_task and not self._frontend_startup_task.done():
                self._frontend_startup_task.cancel()
                try:
                    await asyncio.wait_for(self._frontend_startup_task, timeout=1.0)
                except (asyncio.CancelledError, asyncio.TimeoutError):
                    pass
                self._frontend_startup_task = None


            # Cleanup graceful degradation
            try:
                from backend.core.graceful_degradation import shutdown_degradation
                await shutdown_degradation()
            except Exception:
                pass

            # Cleanup advanced primitives
            try:
                from backend.core.advanced_async_primitives import shutdown_all
                await shutdown_all()
            except Exception:
                pass

        except Exception as e:
            self.logger.error(f"Emergency shutdown error: {e}")

    async def _release_v85_ownership(self) -> None:
        """
        v85.0: Release state coordinator ownership and stop heartbeat.

        This ensures clean ownership transfer when shutting down, preventing
        stale lock files and allowing other processes to acquire ownership.
        """
        try:
            # Stop heartbeat task first
            if hasattr(self, "_heartbeat_task") and self._heartbeat_task:
                self._heartbeat_task.cancel()
                try:
                    await asyncio.wait_for(
                        asyncio.shield(self._heartbeat_task),
                        timeout=2.0,
                    )
                except (asyncio.CancelledError, asyncio.TimeoutError):
                    pass
                self._heartbeat_task = None
                self.logger.debug("[v85.0] Heartbeat task stopped")

            # Release ownership
            if hasattr(self, "_state_coordinator") and self._state_coordinator:
                if hasattr(self, "_ownership_acquired") and self._ownership_acquired:
                    await self._state_coordinator.release_ownership("jarvis")  # v85.0: Unified
                    self._ownership_acquired = False
                    self.logger.info("[v85.0] âœ… Ownership released")
                self._state_coordinator = None

        except Exception as e:
            self.logger.warning(f"[v85.0] Error releasing ownership: {e}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v111.0: In-Process Backend Management (Unified Monolith Mode)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def _start_backend_in_process(self) -> bool:
        """
        v111.0: Start the JARVIS backend in-process using Uvicorn.

        This replaces subprocess spawning with direct in-process execution,
        enabling shared memory and coordinated shutdown.

        Benefits:
        - No IPC overhead (shared memory)
        - Coordinated shutdown (no orphan processes)
        - Faster startup (no process spawn overhead)
        - Unified event loop (better async coordination)

        Returns:
            True if backend started successfully, False otherwise
        """
        if not self._in_process_mode:
            self.logger.info("[v111.0] In-process mode disabled, skipping backend start")
            return True

        try:
            self.logger.info("[v111.0] Starting backend in-process...")
            TerminalUI.print_step("[v111.0] Starting backend in-process")

            # Deferred import to avoid circular dependencies
            import uvicorn
            from backend.main import app

            # Configure Uvicorn for in-process execution
            config = uvicorn.Config(
                app=app,
                host="0.0.0.0",
                port=self._backend_port,
                log_level="info",
                access_log=True,
                # Disable reload in production mode
                reload=False,
                # Use the supervisor's event loop
                loop="auto",
            )

            self._backend_server = uvicorn.Server(config)

            # CRITICAL: Disable Uvicorn's signal handlers
            # The supervisor manages signals - Uvicorn must not intercept them
            self._backend_server.install_signal_handlers = lambda: None

            # Start as background task in supervisor's event loop
            self._backend_task = asyncio.create_task(
                self._backend_server.serve(),
                name="backend-uvicorn-v111"
            )

            # Wait for server to be ready with timeout
            max_wait = float(os.getenv("BACKEND_STARTUP_TIMEOUT", "30.0"))
            start_time = time.time()

            while not self._backend_server.started:
                if time.time() - start_time > max_wait:
                    raise TimeoutError(f"Backend didn't start within {max_wait}s")

                # Check if task failed early
                if self._backend_task.done():
                    exc = self._backend_task.exception()
                    if exc:
                        raise exc
                    raise RuntimeError("Backend task exited unexpectedly")

                await asyncio.sleep(0.1)

            elapsed = time.time() - start_time
            self.logger.info(
                f"[v111.0] âœ… Backend started in-process on port {self._backend_port} "
                f"in {elapsed:.1f}s"
            )
            TerminalUI.print_success(
                f"[v111.0] Backend in-process: port {self._backend_port} ({elapsed:.1f}s)"
            )

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v111.1: IMMEDIATE SERVICE REGISTRY REGISTRATION (Phase 2 - Cross-Repo)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Register jarvis-body with the service registry IMMEDIATELY so external
            # services (J-Prime, Reactor-Core) can discover us without waiting.
            #
            # This eliminates the jarvis-body discovery race condition:
            # - OLD: External services wait for jarvis-body to appear in registry
            # - NEW: We register instantly because backend is in-process (no subprocess)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            try:
                from backend.core.service_registry import ServiceRegistry
                registry = ServiceRegistry()
                await registry.register_service(
                    service_name="jarvis-body",
                    pid=os.getpid(),  # Same PID as supervisor (in-process)
                    port=self._backend_port,
                    host="localhost",
                    health_endpoint="/health",
                    metadata={
                        "mode": "in-process",
                        "version": "v111.1",
                        "supervisor_pid": os.getpid(),
                        "startup_time_ms": int(elapsed * 1000),
                        "unified_monolith": True,
                    },
                    primary_port=self._backend_port,
                    is_fallback_port=False,
                )
                # Mark as healthy immediately since we verified startup
                await registry.heartbeat(
                    "jarvis-body",
                    status="healthy",
                    metadata={
                        "mode": "in-process",
                        "uptime_seconds": elapsed,
                    }
                )
                self.logger.info(
                    "[v111.1] âœ… jarvis-body registered in service registry (in-process mode)"
                )
            except Exception as reg_err:
                # Registry failure is not fatal - log and continue
                self.logger.warning(
                    f"[v111.1] âš ï¸ Service registry registration failed (non-fatal): {reg_err}"
                )

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v111.2: HEARTBEAT FILE WRITER FOR HEARTBEAT VALIDATOR
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # The HeartbeatValidator monitors heartbeat FILES in ~/.jarvis/trinity/heartbeats/
            # We must write heartbeat files for the in-process backend so the validator
            # doesn't mark jarvis_body as "dead" and trigger recovery cascades.
            #
            # CRITICAL FIX: Without this, HeartbeatValidator repeatedly marks
            # jarvis_body_{pid} as "dead" causing recovery loops that shut down the backend.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            try:
                self._inprocess_heartbeat_task = asyncio.create_task(
                    self._run_inprocess_heartbeat_loop(),
                    name="inprocess-heartbeat-v111.2"
                )
                self.logger.info(
                    "[v111.2] âœ… In-process heartbeat writer started (HeartbeatValidator compatibility)"
                )
            except Exception as hb_err:
                self.logger.warning(
                    f"[v111.2] âš ï¸ Heartbeat writer failed to start (non-fatal): {hb_err}"
                )

            return True

        except ImportError as e:
            self.logger.error(f"[v111.0] âŒ Failed to import backend: {e}")
            TerminalUI.print_error(f"[v111.0] Backend import failed: {e}")
            import traceback
            traceback.print_exc()
            return False

        except Exception as e:
            self.logger.error(f"[v111.0] âŒ Failed to start backend in-process: {e}")
            TerminalUI.print_error(f"[v111.0] Backend start failed: {e}")
            import traceback
            traceback.print_exc()
            return False

    async def _stop_backend_in_process(self, timeout: float = 10.0) -> None:
        """
        v111.0: Stop the in-process backend gracefully.

        Shutdown sequence:
        1. Deregister from service registry (v111.1)
        2. Signal server to exit (should_exit = True)
        3. Wait for graceful shutdown with timeout
        4. Cancel task if it doesn't respond
        5. Clean up references

        Args:
            timeout: Maximum time to wait for graceful shutdown
        """
        if not self._backend_server:
            return

        self.logger.info("[v111.0] Stopping backend...")
        TerminalUI.print_step("[v111.0] Stopping in-process backend")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v111.2: STOP HEARTBEAT WRITER FIRST
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Stop writing heartbeats before deregistering so HeartbeatValidator
        # doesn't try to trigger recovery during shutdown.
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if hasattr(self, "_inprocess_heartbeat_task") and self._inprocess_heartbeat_task:
            try:
                self._inprocess_heartbeat_task.cancel()
                await asyncio.wait_for(
                    asyncio.shield(self._inprocess_heartbeat_task),
                    timeout=2.0
                )
            except (asyncio.CancelledError, asyncio.TimeoutError):
                pass
            self._inprocess_heartbeat_task = None
            self.logger.debug("[v111.2] In-process heartbeat writer stopped")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v111.1: DEREGISTER FROM SERVICE REGISTRY FIRST
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Deregister jarvis-body so external services know we're shutting down
        # and don't try to connect to us during the shutdown window.
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        try:
            from backend.core.service_registry import ServiceRegistry
            registry = ServiceRegistry()
            await registry.deregister_service("jarvis-body")
            self.logger.info("[v111.1] âœ… jarvis-body deregistered from service registry")
        except Exception as reg_err:
            self.logger.debug(f"[v111.1] Service registry deregistration: {reg_err}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v111.2: CLEAN UP HEARTBEAT FILE
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        try:
            heartbeat_dir = Path.home() / ".jarvis" / "trinity" / "heartbeats"
            component_id = f"jarvis_body_{os.getpid()}"
            heartbeat_file = heartbeat_dir / f"{component_id}.json"
            if heartbeat_file.exists():
                heartbeat_file.unlink()
                self.logger.debug(f"[v111.2] Heartbeat file cleaned up: {heartbeat_file}")
        except Exception as cleanup_err:
            self.logger.debug(f"[v111.2] Heartbeat file cleanup: {cleanup_err}")

        try:
            # Signal graceful shutdown
            self._backend_server.should_exit = True

            if self._backend_task and not self._backend_task.done():
                try:
                    # Wait for graceful shutdown
                    await asyncio.wait_for(self._backend_task, timeout=timeout)
                    self.logger.info("[v111.0] âœ… Backend stopped gracefully")
                except asyncio.TimeoutError:
                    self.logger.warning(
                        f"[v111.0] Backend didn't stop within {timeout}s, cancelling"
                    )
                    self._backend_task.cancel()
                    try:
                        await self._backend_task
                    except asyncio.CancelledError:
                        pass
                    self.logger.info("[v111.0] âœ… Backend cancelled")

            TerminalUI.print_success("[v111.0] Backend stopped")

        except Exception as e:
            self.logger.error(f"[v111.0] Error stopping backend: {e}")
        finally:
            self._backend_server = None
            self._backend_task = None

    async def _run_inprocess_heartbeat_loop(self) -> None:
        """
        v111.2: Heartbeat file writer for in-process backend.

        Writes heartbeat files to ~/.jarvis/trinity/heartbeats/ so the
        HeartbeatValidator doesn't mark jarvis_body as "dead".

        CRITICAL: Without this, the HeartbeatValidator repeatedly marks
        jarvis_body_{pid} as dead, triggering recovery cascades that
        shut down the in-process backend.

        Heartbeat format matches what HeartbeatValidator expects:
        {
            "component_id": "jarvis_body_{pid}",
            "component_type": "jarvis_body",
            "timestamp": <unix_timestamp>,
            "pid": <pid>,
            "host": <hostname>,
            "version": "v111.2",
            "status": "running",
            "metrics": {...}
        }
        """
        import socket

        heartbeat_dir = Path.home() / ".jarvis" / "trinity" / "heartbeats"
        heartbeat_dir.mkdir(parents=True, exist_ok=True)

        # Also write to cross-repo dir for Trinity synchronization
        cross_repo_dir = Path.home() / ".jarvis" / "cross_repo"
        cross_repo_dir.mkdir(parents=True, exist_ok=True)

        component_id = f"jarvis_body_{os.getpid()}"
        heartbeat_interval = float(os.getenv("JARVIS_HEARTBEAT_INTERVAL", "10.0"))
        hostname = socket.gethostname()
        start_time = time.time()

        self.logger.info(
            f"[v111.2] Heartbeat writer started: {component_id} "
            f"(interval={heartbeat_interval}s)"
        )

        try:
            while True:
                try:
                    uptime = time.time() - start_time

                    heartbeat_data = {
                        "component_id": component_id,
                        "component_type": "jarvis_body",
                        "timestamp": time.time(),
                        "pid": os.getpid(),
                        "host": hostname,
                        "version": "v111.2",
                        "status": "running",
                        "metrics": {
                            "mode": "in-process",
                            "unified_monolith": True,
                            "uptime_seconds": round(uptime, 1),
                            "port": self._backend_port,
                            "backend_started": self._backend_server is not None and
                                              getattr(self._backend_server, "started", False),
                        },
                    }

                    heartbeat_json = json.dumps(heartbeat_data, indent=2)

                    # Write to primary heartbeat directory (atomic write)
                    heartbeat_file = heartbeat_dir / f"{component_id}.json"
                    tmp_file = heartbeat_file.with_suffix(".tmp")
                    tmp_file.write_text(heartbeat_json)
                    tmp_file.rename(heartbeat_file)

                    # Also write to cross-repo directory for Trinity sync
                    cross_repo_file = cross_repo_dir / f"{component_id}.json"
                    tmp_cross = cross_repo_file.with_suffix(".tmp")
                    tmp_cross.write_text(heartbeat_json)
                    tmp_cross.rename(cross_repo_file)

                except Exception as write_err:
                    self.logger.debug(f"[v111.2] Heartbeat write error: {write_err}")

                await asyncio.sleep(heartbeat_interval)

        except asyncio.CancelledError:
            # Clean up on cancellation
            self.logger.debug("[v111.2] Heartbeat writer cancelled")
            raise

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v113.0: LOADING SERVER AND FRONTEND STARTUP METHODS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # These methods fix the ERR_CONNECTION_REFUSED errors on ports 3000/3001
    # by actually starting the loading server and frontend processes.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def _start_loading_server_v113(self) -> bool:
        """
        v113.0: Start the loading server on port 3001 for startup progress display.

        The loading server serves a loading page that shows startup progress
        to the user while the backend initializes. Once the frontend is ready,
        the loading server is stopped and traffic is redirected to port 3000.

        Returns:
            True if loading server started successfully
        """
        self.logger.info(f"[v113.0] Starting loading server on port {self._loading_server_port}...")
        TerminalUI.print_step(f"[v113.0] Starting loading server (port {self._loading_server_port})")

        try:
            # Check for dedicated loading server script
            loading_server_path = self._jarvis_repo / "backend" / "loading_server.py"

            if loading_server_path.exists():
                # Use dedicated loading server
                self._loading_server_process = await asyncio.create_subprocess_exec(
                    sys.executable, str(loading_server_path),
                    env={
                        **os.environ,
                        "LOADING_SERVER_PORT": str(self._loading_server_port),
                    },
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
            else:
                # Fallback: serve frontend/public with Python's HTTP server
                public_dir = self._jarvis_repo / "frontend" / "public"
                if public_dir.exists():
                    self._loading_server_process = await asyncio.create_subprocess_exec(
                        sys.executable, "-m", "http.server", str(self._loading_server_port),
                        cwd=str(public_dir),
                        stdout=asyncio.subprocess.DEVNULL,
                        stderr=asyncio.subprocess.DEVNULL,
                    )
                else:
                    self.logger.warning("[v113.0] No loading server path found - skipping")
                    return False

            # Wait for process to start
            await asyncio.sleep(1.0)

            if self._loading_server_process.returncode is None:
                self.logger.info(
                    f"[v113.0] âœ… Loading server started on port {self._loading_server_port} "
                    f"(pid={self._loading_server_process.pid})"
                )
                TerminalUI.print_success(f"[v113.0] Loading server: port {self._loading_server_port}")
                return True
            else:
                self.logger.warning(
                    f"[v113.0] Loading server exited with code {self._loading_server_process.returncode}"
                )
                return False

        except Exception as e:
            self.logger.warning(f"[v113.0] Failed to start loading server: {e}")
            return False

    async def _start_frontend_v113(self) -> bool:
        """
        v113.0: Start the React frontend on port 3000.

        This should be called AFTER the backend is verified healthy.
        The frontend serves the main JARVIS UI.

        Returns:
            True if frontend started successfully
        """
        self.logger.info(f"[v113.0] Starting frontend on port {self._frontend_port}...")
        TerminalUI.print_step(f"[v113.0] Starting frontend (port {self._frontend_port})")

        try:
            frontend_dir = self._jarvis_repo / "frontend"

            if not frontend_dir.exists():
                self.logger.warning(f"[v113.0] Frontend directory not found: {frontend_dir}")
                return False

            # Check if node_modules exists
            node_modules = frontend_dir / "node_modules"
            if not node_modules.exists():
                self.logger.info("[v113.0] Installing frontend dependencies...")
                TerminalUI.print_step("[v113.0] npm install (first run)")
                npm_install = await asyncio.create_subprocess_exec(
                    "npm", "install",
                    cwd=str(frontend_dir),
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
                try:
                    await asyncio.wait_for(npm_install.wait(), timeout=300.0)
                except asyncio.TimeoutError:
                    self.logger.warning("[v113.0] npm install timed out")
                    return False
                if npm_install.returncode != 0:
                    self.logger.warning("[v113.0] npm install failed")
                    return False

            # Start the frontend dev server
            env = {
                **os.environ,
                "PORT": str(self._frontend_port),
                "BROWSER": "none",  # Don't auto-open browser
                "REACT_APP_BACKEND_URL": f"http://localhost:{self._backend_port}",
            }

            self._frontend_process = await asyncio.create_subprocess_exec(
                "npm", "start",
                cwd=str(frontend_dir),
                env=env,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )

            # Wait for frontend to be ready (poll health endpoint)
            import aiohttp
            deadline = time.time() + 120.0  # 2 minute timeout for webpack
            check_interval = 3.0

            while time.time() < deadline:
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.head(
                            f"http://localhost:{self._frontend_port}/",
                            timeout=aiohttp.ClientTimeout(total=5.0)
                        ) as resp:
                            if resp.status in (200, 304):
                                self.logger.info(
                                    f"[v113.0] âœ… Frontend ready on port {self._frontend_port} "
                                    f"(pid={self._frontend_process.pid})"
                                )
                                TerminalUI.print_success(
                                    f"[v113.0] Frontend: http://localhost:{self._frontend_port}"
                                )
                                return True
                except Exception:
                    pass

                # Check if process died
                if self._frontend_process.returncode is not None:
                    self.logger.warning(
                        f"[v113.0] Frontend exited with code {self._frontend_process.returncode}"
                    )
                    return False

                await asyncio.sleep(check_interval)

            self.logger.warning("[v113.0] Frontend startup timeout (120s)")
            return False

        except Exception as e:
            self.logger.error(f"[v113.0] Failed to start frontend: {e}")
            return False

    async def _stop_loading_server_v113(self) -> None:
        """v113.0: Stop the loading server (called after frontend is ready)."""
        if self._loading_server_process and self._loading_server_process.returncode is None:
            self.logger.info("[v113.0] Stopping loading server...")
            try:
                self._loading_server_process.terminate()
                await asyncio.wait_for(self._loading_server_process.wait(), timeout=5.0)
            except asyncio.TimeoutError:
                self._loading_server_process.kill()
            self._loading_server_process = None

    async def _stop_frontend_v113(self) -> None:
        """v113.0: Stop the frontend (called during shutdown)."""
        if self._frontend_process and self._frontend_process.returncode is None:
            self.logger.info("[v113.0] Stopping frontend...")
            try:
                self._frontend_process.terminate()
                await asyncio.wait_for(self._frontend_process.wait(), timeout=5.0)
            except asyncio.TimeoutError:
                self._frontend_process.kill()
            self._frontend_process = None

    async def _run_with_deep_health(self) -> int:
        """
        v80.0: Run startup with deep health verification.

        This wraps the main startup sequence and verifies components
        are actually functional, not just responding to HTTP.
        """
        try:
            # v108.3: Reset global shutdown flag at the very beginning
            # This clears any stale shutdown state from previous runs or
            # interrupted startups. Critical for ensuring recovery mechanisms
            # work correctly and services don't skip initialization.
            try:
                from backend.core.resilience.graceful_shutdown import reset_global_shutdown
                reset_global_shutdown()
                self.logger.debug("[v108.3] Global shutdown flag reset at startup")
            except ImportError:
                pass  # Module not available
            except Exception as e:
                self.logger.debug(f"[v108.3] Could not reset global shutdown: {e}")

            # Setup signal handlers
            self._setup_signal_handlers()

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v113.0: START LOADING SERVER FIRST (shows progress on port 3001)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # This is the FIX for "ERR_CONNECTION_REFUSED on port 3001".
            # The loading server must be running BEFORE ANYTHING ELSE so users
            # can see startup progress in their browser.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            await self._start_loading_server_v113()

            # Print banner
            TerminalUI.print_banner()

            # v9.0: Log Hyper-Runtime Engine status
            runtime_icons = {3: "âš¡", 2: "ğŸš€", 1: "ğŸ"}
            runtime_descs = {
                3: "Granian Rust/Tokio (3-5x faster)",
                2: "uvloop C/libuv (2-4x faster)",
                1: "asyncio Python (standard)",
            }
            icon = runtime_icons.get(_HYPER_RUNTIME_LEVEL, "â“")
            desc = runtime_descs.get(_HYPER_RUNTIME_LEVEL, "unknown")

            self.logger.info(f"{icon} [HYPER-RUNTIME] {_HYPER_RUNTIME_NAME} Engine ACTIVE - {desc}")
            TerminalUI.print_success(f"Hyper-Runtime: {_HYPER_RUNTIME_NAME} (Level {_HYPER_RUNTIME_LEVEL}/3)")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v87.0: Pre-Flight Resource Checks
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Verify system resources before attempting to acquire ownership.
            # This catches network partitions, filesystem issues, disk space
            # problems, and clock skew early in startup.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            preflight_passed = await self._run_v87_preflight_checks()
            if not preflight_passed:
                self.logger.error("[v87.0] âŒ Pre-flight checks failed - aborting startup")
                TerminalUI.print_error("[v87.0] Pre-flight checks failed")
                return 1

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v4.0: SERVICE REGISTRY PRE-FLIGHT CLEANUP
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # CRITICAL: Clean up stale/dead service entries BEFORE starting
            # any services. This prevents "dead PID" warnings and ensures
            # reliable service discovery across JARVIS, J-Prime, and Reactor-Core.
            #
            # This cleanup:
            # 1. Removes services with dead PIDs (process no longer running)
            # 2. Detects PID reuse (same PID but different process)
            # 3. Removes invalid entries (bad port, bad PID)
            # 4. Reports detailed cleanup statistics
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            try:
                from backend.core.service_registry import ServiceRegistry

                self.logger.info("[v4.0] ğŸ§¹ Running service registry pre-flight cleanup...")
                TerminalUI.print_step("[v4.0] Service registry pre-flight cleanup")

                registry = ServiceRegistry()
                cleanup_stats = await registry.pre_flight_cleanup()

                # Log cleanup statistics
                removed_total = (
                    len(cleanup_stats.get("removed_dead_pid", [])) +
                    len(cleanup_stats.get("removed_pid_reuse", [])) +
                    len(cleanup_stats.get("removed_invalid", []))
                )

                if removed_total > 0:
                    self.logger.info(
                        f"[v4.0] âœ… Cleaned {removed_total} stale entries "
                        f"(dead_pid: {len(cleanup_stats.get('removed_dead_pid', []))}, "
                        f"pid_reuse: {len(cleanup_stats.get('removed_pid_reuse', []))}, "
                        f"invalid: {len(cleanup_stats.get('removed_invalid', []))})"
                    )
                    TerminalUI.print_success(
                        f"[v4.0] Cleaned {removed_total} stale services "
                        f"({cleanup_stats['valid_entries']} valid remain)"
                    )
                else:
                    self.logger.info(
                        f"[v4.0] âœ… Registry clean "
                        f"({cleanup_stats['valid_entries']} valid services)"
                    )
                    TerminalUI.print_success(
                        f"[v4.0] Registry clean ({cleanup_stats['valid_entries']} valid services)"
                    )

                # Store registry reference for later use
                self._service_registry = registry

            except ImportError as e:
                self.logger.warning(f"[v4.0] ServiceRegistry not available: {e}")
            except Exception as e:
                self.logger.warning(
                    f"[v4.0] Service registry cleanup failed (continuing): {e}",
                    exc_info=True
                )

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v88.0: Ultra-Advanced Coordinator Initialization
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Initialize the ultra coordinator with adaptive circuit breakers,
            # backpressure handling, and distributed tracing.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self._ultra_coordinator: Optional[Any] = None
            enable_ultra = os.environ.get("JARVIS_ENABLE_ULTRA_COORD", "true").lower() in ("1", "true", "yes")

            if enable_ultra:
                try:
                    from backend.core.trinity_integrator import get_ultra_coordinator
                    self._ultra_coordinator = await get_ultra_coordinator()

                    status = self._ultra_coordinator.get_status()
                    self.logger.info(f"[v88.0] âœ… Ultra coordinator v{status['version']} initialized")
                    TerminalUI.print_success(f"[v88.0] Ultra coordinator v{status['version']}")

                    # Log container info if detected
                    if status.get("container", {}).get("is_containerized"):
                        cgroup_ver = status["container"].get("cgroup_version")
                        self.logger.info(f"[v88.0] Running in container (cgroup v{cgroup_ver})")
                        TerminalUI.print_step(f"[v88.0] Container detected (cgroup v{cgroup_ver})")

                except Exception as e:
                    self.logger.warning(f"[v88.0] Ultra coordinator init failed (continuing): {e}")
                    self._ultra_coordinator = None
            else:
                self.logger.info("[v88.0] Ultra coordinator disabled via JARVIS_ENABLE_ULTRA_COORD=false")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v85.0: Unified State Coordination - Acquire Exclusive Ownership
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # This prevents race conditions between run_supervisor.py and start_system.py
            # by using atomic fcntl locks with process cookies for reliable ownership
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self._state_coordinator: Optional["UnifiedStateCoordinator"] = None
            self._ownership_acquired = False
            self._heartbeat_task: Optional[asyncio.Task] = None

            try:
                from backend.core.trinity_integrator import (
                    UnifiedStateCoordinator,
                    TrinityEntryPointDetector,
                    ResourceChecker,
                )

                # Detect which entry point we are
                entry_info = TrinityEntryPointDetector.detect_entry_point()
                entry_point = entry_info.get("entry_point", "run_supervisor.py")
                self.logger.info(f"[v85.0] Entry point detected: {entry_point}")
                TerminalUI.print_success(f"[v85.0] Entry point: {entry_point}")

                # v95.0: ATOMIC OWNERSHIP ACQUISITION - fixes TOCTOU race condition
                # Previously: should_manage_trinity() -> acquire_ownership() had a gap
                # where two processes could both get True and race to acquire.
                # Now: Always attempt atomic lock acquisition directly. The fcntl lock
                # ensures only one process wins, eliminating the race condition.
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                self._state_coordinator = UnifiedStateCoordinator()

                # First, cleanup any stale owners (crash recovery)
                try:
                    cleaned = await self._state_coordinator._cleanup_stale_owners()
                    if cleaned > 0:
                        self.logger.info(f"[v95.0] Cleaned {cleaned} stale owner(s)")
                except Exception as e:
                    self.logger.debug(f"[v95.0] Stale cleanup skipped: {e}")

                # ATOMIC: Try to acquire ownership with fcntl lock
                # This is the ONLY place where ownership is decided - no pre-check needed
                acquired, existing_owner = await self._state_coordinator.acquire_ownership(
                    entry_point=entry_point,
                    component="jarvis",  # v85.0: Unified component name
                    timeout=30.0,
                    force=False,  # Don't force - respect existing owners
                )

                if acquired:
                    self._ownership_acquired = True
                    self.logger.info("[v95.0] âœ… Ownership acquired successfully (atomic lock)")
                    TerminalUI.print_success("[v95.0] Exclusive ownership acquired (atomic lock)")

                    # Start heartbeat loop to maintain ownership
                    self._heartbeat_task = await self._state_coordinator.start_heartbeat_loop(
                        component="jarvis",  # v85.0: Unified component name
                        interval=5.0,  # 5-second heartbeat
                    )
                    self.logger.debug("[v95.0] Heartbeat loop started")
                else:
                    # v95.0: Didn't acquire - but this is fine, we continue as subordinate
                    # The atomic lock ensures no race condition - someone else won fairly
                    if existing_owner:
                        owner_pid = getattr(existing_owner, 'pid', 'unknown')
                        owner_entry = getattr(existing_owner, 'entry_point', 'unknown')
                        self.logger.info(
                            f"[v95.0] Another process owns Trinity: "
                            f"PID={owner_pid}, entry={owner_entry} - continuing as subordinate"
                        )
                        TerminalUI.print_warning(
                            f"[v95.0] Trinity managed by PID {owner_pid} ({owner_entry}) - subordinate mode"
                        )
                    else:
                        self.logger.warning("[v95.0] Could not acquire ownership (unknown reason)")

                # v86.0: Resource pre-flight check with auto cloud offloading
                # Verify we have enough resources before proceeding
                # NEW: High CPU/Low Memory now AUTO-ACTIVATES GCP cloud offloading
                can_proceed, issues = await ResourceChecker.check_resources_for_component("jarvis_body")

                # v86.0: Check if cloud offloading was triggered
                if ResourceChecker.is_cloud_offloading_enabled():
                    cloud_reason = ResourceChecker.get_cloud_offloading_reason()
                    TerminalUI.print_success(f"â˜ï¸  [v86.0] GCP Cloud Offloading ACTIVATED")
                    TerminalUI.print_info(f"   â†’ Reason: {cloud_reason}")
                    TerminalUI.print_info(f"   â†’ ML workloads will be processed by GCP Spot VM")
                    self.logger.info(f"[v86.0] â˜ï¸  Cloud offloading enabled: {cloud_reason}")

                if not can_proceed:
                    for issue in issues:
                        TerminalUI.print_error(f"[v86.0] Resource issue: {issue}")
                    self.logger.error(f"[v86.0] Resource check failed: {issues}")
                    # Don't fail hard - log and continue with degraded mode
                    TerminalUI.print_warning("[v86.0] Continuing with degraded resources...")
                else:
                    if ResourceChecker.is_cloud_offloading_enabled():
                        self.logger.info("[v86.0] âœ… Resource check passed (with cloud offloading)")
                    else:
                        self.logger.info("[v86.0] âœ… Resource pre-flight check passed (full local mode)")

            except ImportError as e:
                self.logger.debug(f"[v85.0] State coordination not available: {e}")
            except Exception as e:
                self.logger.warning(f"[v85.0] State coordination error (non-fatal): {e}")
                # Non-fatal - continue without coordination

            # Phase 1: Cleanup existing instances
            self.perf.start("cleanup")
            TerminalUI.print_phase(1, 4, "Checking for existing instances")
            
            cleaner = ParallelProcessCleaner(self.config, self.logger, self.narrator)
            terminated, discovered = await cleaner.discover_and_cleanup()
            
            if discovered:
                TerminalUI.print_process_list(discovered)
                TerminalUI.print_success(f"Terminated {terminated} instance(s)")

                # Wait for ports to be fully released with verification
                await self._wait_for_ports_release()
            else:
                TerminalUI.print_success("No existing instances found")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # CRITICAL: Signal to start_system.py that cleanup was already done
            # This prevents the duplicate "already running on port 8010" warning
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            os.environ["JARVIS_CLEANUP_DONE"] = "1"
            os.environ["JARVIS_CLEANUP_TIMESTAMP"] = str(time.time())
            self.logger.info("Set JARVIS_CLEANUP_DONE=1 - start_system.py will skip redundant cleanup")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v1.0 ProcessCoordinationHub: Enhanced cross-script coordination
            # Provides: supervisor heartbeat, atomic port locks, shared state
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            try:
                from backend.core.trinity_process_coordination import (
                    get_coordination_hub,
                    EntryPoint,
                )

                self._coord_hub = await get_coordination_hub()
                coord_success, coord_warnings = await self._coord_hub.initialize(
                    EntryPoint.RUN_SUPERVISOR
                )

                if coord_warnings:
                    for warn in coord_warnings:
                        self.logger.warning(f"[CoordHub] {warn}")

                # Mark cleanup complete through the hub (persists to shared state file)
                await self._coord_hub.mark_cleanup_complete()
                self.logger.info("[CoordHub] âœ… ProcessCoordinationHub initialized with heartbeat")
                TerminalUI.print_success("[CoordHub] Enhanced coordination active")

            except ImportError as e:
                self.logger.debug(f"[CoordHub] Not available (optional): {e}")
                self._coord_hub = None
            except Exception as e:
                self.logger.warning(f"[CoordHub] Initialization failed (continuing without): {e}")
                self._coord_hub = None

            self.perf.end("cleanup")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v111.3: Start backend in-process BEFORE cross-repo orchestration
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # CRITICAL FIX: The cross-repo orchestrator checks for jarvis-body health
            # and assumes the backend is already running if JARVIS_IN_PROCESS_MODE=true.
            # We MUST start the in-process backend HERE so it's actually running when
            # the orchestrator verifies it.
            #
            # Previous bug: Orchestrator ran first, saw in-process mode, assumed backend
            # was running, but _start_backend_in_process() wasn't called until later.
            # Result: Nothing was actually listening on port 8010.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self._in_process_mode:
                self.logger.info("[v111.3] Starting backend in-process BEFORE cross-repo orchestration...")
                backend_started = await self._start_backend_in_process()
                if not backend_started:
                    self.logger.error("[v111.3] Backend failed to start in-process mode - aborting")
                    TerminalUI.print_error("[v111.3] Backend startup failed")
                    return 1
                self.logger.info("[v111.3] âœ… Backend running - proceeding with cross-repo orchestration")

                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # v113.0: START FRONTEND AS BACKGROUND TASK (non-blocking)
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # CRITICAL: Frontend startup waits for webpack (up to 120s).
                # We MUST run this as a background task so cross-repo orchestration
                # and other critical initialization can proceed in parallel.
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                async def _frontend_startup_background():
                    """Background task for frontend startup."""
                    try:
                        frontend_started = await self._start_frontend_v113()
                        if frontend_started:
                            # Frontend is ready - stop loading server
                            await self._stop_loading_server_v113()
                            self.logger.info(f"ğŸš€ [v113.0] JARVIS is online at http://localhost:{self._frontend_port}")
                            TerminalUI.print_success(f"ğŸš€ JARVIS ready: http://localhost:{self._frontend_port}")
                        else:
                            # Frontend failed - keep loading server running as fallback
                            self.logger.warning("[v113.0] Frontend failed - loading page remains active on port 3001")
                    except asyncio.CancelledError:
                        self.logger.info("[v113.0] Frontend startup cancelled")
                    except Exception as e:
                        self.logger.warning(f"[v113.0] Frontend startup error: {e}")

                # Start frontend in background (non-blocking)
                self._frontend_startup_task = asyncio.create_task(
                    _frontend_startup_background(),
                    name="frontend-startup-v113"
                )
                self.logger.info("[v113.0] Frontend startup initiated (background task)")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v10.1: Cross-Repo Startup Orchestration (MUST RUN BEFORE TrinityIntegrator)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Automatically probe and launch J-Prime and Reactor-Core if not running.
            # This MUST happen before TrinityIntegrator starts so that the services
            # are already running when TrinityIntegrator checks for them.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            try:
                from backend.supervisor.cross_repo_startup_orchestrator import initialize_cross_repo_orchestration

                self.logger.info("ğŸš€ [v10.1] Pre-launching external services...")
                self.logger.info("ğŸ“‹ [v108.1] Non-blocking model loading enabled - services will be 'started' once responding")
                self.logger.info("    (Model loading will continue in background while main JARVIS backend starts)")
                await initialize_cross_repo_orchestration()
                self.logger.info("âœ… [v10.1] External services launched (servers responding)")

            except ImportError as e:
                self.logger.warning(f"âš ï¸ [v10.1] Cross-Repo Orchestrator not available: {e}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ [v10.1] Cross-Repo Orchestration failed: {e}")
                self.logger.debug(f"[v10.1] Error details: {e}", exc_info=True)

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v81.0: TrinityIntegrator - Unified Cross-Repo Integration
            # Provides: orphan cleanup, resilient IPC, port allocation, shutdown
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self._trinity_integrator_enabled:
                try:
                    from backend.core.trinity_integrator import TrinityIntegrator

                    # Create integrator with environment-aware configuration
                    # Environment variables JARVIS_PRIME_ENABLED and REACTOR_CORE_ENABLED
                    # will override the defaults passed here
                    self._trinity_integrator = TrinityIntegrator(
                        enable_jprime=self.config.jarvis_prime_enabled,
                        enable_reactor=self.config.reactor_core_enabled,
                        startup_timeout=self.config.jarvis_prime_startup_timeout,
                        health_check_interval=30.0,
                    )

                    # Start the integrator (orphan cleanup, IPC, port allocation, health)
                    TerminalUI.print_success("[v81.0] TrinityIntegrator starting...")
                    start_success = await self._trinity_integrator.start()

                    if start_success:
                        # Register shutdown hooks with supervisor signals
                        await self._register_trinity_shutdown_hooks()

                        state = self._trinity_integrator.state.value
                        TerminalUI.print_success(f"[v81.0] TrinityIntegrator ready (state={state})")
                        self.logger.info(f"[v81.0] âœ… TrinityIntegrator started successfully (state={state})")

                        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        # v85.0: Comprehensive Startup Verification
                        # Verifies all Trinity components are truly functional:
                        # - Heartbeat files exist and are fresh
                        # - Health endpoints respond (if applicable)
                        # - Cross-repo discovery succeeded
                        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        try:
                            verification_results = await self._verify_trinity_startup_v85()
                            if verification_results["all_verified"]:
                                TerminalUI.print_success("[v85.0] All Trinity components verified and healthy")
                                self.logger.info("[v85.0] âœ… Cross-repo verification complete")
                            else:
                                failed = [k for k, v in verification_results.get("components", {}).items() if not v]
                                if failed:
                                    self.logger.warning(f"[v85.0] Some components not verified: {failed}")
                                    TerminalUI.print_warning(f"[v85.0] Components pending: {', '.join(failed)}")
                        except Exception as verify_err:
                            self.logger.debug(f"[v85.0] Verification check skipped: {verify_err}")

                    else:
                        self.logger.warning("[v81.0] TrinityIntegrator start returned false - continuing with fallback")
                        self._trinity_integrator = None

                except ImportError as e:
                    self.logger.warning(f"[v81.0] TrinityIntegrator not available: {e}")
                    self._trinity_integrator = None
                except Exception as e:
                    self.logger.warning(f"[v81.0] TrinityIntegrator initialization failed: {e}")
                    self._trinity_integrator = None

            # Phase 2: Intelligent Resource Validation & Optimization
            self.perf.start("validation")
            TerminalUI.print_phase(2, 4, "Analyzing system resources")

            orchestrator = IntelligentResourceOrchestrator(self.config, self.logger)
            resources = await orchestrator.validate_and_optimize()

            # Show actions taken (if any)
            if resources.actions_taken:
                for action in resources.actions_taken:
                    print(f"  {TerminalUI.CYAN}âš¡ {action}{TerminalUI.RESET}")

            # Show errors (critical)
            if resources.errors:
                for error in resources.errors:
                    TerminalUI.print_error(error)
                await self.narrator.speak("System resources insufficient. Please check the logs.", wait=True)
                return 1

            # Show warnings
            if resources.warnings:
                for warning in resources.warnings:
                    TerminalUI.print_warning(warning)

            # Show recommendations (intelligent)
            if resources.recommendations:
                for rec in resources.recommendations:
                    print(f"  {TerminalUI.CYAN}{rec}{TerminalUI.RESET}")

            # Show startup mode decision
            if resources.startup_mode:
                mode_display = {
                    "local_full": "ğŸ  Full Local Mode",
                    "cloud_first": "â˜ï¸  Cloud-First Mode",
                    "cloud_only": "â˜ï¸  Cloud-Only Mode"
                }.get(resources.startup_mode, resources.startup_mode)
                print(f"  {TerminalUI.GREEN}Mode: {mode_display}{TerminalUI.RESET}")

                # Voice announcement for cloud mode
                if resources.is_cloud_mode:
                    await self.narrator.speak(
                        "Using cloud mode for optimal performance.",
                        wait=False
                    )

            # Summary
            if not resources.warnings and not resources.errors:
                TerminalUI.print_success(
                    f"Resources OK ({resources.memory_available_gb:.1f}GB RAM, "
                    f"{resources.disk_available_gb:.1f}GB disk)"
                )

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # CRITICAL: Propagate Intelligent Decisions to Child Processes
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # These environment variables are inherited by start_system.py and
            # all JARVIS processes, ensuring the orchestrator's decisions are
            # respected throughout the system.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if resources.startup_mode:
                os.environ["JARVIS_STARTUP_MODE"] = resources.startup_mode
                self.logger.info(f"ğŸ”„ Propagating JARVIS_STARTUP_MODE={resources.startup_mode}")

            if resources.cloud_activated:
                os.environ["JARVIS_CLOUD_ACTIVATED"] = "true"
                os.environ["JARVIS_PREFER_CLOUD_RUN"] = "true"
                self.logger.info("ğŸ”„ Propagating JARVIS_CLOUD_ACTIVATED=true")

            if resources.arm64_simd_available:
                os.environ["JARVIS_ARM64_SIMD"] = "true"
                self.logger.info("ğŸ”„ Propagating JARVIS_ARM64_SIMD=true")

            # Propagate memory constraints for adaptive loading
            os.environ["JARVIS_AVAILABLE_RAM_GB"] = str(round(resources.memory_available_gb, 1))
            os.environ["JARVIS_TOTAL_RAM_GB"] = str(round(resources.memory_total_gb, 1))

            # CRITICAL: Optimize Frontend Memory & Timeout based on resources
            # Reduce Node memory to 2GB (from default 4GB) to prevent relief pressure
            os.environ["JARVIS_FRONTEND_MEMORY_MB"] = "2048"
            # v5.2: Reduced frontend timeout to 60s (was 600s which blocked startup)
            # Frontend is optional - backend can complete startup without it
            # The loading page now has graceful fallback when frontend isn't available
            os.environ["JARVIS_FRONTEND_TIMEOUT"] = "60"
            # Signal that frontend is optional (loading page will show fallback options)
            os.environ["FRONTEND_OPTIONAL"] = "true"
            self.logger.info("ğŸ”„ Configured Frontend: 2GB RAM limit, 60s timeout, optional mode")

            # Propagate warnings for downstream handling
            if resources.warnings:
                os.environ["JARVIS_STARTUP_WARNINGS"] = "|".join(resources.warnings[:5])

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v3.0: Propagate Zero-Touch & AGI OS Settings to Child Processes
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            await self._propagate_zero_touch_settings()
            await self._propagate_agi_os_settings()
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v78.0: Initialize Advanced Startup Orchestrator
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # This enables enterprise-grade startup patterns:
            # - Dependency graph resolution (Kahn's algorithm)
            # - Circuit breakers with exponential backoff
            # - Dynamic configuration discovery (zero hardcoding)
            # - Connection verification loops
            # - Graceful degradation for non-critical components
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            try:
                from core.supervisor_orchestrator_bridge import (
                    enhance_supervisor_with_orchestrator,
                    OrchestratorBridgeConfig,
                )

                orchestrator_config = OrchestratorBridgeConfig.from_env()
                if orchestrator_config.enabled:
                    self._orchestrator_hooks = await enhance_supervisor_with_orchestrator(
                        bootstrapper=self,
                        config=orchestrator_config,
                    )

                    # Log discovered configuration
                    if self._orchestrator_hooks.discovered_config:
                        config = self._orchestrator_hooks.discovered_config
                        self.logger.info("[Orchestrator] Dynamic configuration discovered:")
                        self.logger.info(f"  Repos: {len(config.repo_paths)}")
                        self.logger.info(f"  Trinity dir: {config.trinity_dir}")
                        self.logger.info(f"  Ports: {config.ports}")

                        # Update paths from discovered config
                        from core.advanced_startup_orchestrator import TrinityRepo
                        if TrinityRepo.JARVIS_PRIME in config.repo_paths:
                            self._jprime_repo_path = config.repo_paths[TrinityRepo.JARVIS_PRIME]
                        if TrinityRepo.REACTOR_CORE in config.repo_paths:
                            self._reactor_core_repo_path = config.repo_paths[TrinityRepo.REACTOR_CORE]

                    print(f"  {TerminalUI.GREEN}âœ“ Advanced Startup Orchestrator: Active{TerminalUI.RESET}")
                    print(f"    â†’ Dependency graph, circuit breakers, dynamic discovery enabled")
                else:
                    self._orchestrator_hooks = None
                    self.logger.debug("[Orchestrator] Disabled via configuration")

            except ImportError as e:
                self._orchestrator_hooks = None
                self.logger.debug(f"[Orchestrator] Module not available: {e}")
            except Exception as e:
                self._orchestrator_hooks = None
                self.logger.warning(f"[Orchestrator] Initialization failed: {e}")
                print(f"  {TerminalUI.YELLOW}âš ï¸ Advanced Orchestrator: Not available{TerminalUI.RESET}")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v78.0: Initialize Advanced Process Management Components
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # This enables enterprise-grade process management:
            # - Unified Process Tree: Track entire process hierarchy
            # - Command Buffer: Handle early commands before system ready
            # - Atomic Command Queue: Race-condition-free Trinity transport
            # - Cross-Repo Coordinator: Atomic commits across JARVIS/J-Prime/Reactor
            # - Adaptive Timeout: Dynamic timeouts based on history
            # - Intelligent Retry: Context-aware retry with circuit breakers
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            try:
                from core.coding_council.advanced import (
                    get_process_tree,
                    get_command_buffer,
                    get_atomic_queue,
                    get_transaction_coordinator,
                    get_timeout_manager,
                    get_retry_manager,
                    ProcessRole,
                    JARVIS_TO_JPRIME,
                )
                # NOTE: os is imported at module level (line 236), do not reimport here
                # Reimporting inside the function causes UnboundLocalError at line 2453

                # Initialize Process Tree Manager
                self._process_tree = await get_process_tree()
                await self._process_tree.register_process(
                    pid=os.getpid(),
                    name="run_supervisor.py",
                    role=ProcessRole.SUPERVISOR,
                    parent_pid=None,  # Root of tree
                    critical=True,
                    metadata={"version": "v78.0", "started_at": time.time()},
                )
                await self._process_tree.start_monitoring(interval=10.0)

                # Initialize Command Buffer for early Trinity commands
                self._command_buffer = await get_command_buffer()
                # Executor will be set when Coding Council is ready

                # Initialize Trinity Queues (atomic, race-condition-free)
                self._trinity_queues = {
                    "jarvis_to_jprime": await get_atomic_queue(JARVIS_TO_JPRIME),
                }

                # Initialize Cross-Repo Transaction Coordinator
                self._tx_coordinator = await get_transaction_coordinator()

                # Initialize Adaptive Timeout Manager
                self._timeout_manager = await get_timeout_manager()

                # Initialize Intelligent Retry Manager
                self._retry_manager = await get_retry_manager()

                print(f"  {TerminalUI.GREEN}âœ“ Process Management v78.0: Active{TerminalUI.RESET}")
                print(f"    â†’ Process tree, command buffer, atomic queues, cross-repo tx")
                print(f"    â†’ Adaptive timeouts, intelligent retry enabled")

            except ImportError as e:
                self._process_tree = None
                self._command_buffer = None
                self._trinity_queues = {}
                self._tx_coordinator = None
                self._timeout_manager = None
                self._retry_manager = None
                self.logger.debug(f"[ProcessMgmt] Module not available: {e}")
            except Exception as e:
                self._process_tree = None
                self._command_buffer = None
                self._trinity_queues = {}
                self._tx_coordinator = None
                self._timeout_manager = None
                self._retry_manager = None
                self.logger.warning(f"[ProcessMgmt] Initialization failed: {e}")
                print(f"  {TerminalUI.YELLOW}âš ï¸ Process Management v78.0: Not available{TerminalUI.RESET}")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v5.0: Initialize Intelligent Rate Orchestrator (ML Forecasting)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # This starts the ML-powered rate limiting system that:
            # - Predicts rate limit breaches BEFORE they happen
            # - Adaptively throttles GCP, CloudSQL, and Claude API calls
            # - Uses time-series forecasting for intelligent request scheduling
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            await self._initialize_rate_orchestrator()

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v6.0: Initialize Two-Tier Agentic Security System
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # This enables the Two-Tier security model:
            # - Tier 1 (JARVIS): Safe APIs, read-only, Gemini Flash
            # - Tier 2 (JARVIS ACCESS): Full Computer Use, strict VBIA, Claude
            # - Watchdog monitors heartbeats and can trigger kill switch
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            await self._initialize_agentic_security()

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v7.0: Initialize JARVIS-Prime Tier-0 Local Brain
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # This starts the local inference server for cost-effective AI:
            # - Tier 0 (JARVIS-Prime): Local GGUF model, fast, free
            # - Supports local subprocess, Docker, or Cloud Run deployment
            # - Auto-integrates with Reactor-Core for model hot-swapping
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v107.0: Major initializations with timeout protection
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Each initialization is wrapped with _safe_phase_init() to prevent
            # any single component from blocking the entire startup indefinitely.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            major_init_timeout = float(os.getenv("JARVIS_INIT_TIMEOUT", "60.0"))

            # v9.4: Initialize Model Manager BEFORE JARVIS-Prime
            if self.config.model_manager_enabled:
                await self._safe_phase_init(
                    "Model Manager",
                    self._init_model_manager(),
                    timeout_seconds=major_init_timeout,
                )

            # v7.0: Initialize JARVIS-Prime (Local Brain)
            await self._safe_phase_init(
                "JARVIS-Prime (Local Brain)",
                self._initialize_jarvis_prime(),
                timeout_seconds=major_init_timeout,
            )

            # NOTE: Cross-Repo Orchestration (v10.1) moved earlier (see ~4961)

            # v9.0: Initialize Intelligence Systems (UAE/SAI/Neural Mesh/MAS)
            await self._safe_phase_init(
                "Intelligence Systems (UAE/SAI/MAS)",
                self._initialize_intelligence_systems(),
                timeout_seconds=major_init_timeout,
            )

            # v12.0: Initialize Docker Manager (Self-Healing) BEFORE infrastructure
            # Docker must be healthy before Cloud Run deployments, container builds, etc.
            await self._safe_phase_init(
                "Docker Manager (Self-Healing)",
                self._initialize_docker_manager(),
                timeout_seconds=120,  # Docker startup can take time
            )

            # v9.5: Initialize Infrastructure Orchestrator (On-Demand GCP)
            if self._infra_orchestrator_enabled:
                await self._safe_phase_init(
                    "Infrastructure Orchestrator",
                    self._initialize_infrastructure_orchestrator(),
                    timeout_seconds=major_init_timeout,
                )

            # v10.0: Reactor-Core API Server (Training Pipeline)
            if self._reactor_core_enabled:
                await self._safe_phase_init(
                    "Reactor-Core API",
                    self._initialize_reactor_core_api(),
                    timeout_seconds=major_init_timeout,
                )

            # v11.0: PROJECT TRINITY - Unified Cognitive Architecture
            # This initializes the Trinity network (JARVIS Body â†” J-Prime â†” Reactor Core)
            if self._trinity_enabled:
                await self._safe_phase_init(
                    "PROJECT TRINITY",
                    self._initialize_trinity(),
                    timeout_seconds=major_init_timeout * 2,  # Double timeout for Trinity (calls many PHASES)
                )

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v109.7: COMPREHENSIVE ZOMBIE CLEANUP - Pre-Trinity Cleanup
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Run comprehensive zombie cleanup BEFORE launching Trinity components.
            # This ensures:
            # 1. No stale J-Prime or Reactor-Core processes blocking ports
            # 2. Cross-repo orphans are cleaned from the registry
            # 3. Zombie processes (orphaned, stuck) are terminated
            # 4. Ports 8000 (J-Prime) and 8090 (Reactor-Core) are free
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self._trinity_enabled and self._trinity_auto_launch_enabled:
                try:
                    TerminalUI.print_step("[v109.7] Pre-Trinity zombie cleanup")

                    zombie_cleaner = ComprehensiveZombieCleanup(
                        config=self.config,
                        logger=self.logger,
                        enable_cross_repo=True,
                        enable_memory_aware=True,
                        enable_circuit_breaker=True,
                    )

                    cleanup_result = await zombie_cleaner.run_comprehensive_cleanup()

                    if cleanup_result["zombies_killed"] > 0:
                        TerminalUI.print_success(
                            f"[v109.7] Cleaned {cleanup_result['zombies_killed']} zombie(s), "
                            f"freed ports: {cleanup_result['ports_freed']}"
                        )
                        # Give ports time to fully release
                        await asyncio.sleep(0.5)
                    elif cleanup_result["zombies_found"] > 0:
                        TerminalUI.print_warning(
                            f"[v109.7] Found {cleanup_result['zombies_found']} zombie(s) but cleanup incomplete"
                        )
                    else:
                        TerminalUI.print_success("[v109.7] No zombies found - clean environment")

                    # Store cleanup stats for diagnostics
                    self._pre_trinity_cleanup_stats = zombie_cleaner.get_stats()

                except Exception as e:
                    self.logger.warning(f"[v109.7] Pre-Trinity cleanup error (continuing): {e}")
                    TerminalUI.print_warning(f"[v109.7] Cleanup warning: {e}")

            # v72.0: Auto-Launch Trinity Components (J-Prime + Reactor-Core)
            # v109.5: Use Trinity's configured timeout to avoid timeout hierarchy conflict.
            # The internal launch_timeout_sec is 120s by default; using 60s here caused
            # the outer timeout to kill the inner operation mid-flight.
            if self._trinity_enabled and self._trinity_auto_launch_enabled:
                # Get Trinity config to use its timeout
                try:
                    trinity_config = get_trinity_config()
                    # Add buffer for retry logic and health checks
                    trinity_timeout = trinity_config.launch_timeout_sec + 30.0
                except Exception:
                    # Fallback to generous timeout if config unavailable
                    trinity_timeout = 150.0

                await self._safe_phase_init(
                    "Trinity Components Launch",
                    self._launch_trinity_components(),
                    timeout_seconds=trinity_timeout,
                )

            # v77.0: Initialize Unified Coding Council (Self-Evolution Framework)
            if os.getenv("CODING_COUNCIL_ENABLED", "true").lower() == "true":
                try:
                    from core.coding_council.startup import coding_council_startup_hook
                    await self._safe_phase_init(
                        "Coding Council",
                        coding_council_startup_hook(
                            bootstrapper=self,
                            phase="supervisor_init"
                        ),
                        timeout_seconds=30.0,  # Shorter timeout for Coding Council
                    )
                    print(f"  {TerminalUI.GREEN}âœ“ Coding Council: Self-evolution framework active{TerminalUI.RESET}")
                except ImportError as e:
                    self.logger.info(f"[CodingCouncil] Not available: {e}")
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Coding Council: Not available{TerminalUI.RESET}")

            # v13.0: Initialize Collaboration & IDE Integration System
            if os.getenv("JARVIS_COLLABORATION_ENABLED", "true").lower() == "true":
                await self._safe_phase_init(
                    "Collaboration & IDE Integration",
                    self._initialize_collaboration_and_ide_systems(),
                    timeout_seconds=30.0,
                )

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v78.0: Verify Trinity Connections (Advanced Orchestrator)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Uses the advanced orchestrator to verify all Trinity components
            # are properly connected and healthy before proceeding.
            # v78.1: Pre-startup phase - skip HTTP checks since backend hasn't started yet.
            # Backend/CodingCouncil HTTP endpoints won't exist until supervisor.run() completes.
            # Only verify heartbeat-based components (J-Prime, Reactor-Core) at this stage.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if hasattr(self, '_orchestrator_hooks') and self._orchestrator_hooks:
                try:
                    self.logger.info("[Orchestrator] Verifying Trinity connections (pre-startup)...")
                    health_status = await self._orchestrator_hooks.verify_trinity_connections(
                        timeout=30.0,
                        skip_http_checks=True,  # v78.1: Backend hasn't started yet
                    )

                    if health_status.all_healthy:
                        print(f"  {TerminalUI.GREEN}âœ“ Trinity Connections: All healthy{TerminalUI.RESET}")
                        self.logger.info(f"[Orchestrator] Trinity health: {health_status.summary}")
                    else:
                        print(f"  {TerminalUI.YELLOW}âš ï¸ Trinity Connections: Partial ({health_status.summary}){TerminalUI.RESET}")
                        self.logger.warning(f"[Orchestrator] Trinity partial health: {health_status.summary}")

                        # Log specific statuses
                        if not health_status.jarvis_backend:
                            self.logger.warning("[Orchestrator] Backend not responding - may still be starting")
                        if not health_status.jarvis_prime:
                            self.logger.info("[Orchestrator] J-Prime not connected (optional)")
                        if not health_status.reactor_core:
                            self.logger.info("[Orchestrator] Reactor-Core not connected (optional)")

                    # Store health status for monitoring
                    self._trinity_health_status = health_status

                except Exception as e:
                    self.logger.debug(f"[Orchestrator] Health verification skipped: {e}")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v10.6: Start Real-Time Log Monitor with Voice Narrator Integration
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self._log_monitor_enabled:
                try:
                    self.logger.info("[LogMonitor] Starting real-time log monitoring with voice alerts")

                    # Create narrator callback for log monitor
                    async def log_monitor_narrator(message: str):
                        """Narrator callback for log monitor - speaks critical issues."""
                        try:
                            await self.narrator.speak(message, wait=False, priority=True)
                        except Exception as e:
                            self.logger.debug(f"[LogMonitor] Narrator error: {e}")

                    # Initialize and start log monitor
                    monitor_config = LogMonitorConfig.from_env()
                    self._log_monitor = await get_log_monitor(
                        config=monitor_config,
                        narrator=log_monitor_narrator,
                        logger=self.logger,
                    )

                    await self._log_monitor.start()

                    self.logger.info(
                        "[LogMonitor] Real-time monitoring active",
                        poll_interval=monitor_config.poll_interval,
                        error_threshold=monitor_config.critical_error_threshold,
                        voice_alerts_enabled=True,
                    )

                    print(f"  {TerminalUI.GREEN}âœ“ Real-Time Log Monitor: Active (voice alerts enabled){TerminalUI.RESET}")

                except Exception as e:
                    self.logger.warning(f"[LogMonitor] Failed to start: {e}")
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Real-Time Log Monitor: Disabled ({e}){TerminalUI.RESET}")

            self.perf.end("validation")

            # Phase 3: Initialize supervisor
            self.perf.start("supervisor_init")
            TerminalUI.print_phase(3, 4, "Initializing supervisor")
            print()
            
            from core.supervisor import JARVISSupervisor
            # v3.1: Pass skip_browser_open=True to prevent duplicate browser windows
            # run_supervisor.py handles browser opening in _start_loading_page_ecosystem()
            supervisor = JARVISSupervisor(skip_browser_open=True)
            self._supervisor = supervisor  # Store reference for hot reload
            
            self._print_config_summary(supervisor)
            self.perf.end("supervisor_init")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Phase 3.1: Initialize Enterprise Systems (Resilience + Data Management)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Initialize the enterprise-grade resilience and data management systems
            # that provide fault tolerance, data lifecycle management, and cross-repo
            # synchronization across the JARVIS Trinity ecosystem.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            await self._initialize_enterprise_systems()

            # Phase 3.5: Start Loading Page (BEFORE JARVIS spawns)
            TerminalUI.print_divider()
            print(f"  {TerminalUI.CYAN}ğŸŒ Starting Loading Page Server...{TerminalUI.RESET}")

            await self._start_loading_page_ecosystem()

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # CRITICAL FIX v20.1: Re-broadcast Two-Tier state AFTER loading server starts
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Two-Tier components initialize in Phase 2, BEFORE loading server starts.
            # This causes the frontend to show "Initializing..." forever because it
            # never received the *_ready broadcasts. Re-broadcast here to fix this.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self._loading_server_process:
                await self._rebroadcast_two_tier_state()

            # Phase 4: Start JARVIS
            TerminalUI.print_divider()
            TerminalUI.print_phase(4, 4, "Launching JARVIS Core")
            print()
            
            print(f"  {TerminalUI.YELLOW}ğŸ“¡ Watch real-time progress in the loading page!{TerminalUI.RESET}")
            
            if self.config.voice_enabled:
                print(f"  {TerminalUI.YELLOW}ğŸ”Š Voice narration enabled{TerminalUI.RESET}")
            
            print()
            
            # Run supervisor with startup monitoring
            self.perf.start("jarvis")
            
            # v4.0: Run startup monitoring in parallel with supervisor
            # This ensures completion is broadcast even if jarvis_supervisor's
            # internal monitoring doesn't complete (e.g., due to service initialization delays)
            if self._loading_server_process:
                # Start monitoring as a background task
                monitoring_task = asyncio.create_task(
                    self._monitor_jarvis_startup(max_wait=120.0)
                )
                self.logger.info("ğŸ” Startup monitoring task started")
            else:
                monitoring_task = None
            
            # v5.0: Start hot reload watcher (dev mode)
            await self._hot_reload.start()

            # v91.0: Start advanced monitoring tasks (ML prediction, self-healing, resources)
            await self._start_advanced_monitoring_tasks()

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v111.3: Backend startup now happens EARLIER (before cross-repo orchestration)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # The in-process backend is now started at the v111.3 section above,
            # BEFORE cross-repo orchestration runs. This ensures the backend is
            # actually listening when the orchestrator verifies jarvis-body health.
            #
            # This section is kept for safety - if the backend wasn't started earlier
            # for some reason, we'll start it here as a fallback.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self._in_process_mode and not self._backend_server:
                self.logger.warning("[v111.3] Backend not started earlier - starting now (fallback)")
                backend_started = await self._start_backend_in_process()
                if not backend_started:
                    self.logger.error("[v111.0] Backend failed to start in-process mode")
                    TerminalUI.print_error("[v111.0] Backend failed to start - aborting")
                    return 1

            try:
                await supervisor.run()
            finally:
                # v91.0: Stop advanced monitoring tasks first
                await self._stop_advanced_monitoring_tasks()

                # v111.0: Stop in-process backend (Unified Monolith Mode)
                if self._in_process_mode and self._backend_server:
                    await self._stop_backend_in_process()

                # Cleanup remote resources (VMs)
                await self.cleanup_resources()

                # Stop hot reload watcher
                await self._hot_reload.stop()

                # Cancel monitoring if still running
                if monitoring_task and not monitoring_task.done():
                    monitoring_task.cancel()
                    try:
                        await monitoring_task
                    except asyncio.CancelledError:
                        pass
            
            self.perf.end("jarvis")

            return 0

        except KeyboardInterrupt:
            print(f"\n{TerminalUI.YELLOW}ğŸ‘‹ Supervisor interrupted by user{TerminalUI.RESET}")
            await self.narrator.speak("Supervisor shutting down. Goodbye.", wait=True)
            return 130

        except Exception as e:
            # v88.1: Ultra-robust exception handling with multiple fallback layers
            # This ensures startup NEVER fails silently due to logging issues
            error_msg = f"Bootstrap failed: {e}"

            # Layer 1: Try the exception() method (now available in StructuredLogger)
            try:
                self.logger.exception(error_msg)
            except AttributeError:
                # Layer 2: Fallback to error() with exc_info if exception() doesn't exist
                try:
                    self.logger.error(error_msg, exc_info=True)
                except Exception:
                    # Layer 3: Last resort - print to stderr with full traceback
                    import traceback
                    print(f"ğŸš¨ CRITICAL: {error_msg}", file=sys.stderr)
                    traceback.print_exc(file=sys.stderr)
            except Exception as log_err:
                # Layer 3: Logging itself failed - print to stderr
                import traceback
                print(f"ğŸš¨ CRITICAL: {error_msg}", file=sys.stderr)
                print(f"ğŸš¨ LOGGING ALSO FAILED: {log_err}", file=sys.stderr)
                traceback.print_exc(file=sys.stderr)

            TerminalUI.print_error(error_msg)

            # Try to speak, but don't fail if narrator is unavailable
            try:
                await self.narrator.speak("An error occurred. Please check the logs.", wait=True)
            except Exception:
                pass  # Narrator failure should not cascade

            return 1
        
        finally:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # GRACEFUL SHUTDOWN: Use HTTP API instead of kill signals
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # The loading server v5.0.1 implements intelligent graceful shutdown:
            # 1. We request shutdown via HTTP POST /api/shutdown/graceful
            # 2. Loading server waits for browser to naturally disconnect
            # 3. Then it auto-shuts down, avoiding "window terminated unexpectedly"
            #
            # This eliminates the race condition where killing the server while
            # Chrome is still connected causes: "reason: 'killed', code: '15'"
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self._loading_server_process:
                await self._graceful_shutdown_loading_server()

            # v111.0: Ensure in-process backend is stopped (belt and suspenders)
            # This catches cases where the inner finally didn't run
            if self._in_process_mode and self._backend_server:
                await self._stop_backend_in_process()

            # Shutdown Enterprise Systems (Resilience + Data Management)
            await self._shutdown_enterprise_systems()

            # v10.6: Stop log monitor
            if self._log_monitor:
                try:
                    self.logger.info("[LogMonitor] Stopping real-time monitoring")
                    await self._log_monitor.stop()

                    # Get final stats
                    stats = self._log_monitor.get_stats()
                    self.logger.info(
                        "[LogMonitor] Final statistics",
                        total_logs_analyzed=stats["total_logs_analyzed"],
                        issues_detected=stats["issues_detected"],
                        voice_announcements=stats["voice_announcements"],
                        uptime_seconds=stats.get("uptime_seconds"),
                    )
                except Exception as e:
                    self.logger.debug(f"[LogMonitor] Cleanup error: {e}")

            # Log performance summary
            summary = self.perf.get_summary()
            self.logger.info(f"Bootstrap performance: {summary}")

    async def _initialize_enterprise_systems(self) -> None:
        """
        Phase 3.1: Initialize enterprise-grade resilience and data management systems.

        This initializes:
        1. Unified Resilience Engine - Circuit breakers, bulkheads, rate limiting, chaos engineering
        2. Neural Mesh Resilience - Cross-repo fault tolerance
        3. Unified Data Management - Training data, versioning, validation, privacy, lineage
        4. Cross-Repo Data Bridge - Data synchronization across JARVIS Trinity

        All systems are optional and gracefully degrade if initialization fails.
        """
        print(f"  {TerminalUI.CYAN}ğŸ”§ Initializing Enterprise Systems...{TerminalUI.RESET}")
        self.perf.start("enterprise_systems")

        enterprise_enabled = os.environ.get(
            "JARVIS_ENTERPRISE_SYSTEMS", "true"
        ).lower() in ("1", "true", "yes")

        if not enterprise_enabled:
            self.logger.info("[Phase 3.1] Enterprise systems disabled via JARVIS_ENTERPRISE_SYSTEMS=false")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Enterprise Systems: Disabled{TerminalUI.RESET}")
            return

        errors: List[str] = []

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 1. Initialize Resilience System
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        resilience_enabled = os.environ.get(
            "JARVIS_RESILIENCE_ENABLED", "true"
        ).lower() in ("1", "true", "yes")

        if resilience_enabled:
            try:
                from backend.core.resilience import (
                    initialize_supervisor_resilience,
                    get_supervisor_resilience_status,
                )

                result = await initialize_supervisor_resilience()

                if result.success:
                    components = []
                    if result.engine_initialized:
                        components.append("engine")
                    if result.mesh_bridge_initialized:
                        components.append("mesh_bridge")
                    self.logger.info(
                        f"[Phase 3.1] âœ… Resilience System initialized: "
                        f"{len(components)} components in {result.initialization_time_ms:.0f}ms"
                    )
                    print(f"  {TerminalUI.GREEN}âœ“ Resilience System: {', '.join(components) or 'core'}{TerminalUI.RESET}")
                else:
                    self.logger.warning(
                        f"[Phase 3.1] âš ï¸ Resilience System degraded: {result.errors}"
                    )
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Resilience System: Degraded ({len(result.errors)} errors){TerminalUI.RESET}")
                    errors.extend(result.errors)

            except ImportError as e:
                self.logger.debug(f"[Phase 3.1] Resilience System not available: {e}")
                print(f"  {TerminalUI.DIM}â—‹ Resilience System: Not available{TerminalUI.RESET}")
            except Exception as e:
                error_msg = f"Resilience System init failed: {e}"
                self.logger.warning(f"[Phase 3.1] {error_msg}")
                print(f"  {TerminalUI.YELLOW}âš ï¸ Resilience System: {e}{TerminalUI.RESET}")
                errors.append(error_msg)
        else:
            self.logger.info("[Phase 3.1] Resilience System disabled")
            print(f"  {TerminalUI.DIM}â—‹ Resilience System: Disabled{TerminalUI.RESET}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 2. Initialize Data Management System
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        data_mgmt_enabled = os.environ.get(
            "JARVIS_DATA_MANAGEMENT_ENABLED", "true"
        ).lower() in ("1", "true", "yes")

        if data_mgmt_enabled:
            try:
                from backend.core.data_management import (
                    initialize_data_management_supervisor,
                    get_data_management_status,
                )

                result = await initialize_data_management_supervisor()

                if result.success:
                    self.logger.info(
                        f"[Phase 3.1] âœ… Data Management initialized: "
                        f"{len(result.components)} components in {result.duration_seconds:.2f}s"
                    )
                    print(f"  {TerminalUI.GREEN}âœ“ Data Management: {len(result.components)} components{TerminalUI.RESET}")
                else:
                    self.logger.warning(
                        f"[Phase 3.1] âš ï¸ Data Management degraded: {result.errors}"
                    )
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Data Management: Degraded ({len(result.errors)} errors){TerminalUI.RESET}")
                    errors.extend(result.errors)

            except ImportError as e:
                self.logger.debug(f"[Phase 3.1] Data Management not available: {e}")
                print(f"  {TerminalUI.DIM}â—‹ Data Management: Not available{TerminalUI.RESET}")
            except Exception as e:
                error_msg = f"Data Management init failed: {e}"
                self.logger.warning(f"[Phase 3.1] {error_msg}")
                print(f"  {TerminalUI.YELLOW}âš ï¸ Data Management: {e}{TerminalUI.RESET}")
                errors.append(error_msg)
        else:
            self.logger.info("[Phase 3.1] Data Management disabled")
            print(f"  {TerminalUI.DIM}â—‹ Data Management: Disabled{TerminalUI.RESET}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 3. Initialize Model Management System
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        model_mgmt_enabled = os.environ.get(
            "JARVIS_MODEL_MANAGEMENT_ENABLED", "true"
        ).lower() in ("1", "true", "yes")

        if model_mgmt_enabled:
            try:
                from backend.core.model_management import (
                    initialize_model_management_supervisor,
                    get_model_management_status,
                )

                result = await initialize_model_management_supervisor()

                if result.success:
                    self.logger.info(
                        f"[Phase 3.1] âœ… Model Management initialized: "
                        f"{len(result.components)} components in {result.duration_seconds:.2f}s"
                    )
                    print(f"  {TerminalUI.GREEN}âœ“ Model Management: {len(result.components)} components{TerminalUI.RESET}")
                else:
                    self.logger.warning(
                        f"[Phase 3.1] âš ï¸ Model Management degraded: {result.errors}"
                    )
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Model Management: Degraded ({len(result.errors)} errors){TerminalUI.RESET}")
                    errors.extend(result.errors)

            except ImportError as e:
                self.logger.debug(f"[Phase 3.1] Model Management not available: {e}")
                print(f"  {TerminalUI.DIM}â—‹ Model Management: Not available{TerminalUI.RESET}")
            except Exception as e:
                error_msg = f"Model Management init failed: {e}"
                self.logger.warning(f"[Phase 3.1] {error_msg}")
                print(f"  {TerminalUI.YELLOW}âš ï¸ Model Management: {e}{TerminalUI.RESET}")
                errors.append(error_msg)
        else:
            self.logger.info("[Phase 3.1] Model Management disabled")
            print(f"  {TerminalUI.DIM}â—‹ Model Management: Disabled{TerminalUI.RESET}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 4. Initialize Resource Management System
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        resource_mgmt_enabled = os.environ.get(
            "JARVIS_RESOURCE_MANAGEMENT_ENABLED", "true"
        ).lower() in ("1", "true", "yes")

        if resource_mgmt_enabled:
            try:
                from backend.core.resource_management import (
                    initialize_resource_management_supervisor,
                    get_resource_management_status,
                )

                result = await initialize_resource_management_supervisor()

                if result.success:
                    self.logger.info(
                        f"[Phase 3.1] âœ… Resource Management initialized: "
                        f"{len(result.components_initialized)} components in {result.initialization_time_ms:.1f}ms"
                    )
                    print(f"  {TerminalUI.GREEN}âœ“ Resource Management: {len(result.components_initialized)} components{TerminalUI.RESET}")
                else:
                    self.logger.warning(
                        f"[Phase 3.1] âš ï¸ Resource Management degraded: {result.error_message}"
                    )
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Resource Management: Degraded{TerminalUI.RESET}")
                    if result.error_message:
                        errors.append(result.error_message)

            except ImportError as e:
                self.logger.debug(f"[Phase 3.1] Resource Management not available: {e}")
                print(f"  {TerminalUI.DIM}â—‹ Resource Management: Not available{TerminalUI.RESET}")
            except Exception as e:
                error_msg = f"Resource Management init failed: {e}"
                self.logger.warning(f"[Phase 3.1] {error_msg}")
                print(f"  {TerminalUI.YELLOW}âš ï¸ Resource Management: {e}{TerminalUI.RESET}")
                errors.append(error_msg)
        else:
            self.logger.info("[Phase 3.1] Resource Management disabled")
            print(f"  {TerminalUI.DIM}â—‹ Resource Management: Disabled{TerminalUI.RESET}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 5. Initialize Security System
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        security_enabled = os.environ.get(
            "JARVIS_SECURITY_ENABLED", "true"
        ).lower() in ("1", "true", "yes")

        if security_enabled:
            try:
                from backend.core.security import (
                    initialize_security_supervisor,
                    get_security_status,
                )

                result = await initialize_security_supervisor()

                if result.success:
                    self.logger.info(
                        f"[Phase 3.1] âœ… Security System initialized: "
                        f"{len(result.components_initialized)} components in {result.initialization_time_ms:.1f}ms"
                    )
                    print(f"  {TerminalUI.GREEN}âœ“ Security System: {len(result.components_initialized)} components{TerminalUI.RESET}")
                else:
                    self.logger.warning(
                        f"[Phase 3.1] âš ï¸ Security System degraded: {result.error_message}"
                    )
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Security System: Degraded{TerminalUI.RESET}")
                    if result.error_message:
                        errors.append(result.error_message)

            except ImportError as e:
                self.logger.debug(f"[Phase 3.1] Security System not available: {e}")
                print(f"  {TerminalUI.DIM}â—‹ Security System: Not available{TerminalUI.RESET}")
            except Exception as e:
                error_msg = f"Security System init failed: {e}"
                self.logger.warning(f"[Phase 3.1] {error_msg}")
                print(f"  {TerminalUI.YELLOW}âš ï¸ Security System: {e}{TerminalUI.RESET}")
                errors.append(error_msg)
        else:
            self.logger.info("[Phase 3.1] Security System disabled")
            print(f"  {TerminalUI.DIM}â—‹ Security System: Disabled{TerminalUI.RESET}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 6. Initialize Configuration Management System
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        config_mgmt_enabled = os.environ.get(
            "JARVIS_CONFIG_MANAGEMENT_ENABLED", "true"
        ).lower() in ("1", "true", "yes")

        if config_mgmt_enabled:
            try:
                from backend.core.configuration import (
                    initialize_config_supervisor,
                    get_config_status,
                )

                result = await initialize_config_supervisor()

                if result.success:
                    self.logger.info(
                        f"[Phase 3.1] âœ… Configuration Management initialized: "
                        f"{len(result.components_initialized)} components in {result.initialization_time_ms:.1f}ms"
                    )
                    print(f"  {TerminalUI.GREEN}âœ“ Configuration Management: {len(result.components_initialized)} components{TerminalUI.RESET}")
                else:
                    self.logger.warning(
                        f"[Phase 3.1] âš ï¸ Configuration Management degraded: {result.error_message}"
                    )
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Configuration Management: Degraded{TerminalUI.RESET}")
                    if result.error_message:
                        errors.append(result.error_message)

            except ImportError as e:
                self.logger.debug(f"[Phase 3.1] Configuration Management not available: {e}")
                print(f"  {TerminalUI.DIM}â—‹ Configuration Management: Not available{TerminalUI.RESET}")
            except Exception as e:
                error_msg = f"Configuration Management init failed: {e}"
                self.logger.warning(f"[Phase 3.1] {error_msg}")
                print(f"  {TerminalUI.YELLOW}âš ï¸ Configuration Management: {e}{TerminalUI.RESET}")
                errors.append(error_msg)
        else:
            self.logger.info("[Phase 3.1] Configuration Management disabled")
            print(f"  {TerminalUI.DIM}â—‹ Configuration Management: Disabled{TerminalUI.RESET}")

        self.perf.end("enterprise_systems")

        # Report summary
        if errors:
            self.logger.warning(f"[Phase 3.1] Enterprise Systems initialized with {len(errors)} error(s)")
        else:
            self.logger.info("[Phase 3.1] âœ… All Enterprise Systems initialized successfully")
            print(f"  {TerminalUI.GREEN}âœ“ Enterprise Systems: Ready{TerminalUI.RESET}")

    async def _shutdown_enterprise_systems(self) -> None:
        """
        Gracefully shutdown enterprise systems (Resilience + Data Management).

        This is called during the finally block to ensure proper cleanup
        of all enterprise components including:
        - Resilience engine (circuit breakers, bulkheads, chaos controllers)
        - Data management (training collectors, data bridge, lineage trackers)
        - Cross-repo bridges and synchronization
        """
        self.logger.info("[Shutdown] Shutting down Enterprise Systems...")

        # Shutdown Resilience System
        try:
            from backend.core.resilience import shutdown_supervisor_resilience
            await shutdown_supervisor_resilience()
            self.logger.info("[Shutdown] âœ… Resilience System shutdown complete")
        except ImportError:
            pass  # Not available
        except Exception as e:
            self.logger.debug(f"[Shutdown] Resilience shutdown error (non-fatal): {e}")

        # Shutdown Data Management System
        try:
            from backend.core.data_management import shutdown_data_management_supervisor
            await shutdown_data_management_supervisor()
            self.logger.info("[Shutdown] âœ… Data Management shutdown complete")
        except ImportError:
            pass  # Not available
        except Exception as e:
            self.logger.debug(f"[Shutdown] Data Management shutdown error (non-fatal): {e}")

        # Shutdown Model Management System
        try:
            from backend.core.model_management import shutdown_model_management_supervisor
            await shutdown_model_management_supervisor()
            self.logger.info("[Shutdown] âœ… Model Management shutdown complete")
        except ImportError:
            pass  # Not available
        except Exception as e:
            self.logger.debug(f"[Shutdown] Model Management shutdown error (non-fatal): {e}")

        # Shutdown Resource Management System
        try:
            from backend.core.resource_management import shutdown_resource_management_supervisor
            await shutdown_resource_management_supervisor()
            self.logger.info("[Shutdown] âœ… Resource Management shutdown complete")
        except ImportError:
            pass  # Not available
        except Exception as e:
            self.logger.debug(f"[Shutdown] Resource Management shutdown error (non-fatal): {e}")

        # Shutdown Security System
        try:
            from backend.core.security import shutdown_security_supervisor
            await shutdown_security_supervisor()
            self.logger.info("[Shutdown] âœ… Security System shutdown complete")
        except ImportError:
            pass  # Not available
        except Exception as e:
            self.logger.debug(f"[Shutdown] Security shutdown error (non-fatal): {e}")

        # Shutdown Configuration Management System
        try:
            from backend.core.configuration import shutdown_config_supervisor
            await shutdown_config_supervisor()
            self.logger.info("[Shutdown] âœ… Configuration Management shutdown complete")
        except ImportError:
            pass  # Not available
        except Exception as e:
            self.logger.debug(f"[Shutdown] Configuration Management shutdown error (non-fatal): {e}")

        self.logger.info("[Shutdown] Enterprise Systems shutdown complete")

    async def _start_loading_page_ecosystem(self) -> None:
        """
        Start the loading page ecosystem BEFORE JARVIS spawns.
        
        This method:
        1. Starts the loading_server.py subprocess
        2. Waits for server to be ready (with retries)
        3. Opens Chrome Incognito to the loading page
        4. Sets JARVIS_SUPERVISOR_LOADING=1 so start_system.py knows to skip browser ops
        
        This ensures the user sees the loading page immediately when running
        under supervisor, just like when running start_system.py directly.
        """
        loading_port = self.config.required_ports[2]  # 3001
        loading_url = f"http://localhost:{loading_port}"
        
        try:
            # Step 1: Start loading server subprocess
            loading_server_script = Path(__file__).parent / "loading_server.py"

            if not loading_server_script.exists():
                self.logger.warning(f"Loading server script not found: {loading_server_script}")
                print(f"  {TerminalUI.YELLOW}âš ï¸  Loading server not available - will skip browser{TerminalUI.RESET}")
                return

            self.logger.info(f"Starting loading server: {loading_server_script}")

            # Determine the best Python executable to use:
            # 1. Prefer venv Python for correct dependencies
            # 2. Fall back to sys.executable if venv not found
            project_root = Path(__file__).parent
            venv_python = project_root / "venv" / "bin" / "python3"
            if not venv_python.exists():
                venv_python = project_root / "venv" / "bin" / "python"

            if venv_python.exists():
                python_executable = str(venv_python)
                self.logger.debug(f"Using venv Python: {python_executable}")
            else:
                python_executable = sys.executable
                self.logger.warning(f"Venv not found, using system Python: {python_executable}")

            # Set up environment with PYTHONPATH for proper imports
            env = os.environ.copy()
            pythonpath_parts = [
                str(project_root),
                str(project_root / "backend"),
            ]
            existing_pythonpath = env.get("PYTHONPATH", "")
            if existing_pythonpath:
                pythonpath_parts.append(existing_pythonpath)
            env["PYTHONPATH"] = os.pathsep.join(pythonpath_parts)

            # Create log file for loading server output (helps debugging)
            logs_dir = project_root / "backend" / "logs"
            logs_dir.mkdir(parents=True, exist_ok=True)
            loading_server_log = logs_dir / f"loading_server_{time.strftime('%Y%m%d_%H%M%S')}.log"
            self._loading_server_log_path = loading_server_log

            # Open log file for subprocess output
            log_file = open(loading_server_log, "w")
            self._loading_server_log_file = log_file  # Keep reference for cleanup

            # Start as async subprocess with proper environment
            self._loading_server_process = await asyncio.create_subprocess_exec(
                python_executable,
                str(loading_server_script),
                stdout=log_file,
                stderr=asyncio.subprocess.STDOUT,  # Combine stderr into log
                env=env,
            )

            print(f"  {TerminalUI.GREEN}âœ“ Loading server started (PID {self._loading_server_process.pid}){TerminalUI.RESET}")
            self.logger.debug(f"Loading server log: {loading_server_log}")
            
            # Step 2: Wait for server to be ready (intelligent adaptive health check)
            import aiohttp
            
            server_ready = False
            health_url = f"{loading_url}/health"
            
            # Adaptive retry configuration
            initial_delay = 0.1  # Start fast
            max_delay = 1.0      # Cap at 1 second
            max_wait_time = 15.0 # Total wait budget (seconds)
            timeout_per_request = 1.0  # Generous timeout per request
            
            start_time = time.time()
            attempt = 0
            current_delay = initial_delay
            
            # Single session for connection reuse
            connector = aiohttp.TCPConnector(
                limit=1,
                enable_cleanup_closed=True,
                force_close=False,
            )
            
            async with aiohttp.ClientSession(
                connector=connector,
                timeout=aiohttp.ClientTimeout(total=timeout_per_request)
            ) as session:
                while (time.time() - start_time) < max_wait_time:
                    attempt += 1
                    try:
                        async with session.get(health_url) as resp:
                            if resp.status == 200:
                                server_ready = True
                                elapsed = time.time() - start_time
                                self.logger.info(
                                    f"Loading server ready after {attempt} attempts ({elapsed:.2f}s)"
                                )
                                break
                            else:
                                self.logger.debug(
                                    f"Health check attempt {attempt}: status {resp.status}"
                                )
                    except aiohttp.ClientConnectorError:
                        # Server not listening yet - this is expected during startup
                        pass
                    except asyncio.TimeoutError:
                        # Request timed out - server might be slow
                        self.logger.debug(f"Health check attempt {attempt}: timeout")
                    except Exception as e:
                        self.logger.debug(f"Health check attempt {attempt}: {type(e).__name__}")
                    
                    # Adaptive backoff: start fast, slow down over time
                    await asyncio.sleep(current_delay)
                    current_delay = min(current_delay * 1.5, max_delay)
                    
                    # Progress indicator every ~2 seconds
                    if attempt % 5 == 0:
                        elapsed = time.time() - start_time
                        print(f"  {TerminalUI.CYAN}â³ Waiting for loading server... ({elapsed:.1f}s){TerminalUI.RESET}")
            
            if not server_ready:
                elapsed = time.time() - start_time
                self.logger.warning(
                    f"Loading server didn't respond after {attempt} attempts ({elapsed:.1f}s)"
                )
                # Check if process is still running
                if self._loading_server_process.returncode is not None:
                    print(f"  {TerminalUI.RED}âœ— Loading server exited unexpectedly (code: {self._loading_server_process.returncode}){TerminalUI.RESET}")
                    # Show log file location and last few lines for debugging
                    if hasattr(self, '_loading_server_log_path') and self._loading_server_log_path.exists():
                        print(f"  {TerminalUI.YELLOW}ğŸ“„ Log file: {self._loading_server_log_path}{TerminalUI.RESET}")
                        try:
                            # Flush and read log file
                            if hasattr(self, '_loading_server_log_file'):
                                self._loading_server_log_file.flush()
                            with open(self._loading_server_log_path, 'r') as f:
                                lines = f.readlines()
                                if lines:
                                    print(f"  {TerminalUI.YELLOW}Last log entries:{TerminalUI.RESET}")
                                    for line in lines[-10:]:
                                        print(f"    {line.rstrip()}")
                        except Exception as log_err:
                            self.logger.debug(f"Could not read log file: {log_err}")
                else:
                    print(f"  {TerminalUI.YELLOW}âš ï¸  Loading server slow to respond - continuing (may still be starting){TerminalUI.RESET}")
                    if hasattr(self, '_loading_server_log_path'):
                        print(f"  {TerminalUI.CYAN}ğŸ“„ Log file: {self._loading_server_log_path}{TerminalUI.RESET}")
            else:
                print(f"  {TerminalUI.GREEN}âœ“ Loading server ready at {loading_url}{TerminalUI.RESET}")
            
            # Step 3: Intelligent Chrome window management (v4.0 - Clean Slate)
            # - Close ALL existing JARVIS windows (localhost:3000, :3001, :8010)
            # - Open ONE fresh incognito window
            # - This ensures a clean, predictable single-window experience
            # v5.2: Add frontend_optional parameter to loading URL
            frontend_optional = os.environ.get("FRONTEND_OPTIONAL", "false").lower() == "true"
            loading_url_with_params = f"{loading_url}?frontend_optional={str(frontend_optional).lower()}"
            if platform.system() == "Darwin":  # macOS
                try:
                    opened = await self._ensure_single_jarvis_window(loading_url_with_params)
                    if opened:
                        print(f"  {TerminalUI.GREEN}âœ“ Single JARVIS window ready{TerminalUI.RESET}")
                    else:
                        print(f"  {TerminalUI.YELLOW}âš ï¸  Could not open Chrome automatically{TerminalUI.RESET}")
                        print(f"  {TerminalUI.CYAN}ğŸ’¡ Open manually: {loading_url_with_params}{TerminalUI.RESET}")
                except Exception as e:
                    self.logger.debug(f"Failed to open Chrome: {e}")
                    print(f"  {TerminalUI.YELLOW}âš ï¸  Could not open Chrome automatically{TerminalUI.RESET}")
                    print(f"  {TerminalUI.CYAN}ğŸ’¡ Open manually: {loading_url_with_params}{TerminalUI.RESET}")
            
            # Step 4: Set environment variable to signal start_system.py
            os.environ["JARVIS_SUPERVISOR_LOADING"] = "1"
            self.logger.info("Set JARVIS_SUPERVISOR_LOADING=1")
            
            # Voice narration
            await self.narrator.speak("Loading page ready. Starting JARVIS core.", wait=False)
            
            print()  # Blank line for readability
            
        except Exception as e:
            # v88.1: Robust exception logging with fallback
            error_msg = f"Failed to start loading page ecosystem: {e}"
            try:
                self.logger.exception(error_msg)
            except Exception:
                self.logger.error(error_msg, exc_info=True)
            print(f"  {TerminalUI.YELLOW}âš ï¸  Loading page failed: {e}{TerminalUI.RESET}")
            print(f"  {TerminalUI.CYAN}ğŸ’¡ JARVIS will start without loading page{TerminalUI.RESET}")

    async def _graceful_shutdown_loading_server(self) -> None:
        """
        Gracefully shutdown the loading server using HTTP API.

        This method implements the v5.0.1 graceful shutdown protocol that
        eliminates the "window terminated unexpectedly (reason: 'killed', code: '15')"
        error by:

        1. Requesting graceful shutdown via HTTP POST /api/shutdown/graceful
        2. The loading server waits for browser to naturally disconnect
        3. Then auto-shuts down cleanly without killing active connections

        Falls back to signal-based shutdown if HTTP fails (for resilience).
        """
        if not self._loading_server_process:
            self._cleanup_loading_server_log()  # Cleanup even if no process
            return

        loading_port = self.config.required_ports[2]  # 3001
        shutdown_url = f"http://localhost:{loading_port}/api/shutdown/graceful"
        status_url = f"http://localhost:{loading_port}/api/shutdown/status"

        try:
            import aiohttp

            # Configurable timeouts from environment
            http_timeout = float(os.getenv('LOADING_SERVER_SHUTDOWN_HTTP_TIMEOUT', '5.0'))
            max_wait = float(os.getenv('LOADING_SERVER_SHUTDOWN_MAX_WAIT', '30.0'))
            poll_interval = float(os.getenv('LOADING_SERVER_SHUTDOWN_POLL_INTERVAL', '0.5'))

            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=http_timeout)
            ) as session:
                # Step 1: Request graceful shutdown
                self.logger.info("Requesting graceful shutdown of loading server...")
                try:
                    async with session.post(
                        shutdown_url,
                        json={"reason": "supervisor_shutdown"}
                    ) as resp:
                        if resp.status == 200:
                            result = await resp.json()
                            status = result.get("status", "unknown")
                            connections = result.get("connections", 0)

                            if status == "immediate_shutdown":
                                self.logger.info("Loading server shutting down immediately (no active connections)")
                            elif status == "pending_disconnect":
                                self.logger.info(
                                    f"Loading server waiting for {connections} connection(s) to close..."
                                )
                            elif status == "already_shutting_down":
                                self.logger.debug("Loading server already shutting down")
                            else:
                                self.logger.debug(f"Shutdown response: {result}")
                        else:
                            self.logger.warning(
                                f"Shutdown request returned {resp.status}, falling back to signal"
                            )
                            await self._fallback_signal_shutdown()
                            return
                except aiohttp.ClientError as e:
                    self.logger.debug(f"HTTP shutdown request failed: {e}")
                    await self._fallback_signal_shutdown()
                    return

                # Step 2: Wait for loading server to actually shutdown
                start_time = time.time()
                while (time.time() - start_time) < max_wait:
                    # Check if process has exited
                    if self._loading_server_process.returncode is not None:
                        self.logger.info("Loading server gracefully terminated via HTTP")
                        self._cleanup_loading_server_log()
                        return

                    # Check shutdown status
                    try:
                        async with session.get(status_url) as resp:
                            if resp.status == 200:
                                status_data = await resp.json()
                                if status_data.get("shutdown_initiated"):
                                    # Shutdown is happening, just wait for process exit
                                    self.logger.debug("Shutdown initiated, waiting for process exit...")
                            else:
                                # Server may have already died
                                break
                    except aiohttp.ClientError:
                        # Server not responding, likely already shutdown
                        self.logger.debug("Loading server no longer responding")
                        break

                    await asyncio.sleep(poll_interval)

                # Wait a bit more for process to fully exit
                try:
                    await asyncio.wait_for(
                        self._loading_server_process.wait(),
                        timeout=2.0
                    )
                    self.logger.info("Loading server gracefully terminated")
                    self._cleanup_loading_server_log()
                    return
                except asyncio.TimeoutError:
                    pass

        except ImportError:
            self.logger.debug("aiohttp not available, using signal-based shutdown")
        except Exception as e:
            self.logger.debug(f"HTTP graceful shutdown failed: {e}")

        # Fall back to signal-based shutdown if HTTP approach failed
        await self._fallback_signal_shutdown()

    async def _fallback_signal_shutdown(self) -> None:
        """
        Fallback shutdown using signals (for when HTTP fails).

        This is the legacy approach - used only when the HTTP API is unavailable.
        Includes a delay to give Chrome time to redirect before killing.
        """
        if not self._loading_server_process:
            return

        try:
            startup_complete = os.environ.get("JARVIS_STARTUP_COMPLETE") == "true"

            if startup_complete:
                # Give Chrome time to redirect (legacy workaround)
                self.logger.debug("Waiting for Chrome to complete redirect...")
                await asyncio.sleep(2.0)

            # Try SIGINT first for graceful shutdown
            self._loading_server_process.send_signal(signal.SIGINT)
            try:
                await asyncio.wait_for(self._loading_server_process.wait(), timeout=3.0)
                self.logger.info("Loading server terminated (SIGINT)")
                return
            except asyncio.TimeoutError:
                pass

            # Try SIGTERM
            self._loading_server_process.terminate()
            try:
                await asyncio.wait_for(self._loading_server_process.wait(), timeout=2.0)
                self.logger.info("Loading server terminated (SIGTERM)")
                return
            except asyncio.TimeoutError:
                pass

            # Force kill
            self._loading_server_process.kill()
            self.logger.warning("Loading server force killed (timeout)")

        except ProcessLookupError:
            self.logger.debug("Loading server already exited")
        except Exception as e:
            self.logger.debug(f"Loading server cleanup error: {e}")
        finally:
            # Always cleanup log file handle
            self._cleanup_loading_server_log()

    def _cleanup_loading_server_log(self) -> None:
        """Clean up loading server log file handle."""
        if hasattr(self, '_loading_server_log_file') and self._loading_server_log_file:
            try:
                self._loading_server_log_file.close()
                self.logger.debug("Loading server log file closed")
            except Exception as e:
                self.logger.debug(f"Error closing log file: {e}")
            finally:
                self._loading_server_log_file = None

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # BROWSER LOCK FILE - Prevents race conditions across all processes
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    BROWSER_LOCK_FILE = Path("/tmp/jarvis_browser.lock")
    BROWSER_PID_FILE = Path("/tmp/jarvis_browser_opener.pid")
    
    async def _acquire_browser_lock(self) -> bool:
        """
        Acquire exclusive lock for browser operations.
        
        Uses file-based locking to prevent multiple processes from
        opening browser windows simultaneously.
        
        Returns:
            True if lock acquired, False if another process holds it
        """
        try:
            # Check if lock exists and is recent (within 30 seconds)
            if self.BROWSER_LOCK_FILE.exists():
                lock_age = time.time() - self.BROWSER_LOCK_FILE.stat().st_mtime
                if lock_age < 30:
                    # Check if the PID that created it is still running
                    if self.BROWSER_PID_FILE.exists():
                        try:
                            # v92.0: Use safe file reading to avoid "Bad file descriptor" errors
                            pid_content = _safe_read_file(self.BROWSER_PID_FILE, default="").strip()
                            pid = int(pid_content) if pid_content else 0
                            if not pid:
                                raise ValueError("Empty or invalid PID file")
                            # Check if process is still alive
                            os.kill(pid, 0)
                            self.logger.debug(f"Browser lock held by PID {pid}")
                            return False
                        except (ProcessLookupError, ValueError):
                            # Process is dead, we can take the lock
                            pass
                    else:
                        self.logger.debug(f"Browser lock exists but no PID file, age={lock_age:.1f}s")
                        return False
            
            # Acquire lock
            self.BROWSER_LOCK_FILE.write_text(str(time.time()))
            self.BROWSER_PID_FILE.write_text(str(os.getpid()))
            self.logger.debug(f"Acquired browser lock (PID {os.getpid()})")
            return True
            
        except Exception as e:
            self.logger.debug(f"Lock acquisition error: {e}")
            return False
    
    def _release_browser_lock(self) -> None:
        """Release the browser lock."""
        try:
            if self.BROWSER_LOCK_FILE.exists():
                self.BROWSER_LOCK_FILE.unlink()
            if self.BROWSER_PID_FILE.exists():
                self.BROWSER_PID_FILE.unlink()
            self.logger.debug("Released browser lock")
        except Exception as e:
            self.logger.debug(f"Lock release error: {e}")

    async def _close_all_jarvis_windows(self) -> int:
        """
        AGGRESSIVELY close ALL Chrome incognito windows + JARVIS-related regular windows.
        
        v5.0: Multi-phase approach with verification
        1. First pass: Close all incognito + JARVIS windows
        2. Verify: Check if any remain
        3. Force: Repeatedly close until none remain
        4. Nuclear: Quit Chrome entirely if still stuck
        
        Returns:
            Number of windows closed
        """
        total_closed = 0
        
        # Phase 1: Check if Chrome is even running
        try:
            check_chrome = '''
            tell application "System Events"
                return (exists process "Google Chrome")
            end tell
            '''
            process = await asyncio.create_subprocess_exec(
                "/usr/bin/osascript", "-e", check_chrome,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.DEVNULL,
            )
            stdout, _ = await process.communicate()
            chrome_running = stdout.decode().strip().lower() == "true"
            
            if not chrome_running:
                self.logger.debug("Chrome not running - no windows to close")
                return 0
        except Exception:
            pass
        
        # Phase 2: First pass - close all incognito and JARVIS windows
        try:
            applescript = '''
            tell application "Google Chrome"
                set jarvisPatterns to {"localhost:3000", "localhost:3001", "localhost:8010", "127.0.0.1:3000", "127.0.0.1:3001", "127.0.0.1:8010"}
                set closedCount to 0
                
                set windowCount to count of windows
                repeat with i from windowCount to 1 by -1
                    try
                        set w to window i
                        set shouldClose to false
                        
                        if mode of w is "incognito" then
                            set shouldClose to true
                        else
                            repeat with t in tabs of w
                                set tabURL to URL of t
                                repeat with pattern in jarvisPatterns
                                    if tabURL contains pattern then
                                        set shouldClose to true
                                        exit repeat
                                    end if
                                end repeat
                                if shouldClose then exit repeat
                            end repeat
                        end if
                        
                        if shouldClose then
                            close w
                            set closedCount to closedCount + 1
                            delay 0.2
                        end if
                    end try
                end repeat
                
                return closedCount
            end tell
            '''
            process = await asyncio.create_subprocess_exec(
                "/usr/bin/osascript", "-e", applescript,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.DEVNULL,
            )
            stdout, _ = await process.communicate()
            try:
                closed = int(stdout.decode().strip() or "0")
                total_closed += closed
                if closed > 0:
                    self.logger.info(f"ğŸ—‘ï¸ Phase 1: Closed {closed} window(s)")
            except ValueError:
                pass
        except Exception as e:
            self.logger.debug(f"Phase 1 failed: {e}")
        
        await asyncio.sleep(0.5)
        
        # Phase 3: Force-close any remaining incognito windows (loop until done)
        max_attempts = 10
        for attempt in range(max_attempts):
            try:
                force_script = '''
                tell application "Google Chrome"
                    set incogCount to count of (windows whose mode is "incognito")
                    if incogCount = 0 then
                        return "0:done"
                    end if
                    
                    try
                        close (first window whose mode is "incognito")
                        return "1:closed"
                    on error
                        return "0:error"
                    end try
                end tell
                '''
                process = await asyncio.create_subprocess_exec(
                    "/usr/bin/osascript", "-e", force_script,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.DEVNULL,
                )
                stdout, _ = await process.communicate()
                result = stdout.decode().strip()
                
                if "done" in result:
                    break
                elif "closed" in result:
                    total_closed += 1
                    await asyncio.sleep(0.3)
                else:
                    break
            except Exception:
                break
        
        if total_closed > 0:
            self.logger.info(f"ğŸ§¹ Total: Closed {total_closed} JARVIS window(s)")
            await asyncio.sleep(1.0)  # Let Chrome fully process
        
        return total_closed

    async def _open_fresh_incognito_window(self, url: str) -> bool:
        """
        Open exactly ONE fresh Chrome incognito window with fullscreen mode.
        
        v5.0: Verifies only one window is created after opening.
        
        Args:
            url: The URL to open
            
        Returns:
            True if successful, False otherwise
        """
        # First, count existing incognito windows
        initial_count = 0
        try:
            count_script = '''
            tell application "Google Chrome"
                return count of (windows whose mode is "incognito")
            end tell
            '''
            process = await asyncio.create_subprocess_exec(
                "/usr/bin/osascript", "-e", count_script,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.DEVNULL,
            )
            stdout, _ = await process.communicate()
            initial_count = int(stdout.decode().strip() or "0")
            
            if initial_count > 0:
                self.logger.warning(f"âš ï¸ {initial_count} incognito windows still exist before opening new one!")
        except Exception:
            pass
        
        # Open the window using AppleScript for precise control
        try:
            applescript = f'''
            tell application "Google Chrome"
                -- Create exactly ONE new incognito window
                set newWindow to make new window with properties {{mode:"incognito"}}
                delay 0.5
                
                -- Navigate to URL
                tell newWindow
                    set URL of active tab to "{url}"
                end tell
                
                -- Bring to front and activate
                set index of newWindow to 1
                activate
            end tell
            
            -- Enter fullscreen mode (Cmd+Ctrl+F)
            delay 1.0
            tell application "System Events"
                tell process "Google Chrome"
                    set frontmost to true
                    delay 0.3
                    -- Use menu bar for more reliable fullscreen
                    try
                        click menu item "Enter Full Screen" of menu "View" of menu bar 1
                    on error
                        -- Fallback to keyboard shortcut
                        keystroke "f" using {{command down, control down}}
                    end try
                end tell
            end tell
            
            return true
            '''
            process = await asyncio.create_subprocess_exec(
                "/usr/bin/osascript", "-e", applescript,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.DEVNULL,
            )
            stdout, _ = await process.communicate()
            result = stdout.decode().strip().lower()
            
            if result == "true":
                # Verify we have exactly ONE incognito window
                await asyncio.sleep(0.5)
                try:
                    process = await asyncio.create_subprocess_exec(
                        "/usr/bin/osascript", "-e", count_script,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.DEVNULL,
                    )
                    stdout, _ = await process.communicate()
                    final_count = int(stdout.decode().strip() or "0")
                    
                    if final_count > 1:
                        self.logger.warning(f"âš ï¸ Multiple incognito windows detected ({final_count}), closing extras...")
                        # Close extras
                        for _ in range(final_count - 1):
                            await self._close_one_incognito_window()
                    
                    self.logger.info(f"ğŸŒ Single JARVIS incognito window opened: {url}")
                except Exception:
                    pass
                
                return True
            
        except Exception as e:
            self.logger.debug(f"AppleScript failed: {e}")
        
        # Fallback to command line (less precise)
        try:
            process = await asyncio.create_subprocess_exec(
                "/usr/bin/open", "-na", "Google Chrome",
                "--args", "--incognito", "--new-window", "--start-fullscreen", url,
                stdout=asyncio.subprocess.DEVNULL,
                stderr=asyncio.subprocess.DEVNULL,
            )
            await process.wait()
            self.logger.info(f"ğŸŒ Opened Chrome incognito via command line: {url}")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸ Could not open browser: {e}")
            return False
    
    async def _close_one_incognito_window(self) -> bool:
        """Close a single incognito window."""
        try:
            script = '''
            tell application "Google Chrome"
                try
                    close (first window whose mode is "incognito")
                    return true
                on error
                    return false
                end try
            end tell
            '''
            process = await asyncio.create_subprocess_exec(
                "/usr/bin/osascript", "-e", script,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.DEVNULL,
            )
            stdout, _ = await process.communicate()
            return stdout.decode().strip().lower() == "true"
        except Exception:
            return False

    async def _ensure_single_jarvis_window(self, url: str) -> bool:
        """
        Intelligent window manager: ensures exactly ONE Chrome window for JARVIS.
        
        Strategy (v5.0 - Lock-Based Clean Slate):
        1. Acquire exclusive browser lock (prevents race conditions)
        2. Close ALL existing JARVIS-related windows
        3. Open ONE fresh incognito window
        4. Release lock
        
        The lock prevents multiple processes from opening browsers simultaneously.
        
        Args:
            url: The URL to open (typically localhost:3001 for loading page)
            
        Returns:
            True if successful, False otherwise
        """
        try:
            # Step 0: Acquire exclusive browser lock
            if not await self._acquire_browser_lock():
                self.logger.info("ğŸ”’ Another process is managing browser - skipping")
                # Wait a bit and check if browser is ready
                await asyncio.sleep(2.0)
                return True  # Assume the other process handled it
            
            try:
                # Step 1: Close all existing JARVIS windows
                closed_count = await self._close_all_jarvis_windows()
                
                if closed_count > 0:
                    self.logger.info(f"ğŸ§¹ Cleaned up {closed_count} existing JARVIS window(s)")
                    await asyncio.sleep(1.0)  # Let Chrome fully process closures
                
                # Step 2: Open fresh incognito window
                success = await self._open_fresh_incognito_window(url)
                
                if success:
                    self.logger.info(f"âœ… Single JARVIS window ready at {url}")
                else:
                    self.logger.warning(f"âš ï¸ Failed to open browser - please open {url} manually")
                
                return success
                
            finally:
                # Always release lock when done
                self._release_browser_lock()
            
        except Exception as e:
            # v88.1: Robust exception logging with fallback
            error_msg = f"Window management error: {e}"
            try:
                self.logger.exception(error_msg)
            except Exception:
                self.logger.error(error_msg, exc_info=True)
            self._release_browser_lock()
            return False
    
    async def _broadcast_to_loading_page(
        self,
        stage: str,
        message: str,
        progress: int,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> bool:
        """
        Broadcast progress update to the loading page.
        
        v4.0: Supervisor takes responsibility for completion broadcasting
        when JARVIS_SUPERVISOR_LOADING=1 is set.
        
        Args:
            stage: Current stage name
            message: Human-readable message
            progress: Progress percentage (0-100)
            metadata: Optional metadata dict
            
        Returns:
            True if broadcast succeeded, False otherwise
        """
        if not self._loading_server_process:
            return False
        
        try:
            import aiohttp
            from datetime import datetime
            
            loading_port = self.config.required_ports[2]  # 3001
            url = f"http://localhost:{loading_port}/api/update-progress"
            
            data = {
                "stage": stage,
                "message": message,
                "progress": progress,
                "timestamp": datetime.now().isoformat(),
                "metadata": metadata or {},
            }
            
            async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=2.0)
            ) as session:
                async with session.post(url, json=data) as resp:
                    if resp.status == 200:
                        self.logger.debug(f"ğŸ“¡ Broadcast: {stage} ({progress}%)")
                        return True
                    else:
                        self.logger.debug(f"Broadcast failed: status {resp.status}")
                        return False
                        
        except Exception as e:
            self.logger.debug(f"Broadcast failed: {e}")
            return False

    # Alias for backward compatibility
    async def _broadcast_startup_progress(
        self,
        stage: str,
        message: str,
        progress: int,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> bool:
        """Alias for _broadcast_to_loading_page for semantic clarity."""
        return await self._broadcast_to_loading_page(stage, message, progress, metadata)

    async def _rebroadcast_two_tier_state(self) -> bool:
        """
        v20.1: Re-broadcast Two-Tier security state to loading server.

        CRITICAL FIX: Two-Tier components (Watchdog, Router, VBIA, Voice Auth) are
        initialized in Phase 2, BEFORE the loading server starts in Phase 3.5.
        This causes the frontend to show "Initializing..." forever because it never
        received the *_ready broadcasts.

        This method re-broadcasts the current Two-Tier state AFTER the loading server
        is ready, ensuring the frontend displays the correct component states.

        Returns:
            True if broadcast succeeded, False otherwise
        """
        if not self._loading_server_process:
            return False

        try:
            # Gather current Two-Tier component states
            watchdog_ready = self._watchdog is not None
            router_ready = self._tiered_routing_enabled  # Router is ready if routing is enabled
            vbia_ready = self._vbia_adapter is not None

            # Build the complete Two-Tier state
            two_tier_state = {
                "watchdog_ready": watchdog_ready,
                "router_ready": router_ready,
                "vbia_adapter_ready": vbia_ready,
                "tier1_operational": router_ready,
                "tier2_operational": router_ready,
                "watchdog_status": "active" if watchdog_ready else "disabled",
                "watchdog_mode": "monitoring" if watchdog_ready else "idle",
                "vbia_tier1_threshold": 0.70,
                "vbia_tier2_threshold": 0.85,
                "vbia_liveness_enabled": True,
            }

            # Determine overall status
            all_ready = watchdog_ready and router_ready and vbia_ready
            some_ready = watchdog_ready or router_ready or vbia_ready

            if all_ready:
                two_tier_state["overall_status"] = "ready"
                two_tier_state["message"] = "Two-Tier Security System fully operational"
                progress = 89
            elif some_ready:
                two_tier_state["overall_status"] = "partial"
                components = []
                if watchdog_ready:
                    components.append("Watchdog")
                if router_ready:
                    components.append("Router")
                if vbia_ready:
                    components.append("VBIA")
                two_tier_state["message"] = f"Partial: {', '.join(components)} ready"
                progress = 85
            else:
                two_tier_state["overall_status"] = "initializing"
                two_tier_state["message"] = "Two-Tier Security initializing..."
                progress = 80

            # Broadcast the current state
            success = await self._broadcast_startup_progress(
                stage="two_tier_rebroadcast",
                message=two_tier_state["message"],
                progress=progress,
                metadata={"two_tier": two_tier_state}
            )

            if success:
                self.logger.info(f"ğŸ”„ Re-broadcast Two-Tier state: {two_tier_state['overall_status']}")
                self.logger.info(f"   Watchdog: {'âœ…' if watchdog_ready else 'âŒ'}, "
                               f"Router: {'âœ…' if router_ready else 'âŒ'}, "
                               f"VBIA: {'âœ…' if vbia_ready else 'âŒ'}")
            else:
                self.logger.warning("âš ï¸ Failed to re-broadcast Two-Tier state")

            return success

        except Exception as e:
            self.logger.warning(f"âš ï¸ Two-Tier state re-broadcast error: {e}")
            return False

    async def _broadcast_jarvis_prime_status(
        self,
        tier: str,
        status: str,
        health: Optional[Dict[str, Any]] = None,
    ) -> bool:
        """
        Broadcast JARVIS-Prime status to loading server.

        Args:
            tier: Current tier (local, cloud_run, gemini_api)
            status: Status (starting, ready, fallback, error)
            health: Optional health metrics
        """
        if not self._loading_server_process:
            return False

        try:
            import aiohttp

            loading_port = self.config.required_ports[2]  # 3001
            url = f"http://localhost:{loading_port}/api/jarvis-prime/update"

            data = {
                "tier": tier,
                "status": status,
                "health": health or {},
                "timestamp": datetime.now().isoformat(),
            }

            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=2.0)) as session:
                async with session.post(url, json=data) as resp:
                    if resp.status == 200:
                        self.logger.debug(f"ğŸ“¡ JARVIS-Prime: {tier} ({status})")
                        return True
                    return False

        except Exception as e:
            self.logger.debug(f"JARVIS-Prime broadcast failed: {e}")
            return False

    async def _broadcast_flywheel_status(
        self,
        status: str,
        experiences_collected: int = 0,
        training_schedule: str = "03:00",
        last_training: Optional[str] = None,
        next_training: Optional[str] = None,
    ) -> bool:
        """
        Broadcast Data Flywheel status to loading server.

        Args:
            status: Current status (idle, collecting, training, ready)
            experiences_collected: Number of experiences collected
            training_schedule: Training schedule time
            last_training: Last training timestamp
            next_training: Next scheduled training
        """
        if not self._loading_server_process:
            return False

        try:
            import aiohttp

            loading_port = self.config.required_ports[2]  # 3001
            url = f"http://localhost:{loading_port}/api/flywheel/update"

            data = {
                "status": status,
                "experiences_collected": experiences_collected,
                "training_schedule": training_schedule,
                "last_training": last_training,
                "next_training": next_training,
                "timestamp": datetime.now().isoformat(),
            }

            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=2.0)) as session:
                async with session.post(url, json=data) as resp:
                    if resp.status == 200:
                        self.logger.debug(f"ğŸ“¡ Flywheel: {status}")
                        return True
                    return False

        except Exception as e:
            self.logger.debug(f"Flywheel broadcast failed: {e}")
            return False

    async def _broadcast_reactor_core_status(
        self,
        status: str,
        components: Optional[Dict[str, bool]] = None,
        training_active: bool = False,
        model_version: Optional[str] = None,
    ) -> bool:
        """
        Broadcast Reactor-Core status to loading server.

        Args:
            status: Current status (initializing, ready, training, deploying)
            components: Component availability (jarvis_connector, scout, trainer, watcher)
            training_active: Whether training is in progress
            model_version: Current model version if available
        """
        if not self._loading_server_process:
            return False

        try:
            import aiohttp

            loading_port = self.config.required_ports[2]  # 3001
            url = f"http://localhost:{loading_port}/api/reactor-core/update"

            data = {
                "status": status,
                "components": components or {},
                "training_active": training_active,
                "model_version": model_version,
                "timestamp": datetime.now().isoformat(),
            }

            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=2.0)) as session:
                async with session.post(url, json=data) as resp:
                    if resp.status == 200:
                        self.logger.debug(f"ğŸ“¡ Reactor-Core: {status}")
                        return True
                    return False

        except Exception as e:
            self.logger.debug(f"Reactor-Core broadcast failed: {e}")
            return False

    async def _broadcast_learning_goals_status(
        self,
        goals: List[Dict[str, Any]],
        total_goals: int = 0,
        active_goals: int = 0,
        completed_goals: int = 0,
    ) -> bool:
        """
        Broadcast Learning Goals status to loading server.

        Args:
            goals: List of current learning goals
            total_goals: Total number of goals
            active_goals: Number of active goals
            completed_goals: Number of completed goals
        """
        if not self._loading_server_process:
            return False

        try:
            import aiohttp

            loading_port = self.config.required_ports[2]  # 3001
            url = f"http://localhost:{loading_port}/api/learning-goals/update"

            data = {
                "goals": goals[:10],  # Limit to 10 goals
                "total_goals": total_goals,
                "active_goals": active_goals,
                "completed_goals": completed_goals,
                "timestamp": datetime.now().isoformat(),
            }

            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=2.0)) as session:
                async with session.post(url, json=data) as resp:
                    if resp.status == 200:
                        self.logger.debug(f"ğŸ“¡ Learning Goals: {active_goals} active")
                        return True
                    return False

        except Exception as e:
            self.logger.debug(f"Learning Goals broadcast failed: {e}")
            return False

    async def _broadcast_intelligence_systems_status(
        self,
        uae_status: Optional[Dict[str, Any]] = None,
        sai_status: Optional[Dict[str, Any]] = None,
        neural_mesh_status: Optional[Dict[str, Any]] = None,
        mas_status: Optional[Dict[str, Any]] = None,
        cai_status: Optional[Dict[str, Any]] = None,
    ) -> bool:
        """
        Broadcast Intelligence Systems status to loading server.

        Args:
            uae_status: UAE (Unified Awareness Engine) status
            sai_status: SAI (Situational Awareness Intelligence) status
            neural_mesh_status: Neural Mesh status
            mas_status: MAS (Multi-Agent System) status
            cai_status: CAI (Collective AI Intelligence) status
        """
        return await self._broadcast_startup_progress(
            stage="intelligence_systems",
            message="Intelligence Systems status update",
            progress=90,
            metadata={
                "intelligence_systems": {
                    "uae": uae_status or {"status": "unknown"},
                    "sai": sai_status or {"status": "unknown"},
                    "neural_mesh": neural_mesh_status or {"status": "unknown"},
                    "mas": mas_status or {"status": "unknown"},
                    "cai": cai_status or {"status": "unknown"},
                }
            }
        )

    async def _monitor_jarvis_startup(self, max_wait: float = 120.0) -> bool:
        """
        Monitor JARVIS startup and broadcast progress to loading page.
        
        v4.0: This is the CRITICAL missing piece that was causing the 97% hang.
        v4.1: Now checks /health/ready for OPERATIONAL readiness, not just HTTP response.
              This prevents "OFFLINE - SEARCHING FOR BACKEND" by ensuring services
              are actually ready before broadcasting completion.
        v9.0: ADAPTIVE TIMEOUT - Extends wait time when heavy initialization detected
              (Docker startup, Cloud Run initialization, ML model loading, etc.)
        
        Args:
            max_wait: Maximum time to wait for JARVIS to be ready
            
        Returns:
            True if JARVIS is ready, False if timeout
        """
        import aiohttp
        
        backend_port = self.config.required_ports[0]  # 8010
        frontend_port = self.config.required_ports[1]  # 3000
        
        start_time = time.time()
        last_progress = 0
        backend_http_ready = False
        backend_operational = False
        frontend_ready = False
        last_status = None
        
        # v20.0: CONSERVATIVE timeout tracking (reduced from v9.0)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Key insight: The /health/ready endpoint now reports ready=True as soon as
        # WebSocket is available. We don't need to wait for ML models or voice.
        # These extensions are kept minimal for truly exceptional circumstances.
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        adaptive_max_wait = max_wait
        timeout_extended = False
        extension_reasons: List[str] = []

        # v20.0: REDUCED - Only add minimal extension for Docker (not 60s)
        docker_needs_start = not self._is_docker_running()
        if docker_needs_start:
            adaptive_max_wait += 15  # Reduced from 60s to 15s
            extension_reasons.append("Docker startup (+15s)")

        # Cold start extension reduced
        if os.getenv("JARVIS_COLD_START") == "1":
            adaptive_max_wait += 15  # Reduced from 30s to 15s
            extension_reasons.append("Cold start (+15s)")
        
        if extension_reasons:
            self.logger.info(f"â±ï¸  Adaptive timeout: {adaptive_max_wait:.0f}s (base: {max_wait}s)")
            self.logger.info(f"   Extensions: {', '.join(extension_reasons)}")
        
        self.logger.info("ğŸ” Monitoring JARVIS startup (v9.0 - adaptive timeout)...")

        while (time.time() - start_time) < adaptive_max_wait:
            elapsed = time.time() - start_time
            
            # Phase 1a: Check backend HTTP health
            if not backend_http_ready:
                try:
                    async with aiohttp.ClientSession(
                        timeout=aiohttp.ClientTimeout(total=3.0)
                    ) as session:
                        async with session.get(
                            f"http://localhost:{backend_port}/health"
                        ) as resp:
                            if resp.status == 200:
                                backend_http_ready = True
                                self.logger.info("âœ… Backend HTTP responding")
                                await self._broadcast_to_loading_page(
                                    "backend_http",
                                    "Backend server online",
                                    70,
                                    {"icon": "â³", "label": "Backend Starting"}
                                )
                except Exception:
                    pass
            
            # Phase 1b: Check backend OPERATIONAL readiness
            if backend_http_ready and not backend_operational:
                try:
                    async with aiohttp.ClientSession(
                        timeout=aiohttp.ClientTimeout(total=5.0)
                    ) as session:
                        async with session.get(
                            f"http://localhost:{backend_port}/health/ready"
                        ) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                                status = data.get("status", "unknown")
                                
                                # Log status changes
                                if status != last_status:
                                    self.logger.info(f"ğŸ“Š Backend status: {status}")
                                    last_status = status
                                
                                # Check for operational readiness
                                # v20.0: Progressive readiness - accept when ready=True from backend
                                # Now includes "interactive" status for faster startup
                                is_ready = (
                                    data.get("ready") == True or
                                    data.get("operational") == True or
                                    status in ["ready", "operational", "degraded", "warming_up", "websocket_ready", "interactive"]
                                )
                                
                                # Also accept if WebSocket is ready (core functionality)
                                details = data.get("details", {})
                                websocket_ready = details.get("websocket_ready", False)
                                # v2.0: Check for ParallelInitializer's interactive_ready
                                parallel_interactive = details.get("parallel_initializer_interactive", False)

                                # v20.0: Interactive ready (WebSocket or ParallelInitializer)
                                # No need to wait for ML models - they warm in background
                                if (websocket_ready or parallel_interactive) and not is_ready:
                                    self.logger.info(f"âœ… Interactive ready - accepting (ws={websocket_ready}, pi={parallel_interactive})")
                                    is_ready = True
                                
                                # v10.0: Accept any status where ready=True (backend makes decision)
                                # This trusts the backend's progressive readiness logic
                                if data.get("ready") == True and not is_ready:
                                    self.logger.info(f"âœ… Backend reports ready={data.get('ready')} status={status}")
                                    is_ready = True
                                
                                if is_ready:
                                    backend_operational = True
                                    self.logger.info("âœ… Backend operationally ready")
                                    await self._broadcast_to_loading_page(
                                        "backend_ready",
                                        "Backend services ready",
                                        85,
                                        {"icon": "âœ…", "label": "Backend Ready", "backend_ready": True}
                                    )
                                
                                # v20.0: MINIMAL dynamic timeout extension
                                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                # Key change: We DON'T extend for ML warmup anymore because
                                # /health/ready returns ready=True when WebSocket is available.
                                # ML models load in background - users can interact immediately.
                                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                if not timeout_extended:
                                    # Detect Docker/Cloud Run initialization (these are real blockers)
                                    cloud_init = data.get("cloud_init", {})
                                    docker_starting = cloud_init.get("docker_starting", False)
                                    cloud_run_init = cloud_init.get("cloud_run_initializing", False)

                                    # v20.0: Only extend for cloud services, NOT for ML warmup
                                    # ML warmup no longer blocks readiness
                                    if docker_starting or cloud_run_init:
                                        # Cloud services initializing - moderate extension
                                        adaptive_max_wait = max(adaptive_max_wait, start_time + elapsed + 30)
                                        timeout_extended = True
                                        service = "Docker" if docker_starting else "Cloud Run"
                                        self.logger.info(f"â±ï¸  Extended timeout for {service}: +30s (new max: {adaptive_max_wait - start_time:.0f}s)")
                                        
                except Exception as e:
                    # /health/ready might not exist - fallback after 45s if /health works
                    if backend_http_ready and elapsed > 45:
                        backend_operational = True
                        self.logger.warning(f"âš ï¸ /health/ready unavailable, accepting after {elapsed:.0f}s")
                        await self._broadcast_to_loading_page(
                            "backend_ready",
                            "Backend services ready (fallback)",
                            85,
                            {"icon": "âš ï¸", "label": "Backend Ready", "backend_ready": True}
                        )
            
            # Phase 2: Check frontend (only after backend is operational)
            if backend_operational and not frontend_ready:
                try:
                    async with aiohttp.ClientSession(
                        timeout=aiohttp.ClientTimeout(total=2.0)
                    ) as session:
                        async with session.get(
                            f"http://localhost:{frontend_port}"
                        ) as resp:
                            if resp.status in [200, 304]:
                                frontend_ready = True
                                self.logger.info("âœ… Frontend ready")
                                await self._broadcast_to_loading_page(
                                    "frontend_ready",
                                    "Frontend interface ready",
                                    95,
                                    {"icon": "âœ…", "label": "Frontend Ready", "frontend_ready": True}
                                )
                except Exception:
                    pass
            
            # Phase 3: Both OPERATIONALLY ready = complete!
            if backend_operational and frontend_ready:
                self.logger.info("ğŸ‰ JARVIS startup complete (all systems operational)!")
                
                # Broadcast 100% completion
                await self._broadcast_to_loading_page(
                    "complete",
                    "JARVIS is online!",
                    100,
                    {
                        "icon": "âœ…",
                        "label": "Complete",
                        "sublabel": "All systems operational!",
                        "success": True,
                        "backend_ready": True,
                        "frontend_verified": True,
                        "redirect_url": f"http://localhost:{frontend_port}",
                    }
                )
                
                return True
            
            # Broadcast periodic progress updates
            progress = 50 + int((elapsed / max_wait) * 40)  # 50-90%
            if progress > last_progress + 5:  # Update every 5%
                last_progress = progress
                
                if not backend_http_ready:
                    status_msg = f"Starting backend... ({int(elapsed)}s)"
                    stage = "backend_starting"
                elif not backend_operational:
                    status_msg = f"Backend initializing services... ({int(elapsed)}s)"
                    stage = "backend_init"
                elif not frontend_ready:
                    status_msg = f"Starting frontend... ({int(elapsed)}s)"
                    stage = "frontend_starting"
                else:
                    status_msg = "Finalizing..."
                    stage = "finalizing"
                
                await self._broadcast_to_loading_page(
                    stage,
                    status_msg,
                    min(progress, 94),  # Cap at 94% until truly complete
                    {"icon": "â³", "label": "Starting", "sublabel": status_msg}
                )
            
            await asyncio.sleep(1.5)  # Slightly slower polling
        
        # Timeout - Only broadcast if we have SOME progress
        self.logger.warning(f"âš ï¸ JARVIS startup monitoring timeout after {adaptive_max_wait}s")
        self.logger.warning(f"   Status: http={backend_http_ready}, operational={backend_operational}, frontend={frontend_ready}")
        if extension_reasons:
            self.logger.warning(f"   Timeout was extended for: {', '.join(extension_reasons)}")
        
        # If backend is operational and frontend is ready, we can complete
        if backend_operational and frontend_ready:
            await self._broadcast_to_loading_page(
                "complete",
                "JARVIS is online!",
                100,
                {
                    "icon": "âœ…",
                    "label": "Complete",
                    "sublabel": "System ready!",
                    "success": True,
                    "backend_ready": True,
                    "frontend_verified": True,
                    "redirect_url": f"http://localhost:{frontend_port}",
                }
            )
            return True
        
        # If only backend HTTP is responding (not operational), broadcast warning
        if backend_http_ready:
            await self._broadcast_to_loading_page(
                "startup_slow",
                "JARVIS is starting slowly...",
                90,
                {
                    "icon": "âš ï¸",
                    "label": "Slow Startup",
                    "sublabel": "Services still initializing...",
                    "backend_ready": backend_operational,
                    "frontend_verified": frontend_ready,
                    "warning": "Startup took longer than expected",
                }
            )
        
        # Return true if backend is at least operational
        return backend_operational
    
    def _is_docker_running(self) -> bool:
        """
        v12.0: Check if Docker daemon is currently running using intelligent manager.
        Uses the new DockerDaemonManager with proper macOS process detection.

        Returns:
            True if Docker is running, False otherwise
        """
        # v12.0: Try intelligent Docker manager first (correct macOS detection)
        if _DOCKER_MANAGER_AVAILABLE:
            try:
                # Quick synchronous check using the diagnostic engine
                from backend.infrastructure.docker_daemon_manager import (
                    DockerDiagnosticEngine,
                    DiagnosticConfig,
                )

                # Create minimal diagnostic config for quick check
                config = DiagnosticConfig(
                    process_check_timeout=2.0,
                    socket_check_timeout=2.0,
                    api_check_timeout=3.0,
                    enable_deep_diagnostics=False,
                )
                engine = DockerDiagnosticEngine(config)

                # Run synchronous quick check
                import asyncio
                try:
                    loop = asyncio.get_running_loop()
                    # If we're in an async context, use create_task pattern
                    future = asyncio.ensure_future(engine.quick_health_check())
                    # Give it 3 seconds max
                    result = asyncio.get_event_loop().run_until_complete(
                        asyncio.wait_for(future, timeout=3.0)
                    )
                    return result.is_healthy
                except RuntimeError:
                    # No running loop - create one for this check
                    loop = asyncio.new_event_loop()
                    try:
                        result = loop.run_until_complete(
                            asyncio.wait_for(engine.quick_health_check(), timeout=3.0)
                        )
                        return result.is_healthy
                    finally:
                        loop.close()
            except Exception as e:
                self.logger.debug(f"Docker manager check failed, falling back: {e}")

        # Fallback to simple subprocess check
        try:
            import subprocess
            result = subprocess.run(
                ["docker", "info"],
                capture_output=True,
                timeout=3.0
            )
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError, Exception):
            return False
    
    async def _wait_for_ports_release(self, max_wait: float = 5.0) -> bool:
        """
        Wait for all critical ports to be fully released after cleanup.

        This prevents race conditions where start_system.py checks ports
        before they're fully released by the OS.

        Args:
            max_wait: Maximum time to wait in seconds

        Returns:
            True if all ports are free, False if timeout
        """
        import socket

        start_time = time.time()
        check_interval = 0.2

        while time.time() - start_time < max_wait:
            all_free = True

            for port in self.config.required_ports:
                try:
                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    sock.settimeout(0.1)
                    result = sock.connect_ex(('localhost', port))
                    sock.close()

                    if result == 0:  # Port is still in use
                        all_free = False
                        break
                except Exception:
                    pass  # Error connecting = port is free

            if all_free:
                self.logger.debug(f"All ports released after {time.time() - start_time:.1f}s")
                return True

            await asyncio.sleep(check_interval)

        self.logger.warning(f"Timeout waiting for ports to release after {max_wait}s")
        return False

    # =========================================================================
    # v91.0: ADVANCED SYSTEM PRIMITIVES - ML Prediction, Self-Healing, Coordination
    # =========================================================================

    def _register_degradation_features(self) -> None:
        """
        v91.0: Register features for graceful degradation management.

        Features are registered with priority levels:
        - Priority 10: Core functionality (never disabled)
        - Priority 7-9: Important features (disabled under heavy load)
        - Priority 4-6: Nice-to-have features (disabled under moderate load)
        - Priority 1-3: Non-essential features (disabled early)
        """
        if not self._graceful_degradation_manager:
            return

        # Core features (priority 10 - never disabled)
        self._graceful_degradation_manager.register_feature(
            "voice_recognition", min_level=DegradationLevel.EMERGENCY, priority=10
        )
        self._graceful_degradation_manager.register_feature(
            "command_processing", min_level=DegradationLevel.EMERGENCY, priority=10
        )

        # Important features (priority 7-9)
        self._graceful_degradation_manager.register_feature(
            "neural_mesh", min_level=DegradationLevel.MINIMAL, priority=8
        )
        self._graceful_degradation_manager.register_feature(
            "trinity_coordination", min_level=DegradationLevel.MINIMAL, priority=8
        )
        self._graceful_degradation_manager.register_feature(
            "health_monitoring", min_level=DegradationLevel.MINIMAL, priority=7
        )

        # Nice-to-have features (priority 4-6)
        self._graceful_degradation_manager.register_feature(
            "voice_narration", min_level=DegradationLevel.REDUCED, priority=5
        )
        self._graceful_degradation_manager.register_feature(
            "hot_reload", min_level=DegradationLevel.REDUCED, priority=5
        )
        self._graceful_degradation_manager.register_feature(
            "knowledge_indexing", min_level=DegradationLevel.REDUCED, priority=4
        )

        # Non-essential features (priority 1-3)
        self._graceful_degradation_manager.register_feature(
            "training_scheduler", min_level=DegradationLevel.FULL, priority=3
        )
        self._graceful_degradation_manager.register_feature(
            "model_downloading", min_level=DegradationLevel.FULL, priority=2
        )
        self._graceful_degradation_manager.register_feature(
            "experience_collection", min_level=DegradationLevel.FULL, priority=2
        )

        self.logger.debug("[v91] Registered 11 features for graceful degradation")

    async def _start_advanced_monitoring_tasks(self) -> None:
        """
        v91.0: Start background tasks for advanced system monitoring.

        Tasks:
        - Health prediction: Continuous ML-based failure prediction
        - Self-healing: Automatic remediation of predicted failures
        - Resource monitoring: Track ulimits and quotas
        - Degradation monitoring: Adjust feature levels based on load
        """
        if not _ADVANCED_PRIMITIVES_AVAILABLE:
            self.logger.debug("[v91] Advanced primitives not available - skipping monitoring tasks")
            return

        try:
            # Health prediction task
            if self._health_predictor:
                self._health_prediction_task = asyncio.create_task(
                    self._run_health_prediction_loop(),
                    name="health_prediction_loop"
                )

            # Self-healing task
            if self._self_healing_orchestrator:
                self._self_healing_task = asyncio.create_task(
                    self._run_self_healing_loop(),
                    name="self_healing_loop"
                )

            # Resource monitoring task
            if self._resource_quota_manager:
                self._resource_monitoring_task = asyncio.create_task(
                    self._run_resource_monitoring_loop(),
                    name="resource_monitoring_loop"
                )

            # Degradation monitoring task
            if self._graceful_degradation_manager:
                self._degradation_monitoring_task = asyncio.create_task(
                    self._run_degradation_monitoring_loop(),
                    name="degradation_monitoring_loop"
                )

            self.logger.info("[v91] Started advanced monitoring tasks (health, self-healing, resources, degradation)")

        except Exception as e:
            self.logger.warning(f"[v91] Could not start all monitoring tasks: {e}")

    async def _run_health_prediction_loop(self) -> None:
        """
        v91.0: Continuous loop for ML-based health prediction.

        Records metrics from running processes and predicts failures.
        """
        interval = float(os.getenv("JARVIS_HEALTH_CHECK_INTERVAL", "10.0"))

        while not self._shutdown_event.is_set():
            try:
                await self._record_process_health_metrics()
                await asyncio.sleep(interval)

            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.debug(f"[v91] Health prediction error: {e}")
                await asyncio.sleep(interval)

    async def _record_process_health_metrics(self) -> None:
        """
        v91.0: Record health metrics for all managed processes.

        Feeds data to the ML predictor for failure prediction.
        """
        if not self._health_predictor:
            return

        try:
            psutil = _get_psutil()
            if not psutil:
                return

            # Record metrics for J-Prime if running
            if self._jprime_orchestrator_process and self._jprime_orchestrator_process.returncode is None:
                try:
                    pid = self._jprime_orchestrator_process.pid
                    if pid and psutil.pid_exists(pid):
                        proc = psutil.Process(pid)
                        self._health_predictor.record_metric("jprime", "cpu", proc.cpu_percent())
                        self._health_predictor.record_metric("jprime", "memory", proc.memory_percent())

                        # Check for failure prediction
                        failure_prob = self._health_predictor.predict_failure_probability("jprime")
                        if failure_prob > 0.7:
                            self.logger.warning(
                                f"[v91] J-Prime failure prediction: {failure_prob*100:.1f}% probability in next 60s"
                            )
                except Exception:
                    pass

            # Record metrics for Reactor-Core if running
            if self._reactor_core_orchestrator_process and self._reactor_core_orchestrator_process.returncode is None:
                try:
                    pid = self._reactor_core_orchestrator_process.pid
                    if pid and psutil.pid_exists(pid):
                        proc = psutil.Process(pid)
                        self._health_predictor.record_metric("reactor_core", "cpu", proc.cpu_percent())
                        self._health_predictor.record_metric("reactor_core", "memory", proc.memory_percent())

                        failure_prob = self._health_predictor.predict_failure_probability("reactor_core")
                        if failure_prob > 0.7:
                            self.logger.warning(
                                f"[v91] Reactor-Core failure prediction: {failure_prob*100:.1f}% probability in next 60s"
                            )
                except Exception:
                    pass

            # Record metrics for main JARVIS process
            try:
                main_proc = psutil.Process()
                self._health_predictor.record_metric("jarvis_main", "cpu", main_proc.cpu_percent())
                self._health_predictor.record_metric("jarvis_main", "memory", main_proc.memory_percent())
                self._health_predictor.record_metric("jarvis_main", "fd_count", main_proc.num_fds() if hasattr(main_proc, 'num_fds') else 0)
            except Exception:
                pass

        except Exception as e:
            self.logger.debug(f"[v91] Metrics recording error: {e}")

    async def _run_self_healing_loop(self) -> None:
        """
        v91.0: Continuous loop for automatic self-healing.

        Monitors predicted failures and applies remediation.
        """
        interval = float(os.getenv("JARVIS_SELF_HEAL_INTERVAL", "30.0"))

        while not self._shutdown_event.is_set():
            try:
                await self._check_and_heal_processes()
                await asyncio.sleep(interval)

            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.debug(f"[v91] Self-healing error: {e}")
                await asyncio.sleep(interval)

    async def _check_and_heal_processes(self) -> None:
        """
        v91.0: Check process health and apply remediation if needed.
        v101.0: Integrated with SupervisorRestartManager for reactive restarts.
        """
        # v101.0: Check supervisor restart manager for actual process deaths
        try:
            restarted = await self._supervisor_restart_manager.check_and_restart_all()
            if restarted:
                self.logger.info(f"[v101] Cross-repo processes restarted: {', '.join(restarted)}")
        except Exception as e:
            self.logger.debug(f"[v101] Supervisor restart manager error: {e}")

        # v91.0: ML-based predictive healing (optional, requires advanced primitives)
        if not self._self_healing_orchestrator or not self._health_predictor:
            return

        processes_to_check = [
            ("jprime", self._restart_jprime_process),
            ("reactor_core", self._restart_reactor_core_process),
        ]

        for process_name, restart_func in processes_to_check:
            try:
                failure_prob = self._health_predictor.predict_failure_probability(process_name)
                health_score = self._health_predictor.get_health_score(process_name)

                if failure_prob > 0.8 or health_score < 0.3:
                    self.logger.warning(
                        f"[v91] {process_name} needs healing: "
                        f"failure_prob={failure_prob*100:.1f}%, health={health_score*100:.1f}%"
                    )

                    # Determine failure type based on health metrics
                    failure_type = self._determine_failure_type(process_name, health_score, failure_prob)

                    # Execute self-healing remediation
                    result = await self._self_healing_orchestrator.execute_remediation(
                        process_name=process_name,
                        failure_type=failure_type,
                    )

                    if result and result.action:
                        self.logger.info(f"[v91] Applied remediation to {process_name}: {result.action.name}")

                        # If action suggests restart, do it
                        if result.action in (RemediationAction.RESTART, RemediationAction.KILL_AND_RESTART):
                            await restart_func()

            except Exception as e:
                self.logger.debug(f"[v91] Heal check error for {process_name}: {e}")

    def _determine_failure_type(
        self,
        process_name: str,
        health_score: float,
        failure_prob: float,
    ) -> str:
        """
        v91.0: Determine the type of failure based on health metrics.

        Args:
            process_name: Name of the process
            health_score: Current health score (0.0 - 1.0)
            failure_prob: Failure probability (0.0 - 1.0)

        Returns:
            Failure type string for remediation strategy selection
        """
        if not self._health_predictor:
            return "unknown"

        try:
            # Check CPU metrics
            cpu_key = f"{process_name}:cpu"
            memory_key = f"{process_name}:memory"

            cpu_z = 0.0
            memory_z = 0.0

            # Get z-scores from the predictor's internal data
            if hasattr(self._health_predictor, '_metrics_history'):
                with self._health_predictor._lock:
                    if cpu_key in self._health_predictor._ewma_values:
                        history = self._health_predictor._metrics_history.get(cpu_key, [])
                        if history:
                            latest = history[-1][1]
                            ewma = self._health_predictor._ewma_values.get(cpu_key, latest)
                            std = max(0.001, self._health_predictor._ewma_variance.get(cpu_key, 0.001) ** 0.5)
                            cpu_z = abs(latest - ewma) / std

                    if memory_key in self._health_predictor._ewma_values:
                        history = self._health_predictor._metrics_history.get(memory_key, [])
                        if history:
                            latest = history[-1][1]
                            ewma = self._health_predictor._ewma_values.get(memory_key, latest)
                            std = max(0.001, self._health_predictor._ewma_variance.get(memory_key, 0.001) ** 0.5)
                            memory_z = abs(latest - ewma) / std

            # Determine failure type based on metrics
            if cpu_z > 3.0:
                return "high_cpu"
            elif memory_z > 3.0:
                return "memory_leak"
            elif health_score < 0.2:
                return "unresponsive"
            elif failure_prob > 0.9:
                return "crash"
            else:
                return "unknown"

        except Exception:
            return "unknown"

    async def _restart_jprime_process(self) -> None:
        """v91.0: Restart J-Prime process as part of self-healing."""
        self.logger.info("[v91] Self-healing: Restarting J-Prime process...")
        try:
            await self._stop_jprime_orchestrator()
            await asyncio.sleep(2.0)
            await self._start_jprime_orchestrator()
            self.logger.info("[v91] Self-healing: J-Prime restart complete")
        except Exception as e:
            self.logger.error(f"[v91] Self-healing: J-Prime restart failed: {e}")

    async def _restart_reactor_core_process(self) -> None:
        """v91.0: Restart Reactor-Core process as part of self-healing."""
        self.logger.info("[v91] Self-healing: Restarting Reactor-Core process...")
        try:
            await self._stop_reactor_core_orchestrator()
            await asyncio.sleep(2.0)
            await self._start_reactor_core_orchestrator()
            self.logger.info("[v91] Self-healing: Reactor-Core restart complete")
        except Exception as e:
            self.logger.error(f"[v91] Self-healing: Reactor-Core restart failed: {e}")

    # =========================================================================
    # v102.0: Process Tree Registration Helper
    # =========================================================================
    # This helper ensures Trinity processes (J-Prime, Reactor-Core) are registered
    # in the unified process tree for proper cascading shutdown and monitoring.
    # =========================================================================

    async def _register_trinity_process_in_tree(
        self,
        pid: int,
        name: str,
        role_str: str,
    ) -> None:
        """
        v102.0: Register a Trinity process in the unified process tree.
        v116.0: Also registers in GlobalProcessRegistry for SIGHUP protection.

        This ensures proper tracking and cascading shutdown of Trinity components.

        Args:
            pid: Process ID of the spawned process
            name: Human-readable name (e.g., "J-Prime", "Reactor-Core")
            role_str: Role string - one of "jprime", "reactor", "backend", "service"
        """
        # v116.0: CRITICAL - Register in GlobalProcessRegistry for SIGHUP protection
        # This MUST happen regardless of process tree state to prevent the SIGHUP
        # handler from killing our own spawned processes during restart
        try:
            from backend.core.supervisor_singleton import GlobalProcessRegistry
            port_map = {"jprime": 8000, "reactor": 8090, "backend": 8010, "service": 0}
            GlobalProcessRegistry.register(
                pid=pid,
                component=name,
                port=port_map.get(role_str, 0)
            )
            print(f"  [v116.0] âœ… Launched {name} (PID {pid}) registered in GlobalProcessRegistry")
        except Exception as reg_err:
            print(f"  [v116.0] âš ï¸ {name} GlobalProcessRegistry registration failed: {reg_err}")

        # v102.0: Process tree registration (optional - may not be initialized)
        if not hasattr(self, '_process_tree') or self._process_tree is None:
            self.logger.debug(f"[v102.0] Skipping process tree registration for {name} - tree not initialized")
            return

        try:
            from core.coding_council.advanced import ProcessRole

            # Map role string to ProcessRole enum
            role_map = {
                "jprime": ProcessRole.TRINITY_JPRIME,
                "reactor": ProcessRole.TRINITY_REACTOR,
                "backend": ProcessRole.BACKEND,
                "service": ProcessRole.SERVICE,
            }
            role = role_map.get(role_str, ProcessRole.SERVICE)

            # Register with supervisor as parent
            supervisor_pid = os.getpid()
            await self._process_tree.register_process(
                pid=pid,
                name=name,
                role=role,
                parent_pid=supervisor_pid,
                critical=True,  # Trinity components are critical
                metadata={
                    "version": "v102.0",
                    "component_type": role_str,
                    "started_at": time.time(),
                },
            )
            self.logger.debug(f"[v102.0] Registered {name} (PID {pid}) in process tree")

        except ImportError:
            self.logger.debug(f"[v102.0] ProcessRole not available - skipping registration for {name}")
        except Exception as e:
            self.logger.debug(f"[v102.0] Failed to register {name} in process tree: {e}")

    async def _unregister_trinity_process_from_tree(self, pid: int, name: str) -> None:
        """
        v102.0: Unregister a Trinity process from the unified process tree.
        v116.0: Also deregisters from GlobalProcessRegistry.

        Called when a Trinity process is stopped or crashes.
        """
        # v116.0: CRITICAL - Deregister from GlobalProcessRegistry
        # This allows the SIGHUP handler to kill orphaned processes from previous sessions
        try:
            from backend.core.supervisor_singleton import GlobalProcessRegistry
            GlobalProcessRegistry.deregister(pid)
            self.logger.debug(f"[v116.0] Deregistered {name} (PID {pid}) from GlobalProcessRegistry")
        except Exception as dereg_err:
            self.logger.debug(f"[v116.0] GlobalProcessRegistry deregistration failed for {name}: {dereg_err}")

        # v102.0: Process tree deregistration
        if not hasattr(self, '_process_tree') or self._process_tree is None:
            return

        try:
            await self._process_tree.unregister_process(pid)
            self.logger.debug(f"[v102.0] Unregistered {name} (PID {pid}) from process tree")
        except Exception as e:
            self.logger.debug(f"[v102.0] Failed to unregister {name} from process tree: {e}")

    # =========================================================================
    # v100.4: Unified Stop/Start Methods (Fix for undefined method bug)
    # =========================================================================
    # These methods provide a consistent interface for stopping and starting
    # Trinity components. They integrate with the v100.0 SafeProcess system
    # and handle both process handles and PID-based termination.
    # =========================================================================

    async def _stop_jprime_orchestrator(self) -> None:
        """
        v100.4: Stop J-Prime orchestrator gracefully.

        Handles multiple scenarios:
        1. Process handle is available (started by this session)
        2. Only PID is known (started by previous session)
        3. Process discovered via heartbeat file
        """
        self.logger.info("[v100.4] Stopping J-Prime orchestrator...")

        # Method 1: Use process handle if available
        if self._jprime_orchestrator_process is not None:
            try:
                pid = self._jprime_orchestrator_process.pid
                self.logger.debug(f"[v100.4] Terminating J-Prime via process handle (PID: {pid})")
                self._jprime_orchestrator_process.terminate()
                try:
                    await asyncio.wait_for(
                        self._jprime_orchestrator_process.wait(),
                        timeout=5.0
                    )
                except asyncio.TimeoutError:
                    self.logger.warning("[v100.4] J-Prime didn't terminate gracefully, killing...")
                    self._jprime_orchestrator_process.kill()
                    await self._jprime_orchestrator_process.wait()
            except Exception as e:
                self.logger.warning(f"[v100.4] Error stopping J-Prime process: {e}")
            finally:
                # v102.0: Unregister from process tree
                if pid:
                    await self._unregister_trinity_process_from_tree(pid, "J-Prime")
                self._jprime_orchestrator_process = None
            return

        # Method 2: Find and kill by heartbeat PID
        heartbeat_path = Path.home() / ".jarvis" / "trinity" / "components" / "jarvis_prime.json"
        if heartbeat_path.exists():
            try:
                with open(heartbeat_path) as f:
                    heartbeat = json.load(f)
                pid = heartbeat.get("pid")
                if pid:
                    import psutil
                    try:
                        proc = psutil.Process(pid)
                        if proc.is_running():
                            self.logger.debug(f"[v100.4] Terminating J-Prime via heartbeat PID: {pid}")
                            proc.terminate()
                            proc.wait(timeout=5.0)
                    except psutil.NoSuchProcess:
                        pass
                    except psutil.TimeoutExpired:
                        proc.kill()
            except Exception as e:
                self.logger.debug(f"[v100.4] Heartbeat-based stop failed: {e}")

        self.logger.info("[v100.4] J-Prime stop complete")

    async def _stop_reactor_core_orchestrator(self) -> None:
        """
        v100.4: Stop Reactor-Core orchestrator gracefully.

        Handles multiple scenarios:
        1. Process handle is available (started by this session)
        2. Only PID is known (started by previous session)
        3. Process discovered via heartbeat file
        """
        self.logger.info("[v100.4] Stopping Reactor-Core orchestrator...")

        # Method 1: Use process handle if available
        if self._reactor_core_orchestrator_process is not None:
            try:
                pid = self._reactor_core_orchestrator_process.pid
                self.logger.debug(f"[v100.4] Terminating Reactor-Core via process handle (PID: {pid})")
                self._reactor_core_orchestrator_process.terminate()
                try:
                    await asyncio.wait_for(
                        self._reactor_core_orchestrator_process.wait(),
                        timeout=5.0
                    )
                except asyncio.TimeoutError:
                    self.logger.warning("[v100.4] Reactor-Core didn't terminate gracefully, killing...")
                    self._reactor_core_orchestrator_process.kill()
                    await self._reactor_core_orchestrator_process.wait()
            except Exception as e:
                self.logger.warning(f"[v100.4] Error stopping Reactor-Core process: {e}")
            finally:
                # v102.0: Unregister from process tree
                if pid:
                    await self._unregister_trinity_process_from_tree(pid, "Reactor-Core")
                self._reactor_core_orchestrator_process = None
            return

        # Method 2: Find and kill by heartbeat PID
        heartbeat_path = Path.home() / ".jarvis" / "trinity" / "components" / "reactor_core.json"
        if heartbeat_path.exists():
            try:
                with open(heartbeat_path) as f:
                    heartbeat = json.load(f)
                pid = heartbeat.get("pid")
                if pid:
                    import psutil
                    try:
                        proc = psutil.Process(pid)
                        if proc.is_running():
                            self.logger.debug(f"[v100.4] Terminating Reactor-Core via heartbeat PID: {pid}")
                            proc.terminate()
                            proc.wait(timeout=5.0)
                    except psutil.NoSuchProcess:
                        pass
                    except psutil.TimeoutExpired:
                        proc.kill()
            except Exception as e:
                self.logger.debug(f"[v100.4] Heartbeat-based stop failed: {e}")

        self.logger.info("[v100.4] Reactor-Core stop complete")

    async def _start_jprime_orchestrator(self) -> None:
        """
        v100.4: Start J-Prime orchestrator using the v100.0 launch system.

        This is a thin wrapper that delegates to the robust v100.0 launcher
        with dynamic repo discovery, venv detection, and SafeProcess.
        """
        self.logger.info("[v100.4] Starting J-Prime orchestrator...")

        try:
            # Get Trinity config
            trinity_config = get_trinity_config()

            # Discover J-Prime path dynamically
            repo_discovery = DynamicRepoDiscovery(trinity_config)
            jprime_path = await repo_discovery.discover_jprime()

            if not jprime_path:
                raise FileNotFoundError("J-Prime repository not found")

            # Detect venv
            venv_detector = RobustVenvDetector(trinity_config)

            # Create trace context
            trace_ctx = TrinityTraceContext(
                component="supervisor",
                operation="jprime_restart",
                metadata={"reason": "self_healing"}
            )

            # Launch using v100.0 method
            await self._launch_jprime_orchestrator_v100(
                jprime_path, venv_detector, trinity_config, trace_ctx
            )

            self.logger.info("[v100.4] J-Prime orchestrator started")

        except Exception as e:
            self.logger.error(f"[v100.4] Failed to start J-Prime: {e}")
            raise

    async def _start_reactor_core_orchestrator(self) -> None:
        """
        v100.4: Start Reactor-Core orchestrator using the v100.0 launch system.

        This is a thin wrapper that delegates to the robust v100.0 launcher
        with dynamic repo discovery, venv detection, and SafeProcess.
        """
        self.logger.info("[v100.4] Starting Reactor-Core orchestrator...")

        try:
            # Get Trinity config
            trinity_config = get_trinity_config()

            # Discover Reactor-Core path dynamically
            repo_discovery = DynamicRepoDiscovery(trinity_config)
            reactor_path = await repo_discovery.discover_reactor_core()

            if not reactor_path:
                raise FileNotFoundError("Reactor-Core repository not found")

            # Detect venv
            venv_detector = RobustVenvDetector(trinity_config)

            # Create trace context
            trace_ctx = TrinityTraceContext(
                component="supervisor",
                operation="reactor_restart",
                metadata={"reason": "self_healing"}
            )

            # Launch using v100.0 method
            await self._launch_reactor_core_orchestrator_v100(
                reactor_path, venv_detector, trinity_config, trace_ctx
            )

            self.logger.info("[v100.4] Reactor-Core orchestrator started")

        except Exception as e:
            self.logger.error(f"[v100.4] Failed to start Reactor-Core: {e}")
            raise

    async def _run_resource_monitoring_loop(self) -> None:
        """
        v91.0: Continuous loop for resource quota monitoring.

        Tracks ulimits, file descriptors, memory usage.
        """
        interval = float(os.getenv("JARVIS_RESOURCE_CHECK_INTERVAL", "15.0"))

        while not self._shutdown_event.is_set():
            try:
                if self._resource_quota_manager:
                    usage = self._resource_quota_manager.get_current_usage()

                    # Log warnings for high usage
                    if usage.fd_usage_ratio > 0.8:
                        self.logger.warning(
                            f"[v91] High file descriptor usage: {usage.fd_usage_ratio*100:.1f}% "
                            f"({usage.open_fds}/{usage.max_fds})"
                        )

                    if usage.memory_usage_ratio > 0.85:
                        self.logger.warning(
                            f"[v91] High memory usage: {usage.memory_usage_ratio*100:.1f}% "
                            f"({usage.memory_used_mb:.0f}MB/{usage.memory_total_mb:.0f}MB)"
                        )

                    if usage.cpu_percent > 90:
                        self.logger.warning(f"[v91] High CPU usage: {usage.cpu_percent:.1f}%")

                await asyncio.sleep(interval)

            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.debug(f"[v91] Resource monitoring error: {e}")
                await asyncio.sleep(interval)

    async def _run_degradation_monitoring_loop(self) -> None:
        """
        v91.0: Continuous loop for graceful degradation management.

        Adjusts feature levels based on system load.
        """
        interval = float(os.getenv("JARVIS_DEGRADATION_CHECK_INTERVAL", "20.0"))

        while not self._shutdown_event.is_set():
            try:
                if self._graceful_degradation_manager:
                    new_level = self._graceful_degradation_manager.update_degradation_level()

                    # Log current state periodically
                    enabled = self._graceful_degradation_manager.get_enabled_features()
                    disabled = self._graceful_degradation_manager.get_disabled_features()

                    if disabled:
                        self.logger.info(
                            f"[v91] Degradation level: {new_level.name}, "
                            f"disabled features: {', '.join(disabled)}"
                        )

                await asyncio.sleep(interval)

            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.debug(f"[v91] Degradation monitoring error: {e}")
                await asyncio.sleep(interval)

    def _is_feature_enabled(self, feature_name: str) -> bool:
        """
        v91.0: Check if a feature is currently enabled based on degradation level.

        Args:
            feature_name: Name of the feature to check

        Returns:
            True if feature is enabled or degradation manager not available
        """
        if not self._graceful_degradation_manager:
            return True
        return self._graceful_degradation_manager.is_feature_enabled(feature_name)

    async def _broadcast_state_to_trinity(self, state_key: str, state_value: Any) -> None:
        """
        v91.0: Broadcast state to other Trinity components via distributed coordinator.

        Args:
            state_key: Key for the state
            state_value: Value to broadcast (must be JSON-serializable)
        """
        if not self._distributed_state_coordinator:
            return

        try:
            await self._distributed_state_coordinator.update_state(state_key, state_value)
        except Exception as e:
            self.logger.debug(f"[v91] State broadcast error: {e}")

    async def _sync_state_from_trinity(self, state_key: str) -> Any:
        """
        v91.0: Sync state from other Trinity components.

        Args:
            state_key: Key for the state to retrieve

        Returns:
            The state value, or None if not found
        """
        if not self._distributed_state_coordinator:
            return None

        try:
            return await self._distributed_state_coordinator.get_state(state_key)
        except Exception as e:
            self.logger.debug(f"[v91] State sync error: {e}")
            return None

    async def _stop_advanced_monitoring_tasks(self) -> None:
        """v91.0: Stop all advanced monitoring background tasks."""
        tasks_to_cancel = [
            self._health_prediction_task,
            self._self_healing_task,
            self._resource_monitoring_task,
            self._degradation_monitoring_task,
        ]

        for task in tasks_to_cancel:
            if task and not task.done():
                task.cancel()
                try:
                    await asyncio.wait_for(task, timeout=2.0)
                except (asyncio.CancelledError, asyncio.TimeoutError):
                    pass

        self.logger.debug("[v91] Stopped advanced monitoring tasks")

    def _setup_signal_handlers(self) -> None:
        """
        v123.0: Setup shutdown hooks only - signal handling moved to UnifiedSignalManager.

        The UnifiedSignalManager (installed in main()) handles SIGINT/SIGTERM using
        loop.add_signal_handler() which is the correct async-safe approach.

        DO NOT use signal.signal() here as it would override the UnifiedSignalManager.

        This method now only registers atexit handlers via shutdown_hook.
        """
        # Import and register shutdown hook early (handles atexit only)
        try:
            backend_path = Path(__file__).parent / "backend"
            if str(backend_path) not in sys.path:
                sys.path.insert(0, str(backend_path))

            from backend.scripts.shutdown_hook import register_handlers
            register_handlers()
            self.logger.debug("âœ… Shutdown hook handlers registered (atexit)")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Could not register shutdown hook: {e}")

        # v123.0: DO NOT install signal handlers here!
        # Signal handling is done by UnifiedSignalManager in main()
        # Using signal.signal() here would override the async-safe handlers
        self.logger.debug("[v123.0] Signal handlers managed by UnifiedSignalManager")

    async def _register_trinity_shutdown_hooks(self) -> None:
        """
        v81.0: Register TrinityIntegrator shutdown hooks with the supervisor.

        This integrates the coordinated shutdown manager with the supervisor's
        signal handling, ensuring all Trinity components are properly cleaned up
        on shutdown (SIGTERM, SIGINT, or graceful exit).

        Note: The TrinityIntegrator.stop() method handles health monitoring
        and IPC shutdown internally, so we just register hooks that need
        supervisor-level coordination.
        """
        if self._trinity_integrator is None:
            return

        try:
            # Register supervisor-level shutdown hooks
            shutdown_manager = self._trinity_integrator._shutdown_manager
            if shutdown_manager is None:
                self.logger.debug("[v81.0] No shutdown manager available in TrinityIntegrator")
                return

            # Register hooks for various shutdown phases
            from backend.core.coordinated_shutdown import ShutdownPhase

            # Hook to log shutdown initiation
            async def on_shutdown_start():
                self.logger.info("[v81.0] Shutdown hook: Trinity shutdown initiated...")

            # register_hook(name, phase, callback, priority, timeout, critical)
            shutdown_manager.register_hook(
                name="supervisor_shutdown_announce",
                phase=ShutdownPhase.ANNOUNCE,
                callback=on_shutdown_start,
                priority=100,
            )

            # Hook to flush IPC state (if available)
            async def persist_ipc_state():
                if self._trinity_integrator and self._trinity_integrator._ipc_bus:
                    self.logger.info("[v81.0] Shutdown hook: Persisting IPC state...")
                    ipc_bus = self._trinity_integrator._ipc_bus
                    # Flush any pending commands from fallback queue
                    if hasattr(ipc_bus, '_fallback_queue') and ipc_bus._fallback_queue:
                        try:
                            await ipc_bus._flush_fallback_queue()
                        except Exception as e:
                            self.logger.debug(f"[v81.0] Fallback queue flush skipped: {e}")

            shutdown_manager.register_hook(
                name="persist_trinity_ipc_state",
                phase=ShutdownPhase.SAVE,
                callback=persist_ipc_state,
                priority=50,
            )

            self.logger.info("[v81.0] âœ… Trinity shutdown hooks registered")

        except ImportError:
            self.logger.debug("[v81.0] Shutdown phase imports not available")
        except Exception as e:
            self.logger.warning(f"[v81.0] Failed to register shutdown hooks: {e}")

    async def cleanup_resources(self):
        """
        Cleanup remote resources (GCP VMs, Cloud Run, etc.) and local services on shutdown.

        Uses the enhanced shutdown_hook module which provides:
        - Async-safe cleanup with timeouts
        - Multiple fallback approaches (VM Manager, gcloud CLI)
        - Idempotent execution (safe to call multiple times)

        v9.5: Also destroys infrastructure that WE provisioned via InfrastructureOrchestrator.
        This ensures GCP resources don't stay deployed when JARVIS shuts down.
        """
        # v9.5: Cleanup Infrastructure Orchestrator (destroys Cloud Run we created)
        # This MUST run first to ensure resources are destroyed before VM cleanup
        if self._infra_orchestrator:
            try:
                self.logger.info("ğŸ”§ Destroying on-demand infrastructure...")
                success = await self._infra_orchestrator.cleanup_infrastructure()
                if success:
                    status = self._infra_orchestrator.get_status()
                    self.logger.info(
                        f"âœ… Infrastructure cleanup: destroyed {status['terraform_operations']['destroy_count']} resource(s)"
                    )
                else:
                    self.logger.warning("âš ï¸ Some infrastructure may not have been cleaned up")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Infrastructure cleanup error: {e}")

        # v12.0: Shutdown Docker Manager (save learning data, stop monitoring)
        if self._docker_manager and self._docker_manager_initialized:
            try:
                self.logger.info("ğŸ³ Stopping Docker Manager (saving learning data)...")
                await self._docker_manager.stop()
                self.logger.info("âœ… Docker Manager stopped gracefully")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Docker Manager cleanup error: {e}")

        # v100.0: Shutdown Trinity Voice Coordinator v100.0 (graceful with queue draining)
        try:
            if hasattr(self, '_trinity_voice_coordinator_instance') and self._trinity_voice_coordinator_instance:
                self.logger.info("ğŸ™ï¸  Shutting down Trinity Voice Coordinator v100.0...")

                # Final farewell announcement (CRITICAL priority to ensure it's spoken)
                try:
                    from backend.core.trinity_voice_coordinator import announce, VoiceContext, VoicePriority
                    success, reason = await announce(
                        message="JARVIS systems shutting down gracefully. Goodbye.",
                        context=VoiceContext.RUNTIME,
                        priority=VoicePriority.CRITICAL,
                        source="supervisor",
                        metadata={"event": "shutdown"}
                    )
                    if success:
                        # Give time for final announcement to be spoken
                        await asyncio.sleep(3.0)
                except Exception as e:
                    self.logger.debug(f"Final announcement skipped: {e}")

                # Graceful shutdown with queue draining (v100.0 feature)
                # This will process any remaining CRITICAL/HIGH announcements
                await self._trinity_voice_coordinator_instance.shutdown()
                self.logger.info("âœ… Trinity Voice Coordinator v100.0 stopped (queue drained)")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Trinity Voice Coordinator cleanup error: {e}")

        # v88.0: Shutdown Trinity Knowledge Indexer
        try:
            if hasattr(self, '_trinity_knowledge_indexer') and self._trinity_knowledge_indexer:
                self.logger.info("ğŸ§  Shutting down Trinity Knowledge Indexer...")

                # Stop background indexing and export loops
                await self._trinity_knowledge_indexer.stop()

                self.logger.info("âœ… Trinity Knowledge Indexer stopped")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Trinity Knowledge Indexer cleanup error: {e}")

        # v93.16: Shutdown Trinity Heartbeat System
        try:
            if self._trinity_heartbeat_task:
                self.logger.info("ğŸ’“ Stopping Trinity Heartbeat System...")
                self._trinity_heartbeat_task.cancel()
                try:
                    await self._trinity_heartbeat_task
                except asyncio.CancelledError:
                    pass
                self.logger.info("âœ… Trinity Heartbeat System stopped")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Trinity Heartbeat cleanup error: {e}")

        # v100.0: Shutdown Trinity Core Systems
        try:
            self.logger.info("ğŸ”§ Shutting down Trinity Core Systems...")

            # Cancel monitoring task first
            if self._trinity_core_systems_task:
                self._trinity_core_systems_task.cancel()
                try:
                    await self._trinity_core_systems_task
                except asyncio.CancelledError:
                    pass

            # Shutdown in reverse order
            shutdown_tasks = []

            if self._trinity_monitoring:
                shutdown_tasks.append(("TrinityMonitoring", self._trinity_monitoring.stop()))
            if self._trinity_training_pipeline:
                shutdown_tasks.append(("TrinityTrainingPipeline", self._trinity_training_pipeline.stop()))
            if self._trinity_knowledge_graph:
                shutdown_tasks.append(("TrinityKnowledgeGraph", self._trinity_knowledge_graph.stop()))
            if self._trinity_event_bus:
                shutdown_tasks.append(("TrinityEventBus", self._trinity_event_bus.stop()))

            # Execute shutdowns in parallel with timeout
            for name, coro in shutdown_tasks:
                try:
                    await asyncio.wait_for(coro, timeout=5.0)
                    self.logger.info(f"   âœ… {name} stopped")
                except asyncio.TimeoutError:
                    self.logger.warning(f"   âš ï¸ {name} shutdown timed out")
                except Exception as e:
                    self.logger.warning(f"   âš ï¸ {name} shutdown error: {e}")

            self.logger.info("âœ… Trinity Core Systems stopped")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Trinity Core Systems cleanup error: {e}")

        # v100.0: Shutdown AGI Orchestrator
        try:
            if self._agi_orchestrator:
                self.logger.info("ğŸ§  Shutting down AGI Orchestrator...")
                await asyncio.wait_for(self._agi_orchestrator.stop(), timeout=10.0)
                self.logger.info("âœ… AGI Orchestrator stopped")
        except asyncio.TimeoutError:
            self.logger.warning("âš ï¸ AGI Orchestrator shutdown timed out")
        except Exception as e:
            self.logger.warning(f"âš ï¸ AGI Orchestrator cleanup error: {e}")

        # v100.0: Shutdown Unified Model Serving
        try:
            if self._unified_model_serving:
                self.logger.info("ğŸ¤– Shutting down Unified Model Serving...")
                await asyncio.wait_for(self._unified_model_serving.stop(), timeout=10.0)
                self._unified_model_serving = None
                self.logger.info("âœ… Unified Model Serving stopped")
        except asyncio.TimeoutError:
            self.logger.warning("âš ï¸ Unified Model Serving shutdown timed out")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Unified Model Serving cleanup error: {e}")

        # v100.0: Shutdown Unified Agent Registry
        try:
            if self._unified_agent_registry:
                self.logger.info("ğŸ“‹ Shutting down Unified Agent Registry...")
                await asyncio.wait_for(self._unified_agent_registry.stop(), timeout=10.0)
                self._unified_agent_registry = None
                self.logger.info("âœ… Unified Agent Registry stopped")
        except asyncio.TimeoutError:
            self.logger.warning("âš ï¸ Unified Agent Registry shutdown timed out")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Unified Agent Registry cleanup error: {e}")

        # v100.0: Shutdown Distributed State Manager
        try:
            if self._distributed_state_manager:
                self.logger.info("ğŸ’¾ Shutting down Distributed State Manager...")
                await asyncio.wait_for(self._distributed_state_manager.stop(), timeout=10.0)
                self._distributed_state_manager = None
                self.logger.info("âœ… Distributed State Manager stopped")
        except asyncio.TimeoutError:
            self.logger.warning("âš ï¸ Distributed State Manager shutdown timed out")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Distributed State Manager cleanup error: {e}")

        # v100.0: Shutdown Continuous Learning Orchestrator
        try:
            if self._continuous_learning_orchestrator:
                self.logger.info("ğŸ“š Shutting down Continuous Learning Orchestrator...")
                await asyncio.wait_for(self._continuous_learning_orchestrator.stop(), timeout=15.0)
                self._continuous_learning_orchestrator = None
                self.logger.info("âœ… Continuous Learning Orchestrator stopped")
        except asyncio.TimeoutError:
            self.logger.warning("âš ï¸ Continuous Learning Orchestrator shutdown timed out")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Continuous Learning Orchestrator cleanup error: {e}")

        # v100.0: Shutdown Neural Mesh Registry Bridge
        try:
            if self._neural_mesh_bridge:
                self.logger.info("ğŸ”— Shutting down Neural Mesh Registry Bridge...")
                await asyncio.wait_for(self._neural_mesh_bridge.stop(), timeout=10.0)
                self._neural_mesh_bridge = None
                self.logger.info("âœ… Neural Mesh Registry Bridge stopped")
        except asyncio.TimeoutError:
            self.logger.warning("âš ï¸ Neural Mesh Registry Bridge shutdown timed out")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Neural Mesh Registry Bridge cleanup error: {e}")

        # v100.0: Shutdown Learning State Connector
        try:
            if self._learning_state_connector:
                self.logger.info("ğŸ’¾ Shutting down Learning State Connector...")
                await asyncio.wait_for(self._learning_state_connector.stop(), timeout=10.0)
                self._learning_state_connector = None
                self.logger.info("âœ… Learning State Connector stopped")
        except asyncio.TimeoutError:
            self.logger.warning("âš ï¸ Learning State Connector shutdown timed out")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Learning State Connector cleanup error: {e}")

        # v100.0: Shutdown Cross-Repo Experience Forwarder
        try:
            if self._experience_forwarder:
                self.logger.info("ğŸ”„ Shutting down Experience Forwarder...")
                await asyncio.wait_for(self._experience_forwarder.stop(), timeout=15.0)
                self._experience_forwarder = None
                self.logger.info("âœ… Experience Forwarder stopped")
        except asyncio.TimeoutError:
            self.logger.warning("âš ï¸ Experience Forwarder shutdown timed out")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Experience Forwarder cleanup error: {e}")

        # Cleanup JARVIS-Prime
        try:
            await self._stop_jarvis_prime()
            self.logger.info("âœ… JARVIS-Prime stopped")
        except Exception as e:
            self.logger.warning(f"âš ï¸ JARVIS-Prime cleanup error: {e}")

        # v10.0: Cleanup Reactor-Core API Server
        try:
            await self._shutdown_reactor_core()
        except Exception as e:
            self.logger.warning(f"âš ï¸ Reactor-Core cleanup error: {e}")

        # v104.0: Shutdown Ouroboros Self-Improvement Engine
        if self._ouroboros_engine is not None:
            try:
                self.logger.info("ğŸ Shutting down Ouroboros Self-Improvement Engine...")
                await asyncio.wait_for(
                    self._ouroboros_engine.shutdown(),
                    timeout=10.0
                )
                self._ouroboros_engine = None
                self.logger.info("âœ… Ouroboros shutdown complete")
            except asyncio.TimeoutError:
                self.logger.warning("âš ï¸ Ouroboros shutdown timed out")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Ouroboros cleanup error: {e}")

        # v105.0: Shutdown Advanced Ouroboros Orchestrator
        if self._ouroboros_advanced is not None:
            try:
                await asyncio.wait_for(
                    self._ouroboros_advanced.shutdown(),
                    timeout=5.0
                )
                self._ouroboros_advanced = None
                self.logger.info("âœ… Advanced Orchestrator shutdown complete")
            except asyncio.TimeoutError:
                self.logger.warning("âš ï¸ Advanced Orchestrator shutdown timed out")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Advanced Orchestrator cleanup error: {e}")

        # v105.0: Shutdown Cross-Repo Integration
        if self._ouroboros_cross_repo is not None:
            try:
                await asyncio.wait_for(
                    self._ouroboros_cross_repo.shutdown(),
                    timeout=5.0
                )
                self._ouroboros_cross_repo = None
                self.logger.info("âœ… Cross-Repo Integration shutdown complete")
            except asyncio.TimeoutError:
                self.logger.warning("âš ï¸ Cross-Repo Integration shutdown timed out")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Cross-Repo Integration cleanup error: {e}")

        # v106.0: Shutdown Brain Orchestrator (LLM Infrastructure)
        if self._brain_orchestrator is not None:
            try:
                from backend.core.ouroboros.brain_orchestrator import shutdown_brains
                await asyncio.wait_for(
                    shutdown_brains(),
                    timeout=10.0
                )
                self._brain_orchestrator = None
                self.logger.info("âœ… Brain Orchestrator shutdown complete")
            except asyncio.TimeoutError:
                self.logger.warning("âš ï¸ Brain Orchestrator shutdown timed out")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Brain Orchestrator cleanup error: {e}")

        # v107.0: Shutdown UI Integration
        if self._ouroboros_ui_controller is not None:
            try:
                from backend.core.ouroboros.ui_integration import disconnect_ouroboros_ui
                await asyncio.wait_for(
                    disconnect_ouroboros_ui(),
                    timeout=5.0
                )
                self._ouroboros_ui_controller = None
                self.logger.info("âœ… Ouroboros UI Integration shutdown complete")
            except asyncio.TimeoutError:
                self.logger.warning("âš ï¸ UI Integration shutdown timed out")
            except Exception as e:
                self.logger.warning(f"âš ï¸ UI Integration cleanup error: {e}")

        # v107.0: Shutdown Trinity Integration (unified layer)
        if self._ouroboros_trinity is not None:
            try:
                from backend.core.ouroboros.trinity_integration import shutdown_trinity_integration
                await asyncio.wait_for(
                    shutdown_trinity_integration(),
                    timeout=10.0
                )
                self._ouroboros_trinity = None
                self.logger.info("âœ… Trinity Integration shutdown complete")
            except asyncio.TimeoutError:
                self.logger.warning("âš ï¸ Trinity Integration shutdown timed out")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Trinity Integration cleanup error: {e}")

        # v107.0: Shutdown Neural Mesh
        if self._neural_mesh is not None:
            try:
                from backend.core.ouroboros.neural_mesh import shutdown_neural_mesh
                await asyncio.wait_for(
                    shutdown_neural_mesh(),
                    timeout=10.0
                )
                self._neural_mesh = None
                self.logger.info("âœ… Neural Mesh shutdown complete")
            except asyncio.TimeoutError:
                self.logger.warning("âš ï¸ Neural Mesh shutdown timed out")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Neural Mesh cleanup error: {e}")

        # v107.0: Shutdown Native Self-Improvement
        if self._native_self_improvement is not None:
            try:
                from backend.core.ouroboros.native_integration import shutdown_native_self_improvement
                await asyncio.wait_for(
                    shutdown_native_self_improvement(),
                    timeout=10.0
                )
                self._native_self_improvement = None
                self.logger.info("âœ… Native Self-Improvement shutdown complete")
            except asyncio.TimeoutError:
                self.logger.warning("âš ï¸ Native Self-Improvement shutdown timed out")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Native Self-Improvement cleanup error: {e}")

        # v4.0: Shutdown Autonomous Self-Programming System
        if hasattr(self, '_autonomous_components') and self._autonomous_components is not None:
            try:
                from backend.core.ouroboros.integration import shutdown_autonomous_self_programming_full
                await asyncio.wait_for(
                    shutdown_autonomous_self_programming_full(),
                    timeout=15.0
                )
                self._autonomous_components = None
                self.logger.info("âœ… Autonomous Self-Programming shutdown complete")
            except asyncio.TimeoutError:
                self.logger.warning("âš ï¸ Autonomous Self-Programming shutdown timed out")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Autonomous Self-Programming cleanup error: {e}")

        # v77.0: Shutdown Coding Council
        try:
            from core.coding_council.startup import coding_council_shutdown_hook
            await coding_council_shutdown_hook(bootstrapper=self)
            self.logger.info("âœ… Coding Council shutdown complete")
        except ImportError:
            pass  # Coding Council not available
        except Exception as e:
            self.logger.warning(f"âš ï¸ Coding Council cleanup error: {e}")

        # v72.0: Cleanup Trinity component subprocesses
        await self._shutdown_trinity_components()

        # v90.0: Release all port locks
        if self._port_manager is not None:
            try:
                self.logger.info("ğŸ”“ Releasing port locks...")
                self._port_manager.release_all()
                self.logger.info("âœ… Port locks released")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Port lock release error: {e}")
            finally:
                self._port_manager = None

        # Cleanup GCP resources
        try:
            from backend.scripts.shutdown_hook import cleanup_remote_resources

            self.logger.info("ğŸ§¹ Cleaning up remote resources...")
            result = await cleanup_remote_resources(
                timeout=30.0,
                reason="Supervisor shutdown"
            )

            if result.get("success"):
                vms = result.get("vms_cleaned", 0)
                method = result.get("method", "unknown")
                if vms > 0:
                    self.logger.info(f"âœ… Cleaned {vms} VM(s) via {method}")
                else:
                    self.logger.info("âœ… No VMs to clean up")
            else:
                errors = result.get("errors", [])
                self.logger.warning(f"âš ï¸ Cleanup completed with issues: {errors}")

        except Exception as e:
            self.logger.error(f"âŒ Failed to cleanup remote resources: {e}")
    
    async def _propagate_zero_touch_settings(self) -> None:
        """
        Propagate Zero-Touch settings to child processes via environment variables.
        
        These are read by JARVISSupervisor to configure autonomous update behavior.
        """
        # Zero-Touch master switch
        if self.config.zero_touch_enabled:
            os.environ["JARVIS_ZERO_TOUCH_ENABLED"] = "true"
            self.logger.info("ğŸ¤– Zero-Touch autonomous updates ENABLED")
            
            # Propagate individual settings
            os.environ["JARVIS_ZERO_TOUCH_REQUIRE_IDLE"] = str(self.config.zero_touch_require_idle).lower()
            os.environ["JARVIS_ZERO_TOUCH_CHECK_BUSY"] = str(self.config.zero_touch_check_busy).lower()
            os.environ["JARVIS_ZERO_TOUCH_AUTO_SECURITY"] = str(self.config.zero_touch_auto_security).lower()
            os.environ["JARVIS_ZERO_TOUCH_AUTO_CRITICAL"] = str(self.config.zero_touch_auto_critical).lower()
            os.environ["JARVIS_ZERO_TOUCH_AUTO_MINOR"] = str(self.config.zero_touch_auto_minor).lower()
            os.environ["JARVIS_ZERO_TOUCH_AUTO_MAJOR"] = str(self.config.zero_touch_auto_major).lower()
        
        # Dead Man's Switch settings
        if self.config.dms_enabled:
            os.environ["JARVIS_DMS_ENABLED"] = "true"
            os.environ["JARVIS_DMS_PROBATION_SECONDS"] = str(self.config.dms_probation_seconds)
            os.environ["JARVIS_DMS_MAX_FAILURES"] = str(self.config.dms_max_failures)
            self.logger.info(f"ğŸ¯ Dead Man's Switch: {self.config.dms_probation_seconds}s probation")
    
    async def _propagate_agi_os_settings(self) -> None:
        """
        Propagate AGI OS settings for intelligent integration.
        
        When enabled, AGI OS can:
        - Use VoiceApprovalManager for update consent
        - Push update events to ProactiveEventStream
        - Leverage IntelligentActionOrchestrator for optimal timing
        """
        if self.config.agi_os_enabled:
            os.environ["JARVIS_AGI_OS_ENABLED"] = "true"
            
            if self.config.agi_os_approval_for_updates:
                os.environ["JARVIS_AGI_OS_APPROVAL_UPDATES"] = "true"
                self.logger.info("ğŸ§  AGI OS: Voice approval for updates ENABLED")
            
            # Check if AGI OS is available
            # Use consistent import path (same as internal backend imports)
            try:
                from agi_os import get_agi_os
                self.logger.info("ğŸ§  AGI OS module available - will integrate with supervisor")
            except ImportError:
                self.logger.debug("AGI OS module not available - supervisor will operate independently")
    
    async def _initialize_rate_orchestrator(self) -> None:
        """
        v5.0: Initialize the Intelligent Rate Orchestrator for ML-powered rate limiting.
        
        This starts the ML forecasting system that:
        - Predicts rate limit breaches BEFORE they happen
        - Adaptively throttles GCP, CloudSQL, and Claude API calls
        - Uses Holt-Winters exponential smoothing for time-series forecasting
        - Implements PID control for smooth throttle adjustments
        - Schedules requests with priority-based queuing
        
        The orchestrator runs background tasks for:
        - Continuous throttle adjustment (every 1 second)
        - Forecast model updates (every 1 minute)
        """
        try:
            try:
                from core.intelligent_rate_orchestrator import (
                    get_rate_orchestrator,
                    ServiceType,
                )
            except ImportError:
                # Fallback to backend-prefixed import
                from core.intelligent_rate_orchestrator import (
                    get_rate_orchestrator,
                    ServiceType,
                )
            
            # Initialize and start the orchestrator
            orchestrator = await get_rate_orchestrator()
            
            # Log initial status
            stats = orchestrator.get_stats()
            service_count = len(stats.get("services", {}))
            
            self.logger.info(f"ğŸ¯ Intelligent Rate Orchestrator initialized")
            self.logger.info(f"   â€¢ ML Forecasting: Enabled (Holt-Winters time-series)")
            self.logger.info(f"   â€¢ Adaptive Throttling: Enabled (PID control)")
            self.logger.info(f"   â€¢ Services Configured: {service_count}")
            self.logger.info(f"   â€¢ Background Tasks: Adjustment loop, Forecast loop")
            
            # Propagate rate orchestrator availability to child processes
            os.environ["JARVIS_RATE_ORCHESTRATOR_ENABLED"] = "true"
            os.environ["JARVIS_ML_RATE_FORECASTING"] = "true"
            
            # Print status
            print(f"  {TerminalUI.GREEN}âœ“ Rate Limiting: ML Forecasting + Adaptive Throttling{TerminalUI.RESET}")
            
        except ImportError as e:
            self.logger.warning(f"âš ï¸ Intelligent Rate Orchestrator not available: {e}")
            self.logger.warning("   Rate limiting will use basic fallback mode")
            os.environ["JARVIS_RATE_ORCHESTRATOR_ENABLED"] = "false"
            print(f"  {TerminalUI.YELLOW}âš ï¸ Rate Limiting: Basic mode (ML forecasting unavailable){TerminalUI.RESET}")
            
        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize Rate Orchestrator: {e}")
            os.environ["JARVIS_RATE_ORCHESTRATOR_ENABLED"] = "false"

    async def _initialize_agentic_security(self) -> None:
        """
        v6.0: Initialize the Two-Tier Agentic Security System.

        This starts the safety supervision layer for agentic (Computer Use) execution:
        - AgenticWatchdog: Monitors heartbeats, activity rates, triggers kill switch
        - TieredCommandRouter: Routes commands based on Tier 1 (safe) vs Tier 2 (agentic)
        - VBIA Integration: Strict voice authentication for Tier 2 commands

        Two-Tier Security Model:
        - Tier 1 "JARVIS": Safe APIs, read-only, optional auth, Gemini Flash
        - Tier 2 "JARVIS ACCESS": Full Computer Use, strict VBIA, Claude Sonnet

        Safety Features:
        - Heartbeat monitoring with configurable timeout
        - Activity rate limiting (prevents click storms)
        - Automatic downgrade to passive mode on anomalies
        - Voice announcement of safety events
        - Comprehensive audit logging
        """
        try:
            # Broadcast Two-Tier initialization start
            await self._broadcast_startup_progress(
                stage="two_tier_init",
                message="Initializing Two-Tier Agentic Security System...",
                progress=82,
                metadata={
                    "two_tier": {
                        "overall_status": "initializing",
                        "message": "Starting Two-Tier Security initialization...",
                    }
                }
            )

            # v6.2: Announce two-tier security initialization
            if self.config.voice_enabled:
                await self.narrator.speak("Initializing two-tier security architecture.", wait=False)

            # Initialize Watchdog
            if self._watchdog_enabled:
                try:
                    from core.agentic_watchdog import (
                        start_watchdog,
                        WatchdogConfig,
                        AgenticMode,
                    )

                    # Create TTS callback for watchdog announcements
                    async def watchdog_tts(text: str):
                        await self.narrator.speak(text, wait=False)

                    self._watchdog = await start_watchdog(
                        tts_callback=watchdog_tts if self.config.voice_enabled else None
                    )

                    self.logger.info("ğŸ›¡ï¸ Agentic Watchdog initialized")
                    self.logger.info("   â€¢ Kill Switch: Armed (heartbeat timeout, activity spike)")
                    self.logger.info("   â€¢ Safety Mode: Active monitoring")

                    os.environ["JARVIS_WATCHDOG_ENABLED"] = "true"
                    print(f"  {TerminalUI.GREEN}âœ“ Watchdog: Active safety monitoring{TerminalUI.RESET}")

                    # v6.2: Announce watchdog ready
                    if self.config.voice_enabled:
                        await self.narrator.speak("Agentic watchdog armed. Kill switch ready.", wait=False)

                    # Broadcast Two-Tier Watchdog status to loading page
                    await self._broadcast_startup_progress(
                        stage="two_tier_watchdog",
                        message="Agentic Watchdog initialized - kill switch armed",
                        progress=83,
                        metadata={
                            "two_tier": {
                                "watchdog_ready": True,
                                "watchdog_status": "active",
                                "watchdog_mode": "monitoring",
                                "message": "Watchdog initialized - safety monitoring active",
                            }
                        }
                    )

                except ImportError as e:
                    self.logger.warning(f"âš ï¸ Agentic Watchdog not available: {e}")
                    os.environ["JARVIS_WATCHDOG_ENABLED"] = "false"
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Watchdog: Not available{TerminalUI.RESET}")

            # Initialize Tiered VBIA Adapter
            vbia_adapter = None
            if self._tiered_routing_enabled:
                try:
                    from core.tiered_vbia_adapter import (
                        TieredVBIAAdapter,
                        TieredVBIAConfig,
                        get_tiered_vbia_adapter,
                    )

                    vbia_adapter = await get_tiered_vbia_adapter()
                    self.logger.info("ğŸ” Tiered VBIA Adapter initialized")
                    print(f"  {TerminalUI.GREEN}âœ“ VBIA Adapter: Tiered authentication ready{TerminalUI.RESET}")

                    # v6.2: Announce VBIA ready with visual security
                    if self.config.voice_enabled:
                        # Check if visual security is enabled
                        visual_enabled = os.getenv("JARVIS_VISUAL_SECURITY_ENABLED", "true").lower() == "true"
                        if visual_enabled:
                            await self.narrator.speak("Voice biometric authentication ready. Visual threat detection enabled.", wait=False)
                        else:
                            await self.narrator.speak("Voice biometric authentication ready. Tiered thresholds configured.", wait=False)

                    # Broadcast Two-Tier VBIA status to loading page
                    await self._broadcast_startup_progress(
                        stage="two_tier_vbia",
                        message="Voice biometric authentication ready",
                        progress=85,
                        metadata={
                            "two_tier": {
                                "vbia_adapter_ready": True,
                                "vbia_tier1_threshold": 0.70,
                                "vbia_tier2_threshold": 0.85,
                                "vbia_liveness_enabled": True,
                                "message": "VBIA Adapter ready - tiered thresholds active",
                            }
                        }
                    )

                except ImportError as e:
                    self.logger.warning(f"âš ï¸ Tiered VBIA Adapter not available: {e}")
                    print(f"  {TerminalUI.YELLOW}âš ï¸ VBIA Adapter: Not available (will use fallback){TerminalUI.RESET}")

            # v6.2 NEW: Initialize Cross-Repo State System
            try:
                from core.cross_repo_state_initializer import (
                    initialize_cross_repo_state,
                    CrossRepoStateConfig,
                )

                # Initialize cross-repo communication infrastructure
                cross_repo_success = await initialize_cross_repo_state()

                if cross_repo_success:
                    self.logger.info("ğŸŒ Cross-Repo State System initialized")
                    self.logger.info("   â€¢ JARVIS â†” JARVIS Prime â†” Reactor Core connected")
                    self.logger.info("   â€¢ VBIA events: Real-time sharing enabled")
                    self.logger.info("   â€¢ Visual security: Event emission ready")
                    print(f"  {TerminalUI.GREEN}âœ“ Cross-Repo: VBIA v6.2 event sharing active{TerminalUI.RESET}")

                    # v6.2: Announce cross-repo connection
                    if self.config.voice_enabled:
                        await self.narrator.speak("Cross-repository integration complete. Intelligence shared across all platforms.", wait=False)

                    # Broadcast cross-repo status to loading page
                    await self._broadcast_startup_progress(
                        stage="cross_repo_init",
                        message="Cross-repository communication established",
                        progress=86,
                        metadata={
                            "cross_repo": {
                                "initialized": True,
                                "visual_security_enabled": True,
                                "event_sharing_ready": True,
                                "message": "VBIA v6.2 cross-repo events active",
                            }
                        }
                    )
                else:
                    self.logger.warning("âš ï¸ Cross-Repo State System initialization failed")
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Cross-Repo: Initialization failed (VBIA events disabled){TerminalUI.RESET}")

            except ImportError as e:
                self.logger.debug(f"Cross-Repo State System not available: {e}")
                print(f"  {TerminalUI.YELLOW}âš ï¸ Cross-Repo: Not available (VBIA events disabled){TerminalUI.RESET}")

            # Initialize Tiered Router
            if self._tiered_routing_enabled:
                try:
                    from core.tiered_command_router import (
                        TieredCommandRouter,
                        TieredRouterConfig,
                        set_tiered_router,
                    )

                    # Create TTS callback for router announcements
                    async def router_tts(text: str):
                        await self.narrator.speak(text, wait=False)

                    config = TieredRouterConfig()

                    # Wire up VBIA adapter callbacks
                    vbia_callback = None
                    liveness_callback = None
                    if vbia_adapter:
                        vbia_callback = vbia_adapter.verify_speaker
                        liveness_callback = vbia_adapter.verify_liveness

                    self._tiered_router = TieredCommandRouter(
                        config=config,
                        vbia_callback=vbia_callback,
                        liveness_callback=liveness_callback,
                        tts_callback=router_tts if self.config.voice_enabled else None,
                    )

                    # Register in global registry for API access
                    set_tiered_router(self._tiered_router)

                    self.logger.info("ğŸ¯ Two-Tier Command Router initialized")
                    self.logger.info(f"   â€¢ Tier 1: {config.tier1_backend} (safe, low-auth)")
                    self.logger.info(f"   â€¢ Tier 2: {config.tier2_backend} (agentic, strict-auth)")
                    self.logger.info(f"   â€¢ VBIA Thresholds: T1={config.tier1_vbia_threshold:.0%}, T2={config.tier2_vbia_threshold:.0%}")
                    self.logger.info(f"   â€¢ VBIA Adapter: {'Connected' if vbia_adapter else 'Fallback mode'}")

                    os.environ["JARVIS_TIERED_ROUTING"] = "true"
                    os.environ["JARVIS_TIER2_BACKEND"] = config.tier2_backend
                    print(f"  {TerminalUI.GREEN}âœ“ Two-Tier Security: T1=Gemini, T2=Claude+VBIA{TerminalUI.RESET}")

                    # Store VBIA adapter for later use
                    self._vbia_adapter = vbia_adapter

                    # Broadcast Two-Tier Router status to loading page
                    await self._broadcast_startup_progress(
                        stage="two_tier_router",
                        message="Two-Tier command routing ready",
                        progress=87,
                        metadata={
                            "two_tier": {
                                "router_ready": True,
                                "tier1_operational": True,
                                "tier2_operational": True,
                                "message": "Router ready - Tier 1 (Gemini) + Tier 2 (Claude) active",
                            }
                        }
                    )

                    # Broadcast Two-Tier system fully ready
                    await self._broadcast_startup_progress(
                        stage="two_tier_ready",
                        message="Two-Tier Agentic Security System fully operational",
                        progress=89,
                        metadata={
                            "two_tier": {
                                "watchdog_ready": self._watchdog is not None,
                                "router_ready": True,
                                "vbia_adapter_ready": vbia_adapter is not None,
                                "tier1_operational": True,
                                "tier2_operational": True,
                                "watchdog_status": "active" if self._watchdog else "disabled",
                                "watchdog_mode": "monitoring" if self._watchdog else "idle",
                                "overall_status": "operational",
                                "message": "Two-Tier Security fully operational - all components ready",
                            }
                        }
                    )

                    # v6.2: Announce complete two-tier security with visual protection
                    if self.config.voice_enabled:
                        visual_enabled = os.getenv("JARVIS_VISUAL_SECURITY_ENABLED", "true").lower() == "true"
                        if visual_enabled:
                            await self.narrator.speak("Two-tier security fully operational. I'm protected by voice biometrics and visual threat detection.", wait=False)
                        else:
                            await self.narrator.speak("Two-tier security fully operational. Safe mode and agentic mode ready.", wait=False)

                except ImportError as e:
                    self.logger.warning(f"âš ï¸ Tiered Router not available: {e}")
                    os.environ["JARVIS_TIERED_ROUTING"] = "false"
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Tiered Routing: Not available{TerminalUI.RESET}")

            # Initialize Agentic Task Runner (unified execution engine)
            if self._agentic_runner_enabled:
                try:
                    from core.agentic_task_runner import (
                        AgenticTaskRunner,
                        AgenticRunnerConfig,
                        set_agentic_runner,
                    )

                    # Create TTS callback for runner narration
                    async def runner_tts(text: str):
                        await self.narrator.speak(text, wait=False)

                    runner_config = AgenticRunnerConfig()
                    self._agentic_runner = AgenticTaskRunner(
                        config=runner_config,
                        tts_callback=runner_tts if self.config.voice_enabled else None,
                        watchdog=self._watchdog,
                        logger=self.logger,
                    )

                    # Initialize the runner
                    await self._agentic_runner.initialize()

                    # Set global instance for access by other components
                    set_agentic_runner(self._agentic_runner)

                    # Wire router's execute_tier2 to use the runner
                    if self._tiered_router:
                        self._wire_router_to_runner()

                    self.logger.info("ğŸ¤– Agentic Task Runner initialized")
                    self.logger.info(f"   â€¢ Watchdog: {'Attached' if self._watchdog else 'Independent'}")
                    self.logger.info(f"   â€¢ Ready: {self._agentic_runner.is_ready}")
                    os.environ["JARVIS_AGENTIC_RUNNER"] = "true"
                    print(f"  {TerminalUI.GREEN}âœ“ Agentic Runner: Computer Use ready{TerminalUI.RESET}")

                    # v10.0: Connect Training Status Hub for Feedback Loop
                    # This enables voice announcements during training
                    await self._connect_training_status_hub()

                except ImportError as e:
                    self.logger.warning(f"âš ï¸ Agentic Runner not available: {e}")
                    os.environ["JARVIS_AGENTIC_RUNNER"] = "false"
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Agentic Runner: Not available{TerminalUI.RESET}")

            # Log overall status
            if self._watchdog_enabled or self._tiered_routing_enabled or self._agentic_runner_enabled:
                self.logger.info("âœ… Two-Tier Agentic Security System ready")
            else:
                self.logger.info("â„¹ï¸ Agentic security disabled (use JARVIS_WATCHDOG_ENABLED=true to enable)")

        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize Agentic Security: {e}")
            os.environ["JARVIS_WATCHDOG_ENABLED"] = "false"
            os.environ["JARVIS_TIERED_ROUTING"] = "false"
            os.environ["JARVIS_AGENTIC_RUNNER"] = "false"

    async def _initialize_jarvis_prime(self) -> None:
        """
        v8.0: Initialize JARVIS-Prime Tier-0 Brain with Memory-Aware Hybrid Routing.

        This dynamically decides the optimal mode based on available system memory:
        - RAM â‰¥ 8GB â†’ Local subprocess mode (FREE, fastest)
        - RAM 4-8GB â†’ Cloud Run mode (pay-per-use, ~$0.02/request)
        - RAM < 4GB â†’ Gemini API fallback (cheapest, ~$0.0001/1K tokens)

        Features:
        - Memory-aware automatic routing (no hardcoding!)
        - Multi-tier fallback chain: Local â†’ Cloud Run â†’ Gemini API
        - Circuit breaker pattern for resilience
        - Real-time memory monitoring with dynamic switching
        - OpenAI-compatible API at /v1/chat/completions
        - Reactor-Core integration for auto-deployment of trained models

        Architecture:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  JARVIS-Prime Memory-Aware Hybrid Router (v8.0)                 â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚              Memory Pressure Monitor                       â”‚  â”‚
        â”‚  â”‚  RAM â‰¥ 8GB â†’ LOCAL    RAM 4-8GB â†’ CLOUD    < 4GB â†’ API   â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚                             â”‚                                   â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
        â”‚  â”‚   Local     â”‚   â”‚   Cloud Run  â”‚   â”‚    Gemini API      â”‚   â”‚
        â”‚  â”‚ (Port 8002) â”‚â†’â†’â†’â”‚  (GCR URL)   â”‚â†’â†’â†’â”‚    (Fallback)      â”‚   â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
        â”‚         â†“                 â†“                    â†“               â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚              CircuitBreaker + Retry Logic                 â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        if not self.config.jarvis_prime_enabled:
            self.logger.info("â„¹ï¸ JARVIS-Prime disabled via configuration")
            os.environ["JARVIS_PRIME_ENABLED"] = "false"
            return

        try:
            # Import memory-aware client
            from core.jarvis_prime_client import (
                JarvisPrimeClient,
                JarvisPrimeConfig,
                get_jarvis_prime_client,
                get_system_memory_status,
                RoutingMode,
            )

            # Get current memory status for decision
            memory_status = await get_system_memory_status()
            available_gb = memory_status["available_gb"]
            recommended_mode = memory_status["recommended_mode"]

            self.logger.info(
                f"ğŸ§  Memory Status: {available_gb:.1f}GB available, "
                f"recommended mode: {recommended_mode}"
            )

            # Broadcast JARVIS-Prime initialization start
            await self._broadcast_startup_progress(
                stage="jarvis_prime_init",
                message=f"Initializing JARVIS-Prime (Mode: {recommended_mode}, RAM: {available_gb:.1f}GB)...",
                progress=75,
                metadata={
                    "jarvis_prime": {
                        "status": "initializing",
                        "mode": recommended_mode,
                        "memory_available_gb": available_gb,
                        "memory_status": memory_status,
                    }
                }
            )

            # Build configuration from environment and current state
            client_config = JarvisPrimeConfig(
                # Local settings
                local_host=self.config.jarvis_prime_host,
                local_port=self.config.jarvis_prime_port,
                # Cloud Run settings
                cloud_run_url=self.config.jarvis_prime_cloud_run_url or os.getenv(
                    "JARVIS_PRIME_CLOUD_RUN_URL",
                    "https://jarvis-prime-dev-888774109345.us-central1.run.app"
                ),
                use_cloud_run=self.config.jarvis_prime_use_cloud_run or bool(
                    self.config.jarvis_prime_cloud_run_url
                ),
            )

            # Create the memory-aware client
            self._jarvis_prime_client = JarvisPrimeClient(client_config)

            # Log the decision
            mode, reason = self._jarvis_prime_client.decide_mode()
            self.logger.info(f"ğŸ¯ JARVIS-Prime routing decision: {mode.value} ({reason})")

            # Initialize based on recommended mode
            if mode == RoutingMode.LOCAL:
                # Start local subprocess if not already running
                await self._init_jarvis_prime_local_if_needed()
            elif mode == RoutingMode.CLOUD_RUN:
                # Verify Cloud Run is accessible
                await self._init_jarvis_prime_cloud_run()
            elif mode == RoutingMode.GEMINI_API:
                self.logger.info("ğŸ“¡ Using Gemini API fallback due to low memory")
            else:
                self.logger.warning("âš ï¸ No JARVIS-Prime backends available")

            # Run health check on the selected mode
            if mode != RoutingMode.DISABLED:
                health = await self._jarvis_prime_client.check_health(mode)
                if health.available:
                    self.logger.info(f"âœ… {mode.value} backend healthy (latency: {health.latency_ms:.0f}ms)")
                else:
                    self.logger.warning(f"âš ï¸ {mode.value} backend not healthy: {health.error}")

            # Initialize Reactor-Core watcher for auto-deployment
            if self.config.reactor_core_enabled and self.config.reactor_core_watch_dir:
                await self._init_reactor_core_watcher()

            # v8.0: Initialize Data Flywheel for self-improving learning
            if self.config.data_flywheel_enabled:
                await self._init_data_flywheel()

            # v9.2: Initialize Intelligent Training Orchestrator
            if self.config.training_scheduler_enabled:
                await self._init_training_orchestrator()

            # Start dynamic memory monitoring for automatic mode switching
            await self._jarvis_prime_client.start_monitoring()

            # Register mode change callback
            async def on_mode_change(old_mode, new_mode, reason):
                self.logger.info(
                    f"ğŸ”„ JARVIS-Prime mode changed: {old_mode.value} â†’ {new_mode.value} ({reason})"
                )
                os.environ["JARVIS_PRIME_ROUTING_MODE"] = new_mode.value

                # Broadcast the change
                await self._broadcast_startup_progress(
                    stage="jarvis_prime_mode_change",
                    message=f"JARVIS-Prime switched to {new_mode.value}",
                    progress=100,
                    metadata={
                        "jarvis_prime": {
                            "old_mode": old_mode.value,
                            "new_mode": new_mode.value,
                            "reason": reason,
                        }
                    }
                )

            self._jarvis_prime_client.register_mode_change_callback(on_mode_change)

            # Propagate settings to environment
            os.environ["JARVIS_PRIME_ENABLED"] = "true"
            os.environ["JARVIS_PRIME_HOST"] = self.config.jarvis_prime_host
            os.environ["JARVIS_PRIME_PORT"] = str(self.config.jarvis_prime_port)
            os.environ["JARVIS_PRIME_ROUTING_MODE"] = mode.value

            # Broadcast completion
            client_stats = self._jarvis_prime_client.get_stats()
            await self._broadcast_startup_progress(
                stage="jarvis_prime_ready",
                message=f"JARVIS-Prime Tier-0 Brain online ({mode.value})",
                progress=78,
                metadata={
                    "jarvis_prime": {
                        "status": "ready",
                        "mode": mode.value,
                        "reason": reason,
                        "url": f"http://{self.config.jarvis_prime_host}:{self.config.jarvis_prime_port}",
                        "cloud_run_url": client_config.cloud_run_url if mode == RoutingMode.CLOUD_RUN else None,
                        "memory_available_gb": available_gb,
                        "monitoring_active": True,
                        "circuit_breakers": client_stats.get("circuit_breakers", {}),
                    }
                }
            )

            self.logger.info(f"âœ… JARVIS-Prime Tier-0 Brain ready (mode: {mode.value})")
            self.logger.info(f"ğŸ”„ Dynamic memory monitoring active (interval: 30s)")
            print(f"  {TerminalUI.GREEN}âœ“ JARVIS-Prime: {mode.value} mode ({available_gb:.1f}GB RAM){TerminalUI.RESET}")

        except ImportError as e:
            self.logger.warning(f"âš ï¸ JARVIS-Prime client not available: {e}")
            os.environ["JARVIS_PRIME_ENABLED"] = "false"
            print(f"  {TerminalUI.YELLOW}âš ï¸ JARVIS-Prime: Client not available{TerminalUI.RESET}")

        except Exception as e:
            self.logger.error(f"âŒ Failed to initialize JARVIS-Prime: {e}")
            os.environ["JARVIS_PRIME_ENABLED"] = "false"
            print(f"  {TerminalUI.YELLOW}âš ï¸ JARVIS-Prime: Not available ({e}){TerminalUI.RESET}")

    async def _init_jarvis_prime_local_if_needed(self) -> None:
        """
        Start JARVIS-Prime local subprocess if not already running.

        v117.0: CRITICAL FIX - Prevent duplicate spawning race condition.

        The cross_repo_startup_orchestrator may have ALREADY started jarvis-prime
        (via initialize_cross_repo_orchestration at line ~7347). We must:
        1. Check if the orchestrator already manages jarvis-prime
        2. Wait for it to become healthy (with longer timeout during startup)
        3. Only spawn if orchestrator didn't start it AND it's not running

        Root cause of "address already in use" error:
        - Orchestrator starts jarvis-prime on port 8000
        - This function runs later with only 2s timeout health check
        - Health check fails (model still loading)
        - We try to spawn ANOTHER jarvis-prime â†’ port conflict!
        """
        port = self.config.jarvis_prime_port
        host = self.config.jarvis_prime_host

        # v117.0: STEP 1 - Check if cross_repo_orchestrator already started jarvis-prime
        orchestrator_started = False
        try:
            from backend.supervisor.cross_repo_startup_orchestrator import get_orchestrator
            orchestrator = get_orchestrator()

            # Check if jarvis-prime is in the orchestrator's ready or starting set
            if hasattr(orchestrator, '_services_ready') and "jarvis-prime" in orchestrator._services_ready:
                self.logger.info(
                    "[v117.0] âœ… JARVIS-Prime already started by cross_repo_orchestrator (ready)"
                )
                orchestrator_started = True
            elif hasattr(orchestrator, '_services_starting') and "jarvis-prime" in orchestrator._services_starting:
                self.logger.info(
                    "[v117.0] â³ JARVIS-Prime being started by cross_repo_orchestrator (starting)"
                )
                orchestrator_started = True

                # Wait for orchestrator to finish starting it (up to startup timeout)
                wait_timeout = min(self.config.jarvis_prime_startup_timeout, 120.0)
                self.logger.info(f"    Waiting up to {wait_timeout}s for orchestrator to finish...")

                start_time = time.time()
                while time.time() - start_time < wait_timeout:
                    if "jarvis-prime" in orchestrator._services_ready:
                        self.logger.info("[v117.0] âœ… JARVIS-Prime now ready via orchestrator")
                        break
                    await asyncio.sleep(2.0)
                else:
                    self.logger.warning(
                        "[v117.0] âš ï¸ Orchestrator didn't finish starting jarvis-prime in time"
                    )

        except ImportError:
            self.logger.debug("[v117.0] cross_repo_orchestrator not available")
        except Exception as e:
            self.logger.debug(f"[v117.0] Orchestrator check failed: {e}")

        # v117.0: STEP 2 - Check if port is already bound (regardless of orchestrator)
        import socket
        port_in_use = False
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(1)
            result = sock.connect_ex(('localhost', port))
            sock.close()
            port_in_use = (result == 0)
        except Exception:
            pass

        if port_in_use:
            # Port is bound - check if it's healthy with progressive backoff
            self.logger.info(f"[v117.0] Port {port} is in use - checking health with extended timeout...")

            # Use longer timeout during startup (models may be loading)
            max_wait = 60.0 if not orchestrator_started else 30.0  # Shorter if orchestrator is managing
            check_interval = 3.0
            elapsed = 0.0

            import httpx
            while elapsed < max_wait:
                try:
                    async with httpx.AsyncClient(timeout=5.0) as client:
                        resp = await client.get(f"http://{host}:{port}/health")
                        if resp.status_code == 200:
                            self.logger.info(
                                f"[v117.0] âœ… JARVIS-Prime already running and healthy on port {port}"
                            )
                            return  # Success - no need to spawn
                except Exception as health_err:
                    self.logger.debug(f"    Health check failed (elapsed={elapsed:.1f}s): {health_err}")

                await asyncio.sleep(check_interval)
                elapsed += check_interval

            # Port bound but not healthy after waiting - likely a stale process or conflict
            if orchestrator_started:
                # Let orchestrator handle the restart
                self.logger.warning(
                    f"[v117.0] âš ï¸ Port {port} bound but unhealthy - orchestrator should handle restart"
                )
                return
            else:
                # We need to clean up and start fresh
                self.logger.warning(
                    f"[v117.0] âš ï¸ Port {port} bound but unhealthy - attempting cleanup before spawn"
                )
                await self._cleanup_stale_jarvis_prime_process(port)

        # v117.0: STEP 3 - If orchestrator started it, don't duplicate
        if orchestrator_started:
            self.logger.info(
                "[v117.0] Skipping local spawn - orchestrator is managing jarvis-prime"
            )
            return

        # v117.0: STEP 4 - Final port availability check before spawning
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            sock.settimeout(1)
            sock.bind(('0.0.0.0', port))
            sock.close()
            self.logger.info(f"[v117.0] Port {port} is available - proceeding with spawn")
        except OSError as bind_err:
            self.logger.error(
                f"[v117.0] âŒ Port {port} still unavailable after cleanup: {bind_err}"
            )
            self.logger.error("    Cannot start jarvis-prime - port conflict unresolved")
            return

        # Start local subprocess
        await self._init_jarvis_prime_local()

    async def _cleanup_stale_jarvis_prime_process(self, port: int) -> bool:
        """
        v117.0: Clean up stale jarvis-prime process occupying the port.

        This handles cases where a previous jarvis-prime is stuck or zombie.
        """
        self.logger.info(f"[v117.0] Cleaning up stale process on port {port}...")

        try:
            # Find processes on the port
            result = await asyncio.create_subprocess_exec(
                "lsof", "-i", f":{port}", "-t",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.DEVNULL,
            )
            stdout, _ = await result.communicate()

            if stdout:
                pids = [int(p.strip()) for p in stdout.decode().split() if p.strip().isdigit()]
                current_pid = os.getpid()

                for pid in pids:
                    if pid == current_pid:
                        continue  # Don't kill ourselves

                    try:
                        # Check if it's a jarvis-prime process
                        proc_result = await asyncio.create_subprocess_exec(
                            "ps", "-p", str(pid), "-o", "command=",
                            stdout=asyncio.subprocess.PIPE,
                            stderr=asyncio.subprocess.DEVNULL,
                        )
                        ps_stdout, _ = await proc_result.communicate()
                        cmd = ps_stdout.decode().lower()

                        if "jarvis" in cmd or "prime" in cmd or "uvicorn" in cmd:
                            self.logger.info(f"    Terminating stale process PID {pid}")
                            os.kill(pid, signal.SIGTERM)
                            await asyncio.sleep(1.0)

                            # Force kill if still running
                            try:
                                os.kill(pid, 0)  # Check if still alive
                                os.kill(pid, signal.SIGKILL)
                                self.logger.info(f"    Force killed PID {pid}")
                            except ProcessLookupError:
                                pass  # Already dead
                    except Exception as kill_err:
                        self.logger.debug(f"    Failed to handle PID {pid}: {kill_err}")

                # Wait for port to be released
                await asyncio.sleep(2.0)
                return True

        except Exception as e:
            self.logger.warning(f"[v117.0] Cleanup failed: {e}")

        return False

    async def _init_jarvis_prime_local(self) -> None:
        """Start JARVIS-Prime as a local subprocess."""
        repo_path = self.config.jarvis_prime_repo_path

        if not repo_path.exists():
            self.logger.warning(f"âš ï¸ JARVIS-Prime repo not found: {repo_path}")
            return

        # Check if model exists
        model_path = repo_path / self.config.jarvis_prime_models_dir / "current.gguf"
        if not model_path.exists():
            self.logger.warning(f"âš ï¸ JARVIS-Prime model not found: {model_path}")
            self.logger.info("   Download with: cd jarvis-prime && python -m jarvis_prime.docker.model_downloader tinyllama-chat")
            return

        # Check for venv or use system Python
        venv_python = repo_path / "venv" / "bin" / "python"
        python_cmd = str(venv_python) if venv_python.exists() else sys.executable

        # Build command
        cmd = [
            python_cmd,
            "run_server.py",
            "--host", self.config.jarvis_prime_host,
            "--port", str(self.config.jarvis_prime_port),
            "--model", str(model_path),
        ]

        self.logger.info(f"ğŸš€ Starting JARVIS-Prime local: {' '.join(cmd)}")

        # Start subprocess
        # v95.20: CRITICAL - start_new_session=True isolates from parent signal propagation
        self._jarvis_prime_process = await asyncio.create_subprocess_exec(
            *cmd,
            cwd=str(repo_path),
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            env={**os.environ, "PYTHONPATH": str(repo_path)},
            start_new_session=True,  # v95.20: Prevent signal propagation from parent
        )

        # v101.0: Register with supervisor restart manager for automatic recovery
        self._supervisor_restart_manager.register(
            name="jarvis-prime",
            process=self._jarvis_prime_process,
            restart_func=self._init_jarvis_prime_local,
            port=self.config.jarvis_prime_port,
        )

        # Wait for health check
        await self._wait_for_jarvis_prime_health()

        # Start log reader
        asyncio.create_task(self._read_jarvis_prime_logs())

    async def _init_jarvis_prime_docker(self) -> None:
        """Start JARVIS-Prime as a Docker container."""
        container_name = "jarvis-prime"

        # Stop existing container if any
        try:
            stop_proc = await asyncio.create_subprocess_exec(
                "docker", "stop", container_name,
                stdout=asyncio.subprocess.DEVNULL,
                stderr=asyncio.subprocess.DEVNULL,
            )
            await asyncio.wait_for(stop_proc.wait(), timeout=10.0)

            rm_proc = await asyncio.create_subprocess_exec(
                "docker", "rm", container_name,
                stdout=asyncio.subprocess.DEVNULL,
                stderr=asyncio.subprocess.DEVNULL,
            )
            await rm_proc.wait()
        except Exception:
            pass  # Container might not exist

        # Build docker run command
        models_dir = self.config.jarvis_prime_repo_path / self.config.jarvis_prime_models_dir

        cmd = [
            "docker", "run", "-d",
            "--name", container_name,
            "-p", f"{self.config.jarvis_prime_port}:8000",
            "-v", f"{models_dir}:/app/models:ro",
            "-e", f"LOG_LEVEL=INFO",
            self.config.jarvis_prime_docker_image,
        ]

        self.logger.info(f"ğŸ³ Starting JARVIS-Prime Docker: {' '.join(cmd)}")

        proc = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
        )

        stdout, stderr = await proc.communicate()

        if proc.returncode != 0:
            raise RuntimeError(f"Docker start failed: {stderr.decode()}")

        container_id = stdout.decode().strip()[:12]
        self.logger.info(f"âœ… Docker container started: {container_id}")

        # Wait for health
        await self._wait_for_jarvis_prime_health()

    async def _init_jarvis_prime_cloud_run(self) -> None:
        """Connect to JARVIS-Prime on Cloud Run."""
        url = self.config.jarvis_prime_cloud_run_url

        self.logger.info(f"â˜ï¸ Connecting to JARVIS-Prime Cloud Run: {url}")

        # Just verify connectivity - no process to start
        import aiohttp
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(f"{url}/health", timeout=aiohttp.ClientTimeout(total=10)) as resp:
                    if resp.status == 200:
                        self.logger.info("âœ… Cloud Run JARVIS-Prime is healthy")
                    else:
                        self.logger.warning(f"âš ï¸ Cloud Run returned status {resp.status}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ Cloud Run health check failed: {e}")
                self.logger.info("   This is expected if the service hasn't been deployed yet")
                self.logger.info("   Deploy with: terraform apply -var='enable_jarvis_prime=true'")

    async def _wait_for_jarvis_prime_health(self) -> bool:
        """
        Wait for JARVIS-Prime to become healthy.

        v108.0: Enhanced with early process death detection - if J-Prime crashes
        during startup, we detect it immediately instead of waiting for timeout.
        """
        import aiohttp

        url = f"http://{self.config.jarvis_prime_host}:{self.config.jarvis_prime_port}/health"
        start_time = time.perf_counter()
        timeout = self.config.jarvis_prime_startup_timeout

        self.logger.info(f"â³ Waiting for JARVIS-Prime at {url}...")

        async with aiohttp.ClientSession() as session:
            while (time.perf_counter() - start_time) < timeout:
                # v108.0: Check if process has died during startup
                if self._jarvis_prime_process and self._jarvis_prime_process.returncode is not None:
                    elapsed = time.perf_counter() - start_time
                    exit_code = self._jarvis_prime_process.returncode
                    self.logger.error(
                        f"âŒ JARVIS-Prime process died during startup (exit code: {exit_code}, "
                        f"after {elapsed:.1f}s)"
                    )
                    # Try to capture stderr for diagnostics
                    try:
                        if self._jarvis_prime_process.stderr:
                            stderr_data = await asyncio.wait_for(
                                self._jarvis_prime_process.stderr.read(),
                                timeout=2.0
                            )
                            if stderr_data:
                                stderr_text = stderr_data.decode()[-2000:]  # Last 2000 chars
                                self.logger.error(f"   Last stderr output:\n{stderr_text}")
                    except Exception as e:
                        self.logger.debug(f"   Could not read stderr: {e}")
                    return False

                try:
                    async with session.get(url, timeout=aiohttp.ClientTimeout(total=2)) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            self.logger.info(f"âœ… JARVIS-Prime healthy: {data.get('status', 'ok')}")
                            return True
                except Exception:
                    pass

                await asyncio.sleep(1.0)

        # v108.0: Enhanced timeout message with process status
        if self._jarvis_prime_process:
            if self._jarvis_prime_process.returncode is not None:
                self.logger.error(
                    f"âš ï¸ JARVIS-Prime died with exit code {self._jarvis_prime_process.returncode} "
                    f"during {timeout}s startup timeout"
                )
            else:
                self.logger.warning(
                    f"âš ï¸ JARVIS-Prime health check timed out after {timeout}s "
                    f"(process still running, PID: {self._jarvis_prime_process.pid})"
                )
        else:
            self.logger.warning(f"âš ï¸ JARVIS-Prime health check timed out after {timeout}s")
        return False

    async def _read_jarvis_prime_logs(self) -> None:
        """Read and log JARVIS-Prime subprocess output."""
        if not self._jarvis_prime_process:
            return

        try:
            async for line in self._jarvis_prime_process.stdout:
                text = line.decode().strip()
                if text:
                    self.logger.debug(f"[JarvisPrime] {text}")
        except Exception as e:
            self.logger.debug(f"JARVIS-Prime log reader ended: {e}")

    async def _init_reactor_core_watcher(self) -> None:
        """Initialize Reactor-Core watcher for auto-deployment of trained models."""
        watch_dir = self.config.reactor_core_watch_dir

        if not watch_dir:
            # Use default reactor-core output path
            watch_dir = str(Path.home() / "Documents" / "repos" / "reactor-core" / "output")

        self.logger.info(f"ğŸ”¬ Initializing Reactor-Core watcher: {watch_dir}")

        # Import and start watcher (non-blocking)
        try:
            from autonomy.reactor_core_watcher import (
                ReactorCoreWatcher,
                ReactorCoreConfig,
                DeploymentResult,
            )

            # Configure watcher
            config = ReactorCoreConfig(
                watch_dir=Path(watch_dir),
                local_models_dir=self.config.jarvis_prime_repo_path / self.config.jarvis_prime_models_dir,
                upload_to_gcs=True,
                deploy_local=True,
                auto_activate=self.config.reactor_core_auto_deploy,
            )

            # Create watcher
            self._reactor_core_watcher = ReactorCoreWatcher(config)

            # Register deployment callback
            async def on_model_deployed(result: DeploymentResult):
                """Callback when a new model is deployed from reactor-core."""
                if result.success:
                    self.logger.info(
                        f"ğŸ”¥ Reactor-Core deployed: {result.model_name} "
                        f"({result.model_size_mb:.1f}MB, checksum: {result.checksum})"
                    )

                    # Announce deployment
                    if hasattr(self, 'narrator') and self.narrator:
                        await self.narrator.speak(
                            f"New model deployed from Reactor Core: {result.model_name}",
                            wait=False
                        )

                    # Broadcast progress update
                    await self._broadcast_startup_progress(
                        stage="reactor_core_deploy",
                        message=f"New model deployed: {result.model_name}",
                        progress=100,
                        metadata={
                            "reactor_core": {
                                "model_name": result.model_name,
                                "model_size_mb": result.model_size_mb,
                                "checksum": result.checksum,
                                "local_deployed": result.local_deployed,
                                "gcs_uploaded": result.gcs_uploaded,
                                "gcs_path": result.gcs_path,
                                "hot_swap_notified": result.hot_swap_notified,
                            }
                        }
                    )

                    # Force a mode check in case we should switch to local
                    if self._jarvis_prime_client:
                        await self._jarvis_prime_client.force_mode_check()

                else:
                    self.logger.warning(
                        f"âš ï¸ Reactor-Core deployment failed: {result.model_name} - {result.error}"
                    )

            self._reactor_core_watcher.register_callback(on_model_deployed)

            # Start watching in background
            await self._reactor_core_watcher.start()
            self.logger.info("âœ… Reactor-Core watcher started")
            print(f"  {TerminalUI.GREEN}âœ“ Reactor-Core: Watching for new models{TerminalUI.RESET}")

        except ImportError as e:
            self.logger.warning(f"âš ï¸ Reactor-Core watcher not available: {e}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Reactor-Core watcher failed to start: {e}")

    async def _init_data_flywheel(self) -> None:
        """
        v8.0: Initialize the Unified Data Flywheel for self-improving learning.

        The Data Flywheel connects:
        - JARVIS-AI-Agent (experience recording, observability)
        - reactor-core (Scout web scraping, training, GGUF export)
        - JARVIS-Prime (model deployment, inference)

        Features:
        - Automatic experience collection from JARVIS interactions
        - Intelligent learning goals auto-discovery
        - Scheduled training runs (default: 3 AM daily)
        - Auto-deployment via Reactor-Core Watcher
        """
        if not self.config.data_flywheel_enabled:
            self.logger.info("â„¹ï¸ Data Flywheel disabled via configuration")
            return

        self.logger.info("ğŸ”„ Initializing Data Flywheel (self-improving learning loop)...")

        try:
            from autonomy.unified_data_flywheel import (
                UnifiedDataFlywheel,
                FlywheelConfig,
                FlywheelProgress,
                get_data_flywheel,
            )

            # Configure flywheel
            flywheel_config = FlywheelConfig(
                jarvis_repo=Path(__file__).parent,
                jarvis_prime_repo=self.config.jarvis_prime_repo_path,
                reactor_core_repo=Path.home() / "Documents" / "repos" / "reactor-core",
                auto_train_enabled=self.config.data_flywheel_auto_train,
                training_cooldown_hours=self.config.data_flywheel_cooldown_hours,
                min_experiences_for_training=self.config.data_flywheel_min_experiences,
            )

            # Get or create flywheel instance
            self._data_flywheel = UnifiedDataFlywheel(flywheel_config)

            # Register progress callback for status updates
            def on_flywheel_progress(progress: FlywheelProgress):
                self.logger.debug(
                    f"Flywheel: {progress.stage.value} - "
                    f"experiences={progress.experiences_collected}, "
                    f"web_pages={progress.web_pages_scraped}"
                )

            self._data_flywheel.register_progress_callback(on_flywheel_progress)

            # Initialize learning goals manager if enabled
            if self.config.learning_goals_enabled:
                await self._init_learning_goals_manager()

            # Schedule automatic training if enabled
            if self.config.data_flywheel_auto_train:
                self._training_scheduler_task = asyncio.create_task(
                    self._run_training_scheduler()
                )

            # v9.1: Initialize flywheel components eagerly for faster experience collection
            try:
                await self._data_flywheel._init_components()
                self.logger.info("âœ… Data Flywheel components initialized")

                # Initialize the SQLite training database connection
                if hasattr(self._data_flywheel, '_init_training_db'):
                    await self._data_flywheel._init_training_db()
                    self.logger.info("âœ… Training database connection established")
            except Exception as init_err:
                self.logger.warning(f"âš ï¸ Flywheel component init error (non-fatal): {init_err}")

            # v9.1: Start background experience collection loop
            if self.config.data_flywheel_auto_collect:
                self._experience_collection_task = asyncio.create_task(
                    self._run_experience_collection_loop()
                )
                self.logger.info("âœ… Background experience collection started")

            self.logger.info("âœ… Data Flywheel initialized")
            print(f"  {TerminalUI.GREEN}âœ“ Data Flywheel: Self-improving learning active{TerminalUI.RESET}")

            # Broadcast flywheel ready via general progress
            await self._broadcast_startup_progress(
                stage="data_flywheel_ready",
                message="Data Flywheel ready for self-improving learning",
                progress=82,
                metadata={
                    "data_flywheel": {
                        "status": "ready",
                        "auto_train": self.config.data_flywheel_auto_train,
                        "training_schedule": self.config.data_flywheel_training_schedule,
                        "learning_goals_enabled": self.config.learning_goals_enabled,
                    }
                }
            )

            # v9.1: Also broadcast to specialized flywheel endpoint
            await self._broadcast_flywheel_status(
                status="ready",
                experiences_collected=0,
                training_schedule=self.config.data_flywheel_training_schedule,
            )

        except ImportError as e:
            self.logger.warning(f"âš ï¸ Data Flywheel not available: {e}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Data Flywheel failed to start: {e}")

    async def _init_learning_goals_manager(self) -> None:
        """
        v9.3: Initialize the Intelligent Learning Goals Discovery System.

        This comprehensive system automatically discovers topics JARVIS should learn
        about by analyzing multiple sources and triggering Safe Scout for scraping.

        Discovery Sources:
        - FAILED_INTERACTION: Commands JARVIS couldn't handle (highest priority)
        - CORRECTION: When user corrects JARVIS's response (high priority)
        - USER_QUESTION: Questions about technologies/concepts
        - UNKNOWN_TERM: Technical terms JARVIS didn't recognize
        - TRENDING: Topics appearing frequently in interactions
        - MANUAL: User-requested learning goals

        Integration Points:
        - Reactor-Core TopicDiscovery for intelligent extraction
        - Safe Scout Orchestrator for automated web scraping
        - Training Database for experience analysis
        - Loading Server for real-time status broadcasts
        """
        self.logger.info("ğŸ¯ Initializing Intelligent Learning Goals Discovery...")

        try:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Phase 1: Define Data Structures
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            from dataclasses import dataclass, field as dc_field
            from typing import List, Dict, Any, Optional, Set
            from datetime import datetime, timedelta
            from enum import Enum
            import json
            import re
            import sqlite3

            class DiscoverySource(Enum):
                """Sources for discovering learning topics."""
                FAILED_INTERACTION = "failed_interaction"
                CORRECTION = "correction"
                USER_QUESTION = "user_question"
                UNKNOWN_TERM = "unknown_term"
                TRENDING = "trending"
                MANUAL = "manual"

            @dataclass
            class DiscoveredTopic:
                """A topic discovered for JARVIS to learn."""
                topic: str
                priority: float  # 0.0-10.0 calculated score
                source: DiscoverySource
                confidence: float  # 0.0-1.0 extraction confidence
                frequency: int = 1  # How many times mentioned
                urls: List[str] = dc_field(default_factory=list)
                keywords: List[str] = dc_field(default_factory=list)
                discovered_at: datetime = dc_field(default_factory=datetime.now)
                scraped: bool = False
                scrape_started_at: Optional[datetime] = None
                pages_scraped: int = 0

                def to_dict(self) -> Dict[str, Any]:
                    return {
                        "topic": self.topic,
                        "priority": self.priority,
                        "source": self.source.value,
                        "confidence": self.confidence,
                        "frequency": self.frequency,
                        "urls": self.urls,
                        "keywords": self.keywords,
                        "discovered_at": self.discovered_at.isoformat(),
                        "scraped": self.scraped,
                        "pages_scraped": self.pages_scraped,
                    }

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Phase 2: Intelligent Learning Goals Discovery System
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            class IntelligentLearningGoalsDiscovery:
                """
                v9.3: Comprehensive learning goals discovery with reactor-core integration.

                Features:
                - Multi-source topic extraction (logs, experiences, corrections)
                - Intelligent priority scoring based on source weights
                - Automatic URL generation for documentation
                - Safe Scout integration for automated scraping
                - Real-time progress broadcasts
                """

                def __init__(
                    self,
                    max_topics: int = 50,
                    min_mentions: int = 2,
                    min_confidence: float = 0.5,
                    source_weights: Optional[Dict[str, float]] = None,
                    logger: Optional[Any] = None,
                ):
                    self.max_topics = max_topics
                    self.min_mentions = min_mentions
                    self.min_confidence = min_confidence
                    self.logger = logger

                    # Source weights for priority calculation
                    self.source_weights = source_weights or {
                        DiscoverySource.CORRECTION.value: 1.0,
                        DiscoverySource.FAILED_INTERACTION.value: 0.9,
                        DiscoverySource.USER_QUESTION.value: 0.7,
                        DiscoverySource.UNKNOWN_TERM.value: 0.6,
                        DiscoverySource.TRENDING.value: 0.5,
                        DiscoverySource.MANUAL.value: 1.0,
                    }

                    # Topic storage
                    self.topics: Dict[str, DiscoveredTopic] = {}
                    self.topics_file = Path(__file__).parent / "data" / "discovered_topics.json"
                    self._term_frequency: Dict[str, int] = {}
                    self._last_discovery: Optional[datetime] = None

                    # Reactor-core integration (optional)
                    self._reactor_topic_discovery = None
                    self._safe_scout = None
                    self._topic_queue = None

                    # Load existing topics
                    self._load_topics()

                    # Try to import reactor-core components
                    self._init_reactor_core_integration()

                def _init_reactor_core_integration(self) -> None:
                    """Try to connect to reactor-core for enhanced discovery."""
                    try:
                        reactor_core_path = Path(__file__).parent.parent / "reactor-core"
                        if reactor_core_path.exists():
                            import sys
                            if str(reactor_core_path) not in sys.path:
                                sys.path.insert(0, str(reactor_core_path))

                            # Import TopicDiscovery from reactor-core
                            from reactor_core.scout.topic_discovery import TopicDiscovery
                            self._reactor_topic_discovery = TopicDiscovery()
                            if self.logger:
                                self.logger.debug("âœ“ Reactor-core TopicDiscovery connected")

                            # Import SafeScoutOrchestrator
                            from reactor_core.scout.safe_scout_orchestrator import SafeScoutOrchestrator
                            self._safe_scout = SafeScoutOrchestrator()
                            if self.logger:
                                self.logger.debug("âœ“ Reactor-core SafeScout connected")

                            # Import TopicQueue
                            from reactor_core.scout.topic_queue import TopicQueue
                            queue_db = Path(__file__).parent / "data" / "topic_queue.db"
                            queue_db.parent.mkdir(parents=True, exist_ok=True)
                            self._topic_queue = TopicQueue(db_path=str(queue_db))
                            if self.logger:
                                self.logger.debug("âœ“ Reactor-core TopicQueue connected")

                    except ImportError as e:
                        if self.logger:
                            self.logger.debug(f"Reactor-core not available: {e}")
                    except Exception as e:
                        if self.logger:
                            self.logger.debug(f"Reactor-core init error: {e}")

                def _load_topics(self) -> None:
                    """Load previously discovered topics."""
                    if self.topics_file.exists():
                        try:
                            # v92.0: Use safe file reading to avoid "Bad file descriptor" errors
                            data = _safe_read_json(self.topics_file, default={})
                            for t in data.get("topics", []):
                                topic = DiscoveredTopic(
                                    topic=t["topic"],
                                    priority=t.get("priority", 5.0),
                                    source=DiscoverySource(t.get("source", "manual")),
                                    confidence=t.get("confidence", 0.5),
                                    frequency=t.get("frequency", 1),
                                    urls=t.get("urls", []),
                                    keywords=t.get("keywords", []),
                                    scraped=t.get("scraped", False),
                                    pages_scraped=t.get("pages_scraped", 0),
                                )
                                self.topics[topic.topic.lower()] = topic
                        except Exception as e:
                            if self.logger:
                                self.logger.debug(f"Failed to load topics: {e}")

                def _save_topics(self) -> None:
                    """Persist discovered topics."""
                    self.topics_file.parent.mkdir(parents=True, exist_ok=True)
                    data = {
                        "topics": [t.to_dict() for t in self.topics.values()],
                        "last_discovery": self._last_discovery.isoformat() if self._last_discovery else None,
                    }
                    self.topics_file.write_text(json.dumps(data, indent=2, default=str))

                def _calculate_priority(
                    self,
                    source: DiscoverySource,
                    confidence: float,
                    frequency: int,
                    recency_days: float = 0.0,
                ) -> float:
                    """
                    Calculate topic priority using weighted scoring.

                    Formula: priority = 0.4*confidence + 0.3*frequency_norm + 0.2*recency + 0.1*source_weight
                    Final score scaled to 0-10.
                    """
                    # Normalize frequency (log scale, max 10)
                    import math
                    frequency_norm = min(1.0, math.log10(frequency + 1) / math.log10(11))

                    # Recency score (1.0 for today, decays over 30 days)
                    recency_score = max(0.0, 1.0 - (recency_days / 30.0))

                    # Source weight
                    source_weight = self.source_weights.get(source.value, 0.5)

                    # Weighted combination
                    raw_score = (
                        0.4 * confidence +
                        0.3 * frequency_norm +
                        0.2 * recency_score +
                        0.1 * source_weight
                    )

                    # Scale to 0-10
                    return round(raw_score * 10, 2)

                def _generate_documentation_urls(self, topic: str) -> List[str]:
                    """Generate likely documentation URLs for a topic."""
                    urls = []
                    topic_slug = topic.lower().replace(" ", "-").replace(".", "-")
                    topic_underscore = topic.lower().replace(" ", "_").replace(".", "_")

                    # Common documentation patterns
                    patterns = [
                        f"https://docs.python.org/3/library/{topic_underscore}.html",
                        f"https://{topic_slug}.readthedocs.io/",
                        f"https://github.com/{topic_slug}/{topic_slug}",
                        f"https://pypi.org/project/{topic_slug}/",
                        f"https://developer.mozilla.org/en-US/docs/Web/{topic}",
                        f"https://www.npmjs.com/package/{topic_slug}",
                    ]

                    # Add relevant patterns based on topic keywords
                    topic_lower = topic.lower()
                    if "python" in topic_lower or topic_lower.startswith("py"):
                        urls.append(f"https://docs.python.org/3/search.html?q={topic}")
                    if "react" in topic_lower:
                        urls.append(f"https://react.dev/reference/react/{topic}")
                    if "langchain" in topic_lower:
                        urls.append(f"https://python.langchain.com/docs/")
                    if "llm" in topic_lower or "model" in topic_lower:
                        urls.append("https://huggingface.co/docs")

                    # Add base patterns
                    urls.extend(patterns[:3])  # Limit to avoid too many

                    return urls[:5]  # Cap at 5 URLs

                async def discover_from_experiences(
                    self,
                    db_path: Optional[Path] = None,
                    lookback_days: int = 30,
                ) -> List[DiscoveredTopic]:
                    """
                    Discover learning topics from the training database experiences.

                    Analyzes:
                    - Failed interactions (low quality_score)
                    - Corrected responses (feedback='corrected')
                    - User questions (input contains question patterns)
                    - Unknown terms (technical terms in low-confidence responses)
                    """
                    discovered = []

                    # Default database path
                    if db_path is None:
                        db_path = Path(__file__).parent / "data" / "jarvis_training.db"

                    if not db_path.exists():
                        if self.logger:
                            self.logger.debug(f"Training DB not found: {db_path}")
                        return discovered

                    try:
                        conn = sqlite3.connect(str(db_path))
                        cursor = conn.cursor()

                        # Calculate cutoff timestamp
                        cutoff = datetime.now() - timedelta(days=lookback_days)
                        cutoff_ts = cutoff.timestamp()

                        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        # Source 1: Failed Interactions (low quality_score)
                        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        cursor.execute("""
                            SELECT input_text, context, quality_score
                            FROM experiences
                            WHERE timestamp > ? AND quality_score < 0.4
                            ORDER BY timestamp DESC
                            LIMIT 100
                        """, (cutoff_ts,))

                        for row in cursor.fetchall():
                            input_text, context, quality_score = row
                            terms = self._extract_technical_terms(input_text)
                            for term in terms:
                                topic = self._add_or_update_topic(
                                    term,
                                    DiscoverySource.FAILED_INTERACTION,
                                    confidence=0.3 + (1.0 - quality_score) * 0.5,
                                )
                                if topic:
                                    discovered.append(topic)

                        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        # Source 2: Corrected Responses
                        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        cursor.execute("""
                            SELECT input_text, correction, context
                            FROM experiences
                            WHERE timestamp > ? AND feedback = 'corrected'
                            ORDER BY timestamp DESC
                            LIMIT 100
                        """, (cutoff_ts,))

                        for row in cursor.fetchall():
                            input_text, correction, context = row
                            # Extract terms from both input and correction
                            terms = self._extract_technical_terms(input_text)
                            if correction:
                                terms.extend(self._extract_technical_terms(correction))
                            for term in terms:
                                topic = self._add_or_update_topic(
                                    term,
                                    DiscoverySource.CORRECTION,
                                    confidence=0.85,  # High confidence for corrections
                                )
                                if topic:
                                    discovered.append(topic)

                        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        # Source 3: User Questions
                        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        cursor.execute("""
                            SELECT input_text, context
                            FROM experiences
                            WHERE timestamp > ?
                              AND (input_text LIKE '%what is%'
                                   OR input_text LIKE '%how do%'
                                   OR input_text LIKE '%how does%'
                                   OR input_text LIKE '%explain%'
                                   OR input_text LIKE '%learn about%')
                            ORDER BY timestamp DESC
                            LIMIT 100
                        """, (cutoff_ts,))

                        for row in cursor.fetchall():
                            input_text, context = row
                            terms = self._extract_technical_terms(input_text)
                            for term in terms:
                                topic = self._add_or_update_topic(
                                    term,
                                    DiscoverySource.USER_QUESTION,
                                    confidence=0.7,
                                )
                                if topic:
                                    discovered.append(topic)

                        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        # Source 4: Trending Terms (high frequency)
                        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                        cursor.execute("""
                            SELECT input_text
                            FROM experiences
                            WHERE timestamp > ?
                            ORDER BY timestamp DESC
                            LIMIT 500
                        """, (cutoff_ts,))

                        all_terms = []
                        for row in cursor.fetchall():
                            all_terms.extend(self._extract_technical_terms(row[0]))

                        # Count term frequency
                        from collections import Counter
                        term_counts = Counter(all_terms)

                        # Add trending terms (appearing 3+ times)
                        for term, count in term_counts.most_common(20):
                            if count >= 3:
                                topic = self._add_or_update_topic(
                                    term,
                                    DiscoverySource.TRENDING,
                                    confidence=min(0.9, 0.4 + count * 0.05),
                                    frequency=count,
                                )
                                if topic:
                                    discovered.append(topic)

                        conn.close()

                    except Exception as e:
                        if self.logger:
                            self.logger.warning(f"Experience discovery error: {e}")

                    return discovered

                def _extract_technical_terms(self, text: str) -> List[str]:
                    """
                    Extract technical terms from text using pattern matching.

                    Patterns:
                    - CamelCase words (e.g., LangChain, FastAPI)
                    - snake_case identifiers (e.g., async_generator)
                    - Dotted names (e.g., numpy.array)
                    - Known tech patterns (e.g., React, Python, API)
                    """
                    if not text:
                        return []

                    terms = []

                    # CamelCase pattern
                    camel_pattern = r'\b([A-Z][a-z]+(?:[A-Z][a-z]+)+)\b'
                    terms.extend(re.findall(camel_pattern, text))

                    # snake_case pattern
                    snake_pattern = r'\b([a-z]+(?:_[a-z]+)+)\b'
                    terms.extend(re.findall(snake_pattern, text))

                    # Dotted names (e.g., module.function)
                    dot_pattern = r'\b([a-z]+(?:\.[a-z]+)+)\b'
                    terms.extend(re.findall(dot_pattern, text))

                    # Known technology keywords
                    tech_keywords = [
                        r'\b(Python|JavaScript|TypeScript|Rust|Go|Swift)\b',
                        r'\b(React|Vue|Angular|FastAPI|Flask|Django)\b',
                        r'\b(LangChain|LangGraph|ChromaDB|FAISS)\b',
                        r'\b(Docker|Kubernetes|Terraform|AWS|GCP|Azure)\b',
                        r'\b(PostgreSQL|MongoDB|Redis|SQLite)\b',
                        r'\b(API|REST|GraphQL|WebSocket|gRPC)\b',
                        r'\b(ML|AI|LLM|NLP|transformers?|embeddings?)\b',
                    ]
                    for pattern in tech_keywords:
                        matches = re.findall(pattern, text, re.IGNORECASE)
                        terms.extend(matches)

                    # Clean and deduplicate
                    cleaned = []
                    seen = set()
                    for term in terms:
                        term_lower = term.lower().strip()
                        if len(term_lower) > 2 and term_lower not in seen:
                            # Filter common words
                            if term_lower not in {'the', 'and', 'for', 'with', 'this', 'that'}:
                                cleaned.append(term)
                                seen.add(term_lower)

                    return cleaned

                def _add_or_update_topic(
                    self,
                    term: str,
                    source: DiscoverySource,
                    confidence: float,
                    frequency: int = 1,
                ) -> Optional[DiscoveredTopic]:
                    """Add a new topic or update an existing one."""
                    term_key = term.lower().strip()

                    if len(term_key) < 3:
                        return None

                    if term_key in self.topics:
                        # Update existing topic
                        existing = self.topics[term_key]
                        existing.frequency += frequency
                        # Upgrade source if higher priority
                        if self.source_weights.get(source.value, 0) > \
                           self.source_weights.get(existing.source.value, 0):
                            existing.source = source
                        # Update confidence (weighted average)
                        existing.confidence = (existing.confidence + confidence) / 2
                        # Recalculate priority
                        existing.priority = self._calculate_priority(
                            existing.source,
                            existing.confidence,
                            existing.frequency,
                        )
                        return None  # Not a new discovery
                    else:
                        # Create new topic
                        if len(self.topics) >= self.max_topics:
                            # Remove lowest priority scraped topic
                            scraped = [t for t in self.topics.values() if t.scraped]
                            if scraped:
                                lowest = min(scraped, key=lambda t: t.priority)
                                del self.topics[lowest.topic.lower()]

                        topic = DiscoveredTopic(
                            topic=term,
                            priority=self._calculate_priority(source, confidence, frequency),
                            source=source,
                            confidence=confidence,
                            frequency=frequency,
                            urls=self._generate_documentation_urls(term),
                        )
                        self.topics[term_key] = topic
                        return topic

                async def discover_from_logs(self, log_dir: Path) -> List[DiscoveredTopic]:
                    """Discover topics from JARVIS log files."""
                    discovered = []

                    if not log_dir.exists():
                        return discovered

                    # Patterns for discovering learning opportunities
                    patterns = [
                        (r"(?:learn|study|research|understand)\s+(\w+(?:\s+\w+)?)", DiscoverySource.USER_QUESTION),
                        (r"what\s+is\s+(\w+(?:\s+\w+)?)\??", DiscoverySource.USER_QUESTION),
                        (r"how\s+(?:does|do)\s+(\w+(?:\s+\w+)?)\s+work", DiscoverySource.USER_QUESTION),
                        (r"error:?\s+(?:unknown|unrecognized)\s+(\w+)", DiscoverySource.UNKNOWN_TERM),
                        (r"failed to (?:import|load|find)\s+(\w+)", DiscoverySource.FAILED_INTERACTION),
                    ]

                    # v94.0: Non-blocking file I/O - run in executor to avoid blocking event loop
                    def _read_file_sync(file_path: Path) -> str:
                        """Read file content synchronously (runs in thread pool)."""
                        try:
                            return file_path.read_text(errors='ignore')
                        except Exception:
                            return ""

                    loop = asyncio.get_running_loop()

                    # Scan recent log files (non-blocking)
                    for log_file in sorted(log_dir.glob("*.log"), reverse=True)[:10]:
                        try:
                            # Run blocking I/O in executor to prevent event loop blocking
                            content = await loop.run_in_executor(None, _read_file_sync, log_file)
                            if not content:
                                continue

                            for pattern, source in patterns:
                                matches = re.findall(pattern, content, re.IGNORECASE)
                                for match in matches[:5]:
                                    term = match.strip()
                                    topic = self._add_or_update_topic(
                                        term,
                                        source,
                                        confidence=0.5,
                                    )
                                    if topic:
                                        discovered.append(topic)
                        except Exception:
                            continue

                    return discovered

                async def discover_with_reactor_core(
                    self,
                    events: Optional[List[Dict[str, Any]]] = None,
                ) -> List[DiscoveredTopic]:
                    """
                    Use reactor-core's TopicDiscovery for enhanced extraction.

                    If reactor-core is available, leverages its ML-based
                    topic extraction for higher quality results.
                    """
                    discovered = []

                    if not self._reactor_topic_discovery:
                        return discovered

                    try:
                        # Use reactor-core's analyze_events if available
                        if hasattr(self._reactor_topic_discovery, 'analyze_events') and events:
                            results = await self._reactor_topic_discovery.analyze_events(events)
                            for result in results:
                                topic = self._add_or_update_topic(
                                    result.get('topic', ''),
                                    DiscoverySource(result.get('source', 'trending')),
                                    confidence=result.get('confidence', 0.5),
                                )
                                if topic:
                                    discovered.append(topic)

                        # Use discover_from_jarvis if available
                        if hasattr(self._reactor_topic_discovery, 'discover_from_jarvis'):
                            results = await self._reactor_topic_discovery.discover_from_jarvis()
                            for result in results:
                                topic = self._add_or_update_topic(
                                    result.get('topic', ''),
                                    DiscoverySource.TRENDING,
                                    confidence=result.get('confidence', 0.5),
                                )
                                if topic:
                                    discovered.append(topic)

                    except Exception as e:
                        if self.logger:
                            self.logger.debug(f"Reactor-core discovery error: {e}")

                    return discovered

                def get_pending_topics(self, limit: int = 10) -> List[DiscoveredTopic]:
                    """Get unscraped topics sorted by priority."""
                    pending = [t for t in self.topics.values() if not t.scraped]
                    return sorted(pending, key=lambda t: -t.priority)[:limit]

                def get_pending_goals(self, limit: int = 10) -> List[DiscoveredTopic]:
                    """Alias for get_pending_topics for backward compatibility."""
                    return self.get_pending_topics(limit)

                def get_all_topics(self) -> List[DiscoveredTopic]:
                    """Get all topics sorted by priority."""
                    return sorted(self.topics.values(), key=lambda t: -t.priority)

                def mark_scraped(self, topic: str, pages: int = 0) -> None:
                    """Mark a topic as scraped."""
                    key = topic.lower()
                    if key in self.topics:
                        self.topics[key].scraped = True
                        self.topics[key].pages_scraped = pages
                        self._save_topics()

                def add_manual_topic(
                    self,
                    topic: str,
                    priority: float = 8.0,
                    urls: Optional[List[str]] = None,
                ) -> DiscoveredTopic:
                    """Add a user-requested learning topic (highest priority)."""
                    new_topic = DiscoveredTopic(
                        topic=topic,
                        priority=priority,
                        source=DiscoverySource.MANUAL,
                        confidence=1.0,
                        urls=urls or self._generate_documentation_urls(topic),
                    )
                    self.topics[topic.lower()] = new_topic
                    self._save_topics()
                    return new_topic

                def get_discovery_stats(self) -> Dict[str, Any]:
                    """Get statistics about discovered topics."""
                    all_topics = list(self.topics.values())
                    by_source = {}
                    for source in DiscoverySource:
                        by_source[source.value] = len([
                            t for t in all_topics if t.source == source
                        ])

                    return {
                        "total_topics": len(all_topics),
                        "pending_scrape": len([t for t in all_topics if not t.scraped]),
                        "scraped": len([t for t in all_topics if t.scraped]),
                        "by_source": by_source,
                        "avg_priority": sum(t.priority for t in all_topics) / len(all_topics) if all_topics else 0,
                        "last_discovery": self._last_discovery.isoformat() if self._last_discovery else None,
                    }

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Phase 3: Create and Configure Discovery System
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self._learning_goals_discovery = IntelligentLearningGoalsDiscovery(
                max_topics=self.config.learning_goals_max_topics,
                min_mentions=self.config.learning_goals_min_mentions,
                min_confidence=self.config.learning_goals_min_confidence,
                source_weights={
                    "correction": self.config.learning_goals_weight_corrections,
                    "failed_interaction": self.config.learning_goals_weight_failures,
                    "user_question": self.config.learning_goals_weight_questions,
                    "trending": self.config.learning_goals_weight_trending,
                    "manual": 1.0,
                },
                logger=self.logger,
            )

            # Also keep backward-compatible reference
            self._learning_goals_manager = self._learning_goals_discovery

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Phase 4: Run Initial Discovery
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.config.learning_goals_auto_discover:
                # Discover from multiple sources in parallel
                tasks = []

                # From training database
                db_path = Path(__file__).parent / "data" / "jarvis_training.db"
                tasks.append(self._learning_goals_discovery.discover_from_experiences(
                    db_path=db_path,
                    lookback_days=self.config.learning_goals_lookback_days,
                ))

                # From log files
                log_dir = Path(__file__).parent / "logs"
                tasks.append(self._learning_goals_discovery.discover_from_logs(log_dir))

                # From reactor-core (if available)
                tasks.append(self._learning_goals_discovery.discover_with_reactor_core())

                # Run all discovery tasks in parallel
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # v95.0: Validate results and log exceptions for observability
                total_discovered = 0
                source_names = ["experiences_db", "logs", "reactor_core"]
                for i, result in enumerate(results):
                    if isinstance(result, list):
                        total_discovered += len(result)
                    elif isinstance(result, Exception):
                        source = source_names[i] if i < len(source_names) else f"source_{i}"
                        self.logger.debug(f"Discovery exception from {source}: {result}")

                if total_discovered > 0:
                    self.logger.info(f"ğŸ¯ Discovered {total_discovered} new learning topics")
                    self._discovery_stats["total_discovered"] = total_discovered

                # Save discovered topics
                self._learning_goals_discovery._save_topics()

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Phase 5: Start Discovery Loop (if enabled)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.config.learning_goals_auto_discover:
                self._learning_goals_discovery_task = asyncio.create_task(
                    self._run_learning_goals_discovery_loop()
                )
                self.logger.debug("âœ“ Learning goals discovery loop started")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Phase 6: Start Safe Scout Queue Processor (if auto-scrape enabled)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.config.learning_goals_auto_scrape:
                self._discovery_queue_processor_task = asyncio.create_task(
                    self._run_discovery_queue_processor()
                )
                self.logger.debug("âœ“ Safe Scout queue processor started")

            # Report status
            stats = self._learning_goals_discovery.get_discovery_stats()
            pending = stats.get("pending_scrape", 0)
            self.logger.info(f"âœ… Learning Goals Discovery ready ({pending} pending topics)")

            # Broadcast initial status
            await self._broadcast_learning_goals_status(
                status="ready",
                pending_topics=pending,
                total_topics=stats.get("total_topics", 0),
            )

        except Exception as e:
            self.logger.warning(f"âš ï¸ Learning Goals Discovery failed: {e}")
            import traceback
            self.logger.debug(traceback.format_exc())

    async def _initialize_intelligence_systems(self) -> None:
        """
        v9.0: Initialize Full Agentic OS Intelligence Stack.

        This method initializes all advanced intelligence systems that make JARVIS
        a truly autonomous, self-improving AI agent:

        1. UAE (Unified Awareness Engine) - Screen awareness and computer vision
           - Chain-of-thought reasoning for complex decisions
           - Integration with SAI for spatial understanding
           - Continuous visual monitoring and analysis

        2. SAI (Situational Awareness Intelligence) - Window/app tracking
           - Yabai bridge for macOS window management
           - Cross-space learning and pattern recognition
           - Real-time workspace state tracking

        3. Neural Mesh - Distributed intelligence coordination
           - Inter-system communication and synchronization
           - Shared context propagation across all subsystems
           - Adaptive load balancing for intelligence tasks

        4. MAS (Multi-Agent System) - Coordinated agent execution
           - Parallel task decomposition and execution
           - Agent collaboration and conflict resolution
           - Dynamic agent spawning based on task complexity

        5. CAI (Collective AI Intelligence) - Emergent intelligence aggregation
           - Synthesis of insights from all agents
           - Pattern detection across system boundaries
           - Proactive recommendation generation

        6. Continuous Background Web Scraping - Self-improving knowledge
           - Configurable interval-based scraping (default: every 4 hours)
           - Topic-driven intelligent content discovery
           - Automatic integration with training pipeline

        Architecture:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              Agentic OS Intelligence Stack (v9.0)                    â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚       UAE        â”‚â—„â”€â”€â–ºâ”‚       SAI        â”‚â—„â”€â”€â–ºâ”‚  Neural Mesh  â”‚ â”‚
        â”‚  â”‚  (Vision+Chain)  â”‚    â”‚   (Yabai+Apps)   â”‚    â”‚   (Coord.)    â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚           â”‚                       â”‚                       â”‚         â”‚
        â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
        â”‚                                   â–¼                                 â”‚
        â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
        â”‚                    â”‚    MAS (Multi-Agent)     â”‚                     â”‚
        â”‚                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”      â”‚                     â”‚
        â”‚                    â”‚  â”‚Agent1â”‚ â”‚Agent2â”‚ ...  â”‚                     â”‚
        â”‚                    â”‚  â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜      â”‚                     â”‚
        â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
        â”‚                                 â–¼                                   â”‚
        â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
        â”‚                    â”‚    CAI (Collective AI)   â”‚                     â”‚
        â”‚                    â”‚    Intelligence Layer    â”‚                     â”‚
        â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
        â”‚                                 â–¼                                   â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚          Continuous Background Web Scraping                   â”‚  â”‚
        â”‚  â”‚  Topics â†’ Safe Scout â†’ Training Pipeline â†’ Model Deployment  â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        self.logger.info("â•" * 60)
        self.logger.info("ğŸ§  v9.0: Initializing Full Agentic OS Intelligence Stack...")
        self.logger.info("â•" * 60)

        # Track initialization status for all systems
        initialized_systems: Dict[str, bool] = {
            "uae": False,
            "sai": False,
            "neural_mesh": False,
            "mas": False,
            "cai": False,
            "continuous_scraping": False,
            "reactor_core": False,
        }

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 1: Initialize UAE (Unified Awareness Engine)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.config.uae_enabled:
            try:
                self.logger.info("ğŸ”® Step 1/7: Initializing UAE (Unified Awareness Engine)...")
                self.logger.info("   â€¢ Chain-of-thought reasoning: " + ("Enabled" if self.config.uae_chain_of_thought else "Disabled"))
                self.logger.info("   â€¢ Proactive intelligence: Enabled")
                self.logger.info("   â€¢ Learning database: Enabled")

                from intelligence.uae_integration import (
                    initialize_uae,
                    get_uae,
                    get_enhanced_uae,
                    shutdown_uae,
                )

                # v83.0: Create voice and notification callbacks for Proactive Intelligence
                async def uae_voice_callback(text: str):
                    """Voice callback for UAE proactive suggestions."""
                    try:
                        if hasattr(self, 'narrator') and self.narrator:
                            await self.narrator.speak(text, wait=False)
                            self.logger.debug(f"[UAE-VOICE] Spoke: {text[:50]}...")
                    except Exception as e:
                        self.logger.debug(f"[UAE-VOICE] Error: {e}")

                async def uae_notification_callback(title: str, message: str, priority: str = "low"):
                    """Notification callback for UAE proactive suggestions."""
                    try:
                        self.logger.info(f"[UAE-NOTIFY] [{priority.upper()}] {title}: {message}")
                        # Also speak high-priority notifications
                        if priority in ("high", "critical") and hasattr(self, 'narrator') and self.narrator:
                            await self.narrator.speak(f"{title}. {message}", wait=False)
                    except Exception as e:
                        self.logger.debug(f"[UAE-NOTIFY] Error: {e}")

                # Initialize UAE with all features enabled
                self._uae_engine = await initialize_uae(
                    vision_analyzer=None,  # Will be injected later by main.py
                    sai_monitoring_interval=5.0,  # 5-second monitoring for real-time awareness
                    enable_auto_start=True,
                    enable_learning_db=True,
                    enable_yabai=self.config.sai_yabai_bridge,  # Connect to Yabai if enabled
                    enable_proactive_intelligence=True,  # Phase 4 proactive communication
                    voice_callback=uae_voice_callback,  # v83.0: Enable voice output
                    notification_callback=uae_notification_callback,  # v83.0: Enable notifications
                    enable_chain_of_thought=self.config.uae_chain_of_thought,  # LangGraph reasoning
                    enable_unified_orchestrator=True,  # Full UnifiedIntelligenceOrchestrator
                )

                # Store enhanced UAE for chain-of-thought reasoning
                if self.config.uae_chain_of_thought:
                    self._enhanced_uae = get_enhanced_uae()
                    self.logger.info("âœ… UAE initialized with LangGraph chain-of-thought reasoning")
                else:
                    self._enhanced_uae = None
                    self.logger.info("âœ… UAE initialized (standard mode)")

                initialized_systems["uae"] = True
                os.environ["UAE_ENABLED"] = "true"
                os.environ["UAE_CHAIN_OF_THOUGHT"] = str(self.config.uae_chain_of_thought).lower()

                # Connect UAE to training data flywheel for experience logging
                try:
                    from autonomy.unified_data_flywheel import get_data_flywheel

                    flywheel = get_data_flywheel()
                    if flywheel:
                        # Store flywheel reference for UAE experience logging
                        self._uae_data_flywheel = flywheel

                        # Create callback for logging UAE decisions to training DB
                        async def log_uae_decision(decision_data: Dict[str, Any]) -> None:
                            """Log UAE decisions to training database."""
                            try:
                                experience_context = {
                                    "source": "uae_decision",
                                    "element_id": decision_data.get("element_id"),
                                    "confidence": decision_data.get("confidence", 0.0),
                                    "decision_source": decision_data.get("source", "unknown"),
                                    "chain_of_thought": decision_data.get("reasoning"),
                                    "timestamp": time.time(),
                                }

                                quality_score = min(0.9, decision_data.get("confidence", 0.5) + 0.2)

                                flywheel.add_experience(
                                    source="uae",
                                    input_text=decision_data.get("query", "screen_analysis"),
                                    output_text=str(decision_data.get("result", {})),
                                    context=experience_context,
                                    quality_score=quality_score,
                                )
                            except Exception as flywheel_error:
                                self.logger.debug(f"UAE flywheel logging error: {flywheel_error}")

                        self._uae_decision_callback = log_uae_decision
                        self.logger.info("âœ… UAE connected to training data flywheel")
                except Exception as flywheel_error:
                    self.logger.debug(f"UAE flywheel connection skipped: {flywheel_error}")

                print(f"  {TerminalUI.GREEN}âœ“ UAE: Unified Awareness Engine active{TerminalUI.RESET}")

            except ImportError as e:
                self.logger.warning(f"âš ï¸ UAE not available: {e}")
                os.environ["UAE_ENABLED"] = "false"
                print(f"  {TerminalUI.YELLOW}âš ï¸ UAE: Not available{TerminalUI.RESET}")
            except Exception as e:
                self.logger.error(f"âŒ UAE initialization failed: {e}")
                os.environ["UAE_ENABLED"] = "false"
                print(f"  {TerminalUI.YELLOW}âš ï¸ UAE: Failed ({e}){TerminalUI.RESET}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 2: Initialize SAI (Situational Awareness Intelligence)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.config.sai_enabled:
            try:
                self.logger.info("ğŸ‘ï¸ Step 2/7: Initializing SAI (Situational Awareness Intelligence)...")
                self.logger.info("   â€¢ Yabai bridge: " + ("Enabled" if self.config.sai_yabai_bridge else "Disabled"))
                self.logger.info("   â€¢ 24/7 workspace monitoring: Enabled")

                from intelligence.yabai_sai_integration import (
                    initialize_bridge,
                    get_bridge,
                    YabaiSAIBridge,
                )
                from intelligence.yabai_spatial_intelligence import (
                    get_yabai_intelligence,
                    YabaiSpatialIntelligence,
                )

                # Initialize Yabai Spatial Intelligence
                if self.config.sai_yabai_bridge:
                    self._yabai_intelligence = await get_yabai_intelligence(
                        learning_db=None,  # Will connect to learning DB from UAE
                        monitoring_interval=5.0,
                        enable_24_7_mode=True,
                    )

                    if self._yabai_intelligence and self._yabai_intelligence.yabai_available:
                        self.logger.info("âœ… SAI: Yabai bridge connected (24/7 workspace monitoring)")
                        initialized_systems["sai"] = True
                        os.environ["SAI_YABAI_BRIDGE"] = "true"
                    else:
                        self.logger.warning("âš ï¸ SAI: Yabai not available on this system")
                        os.environ["SAI_YABAI_BRIDGE"] = "false"
                else:
                    self.logger.info("â„¹ï¸ SAI: Running without Yabai bridge")

                initialized_systems["sai"] = True
                os.environ["SAI_ENABLED"] = "true"

                # Connect SAI to training data flywheel for workspace experience logging
                try:
                    from autonomy.unified_data_flywheel import get_data_flywheel

                    flywheel = get_data_flywheel()
                    if flywheel and self._yabai_intelligence:
                        # Store flywheel reference for SAI experience logging
                        self._sai_data_flywheel = flywheel

                        # Create callback for logging SAI workspace events to training DB
                        async def log_sai_workspace_event(event_data: Dict[str, Any]) -> None:
                            """Log SAI workspace events to training database."""
                            try:
                                experience_context = {
                                    "source": "sai_workspace",
                                    "event_type": event_data.get("event_type"),
                                    "space_id": event_data.get("space_id"),
                                    "app_name": event_data.get("app_name"),
                                    "window_count": event_data.get("window_count", 0),
                                    "timestamp": time.time(),
                                }

                                flywheel.add_experience(
                                    source="sai",
                                    input_text=f"workspace_{event_data.get('event_type', 'unknown')}",
                                    output_text=str(event_data.get("result", {})),
                                    context=experience_context,
                                    quality_score=0.7,
                                )
                            except Exception as flywheel_error:
                                self.logger.debug(f"SAI flywheel logging error: {flywheel_error}")

                        self._sai_event_callback = log_sai_workspace_event
                        self.logger.info("âœ… SAI connected to training data flywheel")
                except Exception as flywheel_error:
                    self.logger.debug(f"SAI flywheel connection skipped: {flywheel_error}")

                print(f"  {TerminalUI.GREEN}âœ“ SAI: Situational Awareness active{TerminalUI.RESET}")

            except ImportError as e:
                self.logger.warning(f"âš ï¸ SAI not available: {e}")
                os.environ["SAI_ENABLED"] = "false"
                print(f"  {TerminalUI.YELLOW}âš ï¸ SAI: Not available{TerminalUI.RESET}")
            except Exception as e:
                self.logger.error(f"âŒ SAI initialization failed: {e}")
                os.environ["SAI_ENABLED"] = "false"
                print(f"  {TerminalUI.YELLOW}âš ï¸ SAI: Failed ({e}){TerminalUI.RESET}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 3: Initialize Neural Mesh (Distributed Intelligence Coordination)
        # v9.4: Production-grade Neural Mesh with 60+ agents, knowledge graph,
        # communication bus, and multi-agent orchestration
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.config.neural_mesh_enabled:
            try:
                self.logger.info("ğŸ•¸ï¸ Step 3/7: Initializing Neural Mesh v9.4 (Production Multi-Agent System)...")
                self.logger.info("   â€¢ Production mode: " + str(self.config.neural_mesh_production))
                self.logger.info("   â€¢ Agents enabled: " + str(self.config.neural_mesh_agents_enabled))

                # v6.0+: Narrator announcement - Neural Mesh initialization
                if self.config.voice_enabled:
                    await self.narrator.speak("Initializing Neural Mesh multi-agent system.", wait=False)
                self.logger.info("   â€¢ Knowledge graph: " + str(self.config.neural_mesh_knowledge_graph))
                self.logger.info("   â€¢ JARVIS bridge: " + str(self.config.neural_mesh_jarvis_bridge))
                self.logger.info("   â€¢ Health interval: " + str(self.config.neural_mesh_health_interval) + "s")

                # v9.4: Import production Neural Mesh components
                neural_mesh_available = False
                try:
                    from backend.neural_mesh.neural_mesh_coordinator import (
                        NeuralMeshCoordinator,
                        get_neural_mesh,
                        start_neural_mesh,
                        stop_neural_mesh,
                    )
                    from backend.neural_mesh.jarvis_bridge import (
                        JARVISNeuralMeshBridge,
                        get_jarvis_bridge,
                        start_jarvis_neural_mesh,
                        stop_jarvis_neural_mesh,
                        AgentDiscoveryConfig,
                        SystemCategory,
                    )
                    from backend.neural_mesh.agents.agent_initializer import (
                        AgentInitializer,
                        initialize_production_agents,
                        PRODUCTION_AGENTS,
                    )
                    from backend.neural_mesh.config import NeuralMeshConfig, get_config
                    neural_mesh_available = True
                    self.logger.info("   âœ“ Production Neural Mesh modules imported")
                except ImportError as ie:
                    self.logger.warning(f"   âš  Production Neural Mesh not available: {ie}")
                    neural_mesh_available = False

                if neural_mesh_available and self.config.neural_mesh_production:
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # v9.4: Production Neural Mesh Initialization
                    # Comprehensive 4-tier architecture with 60+ agents
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                    # Get or create configuration
                    mesh_config = get_config()

                    # Override config from supervisor settings if needed
                    mesh_config.orchestrator.default_timeout = 30.0
                    mesh_config.communication_bus.max_queue_size = 10000
                    mesh_config.knowledge_graph.cleanup_interval = 3600.0

                    # Initialize Neural Mesh Coordinator (core orchestration)
                    self.logger.info("   â†’ Initializing Neural Mesh Coordinator...")
                    self._neural_mesh_coordinator = NeuralMeshCoordinator(config=mesh_config)
                    await self._neural_mesh_coordinator.initialize()
                    await self._neural_mesh_coordinator.start()
                    self.logger.info("   âœ“ Neural Mesh Coordinator started")

                    # v6.0+: Narrator announcement - Coordinator online
                    if self.config.voice_enabled:
                        await self.narrator.speak("Neural Mesh coordinator online.", wait=False)

                    # Store coordinator reference for system access
                    self._neural_mesh = self._neural_mesh_coordinator

                    # Initialize production agents if enabled
                    if self.config.neural_mesh_agents_enabled:
                        self.logger.info("   â†’ Initializing production agents...")

                        # Create agent initializer
                        agent_initializer = AgentInitializer(self._neural_mesh_coordinator)
                        self._neural_mesh_agents = await agent_initializer.initialize_all_agents()

                        agent_count = len(self._neural_mesh_agents)
                        self.logger.info(f"   âœ“ {agent_count} production agents registered")

                        # Log registered agents
                        for agent_name, agent in self._neural_mesh_agents.items():
                            self._neural_mesh_stats["agents_registered"] += 1
                            self.logger.debug(f"      â€¢ {agent_name} ({agent.agent_type})")

                        # v6.0+: Narrator announcement - Check for Google Workspace Agent
                        if self.config.voice_enabled:
                            # Detect if GoogleWorkspaceAgent was registered
                            google_workspace_registered = any(
                                "GoogleWorkspace" in agent.agent_type or "GoogleWorkspace" in agent_name
                                for agent_name, agent in self._neural_mesh_agents.items()
                            )

                            if google_workspace_registered:
                                await self.narrator.speak(
                                    "Google Workspace Agent registered. Gmail, Calendar, and Drive ready.",
                                    wait=False
                                )
                            else:
                                await self.narrator.speak(
                                    f"{agent_count} production agents registered and coordinated.",
                                    wait=False
                                )

                    # Initialize JARVIS Bridge if enabled (connects all JARVIS systems)
                    if self.config.neural_mesh_jarvis_bridge:
                        self.logger.info("   â†’ Initializing JARVIS Neural Mesh Bridge...")

                        # Configure agent discovery
                        discovery_config = AgentDiscoveryConfig(
                            enabled_categories={
                                SystemCategory.INTELLIGENCE,
                                SystemCategory.AUTONOMY,
                                SystemCategory.VOICE,
                            },
                            auto_initialize=True,
                            parallel_init=True,
                            max_parallel=10,
                            retry_on_failure=True,
                            max_retries=2,
                        )

                        # Create and start bridge
                        self._neural_mesh_bridge = JARVISNeuralMeshBridge(
                            config=discovery_config,
                            coordinator=self._neural_mesh_coordinator,
                        )
                        await self._neural_mesh_bridge.initialize()
                        await self._neural_mesh_bridge.start()

                        bridge_agents = len(self._neural_mesh_bridge.registered_agents)
                        self.logger.info(f"   âœ“ JARVIS Bridge started with {bridge_agents} system adapters")

                        # Register bridge event callbacks
                        def on_bridge_event(data):
                            self._neural_mesh_stats["messages_sent"] += 1

                        self._neural_mesh_bridge.on("agent_registered", on_bridge_event)

                    # Start health monitoring task
                    if self.config.neural_mesh_health_interval > 0:
                        async def neural_mesh_health_loop():
                            """Background health monitoring for Neural Mesh."""
                            while True:
                                try:
                                    await asyncio.sleep(self.config.neural_mesh_health_interval)

                                    # Get system health
                                    if self._neural_mesh_coordinator:
                                        health = await self._neural_mesh_coordinator.health_check()
                                        metrics = self._neural_mesh_coordinator.get_metrics()

                                        # Update stats
                                        self._neural_mesh_stats["agents_registered"] = metrics.registered_agents
                                        self._neural_mesh_stats["messages_sent"] = metrics.messages_published
                                        self._neural_mesh_stats["knowledge_entries"] = metrics.knowledge_entries
                                        self._neural_mesh_stats["workflows_completed"] = metrics.workflows_completed

                                        # Broadcast status if hub available
                                        if self._progress_hub:
                                            await self._progress_hub.broadcast_system_status(
                                                system="neural_mesh",
                                                status="healthy" if health.get("healthy") else "degraded",
                                                details={
                                                    "agents": self._neural_mesh_stats["agents_registered"],
                                                    "messages": self._neural_mesh_stats["messages_sent"],
                                                    "knowledge": self._neural_mesh_stats["knowledge_entries"],
                                                    "workflows": self._neural_mesh_stats["workflows_completed"],
                                                }
                                            )

                                except asyncio.CancelledError:
                                    break
                                except Exception as e:
                                    self.logger.warning(f"Neural Mesh health check error: {e}")

                        self._neural_mesh_health_task = asyncio.create_task(
                            neural_mesh_health_loop(),
                            name="neural_mesh_health_monitor"
                        )
                        self.logger.info(f"   âœ“ Health monitoring started (interval: {self.config.neural_mesh_health_interval}s)")

                    # Register with progress hub
                    if self._progress_hub:
                        await self._progress_hub.broadcast_system_status(
                            system="neural_mesh",
                            status="ready",
                            details={
                                "production": True,
                                "coordinator": True,
                                "agents": self._neural_mesh_stats["agents_registered"],
                                "bridge": self._neural_mesh_bridge is not None,
                            }
                        )

                    initialized_systems["neural_mesh"] = True
                    os.environ["NEURAL_MESH_ENABLED"] = "true"
                    os.environ["NEURAL_MESH_PRODUCTION"] = "true"

                    total_agents = self._neural_mesh_stats["agents_registered"]
                    bridge_status = "active" if self._neural_mesh_bridge else "disabled"
                    self.logger.info(f"âœ… Neural Mesh v9.4 Production initialized ({total_agents} agents, bridge: {bridge_status})")
                    print(f"  {TerminalUI.GREEN}âœ“ Neural Mesh v9.4: Production multi-agent system active ({total_agents} agents){TerminalUI.RESET}")

                    # v6.0+: Narrator announcement - Neural Mesh fully operational
                    if self.config.voice_enabled:
                        await self.narrator.speak(
                            f"Neural Mesh fully operational. {total_agents} agents coordinated.",
                            wait=False
                        )

                # Define NeuralMeshNode at this scope so it's available for all paths
                from dataclasses import dataclass, field
                from typing import Dict, Any, List, Callable, Optional
                from collections import defaultdict

                @dataclass
                class NeuralMeshNode:
                    """A node in the Neural Mesh network (v10.3 unified)."""
                    node_id: str
                    node_type: str
                    capabilities: List[str] = field(default_factory=list)
                    status: str = "active"
                    last_heartbeat: float = field(default_factory=time.time)
                    metadata: Dict[str, Any] = field(default_factory=dict)

                # Store in instance for later use
                self._NeuralMeshNode = NeuralMeshNode

                if not initialized_systems["neural_mesh"]:
                    # Fallback: Basic Neural Mesh (for compatibility)
                    self.logger.info("   â†’ Using basic Neural Mesh (fallback mode)...")

                    class BasicNeuralMesh:
                        """Basic Neural Mesh for fallback compatibility."""

                        def __init__(self, sync_interval: float = 5.0):
                            self._nodes: Dict[str, NeuralMeshNode] = {}
                            self._context: Dict[str, Any] = {}
                            self._subscribers: Dict[str, List[Callable]] = defaultdict(list)
                            self._sync_interval = sync_interval
                            self._sync_task: Optional[asyncio.Task] = None
                            self._running = False
                            self._logger = logging.getLogger("NeuralMesh")

                        async def register_node(self, node: NeuralMeshNode = None, node_name: str = None, node_type: str = None, capabilities: List[str] = None) -> bool:
                            """Register a node (async to match NeuralMeshCoordinator interface)."""
                            if node is not None:
                                # NeuralMeshNode dataclass provided directly
                                self._nodes[node.node_id] = node
                                self._logger.debug(f"Node registered: {node.node_id}")
                            elif node_name is not None:
                                # Parameters provided directly (NeuralMeshCoordinator style)
                                new_node = NeuralMeshNode(
                                    node_id=node_name,
                                    node_type=node_type or "unknown",
                                    capabilities=capabilities or [],
                                )
                                self._nodes[node_name] = new_node
                                self._logger.debug(f"Node registered: {node_name}")
                            return True

                        async def broadcast(self, event_type: str, data: Dict[str, Any], source: str = None) -> None:
                            for subscriber in self._subscribers.get(event_type, []):
                                try:
                                    if asyncio.iscoroutinefunction(subscriber):
                                        await subscriber({"event_type": event_type, "data": data, "source": source})
                                    else:
                                        subscriber({"event_type": event_type, "data": data, "source": source})
                                except Exception as e:
                                    self._logger.warning(f"Subscriber error: {e}")

                        async def subscribe(self, event_type: str, callback: Callable) -> bool:
                            """Subscribe to events (async to match NeuralMeshCoordinator interface)."""
                            self._subscribers[event_type].append(callback)
                            return True

                        def get_active_nodes(self, node_type: str = None) -> List[NeuralMeshNode]:
                            nodes = list(self._nodes.values())
                            if node_type:
                                nodes = [n for n in nodes if n.node_type == node_type]
                            return [n for n in nodes if n.status == "active"]

                        async def start(self) -> None:
                            self._running = True

                        async def stop(self) -> None:
                            self._running = False

                        def get_stats(self) -> Dict[str, Any]:
                            return {
                                "total_nodes": len(self._nodes),
                                "active_nodes": len([n for n in self._nodes.values() if n.status == "active"]),
                                "mode": "basic_fallback",
                            }

                    self._neural_mesh = BasicNeuralMesh(sync_interval=self.config.neural_mesh_sync_interval)

                    # Register core nodes
                    if initialized_systems["uae"]:
                        await self._neural_mesh.register_node(NeuralMeshNode(
                            node_id="uae-primary",
                            node_type="uae",
                            capabilities=["vision", "screen_capture", "element_detection"],
                        ))

                    if initialized_systems["sai"]:
                        await self._neural_mesh.register_node(NeuralMeshNode(
                            node_id="sai-primary",
                            node_type="sai",
                            capabilities=["window_tracking", "app_focus", "workspace_state"],
                        ))

                    await self._neural_mesh.start()

                    initialized_systems["neural_mesh"] = True
                    os.environ["NEURAL_MESH_ENABLED"] = "true"
                    os.environ["NEURAL_MESH_PRODUCTION"] = "false"
                    self.logger.info(f"âœ… Neural Mesh initialized (basic mode)")
                    print(f"  {TerminalUI.GREEN}âœ“ Neural Mesh: Basic intelligence coordination active{TerminalUI.RESET}")

            except Exception as e:
                self.logger.error(f"âŒ Neural Mesh initialization failed: {e}")
                import traceback
                self.logger.debug(traceback.format_exc())
                os.environ["NEURAL_MESH_ENABLED"] = "false"
                print(f"  {TerminalUI.YELLOW}âš ï¸ Neural Mesh: Failed ({e}){TerminalUI.RESET}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 4: Initialize MAS (Multi-Agent System)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.config.mas_enabled:
            try:
                self.logger.info("ğŸ¤– Step 4/7: Initializing MAS (Multi-Agent System)...")
                self.logger.info("   â€¢ Max concurrent agents: " + str(self.config.mas_max_concurrent_agents))
                self.logger.info("   â€¢ Dynamic spawning: Enabled")

                from dataclasses import dataclass, field
                from typing import Dict, Any, List, Optional, Callable
                from enum import Enum
                import uuid

                class AgentStatus(Enum):
                    IDLE = "idle"
                    RUNNING = "running"
                    WAITING = "waiting"
                    COMPLETED = "completed"
                    FAILED = "failed"

                @dataclass
                class AgentTask:
                    """A task for an agent to execute."""
                    task_id: str
                    goal: str
                    context: Dict[str, Any] = field(default_factory=dict)
                    priority: int = 5  # 1-10
                    dependencies: List[str] = field(default_factory=list)
                    parent_task_id: Optional[str] = None
                    status: AgentStatus = AgentStatus.IDLE
                    result: Optional[Any] = None
                    error: Optional[str] = None
                    created_at: float = field(default_factory=time.time)
                    started_at: Optional[float] = None
                    completed_at: Optional[float] = None

                @dataclass
                class Agent:
                    """An autonomous agent in the MAS."""
                    agent_id: str
                    agent_type: str
                    capabilities: List[str] = field(default_factory=list)
                    current_task: Optional[AgentTask] = None
                    status: AgentStatus = AgentStatus.IDLE
                    metrics: Dict[str, Any] = field(default_factory=dict)

                class MultiAgentSystem:
                    """
                    Multi-Agent System for coordinated autonomous execution.

                    Provides:
                    - Dynamic agent spawning based on task complexity
                    - Task decomposition and parallel execution
                    - Agent collaboration and result aggregation
                    - Conflict resolution for shared resources
                    - Load balancing across available agents
                    """

                    def __init__(self, max_concurrent_agents: int = 5):
                        self._agents: Dict[str, Agent] = {}
                        self._task_queue: asyncio.Queue = asyncio.Queue()
                        self._completed_tasks: Dict[str, AgentTask] = {}
                        self._max_concurrent = max_concurrent_agents
                        self._running = False
                        self._coordinator_task: Optional[asyncio.Task] = None
                        self._agent_executors: Dict[str, Callable] = {}
                        self._logger = logging.getLogger("MAS")

                    def register_agent_type(self, agent_type: str, executor: Callable) -> None:
                        """Register an agent type with its executor function."""
                        self._agent_executors[agent_type] = executor
                        self._logger.debug(f"Agent type registered: {agent_type}")

                    async def spawn_agent(self, agent_type: str, capabilities: List[str] = None) -> Agent:
                        """Spawn a new agent."""
                        if len(self._agents) >= self._max_concurrent:
                            # Find idle agent to reuse
                            for agent in self._agents.values():
                                if agent.status == AgentStatus.IDLE:
                                    agent.capabilities = capabilities or []
                                    return agent
                            raise RuntimeError(f"Max agents ({self._max_concurrent}) reached")

                        agent = Agent(
                            agent_id=f"agent-{uuid.uuid4().hex[:8]}",
                            agent_type=agent_type,
                            capabilities=capabilities or [],
                        )
                        self._agents[agent.agent_id] = agent
                        self._logger.info(f"Agent spawned: {agent.agent_id} ({agent_type})")
                        return agent

                    async def submit_task(self, task: AgentTask) -> str:
                        """Submit a task for execution."""
                        await self._task_queue.put(task)
                        self._logger.debug(f"Task submitted: {task.task_id}")
                        return task.task_id

                    async def decompose_task(self, goal: str, context: Dict[str, Any] = None) -> List[AgentTask]:
                        """Decompose a complex goal into subtasks."""
                        # This would typically use an LLM to break down the task
                        # For now, return as single task
                        task = AgentTask(
                            task_id=f"task-{uuid.uuid4().hex[:8]}",
                            goal=goal,
                            context=context or {},
                        )
                        return [task]

                    async def execute_task(self, task: AgentTask) -> AgentTask:
                        """Execute a single task."""
                        task.status = AgentStatus.RUNNING
                        task.started_at = time.time()

                        try:
                            # Find appropriate agent type
                            agent_type = self._determine_agent_type(task)

                            # Spawn or reuse agent
                            agent = await self.spawn_agent(agent_type)
                            agent.current_task = task
                            agent.status = AgentStatus.RUNNING

                            # Get executor for this agent type
                            executor = self._agent_executors.get(agent_type)
                            if executor:
                                if asyncio.iscoroutinefunction(executor):
                                    result = await executor(task)
                                else:
                                    result = executor(task)
                                task.result = result
                                task.status = AgentStatus.COMPLETED
                            else:
                                task.error = f"No executor for agent type: {agent_type}"
                                task.status = AgentStatus.FAILED

                        except Exception as e:
                            task.error = str(e)
                            task.status = AgentStatus.FAILED
                            self._logger.error(f"Task {task.task_id} failed: {e}")

                        finally:
                            task.completed_at = time.time()
                            if agent:
                                agent.current_task = None
                                agent.status = AgentStatus.IDLE

                        self._completed_tasks[task.task_id] = task
                        return task

                    def _determine_agent_type(self, task: AgentTask) -> str:
                        """Determine the best agent type for a task."""
                        goal_lower = task.goal.lower()

                        if any(w in goal_lower for w in ["search", "find", "look"]):
                            return "explorer"
                        elif any(w in goal_lower for w in ["write", "create", "generate"]):
                            return "creator"
                        elif any(w in goal_lower for w in ["analyze", "review", "check"]):
                            return "analyzer"
                        elif any(w in goal_lower for w in ["scrape", "fetch", "download"]):
                            return "scraper"
                        else:
                            return "general"

                    async def run_goal(self, goal: str, context: Dict[str, Any] = None) -> List[AgentTask]:
                        """Execute a goal by decomposing and running all subtasks."""
                        tasks = await self.decompose_task(goal, context)
                        results = []

                        # Execute tasks (respecting dependencies)
                        for task in tasks:
                            result = await self.execute_task(task)
                            results.append(result)

                        return results

                    async def start(self) -> None:
                        """Start the MAS coordinator."""
                        self._running = True
                        self._coordinator_task = asyncio.create_task(self._coordinate())
                        self._logger.info("MAS coordinator started")

                    async def stop(self) -> None:
                        """Stop the MAS."""
                        self._running = False
                        if self._coordinator_task:
                            self._coordinator_task.cancel()
                            try:
                                await self._coordinator_task
                            except asyncio.CancelledError:
                                pass

                    async def _coordinate(self) -> None:
                        """Background coordination loop."""
                        while self._running:
                            try:
                                # Process queued tasks
                                try:
                                    task = await asyncio.wait_for(
                                        self._task_queue.get(),
                                        timeout=1.0
                                    )
                                    asyncio.create_task(self.execute_task(task))
                                except asyncio.TimeoutError:
                                    pass

                                # Clean up completed agents
                                for agent in list(self._agents.values()):
                                    if agent.status == AgentStatus.COMPLETED:
                                        agent.status = AgentStatus.IDLE

                            except asyncio.CancelledError:
                                break
                            except Exception as e:
                                self._logger.error(f"Coordinator error: {e}")
                                await asyncio.sleep(1)

                    def get_stats(self) -> Dict[str, Any]:
                        """Get MAS statistics."""
                        return {
                            "total_agents": len(self._agents),
                            "active_agents": len([a for a in self._agents.values() if a.status == AgentStatus.RUNNING]),
                            "idle_agents": len([a for a in self._agents.values() if a.status == AgentStatus.IDLE]),
                            "queued_tasks": self._task_queue.qsize(),
                            "completed_tasks": len(self._completed_tasks),
                            "max_concurrent": self._max_concurrent,
                        }

                # Create and start MAS
                self._mas = MultiAgentSystem(max_concurrent_agents=self.config.mas_max_concurrent_agents)

                # Register default agent executors
                async def general_executor(task: AgentTask) -> Dict[str, Any]:
                    """Default general-purpose agent executor."""
                    return {"status": "completed", "message": f"Processed: {task.goal}"}

                async def scraper_executor(task: AgentTask) -> Dict[str, Any]:
                    """Web scraping agent executor."""
                    # This would integrate with Safe Scout
                    return {"status": "completed", "message": f"Scraped: {task.goal}"}

                self._mas.register_agent_type("general", general_executor)
                self._mas.register_agent_type("explorer", general_executor)
                self._mas.register_agent_type("creator", general_executor)
                self._mas.register_agent_type("analyzer", general_executor)
                self._mas.register_agent_type("scraper", scraper_executor)

                # Start MAS
                await self._mas.start()

                # Register MAS with Neural Mesh if available
                if hasattr(self, '_neural_mesh') and self._neural_mesh:
                    # Use stored NeuralMeshNode class or call register_node method
                    if hasattr(self, '_NeuralMeshNode'):
                        NeuralMeshNode = self._NeuralMeshNode
                        if hasattr(self._neural_mesh, 'register_node'):
                            # Check if it's the BasicNeuralMesh (takes NeuralMeshNode) or NeuralMeshCoordinator (takes params)
                            # Register with Neural Mesh (both BasicNeuralMesh and NeuralMeshCoordinator are async)
                            if hasattr(self._neural_mesh, '_nodes'):
                                # BasicNeuralMesh - uses NeuralMeshNode dataclass
                                await self._neural_mesh.register_node(NeuralMeshNode(
                                    node_id="mas-coordinator",
                                    node_type="mas",
                                    capabilities=["task_decomposition", "agent_spawning", "parallel_execution"],
                                ))
                            else:
                                # NeuralMeshCoordinator - uses parameters directly
                                await self._neural_mesh.register_node(
                                    node_name="mas-coordinator",
                                    node_type="mas",
                                    capabilities=["task_decomposition", "agent_spawning", "parallel_execution"],
                                )

                initialized_systems["mas"] = True
                os.environ["MAS_ENABLED"] = "true"
                self.logger.info(f"âœ… MAS initialized (max agents: {self.config.mas_max_concurrent_agents})")
                print(f"  {TerminalUI.GREEN}âœ“ MAS: Multi-Agent System active{TerminalUI.RESET}")

            except Exception as e:
                self.logger.error(f"âŒ MAS initialization failed: {e}")
                os.environ["MAS_ENABLED"] = "false"
                print(f"  {TerminalUI.YELLOW}âš ï¸ MAS: Failed ({e}){TerminalUI.RESET}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 5: Initialize CAI (Collective AI Intelligence)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.config.cai_enabled:
            try:
                self.logger.info("ğŸ§¬ Step 5/7: Initializing CAI (Collective AI Intelligence)...")

                from dataclasses import dataclass, field
                from typing import Dict, Any, List, Optional

                @dataclass
                class InsightSource:
                    """Source of an insight."""
                    system: str  # "uae", "sai", "mas", etc.
                    confidence: float
                    timestamp: float
                    data: Dict[str, Any]

                @dataclass
                class CollectiveInsight:
                    """An insight aggregated from multiple sources."""
                    insight_id: str
                    topic: str
                    sources: List[InsightSource] = field(default_factory=list)
                    aggregated_confidence: float = 0.0
                    recommendations: List[str] = field(default_factory=list)
                    created_at: float = field(default_factory=time.time)

                class CollectiveAI:
                    """
                    Collective AI Intelligence - Emergent intelligence from all subsystems.

                    Provides:
                    - Synthesis of insights from UAE, SAI, MAS
                    - Cross-system pattern detection
                    - Proactive recommendation generation
                    - Adaptive learning from system interactions
                    """

                    def __init__(self):
                        self._insights: Dict[str, CollectiveInsight] = {}
                        self._patterns: List[Dict[str, Any]] = []
                        self._recommendation_callbacks: List[Callable] = []
                        self._logger = logging.getLogger("CAI")

                    def add_insight_source(self, topic: str, source: InsightSource) -> None:
                        """Add an insight source for a topic."""
                        if topic not in self._insights:
                            self._insights[topic] = CollectiveInsight(
                                insight_id=f"insight-{uuid.uuid4().hex[:8]}",
                                topic=topic,
                            )

                        self._insights[topic].sources.append(source)
                        self._recalculate_confidence(topic)

                    def _recalculate_confidence(self, topic: str) -> None:
                        """Recalculate aggregated confidence for a topic."""
                        if topic in self._insights:
                            insight = self._insights[topic]
                            if insight.sources:
                                # Weighted average based on source confidence
                                total = sum(s.confidence for s in insight.sources)
                                insight.aggregated_confidence = total / len(insight.sources)

                    def get_insight(self, topic: str) -> Optional[CollectiveInsight]:
                        """Get the collective insight for a topic."""
                        return self._insights.get(topic)

                    def detect_patterns(self) -> List[Dict[str, Any]]:
                        """Detect patterns across all insights."""
                        # Simple pattern detection - look for topics with multiple high-confidence sources
                        patterns = []
                        for insight in self._insights.values():
                            if len(insight.sources) >= 2 and insight.aggregated_confidence > 0.7:
                                patterns.append({
                                    "topic": insight.topic,
                                    "confidence": insight.aggregated_confidence,
                                    "source_count": len(insight.sources),
                                    "systems": list(set(s.system for s in insight.sources)),
                                })
                        self._patterns = patterns
                        return patterns

                    async def generate_recommendations(self) -> List[str]:
                        """Generate proactive recommendations based on patterns."""
                        recommendations = []
                        patterns = self.detect_patterns()

                        for pattern in patterns:
                            if pattern["confidence"] > 0.8:
                                recommendations.append(
                                    f"High-confidence pattern detected in {pattern['topic']} "
                                    f"across {pattern['systems']}"
                                )

                        # Notify callbacks
                        for callback in self._recommendation_callbacks:
                            try:
                                if asyncio.iscoroutinefunction(callback):
                                    await callback(recommendations)
                                else:
                                    callback(recommendations)
                            except Exception as e:
                                self._logger.warning(f"Recommendation callback error: {e}")

                        return recommendations

                    def register_recommendation_callback(self, callback: Callable) -> None:
                        """Register a callback for recommendations."""
                        self._recommendation_callbacks.append(callback)

                    def get_stats(self) -> Dict[str, Any]:
                        """Get CAI statistics."""
                        return {
                            "total_insights": len(self._insights),
                            "total_patterns": len(self._patterns),
                            "high_confidence_insights": len([
                                i for i in self._insights.values()
                                if i.aggregated_confidence > 0.7
                            ]),
                        }

                # Create CAI instance
                self._cai = CollectiveAI()

                # Connect CAI to Neural Mesh for insight aggregation
                if hasattr(self, '_neural_mesh') and self._neural_mesh:
                    async def on_context_sync(message: Dict[str, Any]) -> None:
                        """Handle context sync from Neural Mesh."""
                        for key, value in message.get("data", {}).items():
                            self._cai.add_insight_source(
                                topic=key,
                                source=InsightSource(
                                    system="mesh",
                                    confidence=0.8,
                                    timestamp=time.time(),
                                    data=value,
                                )
                            )

                    # Properly await the async subscribe method
                    await self._neural_mesh.subscribe("context_sync", on_context_sync)

                initialized_systems["cai"] = True
                os.environ["CAI_ENABLED"] = "true"
                self.logger.info("âœ… CAI initialized (Collective AI Intelligence)")
                print(f"  {TerminalUI.GREEN}âœ“ CAI: Collective AI Intelligence active{TerminalUI.RESET}")

            except Exception as e:
                self.logger.error(f"âŒ CAI initialization failed: {e}")
                os.environ["CAI_ENABLED"] = "false"
                print(f"  {TerminalUI.YELLOW}âš ï¸ CAI: Failed ({e}){TerminalUI.RESET}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 6: Initialize Continuous Background Web Scraping
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.config.continuous_scraping_enabled:
            try:
                self.logger.info("ğŸŒ Step 6/7: Initializing Continuous Background Web Scraping...")

                # Start the continuous scraping background task
                self._continuous_scraping_task = asyncio.create_task(
                    self._run_continuous_scraping()
                )

                initialized_systems["continuous_scraping"] = True
                os.environ["CONTINUOUS_SCRAPING_ENABLED"] = "true"
                self.logger.info(
                    f"âœ… Continuous scraping initialized "
                    f"(interval: {self.config.continuous_scraping_interval_hours}h, "
                    f"max pages: {self.config.continuous_scraping_max_pages})"
                )
                print(f"  {TerminalUI.GREEN}âœ“ Web Scraping: Continuous background learning active{TerminalUI.RESET}")

            except Exception as e:
                self.logger.error(f"âŒ Continuous scraping initialization failed: {e}")
                os.environ["CONTINUOUS_SCRAPING_ENABLED"] = "false"
                print(f"  {TerminalUI.YELLOW}âš ï¸ Web Scraping: Failed ({e}){TerminalUI.RESET}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 7: Initialize Reactor-Core Integration
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.config.reactor_core_integration_enabled:
            try:
                self.logger.info("âš›ï¸ Step 7/7: Initializing Reactor-Core Integration...")

                from autonomy.reactor_core_integration import (
                    get_reactor_core_integration,
                    initialize_reactor_core,
                    initialize_prime_neural_mesh,
                    get_prime_neural_mesh_bridge,
                    ReactorCoreConfig,
                )

                # Get reactor-core integration singleton
                self._reactor_core_integration = get_reactor_core_integration()

                # Configure with paths from supervisor config
                if hasattr(self.config, 'reactor_core_repo_path'):
                    self._reactor_core_integration.config.reactor_core_path = Path(
                        self.config.reactor_core_repo_path
                    )
                if hasattr(self.config, 'jarvis_prime_repo_path'):
                    self._reactor_core_integration.config.jarvis_prime_path = Path(
                        self.config.jarvis_prime_repo_path
                    )

                # Initialize all reactor-core components
                reactor_init_success = await initialize_reactor_core()

                if reactor_init_success:
                    # Connect to Neural Mesh if available
                    if hasattr(self, '_neural_mesh') and self._neural_mesh:
                        # Register reactor-core as a Neural Mesh node
                        if hasattr(self, '_NeuralMeshNode'):
                            NeuralMeshNode = self._NeuralMeshNode
                            # Register with Neural Mesh (both BasicNeuralMesh and NeuralMeshCoordinator are async)
                            if hasattr(self._neural_mesh, '_nodes'):
                                # BasicNeuralMesh - uses NeuralMeshNode dataclass
                                await self._neural_mesh.register_node(NeuralMeshNode(
                                    node_id="reactor_core",
                                    node_type="reactor_core",
                                    capabilities=["training", "scraping", "model_deployment", "experience_collection"],
                                    status="active",
                                ))
                            else:
                                # NeuralMeshCoordinator - uses parameters directly
                                await self._neural_mesh.register_node(
                                    node_name="reactor_core",
                                    node_type="reactor_core",
                                    capabilities=["training", "scraping", "model_deployment", "experience_collection"],
                                )
                        self.logger.info("âœ… Reactor-Core connected to Neural Mesh")

                    # Connect to CAI if available for insight aggregation
                    if hasattr(self, '_cai') and self._cai:
                        async def on_reactor_event(event: Dict[str, Any]) -> None:
                            """Feed reactor-core events into CAI for analysis."""
                            self._cai.add_insight_source(
                                topic=event.get("event_type", "reactor_core"),
                                source=InsightSource(
                                    system="reactor_core",
                                    confidence=0.85,
                                    timestamp=time.time(),
                                    data=event,
                                )
                            )

                        self._reactor_core_integration.register_prime_callback(on_reactor_event)
                        self.logger.info("âœ… Reactor-Core connected to CAI for intelligence aggregation")

                    # Connect to Data Flywheel for experience syncing
                    if hasattr(self, '_data_flywheel') and self._data_flywheel:
                        self.logger.info("âœ… Reactor-Core connected to Data Flywheel")

                    # Initialize Prime Neural Mesh bridge
                    try:
                        prime_mesh_success = await initialize_prime_neural_mesh()
                        if prime_mesh_success:
                            self._prime_neural_mesh_bridge = get_prime_neural_mesh_bridge()
                            self.logger.info("âœ… Prime Neural Mesh bridge initialized")

                            # Connect bridge to CAI if available
                            if hasattr(self, '_cai') and self._cai:
                                async def on_prime_mesh_event(event: Dict[str, Any]) -> None:
                                    """Feed Prime mesh events into CAI for analysis."""
                                    self._cai.add_insight_source(
                                        topic=f"prime_{event.get('event_type', 'unknown')}",
                                        source=InsightSource(
                                            system="prime_neural_mesh",
                                            confidence=0.9,
                                            timestamp=time.time(),
                                            data=event,
                                        )
                                    )

                                self._prime_neural_mesh_bridge.register_callback(on_prime_mesh_event)
                                self.logger.info("âœ… Prime Neural Mesh connected to CAI")

                            print(f"  {TerminalUI.GREEN}âœ“ Prime Neural Mesh: JARVIS-Prime bridge active{TerminalUI.RESET}")
                        else:
                            self.logger.warning("âš ï¸ Prime Neural Mesh bridge initialization incomplete")
                    except Exception as prime_mesh_error:
                        self.logger.warning(f"âš ï¸ Prime Neural Mesh initialization skipped: {prime_mesh_error}")

                    initialized_systems["reactor_core"] = True
                    os.environ["REACTOR_CORE_ENABLED"] = "true"

                    # Get status for logging
                    reactor_status = self._reactor_core_integration.get_status()
                    active_components = [k for k, v in reactor_status["components"].items() if v]

                    self.logger.info(
                        f"âœ… Reactor-Core Integration initialized "
                        f"(components: {', '.join(active_components)})"
                    )
                    print(f"  {TerminalUI.GREEN}âœ“ Reactor-Core: Training pipeline integration active{TerminalUI.RESET}")

                    # v9.1: Broadcast reactor-core status to loading server
                    await self._broadcast_reactor_core_status(
                        status="ready",
                        components=reactor_status.get("components", {}),
                        training_active=False,
                    )
                else:
                    self.logger.warning("âš ï¸ Reactor-Core initialization returned false")
                    os.environ["REACTOR_CORE_ENABLED"] = "false"
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Reactor-Core: Partial initialization{TerminalUI.RESET}")

            except ImportError as e:
                self.logger.warning(f"âš ï¸ Reactor-Core not available: {e}")
                os.environ["REACTOR_CORE_ENABLED"] = "false"
                print(f"  {TerminalUI.YELLOW}âš ï¸ Reactor-Core: Not available{TerminalUI.RESET}")
            except Exception as e:
                self.logger.error(f"âŒ Reactor-Core initialization failed: {e}")
                os.environ["REACTOR_CORE_ENABLED"] = "false"
                print(f"  {TerminalUI.YELLOW}âš ï¸ Reactor-Core: Failed ({e}){TerminalUI.RESET}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 8: Initialize Trinity Knowledge Indexer (Brain Bridge)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # This connects JARVIS's scraped web memory to his conversational abilities
        trinity_indexer_enabled = os.getenv("TRINITY_INDEXER_ENABLED", "true").lower() == "true"
        if trinity_indexer_enabled:
            try:
                self.logger.info("ğŸ§  Step 8: Initializing Trinity Knowledge Indexer...")

                from autonomy.trinity_knowledge_indexer import (
                    get_knowledge_indexer,
                    start_knowledge_indexer,
                )

                # Get or initialize the indexer
                self._trinity_indexer = await get_knowledge_indexer()

                # Start background indexing loop
                await start_knowledge_indexer()

                initialized_systems["trinity_indexer"] = True
                os.environ["TRINITY_INDEXER_ENABLED"] = "true"

                # Get status for logging
                indexer_status = self._trinity_indexer.get_status()
                chunks_indexed = indexer_status.get("metrics", {}).get("total_chunks_indexed", 0)

                self.logger.info(
                    f"âœ… Trinity Knowledge Indexer initialized "
                    f"(ChromaDB: {indexer_status['vector_stores']['chromadb']}, "
                    f"FAISS: {indexer_status['vector_stores']['faiss']}, "
                    f"Chunks indexed: {chunks_indexed})"
                )
                print(f"  {TerminalUI.GREEN}âœ“ Trinity Indexer: Brain Bridge connecting scraped knowledge to RAG{TerminalUI.RESET}")

            except ImportError as e:
                self.logger.warning(f"âš ï¸ Trinity Indexer not available: {e}")
                os.environ["TRINITY_INDEXER_ENABLED"] = "false"
                print(f"  {TerminalUI.YELLOW}âš ï¸ Trinity Indexer: Not available{TerminalUI.RESET}")
            except Exception as e:
                self.logger.error(f"âŒ Trinity Indexer initialization failed: {e}")
                os.environ["TRINITY_INDEXER_ENABLED"] = "false"
                print(f"  {TerminalUI.YELLOW}âš ï¸ Trinity Indexer: Failed ({e}){TerminalUI.RESET}")
        else:
            initialized_systems["trinity_indexer"] = False

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Broadcast Intelligence Systems Status
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        active_systems = [k for k, v in initialized_systems.items() if v]

        await self._broadcast_startup_progress(
            stage="intelligence_systems_ready",
            message=f"Intelligence stack online: {', '.join(active_systems)}",
            progress=85,
            metadata={
                "intelligence_systems": {
                    "uae": initialized_systems["uae"],
                    "sai": initialized_systems["sai"],
                    "neural_mesh": initialized_systems["neural_mesh"],
                    "mas": initialized_systems["mas"],
                    "cai": initialized_systems["cai"],
                    "continuous_scraping": initialized_systems["continuous_scraping"],
                    "reactor_core": initialized_systems["reactor_core"],
                    "active_count": len(active_systems),
                }
            }
        )

        self.logger.info("â•" * 60)
        self.logger.info(f"âœ… Intelligence Stack: {len(active_systems)}/7 systems active")
        self.logger.info("â•" * 60)

        # v9.1: Broadcast comprehensive intelligence systems status
        await self._broadcast_intelligence_systems_status(
            uae_status={
                "status": "ready" if initialized_systems["uae"] else "unavailable",
                "chain_of_thought": self.config.uae_chain_of_thought,
            },
            sai_status={
                "status": "ready" if initialized_systems["sai"] else "unavailable",
                "yabai_bridge": self.config.sai_yabai_bridge,
            },
            neural_mesh_status={
                "status": "ready" if initialized_systems["neural_mesh"] else "unavailable",
                "sync_interval": self.config.neural_mesh_sync_interval,
            },
            mas_status={
                "status": "ready" if initialized_systems["mas"] else "unavailable",
                "max_agents": self.config.mas_max_concurrent_agents,
            },
            cai_status={
                "status": "ready" if initialized_systems["cai"] else "unavailable",
            },
        )

    async def _initialize_collaboration_and_ide_systems(self) -> None:
        """
        v13.0: Initialize Collaboration & IDE Integration System.

        This enables advanced collaboration features and IDE integration:
        - CRDT-based multi-user conflict resolution (concurrent editing)
        - Code ownership awareness (CODEOWNERS, git blame analysis)
        - Review workflow integration (GitHub/GitLab PR/MR workflows)
        - LSP server for completions, diagnostics, code actions
        - VS Code/Cursor extension support with commands & keybindings

        Architecture:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚             Collaboration & IDE Integration (v13.0)                 â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚  Collaboration   â”‚    â”‚  Code Ownership  â”‚    â”‚    Review     â”‚  â”‚
        â”‚  â”‚     Engine       â”‚â—„â”€â”€â–ºâ”‚     Engine       â”‚â—„â”€â”€â–ºâ”‚   Workflow    â”‚  â”‚
        â”‚  â”‚   (CRDT/Sync)    â”‚    â”‚(CODEOWNERS/Blame)â”‚    â”‚ (GitHub/Lab)  â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚           â”‚                       â”‚                       â”‚         â”‚
        â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
        â”‚                                   â–¼                                 â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
        â”‚  â”‚                     LSP Server (JARVIS)                      â”‚   â”‚
        â”‚  â”‚  Completions | Diagnostics | Hover | CodeActions | Definitionâ”‚   â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
        â”‚                                   â–¼                                 â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
        â”‚  â”‚              IDE Integration (VS Code/Cursor)                |   â”‚
        â”‚  â”‚  Commands | KeyBindings | StatusBar | CodeLens | Completions â”‚   â”‚  
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        self.logger.info("â•" * 60)
        self.logger.info("ğŸ¤ v13.0: Initializing Collaboration & IDE Integration System...")
        self.logger.info("â•" * 60)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v108.0: Enterprise DI Container-Based Intelligence Services
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        #
        # FIXES 4 CRITICAL BUGS in the original code:
        # 1. CrossRepoCollaborationCoordinator: Expected config= but got collaboration_engine=
        # 2. CrossRepoOwnershipCoordinator: Expected config= but got ownership_engine=
        # 3. CrossRepoReviewCoordinator: Expected config= but got review_engine=
        # 4. CrossRepoIDECoordinator: Expected config= but got ide_engine=
        # 5. All coordinators: Called .start() but only have .initialize()
        #
        # The DI container handles:
        # - Correct parameter passing (config, not engine instances)
        # - Proper dependency ordering via topological sort
        # - Transactional initialization with rollback
        # - Cycle detection to prevent infinite loops
        # - Lazy async lock initialization (safe before event loop)
        # - Graceful degradation for optional services
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        # Track initialization status
        initialized_systems: Dict[str, bool] = {
            "collaboration_engine": False,
            "code_ownership": False,
            "review_workflow": False,
            "lsp_server": False,
            "ide_integration": False,
        }

        try:
            # Import the DI container and intelligence services module
            from backend.core.di import ServiceContainer, create_container
            from backend.core.di.intelligence_services import (
                register_intelligence_services,
                initialize_intelligence_services,
                get_service_status_display,
            )

            self.logger.info("ğŸ”§ v108.0: Using Enterprise DI Container for service initialization...")

            # Create or get the DI container (lazy lock initialization is safe here)
            if self._di_container is None:
                self._di_container = create_container()
                self.logger.info("   â€¢ DI container created")

            # Register all intelligence services with the container
            # This handles correct parameter passing (config=, not engine=)
            registration_status = register_intelligence_services(self._di_container)
            self.logger.info(f"   â€¢ Services registered: {sum(registration_status.values())}/{len(registration_status)}")

            # Initialize all services (container handles ordering and parallelization)
            # This calls .initialize() on each service, NOT .start()
            init_status = await initialize_intelligence_services(self._di_container)
            self.logger.info(f"   â€¢ Services initialized: {len([s for s in init_status.values() if s == 'initialized'])}")

            # Display status for each service
            for line in get_service_status_display(init_status):
                print(f"  {line}")

            # Resolve services from container and set instance attributes for backward compatibility
            # This allows other parts of the codebase to still access self._collaboration_engine, etc.
            await self._resolve_intelligence_services_from_container()

            # Update initialization status based on what was actually initialized
            if init_status.get("CollaborationEngine") == "initialized":
                initialized_systems["collaboration_engine"] = True
            if init_status.get("CodeOwnershipEngine") == "initialized":
                initialized_systems["code_ownership"] = True
            if init_status.get("ReviewWorkflowEngine") == "initialized":
                initialized_systems["review_workflow"] = True
            if init_status.get("JARVISLSPServer") == "initialized":
                initialized_systems["lsp_server"] = True
            if init_status.get("IDEIntegrationEngine") == "initialized":
                initialized_systems["ide_integration"] = True

            self._intelligence_services_initialized = True
            self.logger.info("âœ… v108.0: Intelligence services initialized via DI container")

        except ImportError as e:
            # Fallback: DI container not available, log warning
            self.logger.warning(f"âš ï¸ v108.0: DI container not available ({e}), falling back to legacy initialization")
            print(f"  {TerminalUI.YELLOW}âš ï¸ DI Container: Not available, using legacy mode{TerminalUI.RESET}")

            # Legacy fallback initialization (original buggy code is not executed)
            # Instead, we just log that intelligence services are not available
            self.logger.info("   â€¢ Intelligence services skipped (DI container required)")

        except Exception as e:
            # Container initialization failed - log and continue
            self.logger.error(f"âŒ v108.0: DI container initialization failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Intelligence Services: Failed ({e}){TerminalUI.RESET}")
            import traceback
            self.logger.debug(f"Traceback: {traceback.format_exc()}")

        # Summary
        active_systems = [name for name, status in initialized_systems.items() if status]

        self.logger.info("â•" * 60)
        self.logger.info(f"âœ… Collaboration & IDE Stack: {len(active_systems)}/5 systems active")
        self.logger.info("â•" * 60)

        # Set environment variables for other systems
        os.environ["JARVIS_COLLAB_ACTIVE"] = str(len(active_systems) > 0).lower()
        os.environ["JARVIS_COLLAB_SYSTEMS"] = ",".join(active_systems)

    async def _resolve_intelligence_services_from_container(self) -> None:
        """
        v108.0: Resolve intelligence services from DI container and set instance attributes.

        This method provides backward compatibility by resolving services from the
        container and setting them as instance attributes. This allows other parts
        of the codebase to continue accessing self._collaboration_engine, etc.

        The container handles:
        - Thread-safe singleton resolution with double-check locking
        - Lazy instantiation (services created on first access)
        - Proper dependency injection
        """
        if self._di_container is None:
            self.logger.warning("Cannot resolve services: DI container not initialized")
            return

        try:
            # Import service types for resolution
            # We use try/except for each to handle optional services gracefully

            # Collaboration Engine
            try:
                from backend.intelligence.collaboration_engine import (
                    CollaborationEngine,
                    CrossRepoCollaborationCoordinator,
                )
                self._collaboration_engine = await self._di_container.resolve(CollaborationEngine)
                self._collab_coordinator = await self._di_container.resolve(CrossRepoCollaborationCoordinator)
                self.logger.debug("Resolved: CollaborationEngine, CrossRepoCollaborationCoordinator")
            except (ImportError, Exception) as e:
                self.logger.debug(f"Could not resolve collaboration services: {e}")

            # Code Ownership Engine
            try:
                from backend.intelligence.code_ownership import (
                    CodeOwnershipEngine,
                    CrossRepoOwnershipCoordinator,
                )
                self._code_ownership_engine = await self._di_container.resolve(CodeOwnershipEngine)
                self._ownership_coordinator = await self._di_container.resolve(CrossRepoOwnershipCoordinator)
                self.logger.debug("Resolved: CodeOwnershipEngine, CrossRepoOwnershipCoordinator")
            except (ImportError, Exception) as e:
                self.logger.debug(f"Could not resolve code ownership services: {e}")

            # Review Workflow Engine
            try:
                from backend.intelligence.review_workflow import (
                    ReviewWorkflowEngine,
                    CrossRepoReviewCoordinator,
                )
                self._review_workflow_engine = await self._di_container.resolve(ReviewWorkflowEngine)
                self._review_coordinator = await self._di_container.resolve(CrossRepoReviewCoordinator)
                self.logger.debug("Resolved: ReviewWorkflowEngine, CrossRepoReviewCoordinator")
            except (ImportError, Exception) as e:
                self.logger.debug(f"Could not resolve review workflow services: {e}")

            # LSP Server
            try:
                from backend.intelligence.lsp_server import JARVISLSPServer
                self._lsp_server = await self._di_container.resolve(JARVISLSPServer)
                self.logger.debug("Resolved: JARVISLSPServer")
            except (ImportError, Exception) as e:
                self.logger.debug(f"Could not resolve LSP server: {e}")

            # IDE Integration Engine
            try:
                from backend.intelligence.ide_integration import (
                    IDEIntegrationEngine,
                    CrossRepoIDECoordinator,
                )
                self._ide_integration_engine = await self._di_container.resolve(IDEIntegrationEngine)
                self._ide_coordinator = await self._di_container.resolve(CrossRepoIDECoordinator)

                # Register built-in commands if IDE engine is available
                if self._ide_integration_engine is not None:
                    try:
                        commands_registered = await self._ide_integration_engine.register_builtin_commands()
                        self.logger.info(f"   â€¢ Built-in commands: {commands_registered} registered")
                    except Exception as e:
                        self.logger.debug(f"Could not register built-in commands: {e}")

                self.logger.debug("Resolved: IDEIntegrationEngine, CrossRepoIDECoordinator")
            except (ImportError, Exception) as e:
                self.logger.debug(f"Could not resolve IDE integration services: {e}")

        except Exception as e:
            self.logger.warning(f"Error resolving services from container: {e}")

    async def _initialize_docker_manager(self) -> None:
        """
        v12.0: Initialize the Intelligent Docker Daemon Manager with Self-Healing.

        This fixes the root cause of Docker timeout failures:
        - Correct macOS process detection (com.docker.backend, NOT 'Docker Desktop')
        - Multi-signal diagnostic engine (15+ health signals)
        - 4-level progressive self-healing recovery
        - Adaptive circuit breaker with exponential cooldown
        - Historical learning database for pattern recognition
        - Cross-repo coordination via Trinity Protocol

        The Docker manager ensures Docker is healthy BEFORE other infrastructure
        components try to use it (Cloud Run deployment, container builds, etc.).
        """
        if not _DOCKER_MANAGER_AVAILABLE:
            self.logger.info("âš ï¸ Docker Manager v12.0 not available - using basic checks")
            return

        self.logger.info("â•" * 60)
        self.logger.info("ğŸ³ v12.0: Initializing Intelligent Docker Manager with Self-Healing...")
        self.logger.info("â•" * 60)

        try:
            # Get or create the Docker manager instance
            self._docker_manager = await get_docker_manager()

            if self._docker_manager is None:
                self.logger.warning("âš ï¸ Docker Manager could not be initialized")
                return

            # Start the manager (initializes learning database, cross-repo coordination)
            await self._docker_manager.start()

            # Log diagnostic capabilities
            self.logger.info("   â€¢ Diagnostic Engine: 15+ health signals")
            self.logger.info("   â€¢ Self-Healing: 4-level progressive recovery")
            self.logger.info("   â€¢ Circuit Breaker: Adaptive exponential cooldown")
            self.logger.info("   â€¢ Learning: Historical pattern recognition")
            self.logger.info("   â€¢ Cross-Repo: Trinity Protocol coordination")

            # Check current Docker health
            health = await self._docker_manager.get_health_status()

            if health.is_healthy:
                self.logger.info(f"   âœ… Docker Status: HEALTHY (score: {health.health_score:.1%})")
                print(f"  {TerminalUI.GREEN}âœ“ Docker: Healthy ({health.health_score:.0%}){TerminalUI.RESET}")
            else:
                self.logger.warning(f"   âš ï¸ Docker Status: UNHEALTHY - initiating self-healing")
                print(f"  {TerminalUI.YELLOW}âš ï¸ Docker: Unhealthy - self-healing in progress{TerminalUI.RESET}")

                # Let the manager attempt recovery
                recovery_result = await self._docker_manager.ensure_healthy()

                if recovery_result.success:
                    self.logger.info(f"   âœ… Docker recovered via {recovery_result.level.name}")
                    print(f"  {TerminalUI.GREEN}âœ“ Docker: Recovered via {recovery_result.level.name}{TerminalUI.RESET}")
                else:
                    self.logger.error(f"   âŒ Docker recovery failed: {recovery_result.error}")
                    print(f"  {TerminalUI.RED}âœ— Docker: Recovery failed - {recovery_result.error}{TerminalUI.RESET}")

            # Store for cross-repo coordination
            self._docker_manager_initialized = True
            os.environ["JARVIS_DOCKER_MANAGER_ENABLED"] = "true"
            self.logger.info("âœ… Docker Manager v12.0 initialized with intelligent self-healing")

        except Exception as e:
            self.logger.error(f"âŒ Docker Manager initialization failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Docker Manager: Failed ({e}){TerminalUI.RESET}")
            self._docker_manager = None
            self._docker_manager_initialized = False

    async def _initialize_infrastructure_orchestrator(self) -> None:
        """
        v10.0: Initialize the Infrastructure Orchestrator with Startup Cost Optimization.

        This fixes the root issue of GCP resources staying deployed when JARVIS is off:
        - Provisions Cloud Run/Redis only when needed (memory pressure, explicit config)
        - Tracks what WE created vs pre-existing resources
        - Automatically destroys OUR resources on shutdown
        - Leaves pre-existing infrastructure alone
        - Supports multi-repo integration (JARVIS, Prime, Reactor Core)

        v10.0 Enhancements:
        - Startup cost check: Detects orphaned resources from crashed sessions
        - Artifact cleanup: Cleans old Docker images to reduce storage costs
        - Cloud SQL management: Can stop/start Cloud SQL for cost savings
        - OrphanDetectionLoop: Background monitoring for cost optimization
        - Cross-repo bridge: Unified cost tracking across JARVIS/Prime/Reactor

        Architecture:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              Infrastructure Orchestrator (v10.0)                     â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚  JARVIS Backend  â”‚    â”‚   JARVIS-Prime   â”‚    â”‚  Reactor-Core â”‚ â”‚
        â”‚  â”‚   (Cloud Run)    â”‚    â”‚   (Cloud Run)    â”‚    â”‚  (Training)   â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â”‚           â”‚                       â”‚                       â”‚         â”‚
        â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
        â”‚                                   â–¼                                 â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚  GCP Cost Optimization Engine (v10.0)                         â”‚  â”‚
        â”‚  â”‚  â”œâ”€â”€ StartupCostCheck: Cleanup orphans on boot               â”‚  â”‚
        â”‚  â”‚  â”œâ”€â”€ OrphanDetectionLoop: Every 5 min background check       â”‚  â”‚
        â”‚  â”‚  â”œâ”€â”€ ArtifactRegistryCleanup: Every 6 hr old image cleanup   â”‚  â”‚
        â”‚  â”‚  â”œâ”€â”€ CloudSQLManager: Stop/start for cost savings            â”‚  â”‚
        â”‚  â”‚  â””â”€â”€ CrossRepoBridge: Unified tracking across all repos      â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚                                   â–¼                                 â”‚
        â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
        â”‚                    â”‚     GCP Cloud Platform   â”‚                     â”‚
        â”‚                    â”‚  Cloud Run | Redis | VMs â”‚                     â”‚
        â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        self.logger.info("â•" * 60)
        self.logger.info("ğŸ”§ v10.0: Initializing Infrastructure Orchestrator with Cost Optimization...")
        self.logger.info("â•" * 60)

        try:
            from backend.core.infrastructure_orchestrator import (
                InfrastructureOrchestrator,
                InfrastructureConfig,
                get_infrastructure_orchestrator,
                start_orphan_detection,
                get_reconciler,
            )

            # Create config from supervisor settings
            config = InfrastructureConfig(
                on_demand_enabled=self.config.infra_on_demand_enabled,
                auto_destroy_on_shutdown=self.config.infra_auto_destroy_on_shutdown,
                terraform_timeout_seconds=self.config.infra_terraform_timeout_seconds,
                memory_pressure_threshold_gb=self.config.infra_memory_threshold_gb,
                daily_cost_limit_usd=self.config.infra_daily_cost_limit_usd,
            )

            # Initialize orchestrator (this also creates the GCPReconciler)
            self._infra_orchestrator = await get_infrastructure_orchestrator()

            # Log configuration
            self.logger.info(f"   â€¢ On-demand provisioning: {config.on_demand_enabled}")
            self.logger.info(f"   â€¢ Auto-destroy on shutdown: {config.auto_destroy_on_shutdown}")
            self.logger.info(f"   â€¢ Memory pressure threshold: {config.memory_pressure_threshold_gb:.1f} GB")
            self.logger.info(f"   â€¢ Daily cost limit: ${config.daily_cost_limit_usd:.2f}")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v10.0: STARTUP COST OPTIMIZATION
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # This runs BEFORE JARVIS fully starts to clean up any resources
            # left over from crashed sessions or previous runs.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            startup_cost_enabled = os.getenv("JARVIS_STARTUP_COST_CHECK", "true").lower() == "true"

            if startup_cost_enabled:
                self.logger.info("ğŸ’° Running startup cost optimization check...")
                await self._run_startup_cost_check()

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v10.0: START ORPHAN DETECTION LOOP
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Background loop that runs every 5 minutes to:
            # - Detect orphaned VMs/Cloud Run from crashed sessions
            # - Clean up old Docker images (every 6 hours)
            # - Report cost savings
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            artifact_cleanup_enabled = os.getenv("JARVIS_ARTIFACT_CLEANUP", "true").lower() == "true"

            self._orphan_loop = await start_orphan_detection(
                auto_cleanup=True,
                artifact_cleanup_enabled=artifact_cleanup_enabled,
            )
            self.logger.info("ğŸ”„ OrphanDetectionLoop started (5 min interval, artifact cleanup enabled)")

            # Check if we need to provision infrastructure now
            should_provision = (
                os.getenv("JARVIS_PROVISION_CLOUD_RUN", "false").lower() == "true" or
                os.getenv("JARVIS_PROVISION_REDIS", "false").lower() == "true"
            )

            if should_provision:
                self.logger.info("ğŸš€ Provisioning requested - starting infrastructure...")
                success = await self._infra_orchestrator.ensure_infrastructure()
                if success:
                    status = self._infra_orchestrator.get_status()
                    self.logger.info(
                        f"âœ… Infrastructure provisioned: "
                        f"{status['terraform_operations']['apply_count']} resource(s)"
                    )
                else:
                    self.logger.warning("âš ï¸ Infrastructure provisioning had issues")
            else:
                self.logger.info("ğŸ“¦ Infrastructure on-demand - will provision when needed")

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # v10.0: CROSS-REPO BRIDGE INITIALIZATION
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Share cost tracking state with JARVIS-Prime and Reactor-Core
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            await self._initialize_cross_repo_bridge()

            # Propagate orchestrator availability to child processes
            os.environ["JARVIS_INFRA_ORCHESTRATOR_ENABLED"] = "true"
            os.environ["JARVIS_COST_OPTIMIZATION_ENABLED"] = "true"

            print(f"  {TerminalUI.GREEN}âœ“ Infrastructure Orchestrator: Ready (on-demand GCP + cost optimization){TerminalUI.RESET}")
            self.logger.info("âœ… Infrastructure Orchestrator v10.0 initialized with cost optimization")

        except ImportError as e:
            self.logger.warning(f"âš ï¸ Infrastructure Orchestrator not available: {e}")
            self.logger.info("   â†’ Will use existing infrastructure (no on-demand provisioning)")
            os.environ["JARVIS_INFRA_ORCHESTRATOR_ENABLED"] = "false"
            print(f"  {TerminalUI.YELLOW}âš ï¸ Infrastructure: Using existing resources{TerminalUI.RESET}")

        except Exception as e:
            self.logger.error(f"âŒ Infrastructure Orchestrator init failed: {e}")
            import traceback
            self.logger.debug(traceback.format_exc())
            os.environ["JARVIS_INFRA_ORCHESTRATOR_ENABLED"] = "false"
            print(f"  {TerminalUI.RED}âœ— Infrastructure: Failed ({e}){TerminalUI.RESET}")

    async def _run_startup_cost_check(self) -> None:
        """
        v10.0: Run cost optimization checks at JARVIS startup.

        This runs BEFORE JARVIS fully starts to:
        1. Detect and cleanup orphaned resources from crashed sessions
        2. Check for excessive storage costs (Artifact Registry)
        3. Optionally stop Cloud SQL if configured
        4. Report potential savings
        """
        try:
            from backend.core.infrastructure_orchestrator import get_reconciler

            reconciler = get_reconciler()
            if not reconciler:
                self.logger.debug("   No reconciler available for startup cost check")
                return

            total_savings = 0.0

            # Step 1: Check for orphaned resources
            self.logger.info("   ğŸ” Checking for orphaned resources...")
            reconcile_result = await reconciler.reconcile_with_gcp()

            if reconcile_result.get("error"):
                self.logger.debug(f"   Reconciliation error: {reconcile_result['error']}")
            else:
                orphan_count = (
                    len(reconcile_result.get("orphaned_vms", [])) +
                    len(reconcile_result.get("orphaned_cloud_run", []))
                )

                if orphan_count > 0:
                    self.logger.warning(f"   âš ï¸ Found {orphan_count} orphaned resource(s) - cleaning up...")
                    cleanup_result = await reconciler.cleanup_orphans(reconcile_result)

                    cleaned = (
                        len(cleanup_result.get("vms_deleted", [])) +
                        len(cleanup_result.get("cloud_run_deleted", []))
                    )

                    if cleaned > 0:
                        # Estimate savings: VMs ~$0.029/hr, assume 2 hrs saved
                        vm_savings = len(cleanup_result.get("vms_deleted", [])) * 0.029 * 2
                        total_savings += vm_savings
                        self.logger.info(f"   âœ… Cleaned {cleaned} orphaned resource(s)")
                else:
                    self.logger.info("   âœ… No orphaned resources found")

            # Step 2: Check Cloud SQL status (optional stop for cost savings)
            stop_sql_on_startup = os.getenv("JARVIS_STOP_SQL_ON_STARTUP", "false").lower() == "true"
            start_sql_on_startup = os.getenv("JARVIS_START_SQL_ON_STARTUP", "true").lower() == "true"

            if start_sql_on_startup:
                self.logger.info("   ğŸ—„ï¸ Ensuring Cloud SQL is running...")
                try:
                    sql_status = await reconciler.get_cloud_sql_status()
                    if sql_status.get("state") != "RUNNABLE" or sql_status.get("activation_policy") == "NEVER":
                        self.logger.info("   ğŸš€ Starting Cloud SQL...")
                        await reconciler.start_cloud_sql()
                except Exception as e:
                    self.logger.debug(f"   Cloud SQL check failed: {e}")

            # Step 3: Report potential artifact savings (don't clean at startup - too slow)
            self.logger.info("   ğŸ“¦ Artifact cleanup will run in background (every 6 hours)")

            # Summary
            if total_savings > 0:
                self.logger.info(f"   ğŸ’° Startup cost optimization saved ~${total_savings:.2f}")
                print(f"  {TerminalUI.GREEN}ğŸ’° Cost optimization: ~${total_savings:.2f} saved{TerminalUI.RESET}")

        except Exception as e:
            self.logger.debug(f"   Startup cost check failed (non-critical): {e}")

    async def _initialize_cross_repo_bridge(self) -> None:
        """
        v10.0: Initialize cross-repo infrastructure bridge.

        This creates a shared state mechanism for cost tracking across:
        - JARVIS-AI-Agent (main backend)
        - JARVIS-Prime (local inference)
        - Reactor-Core (training)

        Uses a file-based state for simplicity (no extra dependencies).
        """
        try:
            bridge_state_dir = Path.home() / ".jarvis" / "cross_repo"
            bridge_state_dir.mkdir(parents=True, exist_ok=True)

            bridge_state = {
                "session_id": os.getenv("JARVIS_SESSION_ID", str(int(time.time()))),
                "started_at": time.time(),
                "repos": {
                    "jarvis": {
                        "path": str(Path(__file__).parent),
                        "status": "active",
                        "pid": os.getpid(),
                    },
                },
                "cost_tracking": {
                    "enabled": True,
                    "daily_limit_usd": float(os.getenv("COST_ALERT_DAILY", "1.0")),
                    "current_cost_usd": 0.0,
                },
            }

            # Check for JARVIS-Prime
            jarvis_prime_path = Path(os.getenv(
                "JARVIS_PRIME_PATH",
                str(Path.home() / "Documents/repos/jarvis-prime")
            ))
            if jarvis_prime_path.exists():
                bridge_state["repos"]["jarvis_prime"] = {
                    "path": str(jarvis_prime_path),
                    "status": "available",
                }
                self.logger.info("   ğŸ”— JARVIS-Prime: Connected")

            # Check for Reactor-Core
            reactor_core_path = Path(os.getenv(
                "REACTOR_CORE_PATH",
                str(Path.home() / "Documents/repos/reactor-core")
            ))
            if reactor_core_path.exists():
                bridge_state["repos"]["reactor_core"] = {
                    "path": str(reactor_core_path),
                    "status": "available",
                }
                self.logger.info("   ğŸ”— Reactor-Core: Connected")

            # Write bridge state
            import json
            bridge_file = bridge_state_dir / "bridge_state.json"
            with open(bridge_file, "w") as f:
                json.dump(bridge_state, f, indent=2)

            # Set environment variable for child processes
            os.environ["JARVIS_CROSS_REPO_BRIDGE"] = str(bridge_file)

            self.logger.info("   âœ… Cross-repo bridge initialized")

        except Exception as e:
            self.logger.debug(f"   Cross-repo bridge init failed (non-critical): {e}")

    async def _initialize_reactor_core_api(self) -> None:
        """
        v10.0: Initialize Reactor-Core API Server.

        This starts the training pipeline API server that enables:
        - Programmatic training triggers from JARVIS
        - Experience log streaming
        - Pipeline status monitoring
        - Scout topic management

        The server is started as a subprocess and managed alongside JARVIS.
        """
        import subprocess

        reactor_core_path = Path(os.getenv(
            "REACTOR_CORE_PATH",
            str(Path.home() / "Documents/repos/reactor-core")
        ))

        if not reactor_core_path.exists():
            self.logger.warning("   âš ï¸ Reactor-Core path not found, skipping API server")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Reactor-Core not found at {reactor_core_path}{TerminalUI.RESET}")
            return

        try:
            # Check if already running
            try:
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        f"http://localhost:{self._reactor_core_port}/health",
                        timeout=aiohttp.ClientTimeout(total=2)
                    ) as response:
                        if response.status == 200:
                            self.logger.info(f"   âœ… Reactor-Core already running on port {self._reactor_core_port}")
                            print(f"  {TerminalUI.GREEN}âœ“ Reactor-Core API already running{TerminalUI.RESET}")
                            return
            except Exception:
                pass  # Not running, we'll start it

            # Start Reactor-Core API server as subprocess
            self.logger.info(f"   ğŸš€ Starting Reactor-Core API on port {self._reactor_core_port}...")
            print(f"  {TerminalUI.CYAN}ğŸš€ Starting Reactor-Core API...{TerminalUI.RESET}")

            # Create startup script
            startup_code = f'''
import sys
sys.path.insert(0, "{reactor_core_path}")
import uvicorn
from reactor_core.api.server import app
uvicorn.run(app, host="0.0.0.0", port={self._reactor_core_port}, log_level="warning")
'''

            # ROOT CAUSE FIX: Proper error logging (no DEVNULL!) and longer timeout
            # Create log directory
            log_dir = Path.home() / ".jarvis" / "logs" / "services"
            log_dir.mkdir(parents=True, exist_ok=True)

            stdout_log = log_dir / "reactor_core_stdout.log"
            stderr_log = log_dir / "reactor_core_stderr.log"

            # Start as subprocess with proper error logging
            self._reactor_core_process = subprocess.Popen(
                [sys.executable, "-c", startup_code],
                cwd=str(reactor_core_path),
                env={**os.environ, "PYTHONPATH": str(reactor_core_path)},
                stdout=open(stdout_log, "w"),  # LOG OUTPUT!
                stderr=open(stderr_log, "w"),  # LOG ERRORS!
                start_new_session=True,
            )

            self.logger.info(f"   Logs: {stdout_log}, {stderr_log}")

            # Wait for startup with intelligent retry (not just 2s!)
            max_retries = int(os.getenv("REACTOR_CORE_MAX_RETRIES", "10"))
            retry_delay = float(os.getenv("REACTOR_CORE_RETRY_DELAY", "2.0"))
            health_timeout = float(os.getenv("REACTOR_CORE_HEALTH_TIMEOUT", "10.0"))

            import aiohttp
            for attempt in range(max_retries):
                await asyncio.sleep(retry_delay)

                # Verify it's running
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.get(
                            f"http://localhost:{self._reactor_core_port}/health",
                            timeout=aiohttp.ClientTimeout(total=health_timeout)
                        ) as response:
                            if response.status == 200:
                                self.logger.info(f"   âœ… Reactor-Core API started (PID: {self._reactor_core_process.pid}, attempts: {attempt + 1})")
                                print(f"  {TerminalUI.GREEN}âœ“ Reactor-Core API started (port {self._reactor_core_port}){TerminalUI.RESET}")

                                # Update bridge state
                                bridge_file = Path.home() / ".jarvis" / "cross_repo" / "bridge_state.json"
                                if bridge_file.exists():
                                    import json
                                    with open(bridge_file, "r") as f:
                                        bridge_state = json.load(f)
                                    bridge_state["repos"]["reactor_core"]["status"] = "running"
                                    bridge_state["repos"]["reactor_core"]["pid"] = self._reactor_core_process.pid
                                    bridge_state["repos"]["reactor_core"]["port"] = self._reactor_core_port
                                    with open(bridge_file, "w") as f:
                                        json.dump(bridge_state, f, indent=2)
                                return
                except Exception as e:
                    if attempt < max_retries - 1:
                        self.logger.debug(f"   Reactor-Core not ready (attempt {attempt + 1}/{max_retries}): {e}")
                    else:
                        # Last attempt - log full error
                        self.logger.warning(f"   Reactor-Core health check failed: {e}")
                        # Read stderr to show actual error
                        try:
                            with open(stderr_log, "r") as f:
                                stderr_content = f.read().strip()
                                if stderr_content:
                                    self.logger.error(f"   Reactor-Core stderr:\n{stderr_content[-500:]}")  # Last 500 chars
                        except Exception:
                            pass

            # Failed to start after all retries
            self.logger.warning(f"   âš ï¸ Reactor-Core API failed to start after {max_retries} attempts. Check logs: {stderr_log}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Reactor-Core API failed (check {stderr_log}){TerminalUI.RESET}")

            if self._reactor_core_process:
                self._reactor_core_process.terminate()
                self._reactor_core_process = None

        except Exception as e:
            self.logger.warning(f"   âš ï¸ Reactor-Core initialization failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Reactor-Core init failed: {e}{TerminalUI.RESET}")

    async def _shutdown_reactor_core(self) -> None:
        """Shutdown the Reactor-Core API server and related tasks."""
        # v103.0: Cancel orchestration status monitor task
        if self._orchestration_status_task:
            try:
                self._orchestration_status_task.cancel()
                await self._orchestration_status_task
            except asyncio.CancelledError:
                pass
            except Exception as e:
                self.logger.debug(f"   Orchestration status monitor shutdown error: {e}")
            self._orchestration_status_task = None
            self.logger.info("   âœ… Orchestration status monitor stopped")

        # v103.0: Shutdown Trinity Orchestration Engine
        if self._trinity_orchestration_engine:
            try:
                from backend.core.trinity.orchestration_engine import stop_trinity
                await stop_trinity()
                self._trinity_orchestration_engine = None
                self.logger.info("   âœ… Trinity Orchestration Engine stopped")
            except Exception as e:
                self.logger.debug(f"   Orchestration engine shutdown error: {e}")

        # Cancel training health monitor task
        if self._training_health_task:
            try:
                self._training_health_task.cancel()
                await self._training_health_task
            except asyncio.CancelledError:
                pass
            except Exception as e:
                self.logger.debug(f"   Training health monitor shutdown error: {e}")
            self._training_health_task = None
            self.logger.info("   âœ… Training health monitor stopped")

        if self._reactor_core_process:
            try:
                self.logger.info("   Stopping Reactor-Core API...")
                self._reactor_core_process.terminate()
                try:
                    self._reactor_core_process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    self._reactor_core_process.kill()
                self._reactor_core_process = None
                self.logger.info("   âœ… Reactor-Core API stopped")
            except Exception as e:
                self.logger.debug(f"   Reactor-Core shutdown error: {e}")

    async def _verify_trinity_startup_v85(self) -> Dict[str, Any]:
        """
        v85.0: Comprehensive Trinity startup verification.

        Verifies all Trinity components are truly functional by checking:
        1. Heartbeat files exist and are fresh (< 30 seconds old)
        2. Health endpoints respond for components that have them
        3. Cross-repo discovery succeeded
        4. Process PIDs are valid and running

        Returns:
            Dict with verification results including:
            - all_verified: bool - True if all enabled components verified
            - components: Dict[str, bool] - Per-component verification status
            - details: Dict[str, Any] - Detailed info about each component
        """
        from pathlib import Path
        import json

        results = {
            "all_verified": False,
            "components": {},
            "details": {},
            "timestamp": time.time(),
        }

        trinity_dir = Path.home() / ".jarvis" / "trinity" / "components"

        # Component definitions with health check URLs
        components = {
            "jarvis_body": {
                "files": ["jarvis_body.json"],
                "health_url": None,  # No HTTP endpoint
                "required": True,
            },
            "jarvis_prime": {
                "files": ["jarvis_prime.json", "j_prime.json"],
                "health_url": f"http://localhost:{os.getenv('JARVIS_PRIME_PORT', '8000')}/health",
                "required": os.getenv("JARVIS_PRIME_ENABLED", "true").lower() == "true",
            },
            "reactor_core": {
                "files": ["reactor_core.json"],
                "health_url": None,  # Uses file-based heartbeat
                "required": os.getenv("REACTOR_CORE_ENABLED", "true").lower() == "true",
            },
            "coding_council": {
                "files": ["coding_council.json"],
                "health_url": None,
                "required": False,  # Optional component
            },
        }

        verified_count = 0
        required_count = 0

        for component_name, config in components.items():
            component_verified = False
            details = {
                "heartbeat_found": False,
                "heartbeat_fresh": False,
                "heartbeat_age": None,
                "health_check_passed": None,
                "pid_valid": False,
                "required": config["required"],
            }

            if config["required"]:
                required_count += 1

            # Check heartbeat files
            for file_name in config["files"]:
                heartbeat_path = trinity_dir / file_name
                if heartbeat_path.exists():
                    try:
                        with open(heartbeat_path) as f:
                            heartbeat_data = json.load(f)

                        details["heartbeat_found"] = True

                        # Check freshness (< 30 seconds)
                        timestamp = heartbeat_data.get("timestamp", 0)
                        age = time.time() - timestamp
                        details["heartbeat_age"] = round(age, 1)

                        if age < 30.0:
                            details["heartbeat_fresh"] = True

                            # Check PID validity
                            pid = heartbeat_data.get("pid")
                            if pid:
                                try:
                                    import psutil
                                    proc = psutil.Process(pid)
                                    if proc.is_running():
                                        details["pid_valid"] = True
                                        details["pid"] = pid
                                except Exception:
                                    pass

                            component_verified = details["heartbeat_fresh"]
                            break

                    except Exception as e:
                        self.logger.debug(f"[v85.0] Error reading {file_name}: {e}")

            # Health check for components with HTTP endpoints
            if config["health_url"] and details["heartbeat_fresh"]:
                try:
                    import aiohttp
                    async with aiohttp.ClientSession(
                        timeout=aiohttp.ClientTimeout(total=5.0)
                    ) as session:
                        async with session.get(config["health_url"]) as resp:
                            if resp.status == 200:
                                details["health_check_passed"] = True
                            else:
                                details["health_check_passed"] = False
                                component_verified = False
                except Exception as e:
                    details["health_check_passed"] = False
                    self.logger.debug(f"[v85.0] Health check failed for {component_name}: {e}")
                    # Don't mark as failed if heartbeat is fresh but health check fails temporarily
                    # (component might still be starting up)

            results["components"][component_name] = component_verified
            results["details"][component_name] = details

            if component_verified:
                verified_count += 1
                self.logger.debug(f"[v85.0] {component_name}: âœ… Verified")
            elif config["required"]:
                self.logger.debug(f"[v85.0] {component_name}: âŒ Not verified (required)")
            else:
                self.logger.debug(f"[v85.0] {component_name}: âš  Not verified (optional)")

        # Check if all required components verified
        all_required_verified = all(
            results["components"].get(name, False)
            for name, config in components.items()
            if config["required"]
        )

        results["all_verified"] = all_required_verified
        results["verified_count"] = verified_count
        results["required_count"] = required_count

        # Also check cross-repo discovery via IntelligentRepoDiscovery
        try:
            from backend.core.trinity_integrator import get_repo_discovery
            discovery = await get_repo_discovery()
            all_repos = await discovery.discover_all()
            results["cross_repo_discovery"] = {
                repo_id: {
                    "found": result.path is not None,
                    "path": str(result.path) if result.path else None,
                    "strategy": result.strategy_used.name if result.path else None,
                    "confidence": result.confidence,
                }
                for repo_id, result in all_repos.items()
            }
        except Exception as e:
            self.logger.debug(f"[v85.0] Cross-repo discovery check skipped: {e}")
            results["cross_repo_discovery"] = None

        return results

    async def _initialize_trinity(self) -> None:
        """
        v11.0: Initialize PROJECT TRINITY - Unified Cognitive Architecture.

        This connects JARVIS Body to the Trinity network, enabling:
        - Cross-repo communication (JARVIS â†” J-Prime â†” Reactor Core)
        - Distributed AI reasoning and plan execution
        - Heartbeat monitoring for component liveness
        - File-based message passing for reliability

        The Trinity architecture models:
        - JARVIS Body = Execution layer (Computer Use, Vision, Actions)
        - J-Prime = Cognitive layer (Reasoning, Planning, Decisions)
        - Reactor Core = Neural layer (Training, Learning, Optimization)
        """
        from pathlib import Path
        import json

        try:
            self.logger.info("=" * 60)
            self.logger.info("PROJECT TRINITY: Initializing JARVIS Body Connection")
            self.logger.info("=" * 60)

            print(f"  {TerminalUI.CYAN}ğŸ”— PROJECT TRINITY: Connecting distributed architecture...{TerminalUI.RESET}")

            # Ensure Trinity directories exist
            trinity_dir = Path.home() / ".jarvis" / "trinity"
            (trinity_dir / "commands").mkdir(parents=True, exist_ok=True)
            (trinity_dir / "heartbeats").mkdir(parents=True, exist_ok=True)
            (trinity_dir / "components").mkdir(parents=True, exist_ok=True)

            # Generate instance ID
            import time
            import os
            self._trinity_instance_id = f"jarvis-{os.getpid()}-{int(time.time())}"

            # Check for connected components
            jprime_online = False
            reactor_online = False

            # Check J-Prime heartbeat
            # v78.1: Support both naming conventions for backwards compatibility
            jprime_state_files = [
                trinity_dir / "components" / "jarvis_prime.json",
                trinity_dir / "components" / "j_prime.json",
            ]
            for jprime_state_file in jprime_state_files:
                if jprime_state_file.exists():
                    try:
                        with open(jprime_state_file) as f:
                            jprime_state = json.load(f)
                        age = time.time() - jprime_state.get("timestamp", 0)
                        if age < 30:  # Consider online if heartbeat < 30s old
                            jprime_online = True
                            self.logger.info("   ğŸ§  J-Prime (Mind): ONLINE")
                            break
                    except Exception:
                        pass

            # Check Reactor Core
            reactor_state_file = trinity_dir / "components" / "reactor_core.json"
            if reactor_state_file.exists():
                try:
                    with open(reactor_state_file) as f:
                        reactor_state = json.load(f)
                    age = time.time() - reactor_state.get("timestamp", 0)
                    if age < 30:
                        reactor_online = True
                        self.logger.info("   âš¡ Reactor Core (Nerves): ONLINE")
                except Exception:
                    pass

            # Write JARVIS Body component state
            jarvis_state = {
                "component_type": "jarvis_body",
                "instance_id": self._trinity_instance_id,
                "timestamp": time.time(),
                "metrics": {
                    "uptime_seconds": 0,
                    "surveillance_active": False,
                    "ghost_display_available": False,
                },
            }

            with open(trinity_dir / "components" / "jarvis_body.json", "w") as f:
                json.dump(jarvis_state, f, indent=2)

            self._trinity_initialized = True

            # Status summary
            components_online = 1 + (1 if jprime_online else 0) + (1 if reactor_online else 0)

            # v93.16: Enhanced Trinity voice announcements using the startup narrator
            try:
                from backend.core.supervisor.startup_narrator import get_startup_narrator
                trinity_narrator = get_startup_narrator()

                if components_online == 3:
                    self.logger.info("=" * 60)
                    self.logger.info("PROJECT TRINITY: FULL DISTRIBUTED MODE")
                    self.logger.info("   Mind â†” Body â†” Nerves: All connected")
                    self.logger.info("=" * 60)
                    print(f"  {TerminalUI.GREEN}âœ“ PROJECT TRINITY: Full distributed mode (3/3 components){TerminalUI.RESET}")

                    # v93.16: Use enhanced Trinity complete announcement
                    startup_duration = (time.time() - self._startup_time) if hasattr(self, '_startup_time') else None
                    await trinity_narrator.announce_trinity_complete(
                        mind_online=jprime_online,
                        body_online=True,  # We're the body
                        nerves_online=reactor_online,
                        startup_duration=startup_duration,
                    )
                else:
                    status_parts = ["Body âœ“"]
                    if jprime_online:
                        status_parts.append("Mind âœ“")
                    if reactor_online:
                        status_parts.append("Nerves âœ“")

                    self.logger.info(f"   Trinity components: {', '.join(status_parts)}")
                    print(f"  {TerminalUI.GREEN}âœ“ PROJECT TRINITY: {components_online}/3 components online{TerminalUI.RESET}")

                    # v93.16: Use partial Trinity announcement
                    await trinity_narrator.announce_trinity_complete(
                        mind_online=jprime_online,
                        body_online=True,
                        nerves_online=reactor_online,
                    )
            except Exception as narrator_err:
                self.logger.debug(f"Trinity narrator unavailable: {narrator_err}")
                # Fallback to basic announcement
                if components_online == 3:
                    await self.narrator.speak(
                        "PROJECT TRINITY connected. Distributed cognitive architecture active.",
                        wait=False,
                    )

            # Broadcast Trinity status to loading server
            await self._broadcast_trinity_status()

            # v79.1: Initialize Trinity Voice Coordination
            await self._initialize_trinity_voice_coordination(
                jprime_online=jprime_online,
                reactor_online=reactor_online
            )

            # v88.0: Initialize Trinity Knowledge Indexer
            await self._initialize_trinity_knowledge_indexer()

            # v80.0: Initialize Advanced Cross-Repo Loading System
            if self._v80_enabled:
                await self._initialize_v80_cross_repo_system(
                    jprime_online=jprime_online,
                    reactor_online=reactor_online
                )

            # v101.0: Initialize UnifiedTrinityConnector with Claude Code-like behaviors
            # This connects enhanced self-improvement, cross-repo orchestration,
            # and real-time communication (voice + websocket)
            await self._initialize_unified_trinity_connector()

            # v93.16: Start Enterprise-Grade Heartbeat System
            # This ensures jarvis_body.json and service registry are continuously updated
            await self._start_trinity_heartbeat_system()

        except Exception as e:
            self.logger.warning(f"   âš ï¸ PROJECT TRINITY initialization failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ PROJECT TRINITY: Running in standalone mode ({e}){TerminalUI.RESET}")
            self._trinity_initialized = False

    async def _initialize_trinity_voice_coordination(
        self,
        jprime_online: bool = False,
        reactor_online: bool = False
    ) -> None:
        """
        v87.0 ULTRA: Initialize Trinity Voice Coordinator for cross-repo voice announcements.

        This is the CENTRAL voice system that coordinates ALL voice announcements across:
        - JARVIS Body (main system)
        - JARVIS-Prime (brain/inference)
        - Reactor-Core (self-improvement/training)

        Features:
        - Multi-engine TTS fallback (MacOS Say â†’ pyttsx3 â†’ Edge TTS)
        - Context-aware voice personalities (startup, narrator, runtime, alert, success, trinity)
        - Intelligent priority queue (CRITICAL â†’ HIGH â†’ NORMAL â†’ LOW â†’ BACKGROUND)
        - Deduplication, rate limiting, coalescing
        - Health monitoring and engine selection
        - Zero hardcoding (environment-driven)
        - UK Daniel voice as JARVIS signature â­
        """
        if not self._trinity_voice_enabled:
            self.logger.debug("[v87.0] Trinity Voice Coordinator disabled via config")
            return

        try:
            self.logger.info("[v100.0] ğŸ™ï¸  Initializing Trinity Voice Coordinator v100.0 (Ultra-Robust)...")

            # Import and initialize the Trinity Voice Coordinator v100.0
            try:
                from backend.core.trinity_voice_coordinator import (
                    get_voice_coordinator,
                    announce,
                    VoiceContext,
                    VoicePriority,
                    VoiceConfig,
                    integrate_with_trinity_ipc,
                )

                # Get the global coordinator instance (auto-initializes)
                coordinator = await get_voice_coordinator()

                # Store reference for later use
                self._trinity_voice_coordinator_instance = coordinator

                # Integrate with Trinity IPC for cross-repo voice commands
                try:
                    await integrate_with_trinity_ipc()
                    self.logger.info("[v100.0] âœ… Trinity IPC voice integration enabled")
                except Exception as ipc_e:
                    self.logger.warning(f"[v100.0] âš ï¸ Trinity IPC integration skipped: {ipc_e}")

                # Test voice system is working (now returns tuple)
                test_success, test_reason = await announce(
                    message="Trinity Voice Coordinator v100.0 initialized. JARVIS systems online.",
                    context=VoiceContext.STARTUP,
                    priority=VoicePriority.CRITICAL,
                    source="supervisor",
                    metadata={
                        "event": "coordinator_init",
                        "jprime_online": jprime_online,
                        "reactor_online": reactor_online,
                        "version": "100.0",
                    }
                )

                if test_success:
                    self.logger.info(
                        f"[v100.0] âœ… Trinity Voice Coordinator v100.0 active with UK Daniel voice "
                        f"(reason={test_reason})"
                    )
                else:
                    self.logger.warning(
                        f"[v100.0] âš ï¸  Voice test: {test_reason}"
                    )

                # Announce component status using convenience methods
                if jprime_online:
                    await coordinator.announce_jarvis_prime_ready()

                if reactor_online:
                    await coordinator.announce_reactor_core_ready()

                # If all components online, announce Trinity online
                if jprime_online and reactor_online:
                    await coordinator.announce_trinity_online()

                # Get comprehensive status from v100.0 coordinator
                coordinator_status = coordinator.get_status()

                # Store status
                self._trinity_voice_coordinator = {
                    "initialized": True,
                    "version": coordinator_status.get("version", "100.0"),
                    "running": coordinator_status["running"],
                    "queue_size": coordinator_status["queue"]["size"],
                    "engines_available": coordinator_status["active_engines"],
                    "workers": coordinator_status["workers"],
                    "rate_limiter": coordinator_status["rate_limiter"],
                    "jprime_online": jprime_online,
                    "reactor_online": reactor_online,
                }

                self.logger.info(
                    f"[v100.0] âœ… Trinity Voice Coordinator v100.0 ready "
                    f"({self._trinity_voice_coordinator['engines_available']} engines, "
                    f"{self._trinity_voice_coordinator['workers']['count']} workers)"
                )

            except ImportError as e:
                self.logger.error(
                    f"[v87.0] âŒ Trinity Voice Coordinator import failed: {e}. "
                    f"Voice announcements will not work."
                )
                self._trinity_voice_coordinator = {"initialized": False, "error": f"Import failed: {e}"}

            except Exception as e:
                self.logger.error(
                    f"[v87.0] âŒ Trinity Voice Coordinator initialization failed: {e}"
                )
                self._trinity_voice_coordinator = {"initialized": False, "error": str(e)}

            # Legacy: Also try to connect old voice systems for backward compatibility
            try:
                from backend.core.coding_council.voice_announcer import get_evolution_announcer
                self._voice_announcer = get_evolution_announcer()
                if self._voice_announcer:
                    self.logger.debug("[v87.0] Legacy voice announcer connected (for compatibility)")
            except:
                pass

        except Exception as e:
            self.logger.error(f"[v87.0] Trinity voice coordination init failed: {e}", exc_info=True)
            self._trinity_voice_coordinator = {"initialized": False, "error": str(e)}

    async def _start_trinity_heartbeat_system(self) -> None:
        """
        v93.16: Enterprise-Grade Heartbeat System for Cross-Repo Synchronization.

        CRITICAL FIX: The jarvis_body.json heartbeat file was being written ONCE during
        initialization but never updated, causing "stale heartbeat" warnings.

        This system provides:
        - Continuous heartbeat updates for jarvis_body.json
        - Service registry heartbeat synchronization
        - Cross-repo heartbeat propagation (Prime, Reactor)
        - Adaptive intervals based on system load
        - Intelligent jitter to prevent thundering herd
        - Graceful degradation on file system errors
        - Metrics collection for observability
        - Self-healing on transient failures

        Architecture:
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚              Trinity Heartbeat System v93.16                    â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
            â”‚  â”‚  Background Task (async infinite loop)                      â”‚â”‚
            â”‚  â”‚    â”œâ”€ Update jarvis_body.json (Trinity heartbeat file)      â”‚â”‚
            â”‚  â”‚    â”œâ”€ Update service registry (jarvis-body entry)           â”‚â”‚
            â”‚  â”‚    â”œâ”€ Collect system metrics (CPU, memory, uptime)          â”‚â”‚
            â”‚  â”‚    â”œâ”€ Adaptive interval adjustment (15s-60s)                â”‚â”‚
            â”‚  â”‚    â””â”€ Error recovery with exponential backoff               â”‚â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        try:
            self.logger.info("[v93.16] ğŸ’“ Starting Enterprise-Grade Heartbeat System...")

            # Initialize metrics
            self._last_heartbeat_metrics = {
                "start_time": time.time(),
                "heartbeats_sent": 0,
                "errors": 0,
                "last_error": None,
                "avg_interval": self._trinity_heartbeat_interval,
            }

            # Create background task
            self._trinity_heartbeat_task = asyncio.create_task(
                self._trinity_heartbeat_loop(),
                name="trinity_heartbeat_system"
            )

            self.logger.info(
                f"[v93.16] âœ… Heartbeat System started "
                f"(interval: {self._trinity_heartbeat_interval}s, "
                f"adaptive: {self._heartbeat_adaptive_enabled})"
            )

        except Exception as e:
            self.logger.error(f"[v93.16] âŒ Heartbeat System start failed: {e}")

    async def _trinity_heartbeat_loop(self) -> None:
        """
        v93.16: Background task that continuously updates heartbeats.

        Features:
        - Atomic file writes with error recovery
        - Service registry synchronization
        - Adaptive intervals based on system state
        - Intelligent jitter (Â±10%) to prevent thundering herd
        - Graceful shutdown handling
        - Comprehensive metrics collection
        """
        import random
        consecutive_errors = 0
        max_consecutive_errors = 10
        base_interval = self._trinity_heartbeat_interval

        # Get paths
        trinity_dir = Path.home() / ".jarvis" / "trinity" / "components"
        jarvis_body_file = trinity_dir / "jarvis_body.json"

        # Ensure directory exists
        trinity_dir.mkdir(parents=True, exist_ok=True)

        self.logger.info(f"[v93.16] Heartbeat loop started, writing to: {jarvis_body_file}")

        while True:
            try:
                # Calculate adaptive interval with jitter
                interval = base_interval
                if self._heartbeat_adaptive_enabled:
                    # Increase interval if system is under load
                    try:
                        cpu_percent = psutil.cpu_percent(interval=0.1)
                        if cpu_percent > 80:
                            interval = min(base_interval * 2, 60.0)  # Max 60s under heavy load
                        elif cpu_percent > 60:
                            interval = min(base_interval * 1.5, 45.0)
                    except Exception:
                        pass

                # Add jitter (Â±10%) to prevent thundering herd
                jitter = interval * 0.1 * (2 * random.random() - 1)
                actual_interval = interval + jitter

                # Wait for interval
                await asyncio.sleep(actual_interval)

                # Collect current metrics
                uptime = time.time() - self._last_heartbeat_metrics.get("start_time", time.time())
                try:
                    cpu_percent = psutil.cpu_percent(interval=0.1)
                    memory_percent = psutil.virtual_memory().percent
                except Exception:
                    cpu_percent = 0.0
                    memory_percent = 0.0

                # Build heartbeat data
                heartbeat_data = {
                    "component_type": "jarvis_body",
                    "instance_id": getattr(self, "_trinity_instance_id", f"jarvis-{os.getpid()}-{int(time.time())}"),
                    "timestamp": time.time(),
                    "version": "93.16",
                    "pid": os.getpid(),
                    "metrics": {
                        "uptime_seconds": uptime,
                        "surveillance_active": getattr(self, "_surveillance_active", False),
                        "ghost_display_available": getattr(self, "_ghost_display_available", False),
                        "cpu_percent": cpu_percent,
                        "memory_percent": memory_percent,
                        "heartbeats_sent": self._last_heartbeat_metrics.get("heartbeats_sent", 0) + 1,
                        "heartbeat_interval": actual_interval,
                    },
                    "health": {
                        "status": "healthy" if consecutive_errors == 0 else "degraded",
                        "consecutive_errors": consecutive_errors,
                    },
                }

                # === Update jarvis_body.json (Trinity heartbeat file) ===
                try:
                    # Atomic write using temp file
                    temp_file = jarvis_body_file.with_suffix(".json.tmp")
                    with open(temp_file, "w") as f:
                        json.dump(heartbeat_data, f, indent=2)
                    temp_file.rename(jarvis_body_file)
                except Exception as file_err:
                    self.logger.debug(f"[v93.16] Trinity heartbeat file write error: {file_err}")
                    # Try direct write as fallback
                    try:
                        with open(jarvis_body_file, "w") as f:
                            json.dump(heartbeat_data, f, indent=2)
                    except Exception:
                        pass

                # === Update service registry ===
                try:
                    from backend.core.service_registry import ServiceRegistry
                    registry = ServiceRegistry()
                    await registry.heartbeat(
                        "jarvis-body",
                        status="healthy" if consecutive_errors == 0 else "degraded",
                        metadata={
                            "uptime_seconds": uptime,
                            "cpu_percent": cpu_percent,
                            "memory_percent": memory_percent,
                            "heartbeat_version": "93.16",
                        }
                    )
                except Exception as reg_err:
                    # Service registry might not have jarvis-body registered - that's OK
                    self.logger.debug(f"[v93.16] Service registry heartbeat: {reg_err}")

                # Update metrics
                self._last_heartbeat_metrics["heartbeats_sent"] = heartbeat_data["metrics"]["heartbeats_sent"]
                self._last_heartbeat_metrics["avg_interval"] = actual_interval
                self._last_heartbeat_metrics["last_success"] = time.time()

                # Reset error counter on success
                consecutive_errors = 0

            except asyncio.CancelledError:
                self.logger.info("[v93.16] Heartbeat loop cancelled, stopping...")
                break

            except Exception as e:
                consecutive_errors += 1
                self._last_heartbeat_metrics["errors"] = self._last_heartbeat_metrics.get("errors", 0) + 1
                self._last_heartbeat_metrics["last_error"] = str(e)

                if consecutive_errors <= 3 or consecutive_errors % 10 == 0:
                    self.logger.warning(
                        f"[v93.16] Heartbeat error #{consecutive_errors}: {e}"
                    )

                if consecutive_errors >= max_consecutive_errors:
                    self.logger.error(
                        f"[v93.16] Too many consecutive heartbeat errors ({consecutive_errors}), "
                        f"backing off significantly"
                    )
                    await asyncio.sleep(60.0)  # Long backoff
                else:
                    # Exponential backoff
                    backoff = min(base_interval * (1.5 ** consecutive_errors), 60.0)
                    await asyncio.sleep(backoff)

        self.logger.info("[v93.16] Heartbeat loop stopped")

    async def _initialize_trinity_knowledge_indexer(self) -> None:
        """
        v88.0: Initialize Trinity Knowledge Indexer for scraped content â†’ vector DB â†’ RAG.

        This is the "missing link" that makes web scraping actually useful by:
        - Converting scraped content into semantic chunks
        - Generating embeddings for vector search
        - Storing in ChromaDB/FAISS for RAG retrieval
        - Exporting training data to Reactor Core

        Pipeline:
            SQLite (scraped_content)
              â†’ Semantic Chunking (intelligent boundary detection)
              â†’ Quality Filtering (min 0.6 score)
              â†’ Parallel Embedding Generation (batch processing)
              â†’ ChromaDB/FAISS Storage (persistent vectors)
              â†’ Training Data Export (JSONL for Reactor Core)
              â†’ Mark as Indexed

        Features:
        - Semantic chunking (not fixed-size, preserves meaning)
        - Parallel batch embedding (32 items/batch, 4 concurrent)
        - Quality scoring and filtering
        - SHA-256 fingerprint deduplication
        - Metadata-rich vector storage
        - Async/parallel processing
        - Zero hardcoding (environment-driven)
        - Incremental indexing (only new content)
        - Graceful degradation (works without ChromaDB/FAISS)
        """
        if not self._trinity_knowledge_indexer_enabled:
            self.logger.debug("[v88.0] Trinity Knowledge Indexer disabled via config")
            return

        try:
            self.logger.info("[v88.0] ğŸ§  Initializing Trinity Knowledge Indexer...")

            # Import and start the indexer
            try:
                from backend.autonomy.trinity_knowledge_indexer import (
                    get_knowledge_indexer,
                    start_knowledge_indexer
                )

                # v2.0: Get the global indexer instance with proper initialization
                # Note: Does NOT raise on degraded mode - allows fallback embedding providers
                indexer = await get_knowledge_indexer(
                    raise_on_failure=False,  # Allow degraded mode with fallback providers
                    raise_on_degraded=False  # Accept TF-IDF/hash fallbacks
                )

                # Store reference for later use
                self._trinity_knowledge_indexer = indexer

                # v2.0: Check initialization status and log appropriately
                if indexer.is_initialized:
                    # Start background indexing and export loops
                    await indexer.start()

                    # Log success with new provider info
                    quality_emoji = {
                        "high": "ğŸŒŸ",
                        "medium": "âš ï¸",
                        "low": "ğŸ“Š",
                        "none": "âŒ"
                    }.get(indexer.embedding_quality, "â“")

                    self.logger.info(
                        f"[v88.0] âœ… Trinity Knowledge Indexer started "
                        f"(indexing every {indexer.config.index_interval_seconds}s, "
                        f"exporting every {indexer.config.export_interval_seconds}s)"
                    )

                    # v2.0: Log embedding provider status
                    self.logger.info(
                        f"[v88.0]    {quality_emoji} Embedding: {indexer.initialization_status}"
                    )
                    self.logger.info(
                        f"[v88.0]    Embedding quality: {indexer.embedding_quality}"
                    )
                    self.logger.info(
                        f"[v88.0]    Chunk size: {indexer.config.chunk_size} tokens"
                    )
                    self.logger.info(
                        f"[v88.0]    Min quality: {indexer.config.min_quality_score}"
                    )
                    self.logger.info(
                        f"[v88.0]    Vector DB: {indexer.config.vector_db_path}"
                    )

                    if indexer.config.export_to_reactor:
                        self.logger.info(
                            f"[v88.0]    Training export: {indexer.config.reactor_export_path}"
                        )

                    # Warn if using degraded embedding mode
                    if indexer.embedding_quality != "high":
                        self.logger.warning(
                            f"[v88.0] âš ï¸ Knowledge Indexer running in degraded mode. "
                            f"Install sentence-transformers for best quality: "
                            f"pip install sentence-transformers"
                        )
                else:
                    self.logger.warning(
                        f"[v88.0] âš ï¸ Trinity Knowledge Indexer initialized but not fully operational: "
                        f"{indexer.initialization_status}"
                    )

            except ImportError as e:
                self.logger.error(
                    f"[v88.0] âŒ Trinity Knowledge Indexer import failed: {e}. "
                    f"Knowledge indexing will not work. "
                    f"Install dependencies: pip install sentence-transformers chromadb faiss-cpu"
                )
                self._trinity_knowledge_indexer = None

            except Exception as e:
                self.logger.error(
                    f"[v88.0] âŒ Trinity Knowledge Indexer initialization failed: {e}"
                )
                self._trinity_knowledge_indexer = None

        except Exception as e:
            self.logger.error(f"[v88.0] Trinity knowledge indexer init failed: {e}", exc_info=True)
            self._trinity_knowledge_indexer = None

        # =====================================================================
        # PHASE 0: Initialize Trinity Directory Lifecycle (v102.0) - MUST BE FIRST
        # Creates all required directories for cross-repo communication BEFORE
        # any component tries to use them.
        # =====================================================================
        if self._trinity_integration_coordinator_enabled:
            await self._initialize_directory_lifecycle()

        # =====================================================================
        # PHASE 2: Start ServiceRegistry for cross-repo health monitoring
        # =====================================================================
        try:
            from backend.engines.rag_engine import get_service_registry
            
            service_registry = get_service_registry()
            await service_registry.start_health_checks()
            
            self.logger.info(
                f"[v88.0] âœ… ServiceRegistry started - "
                f"monitoring {len(service_registry._services)} services"
            )
        except ImportError:
            self.logger.warning("[v88.0] ServiceRegistry not available (import failed)")
        except Exception as e:
            self.logger.warning(f"[v88.0] ServiceRegistry startup failed: {e}")

        # =====================================================================
        # PHASE 3: Initialize Trinity Core Systems (v100.0)
        # =====================================================================
        if self._trinity_core_systems_enabled:
            await self._initialize_trinity_core_systems()

    async def _initialize_trinity_core_systems(self) -> None:
        """
        v100.0: Initialize Trinity Core Systems - Advanced Cross-Repo Infrastructure.

        This initializes the following core systems:
        1. TrinityEventBus: Unified pub/sub messaging with priority queues,
           persistence, and replay capability for cross-repo event streaming
        2. TrinityKnowledgeGraph: Shared knowledge storage across repos with
           semantic search, versioning, and cross-repo synchronization
        3. TrinityTrainingPipeline: End-to-end training integration from
           JARVIS experience capture â†’ Reactor Core training â†’ Prime deployment
        4. TrinityMonitoring: Unified observability with distributed tracing,
           metrics collection, health monitoring, and intelligent alerting
        """
        self.logger.info("=" * 60)
        self.logger.info("[v100.0] Initializing Trinity Core Systems")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ”§ Trinity Core Systems: Initializing advanced infrastructure...{TerminalUI.RESET}")

        initialized_count = 0
        failed_systems = []

        # 1. Initialize Trinity Event Bus
        try:
            from backend.core.trinity_event_bus import get_trinity_event_bus

            # v100.2: Await the async factory function
            self._trinity_event_bus = await get_trinity_event_bus()
            await self._trinity_event_bus.start()

            self.logger.info("[v100.0] âœ… TrinityEventBus initialized - pub/sub messaging active")
            initialized_count += 1
        except ImportError as e:
            self.logger.warning(f"[v100.0] âš ï¸ TrinityEventBus import failed: {e}")
            failed_systems.append("EventBus")
        except Exception as e:
            self.logger.warning(f"[v100.0] âš ï¸ TrinityEventBus initialization failed: {e}")
            failed_systems.append("EventBus")

        # 2. Initialize Trinity Knowledge Graph
        try:
            from backend.core.trinity_knowledge_graph import get_trinity_knowledge_graph

            # v100.2: Await the async factory function
            # v100.3: These classes don't have .start() methods - factory returns ready-to-use object
            self._trinity_knowledge_graph = await get_trinity_knowledge_graph()

            self.logger.info("[v100.0] âœ… TrinityKnowledgeGraph initialized - shared knowledge active")
            initialized_count += 1
        except ImportError as e:
            self.logger.warning(f"[v100.0] âš ï¸ TrinityKnowledgeGraph import failed: {e}")
            failed_systems.append("KnowledgeGraph")
        except Exception as e:
            self.logger.warning(f"[v100.0] âš ï¸ TrinityKnowledgeGraph initialization failed: {e}")
            failed_systems.append("KnowledgeGraph")

        # 3. Initialize Trinity Training Pipeline
        try:
            from backend.core.trinity_training_pipeline import get_training_pipeline

            # v100.2: Await the async factory function
            # v100.3: These classes don't have .start() methods - factory returns ready-to-use object
            self._trinity_training_pipeline = await get_training_pipeline()

            self.logger.info("[v100.0] âœ… TrinityTrainingPipeline initialized - JARVISâ†’Reactorâ†’Prime active")
            initialized_count += 1
        except ImportError as e:
            self.logger.warning(f"[v100.0] âš ï¸ TrinityTrainingPipeline import failed: {e}")
            failed_systems.append("TrainingPipeline")
        except Exception as e:
            self.logger.warning(f"[v100.0] âš ï¸ TrinityTrainingPipeline initialization failed: {e}")
            failed_systems.append("TrainingPipeline")

        # 4. Initialize Trinity Monitoring
        try:
            from backend.core.trinity_monitoring import get_trinity_monitoring

            # v100.2: Await the async factory function
            # v100.3: These classes don't have .start() methods - factory returns ready-to-use object
            self._trinity_monitoring = await get_trinity_monitoring()

            self.logger.info("[v100.0] âœ… TrinityMonitoring initialized - observability active")
            initialized_count += 1
        except ImportError as e:
            self.logger.warning(f"[v100.0] âš ï¸ TrinityMonitoring import failed: {e}")
            failed_systems.append("Monitoring")
        except Exception as e:
            self.logger.warning(f"[v100.0] âš ï¸ TrinityMonitoring initialization failed: {e}")
            failed_systems.append("Monitoring")

        # Summary
        if initialized_count == 4:
            self.logger.info("=" * 60)
            self.logger.info("[v100.0] TRINITY CORE SYSTEMS: FULLY INITIALIZED")
            self.logger.info("   EventBus âœ“ | KnowledgeGraph âœ“ | TrainingPipeline âœ“ | Monitoring âœ“")
            self.logger.info("=" * 60)
            print(f"  {TerminalUI.GREEN}âœ“ Trinity Core Systems: All 4 systems initialized{TerminalUI.RESET}")
        elif initialized_count > 0:
            self.logger.info(f"[v100.0] Trinity Core Systems: {initialized_count}/4 initialized")
            if failed_systems:
                self.logger.info(f"   Failed: {', '.join(failed_systems)}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Trinity Core Systems: {initialized_count}/4 initialized{TerminalUI.RESET}")
        else:
            self.logger.warning("[v100.0] Trinity Core Systems: None initialized (running in basic mode)")
            print(f"  {TerminalUI.RED}âœ— Trinity Core Systems: Not available{TerminalUI.RESET}")

        # Start background monitoring task if monitoring is available
        if self._trinity_monitoring:
            async def _run_trinity_monitoring():
                """Background task for Trinity system monitoring."""
                try:
                    while True:
                        await asyncio.sleep(30)  # Check every 30 seconds

                        # Record health metrics for each component
                        # v100.3: Use hasattr to check if get_stats exists (graceful degradation)
                        if self._trinity_event_bus and hasattr(self._trinity_event_bus, 'get_stats'):
                            try:
                                stats = self._trinity_event_bus.get_stats()
                                await self._trinity_monitoring.record_metric(
                                    name="trinity.event_bus.subscribers",
                                    value=stats.get("total_subscribers", 0),
                                    unit="count",
                                    component="event_bus"
                                )
                                await self._trinity_monitoring.record_metric(
                                    name="trinity.event_bus.events_published",
                                    value=stats.get("events_published", 0),
                                    unit="count",
                                    component="event_bus"
                                )
                            except Exception:
                                pass  # Stats not available - continue

                        if self._trinity_knowledge_graph and hasattr(self._trinity_knowledge_graph, 'get_stats'):
                            try:
                                # v100.2: Properly await async get_stats method
                                # GraphStats is a dataclass, so use getattr for attribute access
                                stats = await self._trinity_knowledge_graph.get_stats()
                                await self._trinity_monitoring.record_metric(
                                    name="trinity.knowledge_graph.nodes",
                                    value=getattr(stats, "total_nodes", 0),
                                    unit="count",
                                    component="knowledge_graph"
                                )
                                await self._trinity_monitoring.record_metric(
                                    name="trinity.knowledge_graph.edges",
                                    value=getattr(stats, "total_edges", 0),
                                    unit="count",
                                    component="knowledge_graph"
                                )
                            except Exception:
                                pass  # Stats not available - continue

                        if self._trinity_training_pipeline and hasattr(self._trinity_training_pipeline, 'get_stats'):
                            try:
                                # PipelineStats is a dataclass, so use getattr for attribute access
                                stats = self._trinity_training_pipeline.get_stats()
                                await self._trinity_monitoring.record_metric(
                                    name="trinity.training.experiences",
                                    value=getattr(stats, "experiences_captured", 0),
                                    unit="count",
                                    component="training_pipeline"
                                )
                                await self._trinity_monitoring.record_metric(
                                    name="trinity.training.active_jobs",
                                    value=getattr(stats, "training_jobs_total", 0) - getattr(stats, "training_jobs_completed", 0),
                                    unit="count",
                                    component="training_pipeline"
                                )
                            except Exception:
                                pass  # Stats not available - continue

                except asyncio.CancelledError:
                    pass
                except Exception as e:
                    self.logger.warning(f"[v100.0] Trinity monitoring task error: {e}")

            self._trinity_core_systems_task = asyncio.create_task(_run_trinity_monitoring())
            self.logger.info("[v100.0] Started Trinity Core Systems monitoring task")

        # =====================================================================
        # PHASE 4-15: Initialize remaining Trinity components with TIMEOUTS
        # v107.0: Each phase wrapped with _safe_phase_init() to prevent blocking
        # =====================================================================
        phase_timeout = float(os.getenv("TRINITY_PHASE_TIMEOUT", "30.0"))  # Configurable timeout

        # PHASE 4: Initialize AGI Orchestrator (v100.0)
        if self._agi_orchestrator_enabled:
            await self._safe_phase_init(
                "PHASE 4: AGI Orchestrator",
                self._initialize_agi_orchestrator(),
                timeout_seconds=phase_timeout,
            )

        # PHASE 5: Initialize Unified Model Serving (v100.0)
        if self._model_serving_enabled:
            await self._safe_phase_init(
                "PHASE 5: Unified Model Serving",
                self._initialize_unified_model_serving(),
                timeout_seconds=phase_timeout,
            )

        # PHASE 6: Initialize Unified Agent Registry (v100.0)
        if self._agent_registry_enabled:
            await self._safe_phase_init(
                "PHASE 6: Unified Agent Registry",
                self._initialize_unified_agent_registry(),
                timeout_seconds=phase_timeout,
            )

        # PHASE 7: Initialize Distributed State Manager (v100.0)
        if self._state_manager_enabled:
            await self._safe_phase_init(
                "PHASE 7: Distributed State Manager",
                self._initialize_distributed_state_manager(),
                timeout_seconds=phase_timeout,
            )

        # PHASE 8: Initialize Continuous Learning Orchestrator (v100.0)
        if self._continuous_learning_enabled:
            await self._safe_phase_init(
                "PHASE 8: Continuous Learning Orchestrator",
                self._initialize_continuous_learning_orchestrator(),
                timeout_seconds=phase_timeout,
            )

        # PHASE 9: Initialize Neural Mesh Registry Bridge (v100.0)
        if self._neural_mesh_bridge_enabled:
            await self._safe_phase_init(
                "PHASE 9: Neural Mesh Registry Bridge",
                self._initialize_neural_mesh_bridge(),
                timeout_seconds=phase_timeout,
            )

        # PHASE 10: Initialize Learning State Connector (v100.0)
        if self._learning_state_connector_enabled:
            await self._safe_phase_init(
                "PHASE 10: Learning State Connector",
                self._initialize_learning_state_connector(),
                timeout_seconds=phase_timeout,
            )

        # PHASE 11: Initialize Cross-Repo Experience Forwarder (v100.0)
        if self._experience_forwarder_enabled:
            await self._safe_phase_init(
                "PHASE 11: Cross-Repo Experience Forwarder",
                self._initialize_experience_forwarder(),
                timeout_seconds=phase_timeout,
            )

        # PHASE 12: Initialize Trinity Bridge Adapter (v101.0) - CRITICAL
        # This MUST be initialized to close the Trinity Loop:
        # Training â†’ MODEL_READY â†’ TrinityBridgeAdapter â†’ Hot-Swap
        if self._trinity_bridge_adapter_enabled:
            await self._safe_phase_init(
                "PHASE 12: Trinity Bridge Adapter",
                self._initialize_trinity_bridge_adapter(),
                timeout_seconds=phase_timeout,
                critical=True,  # Mark as critical
            )

        # PHASE 12.5: Initialize Trinity IPC Hub v4.0 (v104.0) - CRITICAL
        # Enterprise-grade communication with ALL 10 channels
        if self._trinity_ipc_hub_enabled:
            await self._safe_phase_init(
                "PHASE 12.5: Trinity IPC Hub",
                self._initialize_trinity_ipc_hub(),
                timeout_seconds=phase_timeout,
                critical=True,
            )

        # PHASE 12.6: Initialize Trinity State Manager v4.0 (v105.0) - CRITICAL
        # Enterprise-grade distributed state with ALL 8 gaps addressed
        if self._trinity_state_manager_enabled:
            await self._safe_phase_init(
                "PHASE 12.6: Trinity State Manager",
                self._initialize_trinity_state_manager(),
                timeout_seconds=phase_timeout,
                critical=True,
            )

        # PHASE 12.7: Initialize Trinity Observability v4.0 (v106.0) - CRITICAL
        # Enterprise-grade observability with ALL 10 gaps addressed
        if self._trinity_observability_enabled:
            await self._safe_phase_init(
                "PHASE 12.7: Trinity Observability",
                self._initialize_trinity_observability(),
                timeout_seconds=phase_timeout,
                critical=True,
            )

        # PHASE 13: Initialize Cross-Repo Neural Mesh Bridge (v101.0)
        # Registers JARVIS Prime and Reactor Core as Neural Mesh agents
        # THIS IS THE PHASE THAT WAS BLOCKING - now has timeout protection
        if self._cross_repo_neural_mesh_enabled:
            await self._safe_phase_init(
                "PHASE 13: Cross-Repo Neural Mesh Bridge",
                self._initialize_cross_repo_neural_mesh(),
                timeout_seconds=phase_timeout,
            )

        # PHASE 14: Initialize Cross-Repo Cost Sync (v101.0)
        # Unified budget tracking across all repos via Redis
        if self._cross_repo_cost_sync_enabled:
            await self._safe_phase_init(
                "PHASE 14: Cross-Repo Cost Sync",
                self._initialize_cross_repo_cost_sync(),
                timeout_seconds=phase_timeout,
            )

        # PHASE 15: Initialize GCP Hybrid Prime Router (v101.0)
        # Memory-triggered VM provisioning with distributed locking
        if self._gcp_hybrid_router_enabled:
            await self._safe_phase_init(
                "PHASE 15: GCP Hybrid Prime Router",
                self._initialize_gcp_hybrid_router(),
                timeout_seconds=phase_timeout,
            )

        self.logger.info("[v107.0] Trinity Core Systems initialization complete (with timeout protection)")

    async def _initialize_trinity_bridge_adapter(self) -> None:
        """
        v101.0: Initialize Trinity Bridge Adapter - MODEL_READY Event Forwarding.

        CRITICAL: This component closes the Trinity Loop by:
        1. Watching Reactor Core event directories for MODEL_READY events
        2. Parsing event files and forwarding to TrinityEventBus
        3. Triggering model hot-swap in UnifiedModelServing

        Without this, trained models from Reactor Core never reach JARVIS Prime!

        Architecture:
            Reactor Core [Training] â†’ MODEL_READY event file
                                     â†“
            TrinityBridgeAdapter [File Watcher]
                                     â†“
            TrinityEventBus [publish("MODEL_READY", ...)]
                                     â†“
            UnifiedModelServing [hot_swap_model()]
        """
        self.logger.info("=" * 60)
        self.logger.info("[v101.0] Initializing Trinity Bridge Adapter - MODEL_READY Forwarding")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸŒ‰ Trinity Bridge: Connecting Reactor Core â†’ JARVIS model hot-swap...{TerminalUI.RESET}")

        try:
            from backend.system.trinity_bridge_adapter import get_trinity_bridge

            self._trinity_bridge_adapter = await get_trinity_bridge()

            # Get metrics to verify initialization
            metrics = self._trinity_bridge_adapter.get_metrics()
            events_forwarded = metrics.get("events_forwarded", 0)
            event_bus_connected = metrics.get("event_bus_connected", False)
            watchers_active = metrics.get("watchers_active", 0)

            # Connect to model serving for hot-swap notifications
            if self._unified_model_serving:
                # Register MODEL_READY handler for hot-swap
                async def on_model_ready(event_data: dict):
                    """Handle MODEL_READY events from Reactor Core."""
                    model_path = event_data.get("model_path")
                    model_name = event_data.get("model_name", "unknown")
                    training_job_id = event_data.get("training_job_id")

                    self.logger.info(f"[v101.0] ğŸ”¥ MODEL_READY: {model_name} from training job {training_job_id}")

                    if model_path and hasattr(self._unified_model_serving, 'hot_swap_model'):
                        try:
                            await self._unified_model_serving.hot_swap_model(
                                model_path=model_path,
                                model_name=model_name,
                                metadata={"training_job_id": training_job_id},
                            )
                            self.logger.info(f"[v101.0] âœ… Model hot-swap complete: {model_name}")

                            # Voice announcement
                            if self.narrator and self.config.voice_enabled:
                                await self.narrator.speak(
                                    f"New model {model_name} is now active.",
                                    wait=False
                                )
                        except Exception as e:
                            self.logger.error(f"[v101.0] âŒ Model hot-swap failed: {e}")

                # Subscribe to MODEL_READY events
                if self._trinity_event_bus:
                    await self._trinity_event_bus.subscribe("MODEL_READY", on_model_ready)
                    self.logger.info("[v101.0] Registered MODEL_READY handler for hot-swap")

            status_parts = []
            if event_bus_connected:
                status_parts.append("EventBus")
            if watchers_active > 0:
                status_parts.append(f"{watchers_active} watchers")

            if status_parts:
                connections = " + ".join(status_parts)
                print(f"  {TerminalUI.GREEN}âœ… Trinity Bridge: Active ({connections}){TerminalUI.RESET}")
                self.logger.info(f"[v101.0] âœ… Trinity Bridge Adapter initialized: {connections}")

                # Log watched directories
                reactor_events_dir = metrics.get("reactor_events_dir", "unknown")
                trinity_events_dir = metrics.get("trinity_events_dir", "unknown")
                self.logger.info(f"[v101.0] Watching: {reactor_events_dir}")
                self.logger.info(f"[v101.0] Watching: {trinity_events_dir}")
            else:
                print(f"  {TerminalUI.YELLOW}âš ï¸ Trinity Bridge: Initialized but no connections{TerminalUI.RESET}")
                self.logger.warning("[v101.0] âš ï¸ Trinity Bridge: No event bus or watchers connected")

        except ImportError as e:
            self.logger.warning(f"[v101.0] âš ï¸ Trinity Bridge Adapter import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Trinity Bridge: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.error(f"[v101.0] âŒ Trinity Bridge Adapter initialization failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Trinity Bridge: Failed to initialize - {e}{TerminalUI.RESET}")
            import traceback
            self.logger.debug(f"[v101.0] Traceback: {traceback.format_exc()}")

    async def _initialize_trinity_ipc_hub(self) -> None:
        """
        v104.0: Initialize Trinity IPC Hub - Enterprise-Grade Communication Layer.

        CRITICAL: This component enables ALL 10 communication channels between
        JARVIS Body, J-Prime, and Reactor Core:

        1. Direct Body â†’ Reactor Command Channel
        2. Reactor â†’ Body Status Push Channel
        3. Prime â†’ Reactor Feedback Channel
        4. Body â†’ Reactor Training Data Pipeline
        5. Bidirectional Model Metadata Exchange (Model Registry)
        6. Cross-Repo Query Interface
        7. Real-Time Event Streaming
        8. Cross-Repo RPC Layer
        9. Multi-Cast Event Broadcasting (Pub/Sub)
        10. Reliable Message Queue with ACK/NACK

        Features:
        - Circuit breaker for resilience
        - Dead letter queue for failed messages
        - Exactly-once delivery guarantees
        - Async, parallel, non-blocking architecture
        - Zero hardcoded configuration

        Architecture:
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                        Trinity IPC Hub v4.0                              â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚                                                                          â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
            â”‚  â”‚ Command Channelâ”‚  â”‚ Status Push    â”‚  â”‚ Feedback       â”‚              â”‚
            â”‚  â”‚ (Bodyâ†’Reactor) â”‚  â”‚ (Reactorâ†’Body) â”‚  â”‚ (Primeâ†’Reactor)â”‚              â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
            â”‚          â”‚                   â”‚                   â”‚                        â”‚
            â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
            â”‚                              â”‚                                            â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
            â”‚  â”‚ Training       â”‚  â”‚ Message Bus    â”‚  â”‚ Model Registry â”‚              â”‚
            â”‚  â”‚ Pipeline       â”‚â—„â”€â”¤ (Transport)    â”œâ”€â–ºâ”‚ (Metadata)     â”‚              â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
            â”‚                              â”‚                                            â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
            â”‚  â”‚ Query Interfaceâ”‚  â”‚ Event Stream   â”‚  â”‚ RPC Layer      â”‚              â”‚
            â”‚  â”‚ (Cross-Repo)   â”‚  â”‚ (Real-Time)    â”‚  â”‚ (Remote Calls) â”‚              â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
            â”‚                                                                          â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
            â”‚  â”‚ Event Bus      â”‚  â”‚ Message Queue  â”‚                                  â”‚
            â”‚  â”‚ (Pub/Sub)      â”‚  â”‚ (ACK/NACK)     â”‚                                  â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        self.logger.info("=" * 60)
        self.logger.info("[v104.0] Initializing Trinity IPC Hub - All 10 Communication Channels")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ”Œ Trinity IPC Hub: Initializing enterprise-grade communication...{TerminalUI.RESET}")

        try:
            from backend.core.trinity_ipc_hub import TrinityIPCHub, TrinityIPCConfig
            from backend.core.trinity_bridge import TrinityBridge, get_trinity_bridge

            # Initialize IPC Hub with all 10 channels
            ipc_config = TrinityIPCConfig()
            self._trinity_ipc_hub = TrinityIPCHub(config=ipc_config)
            await self._trinity_ipc_hub.start()

            # Get metrics to verify initialization
            metrics = self._trinity_ipc_hub.get_metrics()
            channels_active = sum([
                1 if metrics.get("command_channel", {}).get("running", False) else 0,
                1 if metrics.get("status_push", {}).get("running", False) else 0,
                1 if metrics.get("feedback_channel", {}).get("running", False) else 0,
                1 if metrics.get("training_pipeline", {}).get("initialized", False) else 0,
                1 if metrics.get("model_registry", {}).get("models_registered", 0) >= 0 else 0,
                1 if metrics.get("query_interface", {}).get("connected", False) else 0,
                1,  # Event stream
                1,  # RPC layer
                1 if metrics.get("event_bus", {}).get("running", False) else 0,
                1 if metrics.get("message_queue", {}).get("queues_active", 0) >= 0 else 0,
            ])

            # Initialize Trinity Bridge v4.0 for unified control
            self._trinity_bridge_v4 = await get_trinity_bridge()
            bridge_status = await self._trinity_bridge_v4.get_status()

            # Log channel status
            self.logger.info(f"[v104.0] âœ… IPC Hub initialized: {channels_active}/10 channels active")
            self.logger.info(f"[v104.0] âœ… Trinity Bridge v4.0 status: {bridge_status.get('state', 'unknown')}")

            # Print status
            if channels_active >= 8:
                print(f"  {TerminalUI.GREEN}âœ… Trinity IPC Hub: {channels_active}/10 channels active{TerminalUI.RESET}")
                print(f"  {TerminalUI.GREEN}âœ… Trinity Bridge v4.0: Connected{TerminalUI.RESET}")

                # Voice announcement for full IPC Hub
                await self.narrator.speak(
                    "Trinity communication hub online. All channels operational.",
                    wait=False,
                )
            else:
                print(f"  {TerminalUI.YELLOW}âš ï¸ Trinity IPC Hub: {channels_active}/10 channels (degraded){TerminalUI.RESET}")
                self.logger.warning(f"[v104.0] âš ï¸ IPC Hub running in degraded mode: only {channels_active} channels")

            # Log individual channel status
            channel_names = [
                ("Command Channel", "command_channel", "running"),
                ("Status Push", "status_push", "running"),
                ("Feedback Channel", "feedback_channel", "running"),
                ("Training Pipeline", "training_pipeline", "initialized"),
                ("Model Registry", "model_registry", "models_registered"),
                ("Query Interface", "query_interface", "connected"),
                ("Event Bus", "event_bus", "running"),
                ("Message Queue", "message_queue", "queues_active"),
            ]
            for name, key, status_key in channel_names:
                channel_metrics = metrics.get(key, {})
                status = channel_metrics.get(status_key, False)
                if isinstance(status, bool):
                    status_str = "âœ“" if status else "âœ—"
                else:
                    status_str = f"âœ“ ({status})"
                self.logger.debug(f"[v104.0]   {name}: {status_str}")

        except ImportError as e:
            self.logger.warning(f"[v104.0] âš ï¸ Trinity IPC Hub import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Trinity IPC Hub: Not available (import error){TerminalUI.RESET}")
        except Exception as e:
            self.logger.error(f"[v104.0] âŒ Trinity IPC Hub initialization failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Trinity IPC Hub: Failed to initialize - {e}{TerminalUI.RESET}")
            import traceback
            self.logger.debug(f"[v104.0] Traceback: {traceback.format_exc()}")

    async def _initialize_trinity_state_manager(self) -> None:
        """
        v105.0: Initialize Trinity State Manager - Enterprise-Grade Distributed State.

        CRITICAL: This component addresses ALL 8 state management gaps:

        Gap 1: Unified State Coordinator - Single source of truth with atomic updates
        Gap 2: Distributed State Synchronization - CRDT-based replication
        Gap 3: State Versioning - History tracking and rollback capability
        Gap 4: State Conflict Resolution - Vector clocks and merge strategies
        Gap 5: State Snapshot & Restore - Full and incremental backups
        Gap 6: State Partitioning - Namespace-based isolation
        Gap 7: State Compression - LZ4/zlib for large states
        Gap 8: State Access Control - Role-based access control with tokens

        Features:
        - Vector clocks for causality tracking
        - CRDTs (GCounter, PNCounter, LWWRegister, ORSet)
        - Automatic conflict resolution
        - File-based synchronization between repos
        - Audit logging for compliance

        Architecture:
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                      Trinity State Manager v4.0                          â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
            â”‚  â”‚ State          â”‚  â”‚ Version        â”‚  â”‚ Conflict       â”‚              â”‚
            â”‚  â”‚ Coordinator    â”‚  â”‚ Manager        â”‚  â”‚ Resolver       â”‚              â”‚
            â”‚  â”‚ [Gap 1]        â”‚  â”‚ [Gap 3]        â”‚  â”‚ [Gap 4]        â”‚              â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
            â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
            â”‚                              â”‚                                           â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
            â”‚  â”‚ Synchronizer   â”‚  â”‚ PARTITIONER    â”‚  â”‚ Snapshot       â”‚              â”‚
            â”‚  â”‚ [Gap 2]        â”‚â—„â”€â”¤ [Gap 6]        â”œâ”€â–ºâ”‚ Manager [Gap5] â”‚              â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
            â”‚                              â”‚                                           â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
            â”‚  â”‚ Compression    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Access Control â”‚               â”‚
            â”‚  â”‚ [Gap 7]        â”‚                     â”‚ [Gap 8] (RBAC) â”‚               â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        self.logger.info("=" * 60)
        self.logger.info("[v105.0] Initializing Trinity State Manager - Distributed State")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ—ƒï¸ Trinity State: Initializing distributed state management...{TerminalUI.RESET}")

        try:
            from backend.core.trinity_state_manager import (
                TrinityStateManager,
                StateManagerConfig,
                StateNamespace,
                get_state_manager
            )

            # Create state manager with custom config
            config = StateManagerConfig()
            self._trinity_state_manager = await TrinityStateManager.create(
                config=config,
                node_id=f"jarvis-supervisor-{os.getpid()}"
            )

            # Register sync peers for other Trinity components
            await self._trinity_state_manager.add_peer("jarvis-prime")
            await self._trinity_state_manager.add_peer("reactor-core")

            # Get metrics
            metrics = self._trinity_state_manager.get_metrics()
            state_entries = metrics.get("state_entries", 0)
            peers = metrics.get("peers", [])
            partitions = metrics.get("partitions", {})

            # Log status
            self.logger.info(f"[v105.0] âœ… State Manager initialized:")
            self.logger.info(f"[v105.0]    Node ID: {metrics.get('node_id', 'unknown')}")
            self.logger.info(f"[v105.0]    State entries: {state_entries}")
            self.logger.info(f"[v105.0]    Sync peers: {peers}")
            self.logger.info(f"[v105.0]    Partitions: {len(partitions)}")

            # Print status
            print(f"  {TerminalUI.GREEN}âœ… Trinity State Manager: {state_entries} entries, {len(peers)} peers{TerminalUI.RESET}")

            # Voice announcement
            await self.narrator.speak(
                "Distributed state management online.",
                wait=False,
            )

            # Store initial system state
            await self._trinity_state_manager.set(
                "supervisor_started",
                {
                    "timestamp": time.time(),
                    "pid": os.getpid(),
                    "hostname": os.uname().nodename
                },
                namespace=StateNamespace.SYSTEM
            )

        except ImportError as e:
            self.logger.warning(f"[v105.0] âš ï¸ Trinity State Manager import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Trinity State Manager: Not available (import error){TerminalUI.RESET}")
        except Exception as e:
            self.logger.error(f"[v105.0] âŒ Trinity State Manager initialization failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Trinity State Manager: Failed - {e}{TerminalUI.RESET}")
            import traceback
            self.logger.debug(f"[v105.0] Traceback: {traceback.format_exc()}")

    async def _initialize_trinity_observability(self) -> None:
        """
        v106.0: Initialize Trinity Observability System v4.0.

        Provides enterprise-grade observability with ALL 10 gaps addressed:
        1. Distributed Tracing (W3C Trace Context propagation)
        2. Cross-Repo Metrics (Prometheus-compatible)
        3. Centralized Logging (structured JSON)
        4. Performance Profiling (flame graphs)
        5. Error Aggregation (Sentry-style)
        6. Health Dashboard (unified view)
        7. Alert System (deduplication)
        8. Dependency Graph (Mermaid/GraphViz)
        9. Request Flow (bottleneck detection)
        10. Resource Monitoring (CPU/Memory/Disk/Network)

        Architecture:
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                  Trinity Observability v4.0                       â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
            â”‚  â”‚  Tracer     â”‚ â”‚  Metrics    â”‚ â”‚  Logger     â”‚ â”‚  Profiler   â”‚ â”‚
            â”‚  â”‚  (W3C)      â”‚ â”‚  (Prom)     â”‚ â”‚  (JSON)     â”‚ â”‚  (Flame)    â”‚ â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
            â”‚  â”‚  Errors     â”‚ â”‚  Health     â”‚ â”‚  Alerts     â”‚ â”‚  Deps       â”‚ â”‚
            â”‚  â”‚  (Sentry)   â”‚ â”‚  (Dash)     â”‚ â”‚  (Dedup)    â”‚ â”‚  (Graph)    â”‚ â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
            â”‚  â”‚  Flows      â”‚ â”‚  Resources  â”‚                                 â”‚
            â”‚  â”‚  (Track)    â”‚ â”‚  (Monitor)  â”‚                                 â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        try:
            self.logger.info("[v106.0] Initializing Trinity Observability v4.0...")
            print(f"  {TerminalUI.CYAN}ğŸ”­ Initializing Trinity Observability v4.0...{TerminalUI.RESET}")

            from backend.core.trinity_observability import (
                TrinityObservability,
                ObservabilityConfig
            )

            # Create configuration
            config = ObservabilityConfig(
                node_id=f"jarvis-{os.getpid()}",
                service_name="jarvis"
            )

            # Create and start observability system
            self._trinity_observability = await TrinityObservability.create(
                config=config,
                auto_start=True
            )

            # Log initial metrics
            metrics = self._trinity_observability.get_metrics()
            self.logger.info(f"[v106.0] âœ… Trinity Observability initialized: {metrics}")

            print(f"  {TerminalUI.GREEN}âœ“ Trinity Observability v4.0: 10 components active{TerminalUI.RESET}")
            print(f"    {TerminalUI.CYAN}â”œâ”€ Distributed Tracing: W3C Trace Context{TerminalUI.RESET}")
            print(f"    {TerminalUI.CYAN}â”œâ”€ Metrics: Prometheus-compatible{TerminalUI.RESET}")
            print(f"    {TerminalUI.CYAN}â”œâ”€ Logging: Centralized JSON{TerminalUI.RESET}")
            print(f"    {TerminalUI.CYAN}â”œâ”€ Profiling: Flame graphs{TerminalUI.RESET}")
            print(f"    {TerminalUI.CYAN}â”œâ”€ Errors: Sentry-style aggregation{TerminalUI.RESET}")
            print(f"    {TerminalUI.CYAN}â”œâ”€ Health: Unified dashboard{TerminalUI.RESET}")
            print(f"    {TerminalUI.CYAN}â”œâ”€ Alerts: Deduplication{TerminalUI.RESET}")
            print(f"    {TerminalUI.CYAN}â”œâ”€ Dependencies: Graph visualization{TerminalUI.RESET}")
            print(f"    {TerminalUI.CYAN}â”œâ”€ Flows: Request tracking{TerminalUI.RESET}")
            print(f"    {TerminalUI.CYAN}â””â”€ Resources: CPU/Memory/Disk{TerminalUI.RESET}")

            # Log startup event
            await self._trinity_observability.log_info(
                "Trinity Observability v4.0 started",
                component="supervisor",
                version="106.0"
            )

        except ImportError as e:
            self.logger.warning(f"[v106.0] âš ï¸ Trinity Observability import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Trinity Observability: Not available (import error){TerminalUI.RESET}")
        except Exception as e:
            self.logger.error(f"[v106.0] âŒ Trinity Observability initialization failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Trinity Observability: Failed - {e}{TerminalUI.RESET}")
            import traceback
            self.logger.debug(f"[v106.0] Traceback: {traceback.format_exc()}")

    async def _initialize_cross_repo_neural_mesh(self) -> None:
        """
        v101.0: Initialize Cross-Repo Neural Mesh Bridge.

        Registers JARVIS Prime and Reactor Core as Neural Mesh agents for:
        1. Unified capability discovery across all repos
        2. Task routing with capability matching
        3. Health monitoring via heartbeat files
        4. Automatic failover when repos are unavailable

        Architecture:
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚              CrossRepoNeuralMeshBridge                       â”‚
            â”‚  â”œâ”€â”€ JARVIS Prime Agent (local inference, GPU compute)       â”‚
            â”‚  â””â”€â”€ Reactor Core Agent (training, fine-tuning)             â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        self.logger.info("=" * 60)
        self.logger.info("[v101.0] Initializing Cross-Repo Neural Mesh Bridge")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ•¸ï¸ Neural Mesh: Registering external repos as agents...{TerminalUI.RESET}")

        try:
            from backend.core.registry.cross_repo_neural_mesh import get_cross_repo_neural_mesh

            # v95.19: Use timeout to prevent phase blocking
            op_timeout = float(os.getenv("NEURAL_MESH_INIT_TIMEOUT", "10.0"))
            self._cross_repo_neural_mesh = await asyncio.wait_for(
                get_cross_repo_neural_mesh(),
                timeout=op_timeout,
            )

            if self._cross_repo_neural_mesh is None:
                print(f"  {TerminalUI.YELLOW}âš ï¸ Neural Mesh Bridge: Not yet initialized{TerminalUI.RESET}")
                self.logger.warning("[v101.0] âš ï¸ Neural Mesh Bridge: Initialization pending")
                return

            # Get status
            metrics = self._cross_repo_neural_mesh.get_metrics()
            prime_healthy = metrics.get("prime", {}).get("is_healthy", False)
            reactor_healthy = metrics.get("reactor", {}).get("is_healthy", False)

            status_parts = []
            if prime_healthy:
                status_parts.append("Prime")
            if reactor_healthy:
                status_parts.append("Reactor")

            if status_parts:
                connections = " + ".join(status_parts)
                print(f"  {TerminalUI.GREEN}âœ… Neural Mesh Bridge: {connections} registered{TerminalUI.RESET}")
                self.logger.info(f"[v101.0] âœ… Neural Mesh Bridge: {connections} registered as agents")

                # v95.19: DEFER probe_all() to background task - don't block startup
                # The health loop in CrossRepoNeuralMeshBridge will do probing
                self.logger.info("[v95.19] Health probes deferred to background task (prevents startup blocking)")
            else:
                print(f"  {TerminalUI.YELLOW}âš ï¸ Neural Mesh Bridge: No external repos detected (probing in background){TerminalUI.RESET}")
                self.logger.warning("[v101.0] âš ï¸ Neural Mesh Bridge: No external repos available yet")

        except asyncio.TimeoutError:
            self.logger.warning(f"[v95.19] âš ï¸ Neural Mesh Bridge initialization timed out ({op_timeout}s)")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Neural Mesh Bridge: Timed out (will retry in background){TerminalUI.RESET}")
        except ImportError as e:
            self.logger.warning(f"[v101.0] âš ï¸ Neural Mesh Bridge import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Neural Mesh Bridge: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.error(f"[v101.0] âŒ Neural Mesh Bridge initialization failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Neural Mesh Bridge: Failed - {e}{TerminalUI.RESET}")

    async def _initialize_cross_repo_cost_sync(self) -> None:
        """
        v101.0: Initialize Cross-Repo Cost Sync.

        Unified budget tracking across all repos via Redis for:
        1. Real-time cost synchronization between JARVIS, Prime, and Reactor
        2. Atomic budget checks to prevent concurrent overruns
        3. Auto-reconnecting Redis client with file fallback
        4. Budget alerts and enforcement across repos

        Architecture:
            JARVIS â”€â”¬â”€â–º Redis (real-time sync) â—„â”€â”¬â”€ JARVIS Prime
                    â”‚                            â”‚
                    â””â”€â”€â–º File fallback â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        self.logger.info("=" * 60)
        self.logger.info("[v101.0] Initializing Cross-Repo Cost Sync")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ’° Cost Sync: Connecting budget tracking across repos...{TerminalUI.RESET}")

        try:
            from backend.core.cross_repo_cost_sync import get_cross_repo_cost_sync

            self._cross_repo_cost_sync = await get_cross_repo_cost_sync("jarvis")

            # Get status
            metrics = self._cross_repo_cost_sync.get_metrics()
            # v102.0: Fixed - use correct key 'redis_available' (not 'redis_connected')
            redis_connected = metrics.get("redis_available", False)
            # v102.0: Fixed - access costs from unified_state dict
            unified_state = metrics.get("unified_state", {})
            current_cost = unified_state.get("total_daily_cost", 0.0)
            daily_limit = unified_state.get("daily_budget", 1.0)

            if redis_connected:
                print(f"  {TerminalUI.GREEN}âœ… Cost Sync: Redis connected (${current_cost:.4f}/${daily_limit:.2f}){TerminalUI.RESET}")
                self.logger.info(f"[v101.0] âœ… Cost Sync: Redis connected, current=${current_cost:.4f}")
            else:
                print(f"  {TerminalUI.YELLOW}âš ï¸ Cost Sync: File fallback mode (${current_cost:.4f}/${daily_limit:.2f}){TerminalUI.RESET}")
                self.logger.warning("[v101.0] âš ï¸ Cost Sync: Running in file fallback mode")

            # Register cost alert callback
            async def on_budget_alert(alert_type: str, current: float, limit: float):
                """Handle budget alerts."""
                pct = (current / limit) * 100 if limit > 0 else 0
                self.logger.warning(f"[v101.0] ğŸ’¸ Budget Alert: {alert_type} - ${current:.4f}/${limit:.2f} ({pct:.1f}%)")

                if self.narrator and self.config.voice_enabled:
                    if alert_type == "WARNING":
                        await self.narrator.speak(f"Budget at {pct:.0f} percent.", wait=False)
                    elif alert_type == "CRITICAL":
                        await self.narrator.speak("Budget limit reached. Switching to local-only mode.", wait=False)

            self._cross_repo_cost_sync.on_budget_alert(on_budget_alert)

        except ImportError as e:
            self.logger.warning(f"[v101.0] âš ï¸ Cost Sync import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Cost Sync: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.error(f"[v101.0] âŒ Cost Sync initialization failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Cost Sync: Failed - {e}{TerminalUI.RESET}")

    async def _initialize_gcp_hybrid_router(self) -> None:
        """
        v101.0: Initialize GCP Hybrid Prime Router.

        Intelligent routing between local/GCP/cloud tiers with:
        1. Memory pressure-triggered VM provisioning
        2. Redis distributed locking for cross-repo VM creation
        3. Failure classification with intelligent retry
        4. Cost-aware routing decisions

        Architecture:
            Request â”€â–º GCPHybridPrimeRouter
                       â”œâ”€â–º Local Prime (free, fast, RAM check)
                       â”œâ”€â–º GCP VM (spot pricing, memory overflow)
                       â”œâ”€â–º Cloud Run (serverless, pay-per-use)
                       â””â”€â–º Cloud Claude (API fallback)
        """
        self.logger.info("=" * 60)
        self.logger.info("[v101.0] Initializing GCP Hybrid Prime Router")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ”€ Hybrid Router: Initializing cost-aware tier routing...{TerminalUI.RESET}")

        # v95.19: Internal operation timeout to prevent phase blocking
        op_timeout = float(os.getenv("GCP_ROUTER_INIT_TIMEOUT", "15.0"))

        try:
            from backend.core.gcp_hybrid_prime_router import get_gcp_hybrid_prime_router

            # v95.19: Wrap in timeout to prevent cross-phase blocking
            self._gcp_hybrid_router = await asyncio.wait_for(
                get_gcp_hybrid_prime_router(),
                timeout=op_timeout,
            )

            # Connect to cost sync for budget-aware routing
            if self._cross_repo_cost_sync:
                # The router will check budget via the cost sync instance
                self.logger.info("[v101.0] Hybrid Router connected to Cost Sync for budget checks")

            # Get status
            metrics = self._gcp_hybrid_router.get_metrics()
            resilience_enabled = metrics.get("resilience_enabled", False)
            circuit_breakers = metrics.get("circuit_breakers", {})

            status_parts = []
            if resilience_enabled:
                status_parts.append("resilience")

            # Count healthy tiers
            healthy_tiers = sum(1 for cb in circuit_breakers.values() if isinstance(cb, dict) and cb.get("state") == "closed")
            total_tiers = len(circuit_breakers) if circuit_breakers else 0
            if healthy_tiers > 0:
                status_parts.append(f"{healthy_tiers} tiers")

            if status_parts:
                features = " + ".join(status_parts)
                print(f"  {TerminalUI.GREEN}âœ… Hybrid Router: Active ({features}){TerminalUI.RESET}")
                self.logger.info(f"[v101.0] âœ… Hybrid Router initialized: {features}")
            else:
                # v16.0: Provide more informative status about basic mode
                if total_tiers == 0:
                    # No tiers configured - this is expected on startup
                    print(f"  {TerminalUI.GREEN}âœ“ Hybrid Router: Ready (tiers will activate on first request){TerminalUI.RESET}")
                    self.logger.info("[v101.0] âœ“ Hybrid Router: Ready - tiers lazily initialized")
                elif not resilience_enabled:
                    # Tiers exist but resilience not enabled
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Hybrid Router: Basic mode ({total_tiers} tiers, resilience disabled){TerminalUI.RESET}")
                    self.logger.warning(f"[v101.0] âš ï¸ Hybrid Router: Basic mode - {total_tiers} tiers without resilience")
                else:
                    # All tiers are unhealthy
                    unhealthy_tiers = [
                        name for name, cb in circuit_breakers.items()
                        if isinstance(cb, dict) and cb.get("state") != "closed"
                    ]
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Hybrid Router: Degraded (unhealthy: {', '.join(unhealthy_tiers) or 'all tiers'}){TerminalUI.RESET}")
                    self.logger.warning(f"[v101.0] âš ï¸ Hybrid Router: Degraded - unhealthy tiers: {unhealthy_tiers}")

            # v95.0: Pre-initialize VM manager if memory pressure is high
            # This ensures the _gcp_vm_manager global is set for monitoring displays
            try:
                import psutil
                mem_percent = psutil.virtual_memory().percent
                if mem_percent >= 70:  # Same threshold as GCPHybridPrimeRouter
                    self.logger.info(f"[v95.0] High memory ({mem_percent:.1f}%) - pre-initializing GCP VM manager")
                    from backend.core.gcp_vm_manager import get_gcp_vm_manager_safe
                    vm_manager = await get_gcp_vm_manager_safe()
                    if vm_manager:
                        self.logger.info("[v95.0] âœ… GCP VM manager pre-initialized for high memory handling")
                        print(f"  {TerminalUI.CYAN}â„¹ VM Manager: Pre-initialized (memory: {mem_percent:.1f}%){TerminalUI.RESET}")
                    else:
                        self.logger.debug("[v95.0] VM manager not available (GCP credentials may be missing)")
            except Exception as vm_init_error:
                self.logger.debug(f"[v95.0] VM manager pre-init skipped: {vm_init_error}")

        except asyncio.TimeoutError:
            self.logger.warning(f"[v95.19] âš ï¸ Hybrid Router initialization timed out ({op_timeout}s)")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Hybrid Router: Timed out (background initialization){TerminalUI.RESET}")
        except ImportError as e:
            self.logger.warning(f"[v101.0] âš ï¸ Hybrid Router import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Hybrid Router: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.error(f"[v101.0] âŒ Hybrid Router initialization failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Hybrid Router: Failed - {e}{TerminalUI.RESET}")

        # =====================================================================
        # PHASE 16: Initialize Trinity Integration Coordinator (v102.0)
        # Advanced cross-repo orchestration with causal event delivery
        # =====================================================================
        if self._trinity_integration_coordinator_enabled:
            await self._initialize_trinity_integration_coordinator()

        # =====================================================================
        # PHASE 17: Initialize Reactor Core Bridge (v102.0)
        # Training pipeline integration for MODEL_READY events
        # =====================================================================
        if self._reactor_core_bridge_enabled:
            await self._initialize_reactor_core_bridge()

        # =====================================================================
        # PHASE 18: Initialize Trinity Orchestration Engine (v103.0)
        # God Process for Trinity ecosystem orchestration
        # =====================================================================
        if self._trinity_orchestration_engine_enabled:
            await self._initialize_trinity_orchestration_engine()

        # =====================================================================
        # PHASE 19: Initialize Ouroboros Self-Improvement Engine (v104.0)
        # Autonomous code evolution system
        # =====================================================================
        if self._ouroboros_enabled:
            await self._initialize_ouroboros_engine()

    async def _initialize_ouroboros_engine(self) -> None:
        """
        v104.0: Initialize Ouroboros Self-Improvement Engine.

        The autonomous code evolution system that uses JARVIS Prime (local LLM)
        to improve its own codebase. Named after the ancient symbol of a serpent
        eating its own tail - representing eternal cyclic renewal.

        Architecture:
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                    OUROBOROS SELF-IMPROVEMENT                       â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚  Request â”€â”€â–¶ Analyze â”€â”€â–¶ Generate â”€â”€â–¶ Validate â”€â”€â–¶ Apply â”€â”€â–¶ Learn  â”‚
            â”‚     â”‚          â”‚           â”‚            â”‚           â”‚          â”‚    â”‚
            â”‚     â–¼          â–¼           â–¼            â–¼           â–¼          â–¼    â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”‚
            â”‚  â”‚ Goal â”‚  â”‚ AST  â”‚   â”‚Prime â”‚    â”‚pytestâ”‚    â”‚ Git  â”‚   â”‚Memoryâ”‚   â”‚
            â”‚  â”‚ File â”‚  â”‚Contextâ”‚   â”‚ LLM â”‚    â”‚Tests â”‚    â”‚Commitâ”‚   â”‚Learn â”‚   â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜   â”‚
            â”‚                                                                     â”‚
            â”‚                        THE RALPH LOOP                               â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
            â”‚  â”‚  Improve â”€â”€â–¶ Test â”€â”€â–¶ Pass? â”€â”€â–¶ Commit â”€â”€â–¶ Learn              â”‚  â”‚
            â”‚  â”‚     â–²          â”‚         â”‚ (No)              â”‚                â”‚  â”‚
            â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€ Retry with Error Log â—€â”€â”€â”€â”˜                â”‚  â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        self.logger.info("=" * 60)
        self.logger.info("[v104.0] ğŸ Initializing Ouroboros Self-Improvement Engine")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ Ouroboros: Initializing autonomous evolution...{TerminalUI.RESET}")

        # v106.0: Start Brain Orchestrator first (LLM Infrastructure)
        try:
            from backend.core.ouroboros.brain_orchestrator import (
                get_brain_orchestrator,
                ignite_brains,
            )

            self._brain_orchestrator = get_brain_orchestrator()
            brains_ready = await ignite_brains()

            if brains_ready:
                print(f"  {TerminalUI.GREEN}    â”œâ”€ Brain Orchestrator: LLM infrastructure ready{TerminalUI.RESET}")
                self.logger.info("[v106.0] âœ… Brain Orchestrator initialized - LLM infrastructure ready")

                # Log provider status
                brain_status = self._brain_orchestrator.get_status()
                for provider_name, provider_info in brain_status.get("providers", {}).items():
                    state = provider_info.get("state", "unknown")
                    endpoint = provider_info.get("endpoint", "N/A")
                    icon = "âœ…" if state == "healthy" else ("âš ï¸" if state == "degraded" else "âŒ")
                    self.logger.info(f"[v106.0]   {icon} {provider_name}: {state} @ {endpoint}")
            else:
                print(f"  {TerminalUI.YELLOW}    â”œâ”€ Brain Orchestrator: No LLM providers available{TerminalUI.RESET}")
                self.logger.warning("[v106.0] âš ï¸ No LLM providers available - Ouroboros may be limited")

        except Exception as e:
            self._brain_orchestrator = None
            self.logger.warning(f"[v106.0] âš ï¸ Brain Orchestrator failed: {e}")
            print(f"  {TerminalUI.YELLOW}    â”œâ”€ Brain Orchestrator: {e}{TerminalUI.RESET}")

        try:
            from backend.core.ouroboros.engine import (
                get_ouroboros_engine,
                OuroborosConfig,
            )

            # Get engine instance
            self._ouroboros_engine = get_ouroboros_engine()

            # Initialize the engine
            await self._ouroboros_engine.initialize()

            # Get status
            status = self._ouroboros_engine.get_status()

            print(f"  {TerminalUI.GREEN}âœ… Ouroboros: Initialized{TerminalUI.RESET}")
            self.logger.info("[v104.0] âœ… Ouroboros Self-Improvement Engine initialized")

            # Log configuration
            config = status.get("config", {})
            self.logger.info(f"[v104.0] Configuration:")
            self.logger.info(f"  - JARVIS Prime API: {config.get('prime_api_base', 'N/A')}")
            self.logger.info(f"  - Model: {config.get('prime_model', 'N/A')}")
            self.logger.info(f"  - Max Retries: {config.get('max_retries', 10)}")
            self.logger.info(f"  - Population Size: {config.get('population_size', 3)}")

            # Print features
            print(f"  {TerminalUI.CYAN}    â”œâ”€ Ralph Loop: Iterative improvement{TerminalUI.RESET}")
            print(f"  {TerminalUI.CYAN}    â”œâ”€ Genetic Evolution: Multi-path optimization{TerminalUI.RESET}")
            print(f"  {TerminalUI.CYAN}    â”œâ”€ AST Analysis: Code context understanding{TerminalUI.RESET}")
            print(f"  {TerminalUI.CYAN}    â””â”€ Rollback Protection: Git-based safety{TerminalUI.RESET}")

            # v108.0: Display "God Mode" Status (Oracle + Watcher + Simulator)
            god_mode = status.get("god_mode", {})
            if god_mode:
                print(f"\n  {TerminalUI.BOLD}{TerminalUI.MAGENTA}=== GOD MODE STATUS ==={TerminalUI.RESET}")

                # The Oracle - GraphRAG
                if god_mode.get("oracle_enabled"):
                    oracle_info = god_mode.get("oracle", {})
                    print(f"  {TerminalUI.GREEN}âœ… The Oracle (GraphRAG):{TerminalUI.RESET}")
                    print(f"     â”œâ”€ Nodes: {oracle_info.get('total_nodes', 0)}")
                    print(f"     â”œâ”€ Edges: {oracle_info.get('total_edges', 0)}")
                    print(f"     â””â”€ Files indexed: {oracle_info.get('files_indexed', 0)}")
                    self.logger.info(f"[v108.0] âœ… Oracle: {oracle_info.get('total_nodes', 0)} nodes, {oracle_info.get('total_edges', 0)} edges")
                else:
                    print(f"  {TerminalUI.YELLOW}âš ï¸ The Oracle: Disabled{TerminalUI.RESET}")

                # The Watcher - LSP
                if god_mode.get("watcher_enabled"):
                    watcher_info = god_mode.get("watcher", {})
                    print(f"  {TerminalUI.GREEN}âœ… The Watcher (LSP v{watcher_info.get('version', '2.0')}):{TerminalUI.RESET}")
                    print(f"     â”œâ”€ Server: {watcher_info.get('server', 'unknown')}")
                    print(f"     â”œâ”€ Path: {watcher_info.get('server_path', 'N/A')}")
                    print(f"     â”œâ”€ Connected: {watcher_info.get('connected', False)}")
                    caps = watcher_info.get('capabilities', [])
                    print(f"     â””â”€ Capabilities: {', '.join(caps[:4])}{'...' if len(caps) > 4 else ''}")
                    self.logger.info(f"[v108.0] âœ… Watcher: {watcher_info.get('server', 'unknown')} @ {watcher_info.get('server_path', 'N/A')}")
                else:
                    print(f"  {TerminalUI.YELLOW}âš ï¸ The Watcher: Disabled{TerminalUI.RESET}")

                # The Simulator - Runtime Introspection
                if god_mode.get("simulator_enabled"):
                    simulator_info = god_mode.get("simulator", {})
                    sim_config = simulator_info.get("config", {})
                    print(f"  {TerminalUI.GREEN}âœ… The Simulator (Runtime Introspection):{TerminalUI.RESET}")
                    print(f"     â”œâ”€ Max execution time: {sim_config.get('max_time', 30)}s")
                    print(f"     â”œâ”€ Max memory: {sim_config.get('max_memory_mb', 512)}MB")
                    print(f"     â””â”€ Cache size: {simulator_info.get('cache_size', 0)}")
                    self.logger.info(f"[v108.0] âœ… Simulator: Ready (sandbox execution enabled)")
                else:
                    print(f"  {TerminalUI.YELLOW}âš ï¸ The Simulator: Disabled{TerminalUI.RESET}")

                print(f"  {TerminalUI.BOLD}{TerminalUI.MAGENTA}========================{TerminalUI.RESET}\n")

            # v105.0: Initialize Advanced Orchestrator
            try:
                from backend.core.ouroboros.advanced_orchestrator import get_advanced_orchestrator
                self._ouroboros_advanced = get_advanced_orchestrator()
                await self._ouroboros_advanced.initialize()
                print(f"  {TerminalUI.GREEN}    â”œâ”€ Advanced Orchestrator: Ready{TerminalUI.RESET}")
                self.logger.info("[v105.0] âœ… Advanced Ouroboros Orchestrator initialized")

                # Log advanced features
                adv_status = self._ouroboros_advanced.get_status()
                self.logger.info(f"[v105.0] Advanced features:")
                self.logger.info(f"  - Rate limiting: {adv_status.get('rate_limiter', {}).get('rate_per_second', 0):.1f}/s")
                self.logger.info(f"  - Cache size: {adv_status.get('cache', {}).get('max_size', 0)}")
                self.logger.info(f"  - Degradation: {adv_status.get('degradation_level', 'unknown')}")
            except Exception as e:
                self._ouroboros_advanced = None
                self.logger.warning(f"[v105.0] âš ï¸ Advanced orchestrator unavailable: {e}")

            # v105.0: Initialize Cross-Repo Integration
            try:
                from backend.core.ouroboros.cross_repo import get_cross_repo_orchestrator
                self._ouroboros_cross_repo = get_cross_repo_orchestrator()
                await self._ouroboros_cross_repo.initialize()
                print(f"  {TerminalUI.GREEN}    â””â”€ Cross-Repo Integration: Connected{TerminalUI.RESET}")
                self.logger.info("[v105.0] âœ… Cross-Repo Orchestrator initialized")

                # Log connected repos
                cross_status = self._ouroboros_cross_repo.get_status()
                for repo, info in cross_status.get("repositories", {}).items():
                    status_icon = "âœ…" if info.get("healthy") else "âŒ"
                    self.logger.info(f"  - {repo}: {status_icon} {info.get('path', 'N/A')}")
            except Exception as e:
                self._ouroboros_cross_repo = None
                self.logger.warning(f"[v105.0] âš ï¸ Cross-repo integration unavailable: {e}")

            # v107.0: Initialize Native Self-Improvement (Motor Function)
            try:
                from backend.core.ouroboros.native_integration import (
                    get_native_self_improvement,
                    initialize_native_self_improvement,
                )
                self._native_self_improvement = get_native_self_improvement()
                await initialize_native_self_improvement()
                print(f"  {TerminalUI.GREEN}    â”œâ”€ Native Self-Improvement: Ready (Motor Function){TerminalUI.RESET}")
                self.logger.info("[v107.0] âœ… Native Self-Improvement Engine initialized")

                # Log native features
                native_status = self._native_self_improvement.get_status()
                self.logger.info(f"[v107.0] Native features:")
                self.logger.info(f"  - Security validation: Enabled")
                self.logger.info(f"  - Thread-safe metrics: Active")
                self.logger.info(f"  - Progress broadcasting: Ready")
            except Exception as e:
                self._native_self_improvement = None
                self.logger.warning(f"[v107.0] âš ï¸ Native self-improvement unavailable: {e}")

            # v107.0: Initialize Neural Mesh (Cross-Repo Connection)
            try:
                from backend.core.ouroboros.neural_mesh import (
                    get_neural_mesh,
                    initialize_neural_mesh,
                )
                self._neural_mesh = get_neural_mesh()
                mesh_connected = await initialize_neural_mesh()
                if mesh_connected:
                    print(f"  {TerminalUI.GREEN}    â”œâ”€ Neural Mesh: Connected (JARVIS â†” Prime â†” Reactor){TerminalUI.RESET}")
                    self.logger.info("[v107.0] âœ… Neural Mesh connected")
                else:
                    print(f"  {TerminalUI.YELLOW}    â”œâ”€ Neural Mesh: Standalone mode{TerminalUI.RESET}")
                    self.logger.warning("[v107.0] âš ï¸ Neural Mesh running in standalone mode")

                # Log mesh status
                mesh_status = self._neural_mesh.get_status()
                for node, info in mesh_status.get("connections", {}).items():
                    status_icon = "âœ…" if info.get("connected") else "âŒ"
                    self.logger.info(f"[v107.0]   {status_icon} {node}: {info.get('connection_type', 'N/A')}")
            except Exception as e:
                self._neural_mesh = None
                self.logger.warning(f"[v107.0] âš ï¸ Neural mesh unavailable: {e}")

            # v107.0: Initialize UI Integration
            try:
                from backend.core.ouroboros.ui_integration import (
                    get_ouroboros_ui_controller,
                    connect_ouroboros_ui,
                )
                self._ouroboros_ui_controller = await connect_ouroboros_ui()
                print(f"  {TerminalUI.GREEN}    â”œâ”€ UI Integration: Ready (Progress Broadcasting){TerminalUI.RESET}")
                self.logger.info("[v107.0] âœ… Ouroboros UI Integration connected")
            except Exception as e:
                self._ouroboros_ui_controller = None
                self.logger.warning(f"[v107.0] âš ï¸ UI integration unavailable: {e}")

            # v107.0: Initialize Trinity Integration (Unified Layer)
            try:
                from backend.core.ouroboros.trinity_integration import (
                    get_trinity_integration,
                    initialize_trinity_integration,
                )
                self._ouroboros_trinity = get_trinity_integration()
                trinity_ok = await initialize_trinity_integration()
                if trinity_ok:
                    print(f"  {TerminalUI.GREEN}    â””â”€ Trinity Integration: Ready (Unified Layer){TerminalUI.RESET}")
                    self.logger.info("[v107.0] âœ… Trinity Integration initialized")

                    # Log Trinity health status
                    trinity_status = self._ouroboros_trinity.get_status()
                    health = trinity_status.get("health", {})
                    overall_health = health.get("overall", "unknown")
                    self.logger.info(f"[v107.0] Trinity health: {overall_health}")
                    for comp, status in health.get("components", {}).items():
                        status_icon = "âœ…" if status == "healthy" else ("âš ï¸" if status == "degraded" else "âŒ")
                        self.logger.info(f"[v107.0]   {status_icon} {comp}: {status}")
                else:
                    print(f"  {TerminalUI.YELLOW}    â””â”€ Trinity Integration: Degraded mode{TerminalUI.RESET}")
                    self.logger.warning("[v107.0] âš ï¸ Trinity Integration running in degraded mode")
            except Exception as e:
                self._ouroboros_trinity = None
                self.logger.warning(f"[v107.0] âš ï¸ Trinity Integration unavailable: {e}")

            # v3.0: Initialize Advanced Multi-LLM Integration
            try:
                from backend.core.ouroboros.integration import (
                    initialize_trinity,
                    get_intelligent_ouroboros_selector,
                    MultiModelOrchestrator,
                    SelectionStrategy,
                )

                # Initialize Trinity Coordinator for cross-repo management
                self._trinity_coordinator = await initialize_trinity()
                self._model_selector = get_intelligent_ouroboros_selector()
                self._multi_model_orchestrator = MultiModelOrchestrator(self._model_selector)

                # Get coordinator status
                trinity_status = await self._trinity_coordinator.get_system_status()
                coordinator_version = trinity_status.get("coordinator", {}).get("version", "unknown")

                print(f"  {TerminalUI.GREEN}    â””â”€ v3.0 Multi-LLM Integration: Active (v{coordinator_version}){TerminalUI.RESET}")
                self.logger.info(f"[v3.0] âœ… Multi-LLM Integration initialized")

                # Log model selection health
                model_health = await self._model_selector.get_health()
                models_total = model_health.get("jarvis_prime_models_total", 0)
                models_loaded = model_health.get("jarvis_prime_models_loaded", 0)
                self.logger.info(f"[v3.0] Model selection: {models_total} models available, {models_loaded} loaded")

                # Log repo health
                for repo_id, repo_status in trinity_status.get("repositories", {}).items():
                    healthy = repo_status.get("healthy", False)
                    status_icon = "âœ…" if healthy else "âŒ"
                    self.logger.info(f"[v3.0]   {status_icon} {repo_id}: {'healthy' if healthy else 'unavailable'}")

            except Exception as e:
                self._trinity_coordinator = None
                self._model_selector = None
                self._multi_model_orchestrator = None
                self.logger.warning(f"[v3.0] âš ï¸ Multi-LLM Integration unavailable: {e}")

            # v4.0: Initialize Autonomous Self-Programming System
            # This is the "missing 20%" - autonomous decision-making layer
            autonomous_enabled = os.getenv("AUTONOMOUS_SELF_PROGRAMMING", "true").lower() == "true"
            autonomous_loops = os.getenv("AUTONOMOUS_START_LOOPS", "false").lower() == "true"

            if autonomous_enabled:
                try:
                    from backend.core.ouroboros.integration import (
                        initialize_autonomous_self_programming_full,
                        get_cross_repo_autonomous_integration,
                    )

                    # Initialize all autonomous components
                    self._autonomous_components = await initialize_autonomous_self_programming_full(
                        start_loops=autonomous_loops,
                    )

                    if self._autonomous_components:
                        print(f"  {TerminalUI.GREEN}    â”œâ”€ v4.0 Autonomous Self-Programming: Active{TerminalUI.RESET}")
                        self.logger.info("[v4.0] âœ… Autonomous Self-Programming System initialized")

                        # Log component status
                        for comp_name, comp in self._autonomous_components.items():
                            if hasattr(comp, 'get_status'):
                                comp_status = comp.get_status()
                                status_str = "ready" if comp_status else "unknown"
                                self.logger.info(f"[v4.0]   âœ… {comp_name}: {status_str}")
                            else:
                                self.logger.info(f"[v4.0]   âœ… {comp_name}: initialized")

                        # Print features
                        print(f"  {TerminalUI.CYAN}        â”œâ”€ GoalDecompositionEngine: LLM-powered task breakdown{TerminalUI.RESET}")
                        print(f"  {TerminalUI.CYAN}        â”œâ”€ TechnicalDebtDetector: Autonomous issue detection{TerminalUI.RESET}")
                        print(f"  {TerminalUI.CYAN}        â”œâ”€ DualAgentSystem: Architect/Reviewer pattern{TerminalUI.RESET}")
                        print(f"  {TerminalUI.CYAN}        â”œâ”€ CodeMemoryRAG: Oracle + ChromaDB fusion{TerminalUI.RESET}")
                        print(f"  {TerminalUI.CYAN}        â”œâ”€ AutoTestGenerator: Test generation for untested code{TerminalUI.RESET}")

                        if autonomous_loops:
                            print(f"  {TerminalUI.YELLOW}        â”œâ”€ AutonomousSelfRefinementLoop: RUNNING{TerminalUI.RESET}")
                            print(f"  {TerminalUI.YELLOW}        â”œâ”€ SystemFeedbackLoop: RUNNING{TerminalUI.RESET}")
                            self.logger.warning("[v4.0] âš ï¸ Autonomous improvement loops ENABLED")
                        else:
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ AutonomousSelfRefinementLoop: Ready (not started){TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ SystemFeedbackLoop: Ready (not started){TerminalUI.RESET}")

                        # v6.0 + v7.0: Display advanced autonomous system components
                        v6_components = [
                            ("code_sanitizer", "Web code validation & security"),
                            ("dependency_installer", "Auto-install missing packages"),
                            ("file_lock_manager", "User edit conflict detection"),
                            ("reactor_feedback_receiver", "Bidirectional Reactor Core sync"),
                            ("prime_training_integration", "JARVIS Prime training feedback"),
                            ("model_update_notifier", "Model update notifications"),
                            ("autonomous_loop_controller", "Unified loop management"),
                            ("cross_repo_sync_manager", "Cross-repo state synchronization"),
                        ]

                        v7_components = [
                            ("adaptive_lock_manager", "ML-based file lock sensitivity"),
                            ("sanitization_whitelist", "Safe pattern whitelist"),
                            ("conflict_resolver", "Dependency conflict resolution"),
                            ("multi_format_handler", "Multi-format Reactor events"),
                            ("hot_swap_manager", "Zero-downtime model switching"),
                            ("retry_manager", "Exponential backoff retry"),
                            ("health_monitor", "Event-based health monitoring"),
                            ("import_updater", "Auto import path updates"),
                            ("file_chunker", "Intelligent large file chunking"),
                            ("dashboard", "Real-time status dashboard"),
                        ]

                        v6_active = 0
                        v7_active = 0
                        for comp_name, desc in v6_components:
                            if comp_name in self._autonomous_components:
                                v6_active += 1
                                self.logger.info(f"[v6.0]   âœ… {comp_name}: {desc}")

                        for comp_name, desc in v7_components:
                            if comp_name in self._autonomous_components:
                                v7_active += 1
                                self.logger.info(f"[v7.0]   âœ… {comp_name}: {desc}")

                        if v6_active > 0:
                            print(f"  {TerminalUI.GREEN}    â”œâ”€ v6.0 Autonomous System: {v6_active}/8 components{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ CodeSanitizer: Web code security validation{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ DependencyAutoInstaller: Missing package detection{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ FileLockManager: User edit conflict prevention{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ ReactorCoreFeedbackReceiver: Bidirectional sync{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ PrimeTrainingIntegration: Model feedback{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ ModelUpdateNotifier: Model notifications{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ AutonomousLoopController: Loop management{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â””â”€ CrossRepoSyncManager: Cross-repo sync{TerminalUI.RESET}")

                        if v7_active > 0:
                            print(f"  {TerminalUI.GREEN}    â”œâ”€ v7.0 Enterprise System: {v7_active}/10 components{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ AdaptiveFileLockManager: ML-tuned sensitivity{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ CodeSanitizationWhitelist: Safe pattern allow-list{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ DependencyConflictResolver: Version conflict fix{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ MultiFormatReactorEventHandler: Event normalization{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ ModelHotSwapManager: Zero-downtime updates{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ CrossRepoSyncRetryManager: Exponential backoff{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ EventBasedHealthMonitor: Circuit breakers{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ ImportPathAutoUpdater: Auto import fixes{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ IntelligentFileChunker: Large file handling{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â””â”€ AutonomousSystemDashboard: Real-time status{TerminalUI.RESET}")

                        total_active = v6_active + v7_active
                        self.logger.info(f"[v7.0] âœ… Enterprise Autonomous System: {total_active}/18 components active")

                        # v8.0: "Improve Yourself" System
                        v8_components = ["file_selector", "improvement_engine", "voice_handler"]
                        v8_active = sum(1 for c in v8_components if components.get(c) is not None)

                        if v8_active > 0:
                            print(f"  {TerminalUI.GREEN}    â”œâ”€ v8.0 'Improve Yourself' System: {v8_active}/3 components{TerminalUI.RESET}")
                            print(f"  {TerminalUI.MAGENTA}        â”œâ”€ IntelligentFileSelector: Auto file selection{TerminalUI.RESET}")
                            print(f"  {TerminalUI.MAGENTA}        â”œâ”€ AutonomousImprovementEngine: Self-improvement{TerminalUI.RESET}")
                            print(f"  {TerminalUI.MAGENTA}        â””â”€ VoiceCommandHandler: Voice-activated improvement{TerminalUI.RESET}")
                            self.logger.info(f"[v8.0] âœ… 'Improve Yourself' System: {v8_active}/3 components active")
                            print(f"  {TerminalUI.CYAN}        ğŸ“¢ Say 'JARVIS, improve yourself' to trigger{TerminalUI.RESET}")

                        # v9.0: Multi-Language Support System
                        v9_components = ["language_registry", "ast_parser", "symbol_tracker",
                                        "cross_language_refactorer", "multi_language_selector", "language_analyzer"]
                        v9_active = sum(1 for c in v9_components if components.get(c) is not None)

                        if v9_active > 0:
                            print(f"  {TerminalUI.GREEN}    â”œâ”€ v9.0 Multi-Language Support: {v9_active}/6 components{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ LanguageRegistry: 22 languages supported{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ UniversalASTParser: Cross-language AST parsing{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ CrossLanguageSymbolTracker: Symbol indexing{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ CrossLanguageRefactorer: Multi-language refactoring{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ MultiLanguageFileSelector: Language-aware selection{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â””â”€ LanguageSpecificAnalyzer: Best practices analysis{TerminalUI.RESET}")
                            self.logger.info(f"[v9.0] âœ… Multi-Language Support: {v9_active}/6 components active")
                            # List supported languages
                            langs = ["Python", "JavaScript", "TypeScript", "Go", "Rust", "Java", "C", "C++", "Ruby", "PHP", "Swift", "Kotlin"]
                            print(f"  {TerminalUI.CYAN}        ğŸŒ Languages: {', '.join(langs[:6])}...{TerminalUI.RESET}")

                        # v10.0: Real-Time Code Intelligence System
                        v10_components = ["completion_engine", "error_detector", "suggestion_provider",
                                        "explanation_engine", "comment_generator", "interactive_reviewer"]
                        v10_active = sum(1 for c in v10_components if components.get(c) is not None)

                        if v10_active > 0:
                            print(f"  {TerminalUI.GREEN}    â”œâ”€ v10.0 Real-Time Code Intelligence: {v10_active}/6 components{TerminalUI.RESET}")
                            print(f"  {TerminalUI.MAGENTA}        â”œâ”€ LiveCodeCompletionEngine: Real-time autocomplete{TerminalUI.RESET}")
                            print(f"  {TerminalUI.MAGENTA}        â”œâ”€ RealTimeErrorDetector: Live error detection{TerminalUI.RESET}")
                            print(f"  {TerminalUI.MAGENTA}        â”œâ”€ InlineSuggestionProvider: Contextual suggestions{TerminalUI.RESET}")
                            print(f"  {TerminalUI.MAGENTA}        â”œâ”€ ChangeExplanationEngine: Code change reasoning{TerminalUI.RESET}")
                            print(f"  {TerminalUI.MAGENTA}        â”œâ”€ InlineCommentGenerator: Auto-generate comments{TerminalUI.RESET}")
                            print(f"  {TerminalUI.MAGENTA}        â””â”€ InteractiveCodeReviewer: Cherry-pick changes{TerminalUI.RESET}")
                            self.logger.info(f"[v10.0] âœ… Real-Time Code Intelligence: {v10_active}/6 components active")
                            print(f"  {TerminalUI.CYAN}        ğŸ§  Features: Completions, Errors, Suggestions, Explanations{TerminalUI.RESET}")

                        # v11.0: Resilient Service Mesh
                        v11_components = ["resilient_mesh", "handshake_protocol", "heartbeat_watchdog",
                                        "recovery_manager", "cascade_preventor", "degradation_router"]
                        v11_active = sum(1 for c in v11_components if components.get(c) is not None)

                        if v11_active > 0:
                            print(f"  {TerminalUI.GREEN}    â”œâ”€ v11.0 Resilient Service Mesh: {v11_active}/6 components{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ StartupHandshakeProtocol: Race condition prevention{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ IntelligentCircuitBreaker: Fast recovery{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ AdaptiveHealthMonitor: Smart health checks{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ SelfHealingServiceManager: Auto-recovery{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ CascadeFailurePreventor: Failure isolation{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â””â”€ HeartbeatWatchdog: Stale detection{TerminalUI.RESET}")
                            self.logger.info(f"[v11.0] âœ… Resilient Service Mesh: {v11_active}/6 components active")
                            print(f"  {TerminalUI.CYAN}        ğŸ›¡ï¸ Fixes: Race conditions, Cascades, Stale services{TerminalUI.RESET}")

                        # v12.0: Resilient Experience Mesh
                        v12_components = ["experience_mesh", "memory_store", "sqlite_store",
                                        "file_store", "backend_selector", "event_bus_monitor", "degraded_manager"]
                        v12_active = sum(1 for c in v12_components if components.get(c) is not None)

                        if v12_active > 0:
                            print(f"  {TerminalUI.GREEN}    â”œâ”€ v12.0 Resilient Experience Mesh: {v12_active}/7 components{TerminalUI.RESET}")
                            print(f"  {TerminalUI.YELLOW}        â”œâ”€ InMemoryExperienceStore: LRU cache (never fails){TerminalUI.RESET}")
                            print(f"  {TerminalUI.YELLOW}        â”œâ”€ SQLiteExperienceStore: WAL mode persistence{TerminalUI.RESET}")
                            print(f"  {TerminalUI.YELLOW}        â”œâ”€ FileExperienceStore: JSONL ultimate fallback{TerminalUI.RESET}")
                            print(f"  {TerminalUI.YELLOW}        â”œâ”€ AdaptiveBackendSelector: Smart routing{TerminalUI.RESET}")
                            print(f"  {TerminalUI.YELLOW}        â”œâ”€ EventBusHealthMonitor: Trinity Bus health{TerminalUI.RESET}")
                            print(f"  {TerminalUI.YELLOW}        â”œâ”€ DegradedModeManager: Graceful degradation{TerminalUI.RESET}")
                            print(f"  {TerminalUI.YELLOW}        â””â”€ ResilientExperienceMesh: Multi-backend orchestrator{TerminalUI.RESET}")
                            self.logger.info(f"[v12.0] âœ… Resilient Experience Mesh: {v12_active}/7 components active")
                            print(f"  {TerminalUI.YELLOW}        ğŸ“¦ Backends: Redis â†’ SQLite â†’ Memory â†’ File{TerminalUI.RESET}")

                        # v13.0: Bulletproof Orchestration Mesh
                        v13_components = ["bulletproof_mesh", "lock_guard", "task_supervisor",
                                        "atomic_file_manager", "shutdown_orchestrator", "health_coordinator",
                                        "event_loss_preventor", "startup_sequencer"]
                        v13_active = sum(1 for c in v13_components if components.get(c) is not None)

                        if v13_active > 0:
                            print(f"  {TerminalUI.GREEN}    â””â”€ v13.0 Bulletproof Orchestration Mesh: {v13_active}/8 components{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ ValidatedTimeouts: Config validation{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ AsyncLockGuard: Deadlock prevention{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ TaskSupervisor: Crash recovery{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ AtomicFileManager: Safe writes{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ GracefulShutdownOrchestrator: Ordered shutdown{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ CrossRepoHealthCoordinator: Active health probes{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â”œâ”€ EventLossPreventor: Dead Letter Queue{TerminalUI.RESET}")
                            print(f"  {TerminalUI.CYAN}        â””â”€ StartupSequencer: Dependency ordering{TerminalUI.RESET}")
                            self.logger.info(f"[v13.0] âœ… Bulletproof Orchestration Mesh: {v13_active}/8 components active")
                            print(f"  {TerminalUI.CYAN}        ğŸ›¡ï¸ Fixes: 20 critical/high issues{TerminalUI.RESET}")

                        total_all_versions = total_active + v8_active + v9_active + v10_active + v11_active + v12_active + v13_active
                        self.logger.info(f"[v13.0] âœ… Total Autonomous System: {total_all_versions}/54 components active")

                        # Get cross-repo integration status
                        cross_repo = get_cross_repo_autonomous_integration()
                        cross_status = cross_repo.get_status()
                        for repo, path in cross_status.get("repos", {}).items():
                            self.logger.info(f"[v4.0]   ğŸ“ {repo}: {path}")
                    else:
                        print(f"  {TerminalUI.YELLOW}    â”œâ”€ v4.0 Autonomous Self-Programming: No components{TerminalUI.RESET}")
                        self.logger.warning("[v4.0] âš ï¸ Autonomous Self-Programming - no components initialized")

                except ImportError as e:
                    self._autonomous_components = None
                    self.logger.warning(f"[v4.0] âš ï¸ Autonomous Self-Programming import failed: {e}")
                    print(f"  {TerminalUI.YELLOW}    â”œâ”€ v4.0 Autonomous Self-Programming: Import error{TerminalUI.RESET}")
                except Exception as e:
                    self._autonomous_components = None
                    self.logger.warning(f"[v4.0] âš ï¸ Autonomous Self-Programming unavailable: {e}")
                    print(f"  {TerminalUI.YELLOW}    â”œâ”€ v4.0 Autonomous Self-Programming: {e}{TerminalUI.RESET}")
            else:
                self.logger.info("[v4.0] Autonomous Self-Programming disabled via AUTONOMOUS_SELF_PROGRAMMING=false")

            # Note about auto-improvement
            if self._ouroboros_auto_improve:
                print(f"  {TerminalUI.YELLOW}    âš ï¸  Auto-improvement ENABLED{TerminalUI.RESET}")
                self.logger.warning("[v104.0] âš ï¸ Ouroboros auto-improvement is ENABLED")

        except ImportError as e:
            self.logger.warning(f"[v104.0] âš ï¸ Ouroboros import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Ouroboros: Not available - {e}{TerminalUI.RESET}")
        except Exception as e:
            self.logger.error(f"[v104.0] âŒ Ouroboros initialization failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Ouroboros: Failed - {e}{TerminalUI.RESET}")

    async def improve_self(
        self,
        target_file: str,
        goal: str,
        test_command: Optional[str] = None,
    ) -> bool:
        """
        v104.0: Trigger self-improvement on a file.

        This is the main entry point for autonomous code evolution.

        Args:
            target_file: Path to the file to improve
            goal: Description of the improvement goal
            test_command: Optional test command to validate

        Returns:
            True if improvement was successful
        """
        if not self._ouroboros_engine:
            self.logger.error("[v104.0] Ouroboros engine not initialized")
            return False

        self.logger.info(f"[v104.0] ğŸ Starting self-improvement: {goal}")
        self.logger.info(f"[v104.0] Target file: {target_file}")

        try:
            from backend.core.ouroboros.engine import improve_file

            result = await improve_file(
                target_file=target_file,
                goal=goal,
                test_command=test_command,
            )

            if result.success:
                self.logger.info(f"[v104.0] âœ… Self-improvement successful after {result.iterations} iterations")
                self.logger.info(f"[v104.0] Time: {result.total_time:.2f}s")
                return True
            else:
                self.logger.warning(f"[v104.0] âŒ Self-improvement failed after {result.iterations} iterations")
                if result.error_history:
                    self.logger.warning(f"[v104.0] Last error: {result.error_history[-1][:200]}")
                return False

        except Exception as e:
            self.logger.error(f"[v104.0] Self-improvement error: {e}")
            return False

    async def _initialize_trinity_orchestration_engine(self) -> None:
        """
        v103.0: Initialize Trinity Orchestration Engine - The God Process.

        Advanced features:
        - Distributed consensus with Raft-inspired leader election
        - Predictive auto-scaling with Holt-Winters forecasting
        - Anomaly detection using Modified Z-Score
        - Graceful degradation with component fallback modes
        - Dead letter queue for failed event recovery
        - Resource governance with memory limits and GC triggers
        - Split-brain detection and automatic recovery

        Architecture:
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                    Trinity Orchestration Engine                      â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚  Consensus Protocol â”‚ Backpressure â”‚ Experience Pipeline            â”‚
            â”‚  (Raft-inspired)    â”‚ Controller   â”‚ (Guaranteed Delivery)          â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚  Auto-Scaler (Holt-Winters) â”‚ Dead Letter Queue â”‚ Resource Governor â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        self.logger.info("=" * 60)
        self.logger.info("[v103.0] ğŸ§¬ Initializing Trinity Orchestration Engine")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ§¬ Orchestration Engine: Initializing God Process...{TerminalUI.RESET}")

        try:
            from backend.core.trinity.orchestration_engine import (
                get_orchestration_engine,
                TrinityConfig,
            )

            # Get orchestration engine instance
            self._trinity_orchestration_engine = get_orchestration_engine()

            # Note: We don't call engine.start() here because this supervisor IS the body
            # The engine.start() would try to spawn a new JARVIS process
            # Instead, we just use the engine for its coordination features

            # Get status
            status = self._trinity_orchestration_engine.get_status()
            instance_id = status.get("instance_id", "unknown")

            print(f"  {TerminalUI.GREEN}âœ… Orchestration Engine: Initialized (ID: {instance_id}){TerminalUI.RESET}")
            self.logger.info(f"[v103.0] âœ… Trinity Orchestration Engine initialized: {instance_id}")

            # Log component configuration
            self.logger.info(f"[v103.0] Configuration:")
            self.logger.info(f"  - JARVIS Path: {TrinityConfig.JARVIS_PATH}")
            self.logger.info(f"  - Prime Path: {TrinityConfig.PRIME_PATH}")
            self.logger.info(f"  - Reactor Path: {TrinityConfig.REACTOR_PATH}")
            self.logger.info(f"  - Heartbeat Interval: {TrinityConfig.HEARTBEAT_INTERVAL}s")
            self.logger.info(f"  - Health Check Interval: {TrinityConfig.HEALTH_CHECK_INTERVAL}s")

            # Print features
            print(f"  {TerminalUI.CYAN}    â”œâ”€ Consensus Protocol: Raft-inspired leader election{TerminalUI.RESET}")
            print(f"  {TerminalUI.CYAN}    â”œâ”€ Auto-Scaler: Holt-Winters forecasting{TerminalUI.RESET}")
            print(f"  {TerminalUI.CYAN}    â”œâ”€ Dead Letter Queue: Failed event recovery{TerminalUI.RESET}")
            print(f"  {TerminalUI.CYAN}    â””â”€ Graceful Degradation: Component fallback modes{TerminalUI.RESET}")

            # Start status monitoring task
            self._orchestration_status_task = asyncio.create_task(
                self._monitor_orchestration_status(),
                name="orchestration_status_monitor"
            )

        except ImportError as e:
            self.logger.warning(f"[v103.0] âš ï¸ Orchestration Engine import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Orchestration Engine: Not available - {e}{TerminalUI.RESET}")
        except Exception as e:
            self.logger.error(f"[v103.0] âŒ Orchestration Engine initialization failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Orchestration Engine: Failed - {e}{TerminalUI.RESET}")

    async def _monitor_orchestration_status(self) -> None:
        """
        v103.0: Monitor Trinity Orchestration Engine status.

        Periodically checks:
        - Component health and degradation status
        - Auto-scaling recommendations
        - Dead letter queue size
        - Split-brain events
        """
        check_interval = float(os.getenv("ORCHESTRATION_STATUS_INTERVAL", "60.0"))

        while True:
            try:
                await asyncio.sleep(check_interval)

                if not self._trinity_orchestration_engine:
                    continue

                status = self._trinity_orchestration_engine.get_status()

                # Log metrics
                metrics = status.get("metrics", {})
                dlq = status.get("dead_letter_queue", {})
                degradation = status.get("graceful_degradation", {})

                if metrics.get("split_brain_events", 0) > 0:
                    self.logger.warning(
                        f"[v103.0] ğŸ§  Split-brain events detected: {metrics['split_brain_events']}"
                    )

                if dlq.get("size", 0) > 0:
                    self.logger.info(
                        f"[v103.0] ğŸ“‹ DLQ has {dlq['size']} pending events"
                    )

                degraded = degradation.get("degraded_components", [])
                if degraded:
                    self.logger.warning(
                        f"[v103.0] âš ï¸ Degraded components: {', '.join(degraded)}"
                    )

            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"[v103.0] Orchestration status monitor error: {e}")

    async def _initialize_directory_lifecycle(self) -> None:
        """
        v102.0: Initialize Trinity Directory Lifecycle - MUST BE FIRST.

        Creates all required directories for cross-repo IPC:
        - ~/.jarvis/trinity/events/ (JARVIS â†’ Reactor Core events)
        - ~/.jarvis/trinity/state/ (Shared state)
        - ~/.jarvis/trinity/models/ (Model artifacts)
        - ~/.jarvis/reactor/events/ (Reactor Core â†’ JARVIS events)
        - ~/.jarvis/cross_repo/ (Cross-repo coordination)

        This MUST run before any other component tries to use these directories.
        """
        self.logger.info("=" * 60)
        self.logger.info("[v102.0] Initializing Trinity Directory Lifecycle")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ“ Directory Lifecycle: Creating cross-repo directories...{TerminalUI.RESET}")

        try:
            from backend.core.trinity.integration_coordinator import DirectoryLifecycleManager

            directory_manager = DirectoryLifecycleManager()
            success, errors = await directory_manager.initialize()

            if success:
                print(f"  {TerminalUI.GREEN}âœ… Directory Lifecycle: All directories created{TerminalUI.RESET}")
                self.logger.info("[v102.0] âœ… Directory Lifecycle: All directories initialized")

                # Cleanup old files
                removed = await directory_manager.cleanup_old_files()
                if removed > 0:
                    self.logger.info(f"[v102.0] Cleaned up {removed} old event files")
            else:
                print(f"  {TerminalUI.YELLOW}âš ï¸ Directory Lifecycle: Partial initialization{TerminalUI.RESET}")
                for error in errors:
                    self.logger.warning(f"[v102.0] Directory error: {error}")

        except ImportError as e:
            self.logger.warning(f"[v102.0] âš ï¸ Directory Lifecycle import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Directory Lifecycle: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.error(f"[v102.0] âŒ Directory Lifecycle initialization failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Directory Lifecycle: Failed - {e}{TerminalUI.RESET}")

    async def _initialize_trinity_integration_coordinator(self) -> None:
        """
        v102.0: Initialize Trinity Integration Coordinator - Advanced Cross-Repo Orchestration.

        This is the central coordinator for all cross-repo communication with:
        1. Causal event delivery with vector clocks
        2. Distributed locking for model hot-swap operations
        3. Health monitoring with auto-recovery
        4. Experience validation and schema enforcement
        5. Event sequencing with gap detection

        Architecture:
            JARVIS â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Reactor Core
                    â”‚   TrinityIntegrationCoordinator           â”‚
                    â”‚   â”œâ”€â”€ EventSequencer (sequence + gaps)    â”‚
                    â”‚   â”œâ”€â”€ CausalDelivery (ordering)           â”‚
                    â”‚   â”œâ”€â”€ HealthMonitor (auto-recovery)       â”‚
                    â”‚   â”œâ”€â”€ HotSwapManager (model swap)         â”‚
                    â”‚   â””â”€â”€ ExperienceValidator (validation)    â”‚
                    â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
        """
        self.logger.info("=" * 60)
        self.logger.info("[v102.0] Initializing Trinity Integration Coordinator")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ”„ Integration Coordinator: Initializing cross-repo orchestration...{TerminalUI.RESET}")

        try:
            from backend.core.trinity.integration_coordinator import get_trinity_coordinator

            # Get Redis client if available
            redis_client = None
            if hasattr(self, '_redis_client') and self._redis_client:
                redis_client = self._redis_client

            self._trinity_integration_coordinator = await get_trinity_coordinator(
                repo_id="jarvis",
                redis_client=redis_client,
            )

            # Get metrics
            metrics = self._trinity_integration_coordinator.get_metrics()
            health_data = metrics.get("health", {})
            health_status = health_data.get("overall_status", "unknown")
            components = health_data.get("components", {})

            # v16.0: Provide more informative status messages
            if health_status == "healthy":
                print(f"  {TerminalUI.GREEN}âœ… Integration Coordinator: Fully operational{TerminalUI.RESET}")
                self.logger.info("[v102.0] âœ… Integration Coordinator: All components healthy")
            elif health_status == "unknown":
                # "unknown" typically means no components have registered yet - this is expected at startup
                component_count = len(components)
                if component_count == 0:
                    print(f"  {TerminalUI.GREEN}âœ“ Integration Coordinator: Ready (awaiting component registration){TerminalUI.RESET}")
                    self.logger.info("[v102.0] âœ“ Integration Coordinator: Ready - components will register during startup")
                else:
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Integration Coordinator: {component_count} components, status pending{TerminalUI.RESET}")
                    self.logger.warning(f"[v102.0] âš ï¸ Integration Coordinator: {component_count} components registered, status unknown")
            elif health_status == "degraded":
                degraded_components = [
                    name for name, data in components.items()
                    if data.get("status") in ("degraded", "unhealthy")
                ]
                print(f"  {TerminalUI.YELLOW}âš ï¸ Integration Coordinator: Degraded ({', '.join(degraded_components) or 'some components'}){TerminalUI.RESET}")
                self.logger.warning(f"[v102.0] âš ï¸ Integration Coordinator: Degraded - {degraded_components}")
            else:
                print(f"  {TerminalUI.YELLOW}âš ï¸ Integration Coordinator: Status - {health_status}{TerminalUI.RESET}")
                self.logger.warning(f"[v102.0] âš ï¸ Integration Coordinator: {health_status}")

            # Log component status
            self.logger.info(f"[v102.0] Events sent: {metrics.get('events_sent', 0)}")
            self.logger.info(f"[v102.0] Events received: {metrics.get('events_received', 0)}")

        except ImportError as e:
            self.logger.warning(f"[v102.0] âš ï¸ Integration Coordinator import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Integration Coordinator: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.error(f"[v102.0] âŒ Integration Coordinator initialization failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Integration Coordinator: Failed - {e}{TerminalUI.RESET}")

    async def _initialize_reactor_core_bridge(self) -> None:
        """
        v102.0: Initialize Reactor Core Bridge - Training Pipeline Integration.

        Provides integration with Reactor Core for:
        1. Publishing MODEL_READY events when training completes
        2. Receiving experience batches from JARVIS
        3. Training pipeline lifecycle coordination
        4. Model artifact management

        This component should also be deployed to Reactor Core repo to enable
        bidirectional communication.

        Architecture:
            Reactor Core Training Pipeline
                    â”‚
                    â–¼
            ReactorCoreBridge.training.on_training_complete()
                    â”‚
                    â–¼
            ReactorCorePublisher.publish_model_ready()
                    â”‚
                    â–¼
            ~/.jarvis/reactor/events/MODEL_READY_*.json
                    â”‚
                    â–¼
            TrinityBridgeAdapter (JARVIS)
                    â”‚
                    â–¼
            UnifiedModelServing.hot_swap_model()
        """
        self.logger.info("=" * 60)
        self.logger.info("[v102.0] Initializing Reactor Core Bridge")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸŒ‰ Reactor Bridge: Connecting to training pipeline...{TerminalUI.RESET}")

        try:
            from backend.core.trinity.reactor_bridge import get_reactor_bridge

            # Get Redis client if available
            redis_client = None
            if hasattr(self, '_redis_client') and self._redis_client:
                redis_client = self._redis_client

            self._reactor_core_bridge = await get_reactor_bridge(redis_client=redis_client)

            # Register experience callback to forward to training pipeline
            if self._trinity_training_pipeline:
                async def on_experience_batch(experiences):
                    """Forward validated experiences to training pipeline."""
                    try:
                        for exp in experiences:
                            await self._trinity_training_pipeline.add_experience(
                                query=exp.get("query", ""),
                                response=exp.get("response", ""),
                                feedback=exp.get("feedback"),
                                context=exp.get("context"),
                                metadata=exp.get("metadata"),
                            )
                        self.logger.debug(f"[v102.0] Forwarded {len(experiences)} experiences to training pipeline")
                    except Exception as e:
                        self.logger.error(f"[v102.0] Experience forwarding failed: {e}")

                self._reactor_core_bridge.on_experience_batch(on_experience_batch)
                self.logger.info("[v102.0] Experience callback registered with training pipeline")

            print(f"  {TerminalUI.GREEN}âœ… Reactor Bridge: Training pipeline connected{TerminalUI.RESET}")
            self.logger.info("[v102.0] âœ… Reactor Bridge: Ready for training events")

            # Start training health monitor as background task
            self._training_health_task = asyncio.create_task(
                self._monitor_reactor_training_health(),
                name="reactor_training_health_monitor"
            )
            self.logger.info("[v102.1] Started training health monitor")

        except ImportError as e:
            self.logger.warning(f"[v102.0] âš ï¸ Reactor Bridge import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Reactor Bridge: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.error(f"[v102.0] âŒ Reactor Bridge initialization failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Reactor Bridge: Failed - {e}{TerminalUI.RESET}")

    async def _monitor_reactor_training_health(self) -> None:
        """
        v102.1: Monitor Reactor Core Training Health.

        Periodically checks Reactor Core training subsystem health and:
        1. Updates cross-repo health monitor with training status
        2. Triggers voice alerts on health changes
        3. Auto-restarts failed components if configured
        4. Publishes health metrics for dashboard

        Architecture:
            Reactor Core Training Health API
                    â”‚
                    â–¼
            /training/health/deep
                    â”‚
                    â–¼
            JARVIS Health Aggregation
                    â”‚
                    â–¼
            Voice Alerts / Dashboard / Auto-Recovery
        """
        import aiohttp

        check_interval = float(os.getenv("REACTOR_TRAINING_HEALTH_INTERVAL", "30.0"))
        health_endpoint = f"http://localhost:{self._reactor_core_port}/training/health/deep"

        last_status = "unknown"

        while self._running:
            try:
                await asyncio.sleep(check_interval)

                if not self._running:
                    break

                # Check training health
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.get(
                            health_endpoint,
                            timeout=aiohttp.ClientTimeout(total=10),
                        ) as response:
                            if response.status == 200:
                                health_data = await response.json()
                                status = health_data.get("status", "unknown")

                                # Update cross-repo health monitor
                                if self._cross_repo_health_monitor:
                                    self._cross_repo_health_monitor._health_cache["reactor_training"] = {
                                        "status": status,
                                        "latency_ms": 0,
                                        "last_check": asyncio.get_event_loop().time(),
                                        "details": health_data.get("components", {}),
                                    }

                                # Voice alert on status change
                                if status != last_status:
                                    if status == "unhealthy":
                                        msg = "Reactor Core training pipeline is experiencing issues."
                                        self.logger.warning(f"[v102.1] Training health degraded: {status}")
                                        if self.narrator and self.config.voice_enabled:
                                            await self.narrator.speak(msg, wait=False)
                                    elif status == "healthy" and last_status in ("unhealthy", "degraded"):
                                        msg = "Reactor Core training pipeline has recovered."
                                        self.logger.info(f"[v102.1] Training health recovered: {status}")
                                        if self.narrator and self.config.voice_enabled:
                                            await self.narrator.speak(msg, wait=False)

                                    last_status = status

                            else:
                                self.logger.warning(f"[v102.1] Training health check returned {response.status}")
                                last_status = "unreachable"

                except aiohttp.ClientError as e:
                    self.logger.debug(f"[v102.1] Training health check failed: {e}")
                    last_status = "unreachable"

            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"[v102.1] Training health monitor error: {e}")
                await asyncio.sleep(10.0)

    async def _initialize_agi_orchestrator(self) -> None:
        """
        v100.0: Initialize AGI Orchestrator - Unified Cognitive Architecture.

        This initializes the central AGI coordinator that ties together:
        1. MetaCognitiveEngine: Self-aware reasoning and introspection
        2. MultiModalPerceptionFusion: Vision + voice + text integration
        3. ContinuousImprovementEngine: Self-improving learning loop
        4. EmotionalIntelligenceModule: Empathetic response system
        """
        self.logger.info("=" * 60)
        self.logger.info("[v100.0] Initializing AGI Orchestrator")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ§  AGI Orchestrator: Initializing unified cognitive architecture...{TerminalUI.RESET}")

        try:
            from backend.intelligence.agi_orchestrator import get_agi_orchestrator

            self._agi_orchestrator = await get_agi_orchestrator()

            # Get status
            stats = self._agi_orchestrator.get_stats()
            component_count = len(stats.get("components", {}))
            healthy_count = sum(
                1 for c in stats.get("components", {}).values()
                if c.get("health") == "healthy"
            )

            self.logger.info(f"[v100.0] âœ… AGI Orchestrator initialized")
            self.logger.info(f"   Components: {healthy_count}/{component_count} healthy")
            print(f"  {TerminalUI.GREEN}âœ“ AGI Orchestrator: {healthy_count}/{component_count} cognitive components ready{TerminalUI.RESET}")

        except ImportError as e:
            self.logger.warning(f"[v100.0] âš ï¸ AGI Orchestrator import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ AGI Orchestrator: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.warning(f"[v100.0] âš ï¸ AGI Orchestrator initialization failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ AGI Orchestrator: Failed to initialize{TerminalUI.RESET}")

    async def _initialize_unified_model_serving(self) -> None:
        """
        v100.0: Initialize Unified Model Serving - Prime + Claude Fallback.

        This initializes the unified model serving layer that provides:
        1. PrimeLocalClient: Local GGUF model inference (primary)
        2. PrimeCloudRunClient: Cloud Run Prime deployment (backup)
        3. ClaudeClient: Claude API fallback (ultimate fallback)
        4. CircuitBreaker: Automatic failure detection and recovery
        5. ModelRouter: Task-based intelligent routing
        6. Cost tracking and optimization

        Fallback chain: Prime Local â†’ Prime Cloud Run â†’ Claude
        """
        self.logger.info("=" * 60)
        self.logger.info("[v100.0] Initializing Unified Model Serving")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ¤– Model Serving: Initializing Prime + Claude fallback...{TerminalUI.RESET}")

        try:
            from backend.intelligence.unified_model_serving import get_model_serving

            self._unified_model_serving = await get_model_serving()

            # Get status
            stats = self._unified_model_serving.get_stats()
            providers = stats.get("providers_available", [])
            provider_count = len(providers)

            self.logger.info(f"[v100.0] âœ… Unified Model Serving initialized")
            self.logger.info(f"   Providers: {', '.join(providers) if providers else 'none'}")

            # Display provider chain
            provider_display = " â†’ ".join(providers) if providers else "No providers available"
            print(f"  {TerminalUI.GREEN}âœ“ Model Serving: {provider_count} providers ready ({provider_display}){TerminalUI.RESET}")

            if "claude" in providers:
                print(f"  {TerminalUI.GREEN}   â”œâ”€ Claude API fallback enabled{TerminalUI.RESET}")
            if "prime_local" in providers:
                print(f"  {TerminalUI.GREEN}   â”œâ”€ Prime Local model ready{TerminalUI.RESET}")
            if "prime_cloud_run" in providers:
                print(f"  {TerminalUI.GREEN}   â””â”€ Prime Cloud Run ready{TerminalUI.RESET}")

        except ImportError as e:
            self.logger.warning(f"[v100.0] âš ï¸ Unified Model Serving import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Model Serving: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.warning(f"[v100.0] âš ï¸ Unified Model Serving initialization failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Model Serving: Failed to initialize{TerminalUI.RESET}")

    async def _initialize_unified_agent_registry(self) -> None:
        """
        v100.0: Initialize Unified Agent Registry - Distributed Service Discovery.

        This initializes the distributed agent registry that provides:
        1. Service discovery with capability-based routing
        2. Redis-backed state for multi-instance coordination
        3. Pub/sub for real-time agent status updates
        4. Circuit breaker for failing agents
        5. Load balancing with health-aware routing
        6. Automatic failover and recovery
        """
        self.logger.info("=" * 60)
        self.logger.info("[v100.0] Initializing Unified Agent Registry")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ“‹ Agent Registry: Initializing distributed registry...{TerminalUI.RESET}")

        try:
            from backend.core.registry import get_agent_registry

            self._unified_agent_registry = await get_agent_registry()

            # Get status
            metrics = self._unified_agent_registry.get_metrics()
            total_agents = metrics.get("total_registered", 0)
            redis_connected = metrics.get("redis_connected", False)
            is_leader = metrics.get("is_leader", False)

            self.logger.info(f"[v100.0] âœ… Unified Agent Registry initialized")
            self.logger.info(f"   Agents: {total_agents}, Redis: {redis_connected}, Leader: {is_leader}")

            # Display status
            mode = "distributed (Redis)" if redis_connected else "local only"
            leader_status = "leader" if is_leader else "follower"
            print(f"  {TerminalUI.GREEN}âœ“ Agent Registry: {total_agents} agents, {mode}, {leader_status}{TerminalUI.RESET}")

            if redis_connected:
                print(f"  {TerminalUI.GREEN}   â”œâ”€ Redis-backed state enabled{TerminalUI.RESET}")
                print(f"  {TerminalUI.GREEN}   â”œâ”€ Pub/sub for real-time updates{TerminalUI.RESET}")
                print(f"  {TerminalUI.GREEN}   â””â”€ Circuit breaker protection{TerminalUI.RESET}")
            else:
                # v93.14: Changed from YELLOW to DIM - Redis is optional, local mode is fine
                print(f"  {TerminalUI.DIM}   â””â”€ Local mode (Redis optional){TerminalUI.RESET}")

        except ImportError as e:
            self.logger.warning(f"[v100.0] âš ï¸ Unified Agent Registry import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Agent Registry: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.warning(f"[v100.0] âš ï¸ Unified Agent Registry initialization failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Agent Registry: Failed to initialize{TerminalUI.RESET}")

    async def _initialize_distributed_state_manager(self) -> None:
        """
        v100.0: Initialize Distributed State Manager - Transactional State Coordination.

        This initializes the unified state management layer that provides:
        1. Transactional state updates with atomicity guarantees
        2. Redis-backed distributed state with local fallback
        3. Leader election for coordination tasks
        4. State snapshots and recovery
        5. Pub/sub for state change notifications
        6. Conflict resolution for concurrent updates
        """
        self.logger.info("=" * 60)
        self.logger.info("[v100.0] Initializing Distributed State Manager")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ’¾ State Manager: Initializing distributed state...{TerminalUI.RESET}")

        try:
            from backend.core.state import get_state_manager

            self._distributed_state_manager = await get_state_manager()

            # Get status
            metrics = self._distributed_state_manager.get_metrics()
            redis_available = metrics.get("redis_available", False)
            is_leader = metrics.get("is_leader", False)
            mode = metrics.get("mode", "local")

            self.logger.info(f"[v100.0] âœ… Distributed State Manager initialized")
            self.logger.info(f"   Mode: {mode}, Leader: {is_leader}")

            # Display status
            leader_status = "leader" if is_leader else "follower"
            print(f"  {TerminalUI.GREEN}âœ“ State Manager: {mode} mode, {leader_status}{TerminalUI.RESET}")

            if redis_available:
                print(f"  {TerminalUI.GREEN}   â”œâ”€ Redis-backed distributed state{TerminalUI.RESET}")
                print(f"  {TerminalUI.GREEN}   â”œâ”€ Transactional updates enabled{TerminalUI.RESET}")
                print(f"  {TerminalUI.GREEN}   â””â”€ Leader election active{TerminalUI.RESET}")
            else:
                # v93.14: Changed from YELLOW to DIM - local mode is valid fallback
                print(f"  {TerminalUI.DIM}   â””â”€ Local mode (Redis optional){TerminalUI.RESET}")

        except ImportError as e:
            self.logger.warning(f"[v100.0] âš ï¸ Distributed State Manager import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ State Manager: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.warning(f"[v100.0] âš ï¸ Distributed State Manager initialization failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ State Manager: Failed to initialize{TerminalUI.RESET}")

    async def _initialize_continuous_learning_orchestrator(self) -> None:
        """
        v100.0: Initialize Continuous Learning Orchestrator - Unified Learning Pipeline.

        This initializes the continuous learning system that provides:
        1. Experience aggregation from all JARVIS components
        2. Intelligent training job scheduling with priority queues
        3. A/B testing with statistical significance analysis
        4. Model performance tracking with trend detection
        5. Cross-repo experience forwarding to Reactor Core
        6. Automatic model promotion/rollback based on performance
        """
        self.logger.info("=" * 60)
        self.logger.info("[v100.0] Initializing Continuous Learning Orchestrator")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ“š Learning Orchestrator: Initializing continuous learning...{TerminalUI.RESET}")

        try:
            from backend.intelligence.continuous_learning_orchestrator import (
                get_learning_orchestrator
            )

            self._continuous_learning_orchestrator = await get_learning_orchestrator()

            # Get status
            stats = await self._continuous_learning_orchestrator.get_stats()
            metrics = stats.get("metrics", {})
            experiences_collected = metrics.get("experiences_collected", 0)
            training_jobs_completed = metrics.get("training_jobs_completed", 0)
            auto_training = stats.get("aggregator", {}).get("buffer_size", 0)

            self.logger.info(f"[v100.0] âœ… Continuous Learning Orchestrator initialized")
            self.logger.info(f"   Experiences: {experiences_collected}, Jobs: {training_jobs_completed}")

            # Display status
            print(f"  {TerminalUI.GREEN}âœ“ Learning Orchestrator: ready{TerminalUI.RESET}")
            print(f"  {TerminalUI.GREEN}   â”œâ”€ Experience aggregation active{TerminalUI.RESET}")
            print(f"  {TerminalUI.GREEN}   â”œâ”€ Training scheduler ready{TerminalUI.RESET}")
            print(f"  {TerminalUI.GREEN}   â”œâ”€ A/B testing coordinator ready{TerminalUI.RESET}")
            print(f"  {TerminalUI.GREEN}   â””â”€ Performance tracking enabled{TerminalUI.RESET}")

            # Connect to Trinity systems for experience forwarding
            await self._connect_learning_to_trinity()

        except ImportError as e:
            self.logger.warning(f"[v100.0] âš ï¸ Continuous Learning Orchestrator import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Learning Orchestrator: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.warning(f"[v100.0] âš ï¸ Continuous Learning Orchestrator initialization failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Learning Orchestrator: Failed to initialize{TerminalUI.RESET}")

    async def _connect_learning_to_trinity(self) -> None:
        """
        Connect Continuous Learning Orchestrator to Trinity systems for cross-repo experience forwarding.
        """
        try:
            # Connect to Trinity Event Bus for experience streaming
            if self._trinity_event_bus:
                from backend.intelligence.continuous_learning_orchestrator import ExperienceType

                async def on_trinity_event(event):
                    """Forward Trinity events to learning orchestrator."""
                    if not self._continuous_learning_orchestrator:
                        return

                    event_type = event.get("type", "")
                    data = event.get("data", {})

                    # Map Trinity events to learning experiences
                    if "voice" in event_type or "auth" in event_type:
                        exp_type = ExperienceType.VOICE_AUTH
                    elif "inference" in event_type:
                        exp_type = ExperienceType.INFERENCE
                    elif "error" in event_type:
                        exp_type = ExperienceType.ERROR
                    else:
                        exp_type = ExperienceType.INTERACTION

                    await self._continuous_learning_orchestrator.collect_experience(
                        experience_type=exp_type,
                        input_data=data.get("input", {}),
                        output_data=data.get("output", {}),
                        quality_score=data.get("quality", 0.5),
                        confidence=data.get("confidence", 0.5),
                        success=data.get("success", True),
                        component=data.get("component", "trinity"),
                    )

                # Subscribe to Trinity events
                await self._trinity_event_bus.subscribe("jarvis.*", on_trinity_event)
                await self._trinity_event_bus.subscribe("prime.*", on_trinity_event)
                await self._trinity_event_bus.subscribe("reactor.*", on_trinity_event)

                self.logger.info("[v100.0] Connected learning orchestrator to Trinity Event Bus")

        except Exception as e:
            self.logger.warning(f"[v100.0] Failed to connect learning to Trinity: {e}")

    async def _initialize_neural_mesh_bridge(self) -> None:
        """
        v100.0: Initialize Neural Mesh Registry Bridge.

        Creates bidirectional synchronization between UnifiedAgentRegistry and
        Neural Mesh AgentRegistry for unified agent management.

        Features:
        - Automatic agent synchronization between registries
        - Data model translation (AgentInfo â†” NeuralMeshAgentInfo)
        - Unified capability queries across both systems
        - Cross-system event propagation
        - Health status harmonization
        """
        self.logger.info("=" * 60)
        self.logger.info("[v100.0] Initializing Neural Mesh Registry Bridge")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ”— Registry Bridge: Connecting registries...{TerminalUI.RESET}")

        try:
            from backend.core.registry.neural_mesh_bridge import get_registry_bridge

            self._neural_mesh_bridge = await get_registry_bridge()

            # Get metrics
            metrics = self._neural_mesh_bridge.get_metrics()
            unified_connected = metrics.get("unified_connected", False)
            mesh_connected = metrics.get("mesh_connected", False)

            status_parts = []
            if unified_connected:
                status_parts.append("UnifiedRegistry")
            if mesh_connected:
                status_parts.append("NeuralMesh")

            if status_parts:
                connections = " + ".join(status_parts)
                print(f"  {TerminalUI.GREEN}âœ… Registry Bridge: Connected to {connections}{TerminalUI.RESET}")
                self.logger.info(f"[v100.0] âœ… Neural Mesh Registry Bridge connected to: {connections}")

                # Perform initial sync
                sync_results = await self._neural_mesh_bridge.sync_all()
                total_synced = sum(r.agents_synced for r in sync_results)
                if total_synced > 0:
                    print(f"  {TerminalUI.GREEN}   â””â”€ Initial sync: {total_synced} agents synchronized{TerminalUI.RESET}")
                    self.logger.info(f"[v100.0] Initial sync: {total_synced} agents synchronized")
            else:
                print(f"  {TerminalUI.YELLOW}âš ï¸ Registry Bridge: No registries connected{TerminalUI.RESET}")
                self.logger.warning("[v100.0] âš ï¸ Neural Mesh Registry Bridge: No registries connected")

        except ImportError as e:
            self.logger.warning(f"[v100.0] âš ï¸ Neural Mesh Registry Bridge import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Registry Bridge: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.warning(f"[v100.0] âš ï¸ Neural Mesh Registry Bridge initialization failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Registry Bridge: Failed to initialize{TerminalUI.RESET}")

    async def _initialize_learning_state_connector(self) -> None:
        """
        v100.0: Initialize Learning State Connector.

        Connects ContinuousLearningOrchestrator with DistributedStateManager for:
        1. Training job state persistence across restarts
        2. A/B test state synchronization
        3. Experience buffer coordination
        4. Cross-instance training coordination via leader election
        """
        self.logger.info("=" * 60)
        self.logger.info("[v100.0] Initializing Learning State Connector")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ’¾ State Connector: Connecting learning systems...{TerminalUI.RESET}")

        try:
            from backend.intelligence.learning_state_connector import get_learning_state_connector

            self._learning_state_connector = await get_learning_state_connector()

            # Connect to learning orchestrator if available
            if self._continuous_learning_orchestrator:
                await self._learning_state_connector.connect_orchestrator(
                    self._continuous_learning_orchestrator
                )

            # Get metrics
            metrics = self._learning_state_connector.get_metrics()
            is_leader = metrics.get("is_leader", False)
            state_connected = metrics.get("state_manager_connected", False)

            status_parts = []
            if state_connected:
                status_parts.append("StateManager")
            if self._continuous_learning_orchestrator:
                status_parts.append("LearningOrchestrator")

            if status_parts:
                connections = " + ".join(status_parts)
                leader_status = " (training leader)" if is_leader else ""
                print(f"  {TerminalUI.GREEN}âœ… State Connector: Connected to {connections}{leader_status}{TerminalUI.RESET}")
                self.logger.info(f"[v100.0] âœ… Learning State Connector connected to: {connections}")
            else:
                print(f"  {TerminalUI.YELLOW}âš ï¸ State Connector: No systems connected{TerminalUI.RESET}")
                self.logger.warning("[v100.0] âš ï¸ Learning State Connector: No systems connected")

        except ImportError as e:
            self.logger.warning(f"[v100.0] âš ï¸ Learning State Connector import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ State Connector: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.warning(f"[v100.0] âš ï¸ Learning State Connector initialization failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ State Connector: Failed to initialize{TerminalUI.RESET}")

    async def _initialize_experience_forwarder(self) -> None:
        """
        v100.0: Initialize Cross-Repo Experience Forwarder.

        Forwards learning experiences to Reactor Core for:
        1. Distributed model training across repos
        2. Experience aggregation at scale
        3. Cross-repo model performance tracking
        4. Coordinated A/B testing
        """
        self.logger.info("=" * 60)
        self.logger.info("[v100.0] Initializing Cross-Repo Experience Forwarder")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ”„ Experience Forwarder: Connecting to Reactor Core...{TerminalUI.RESET}")

        try:
            from backend.intelligence.cross_repo_experience_forwarder import get_experience_forwarder

            self._experience_forwarder = await get_experience_forwarder()

            # Get metrics (async method requires await)
            metrics = await self._experience_forwarder.get_metrics()
            reactor_available = metrics.get("reactor_core_available", False)
            event_bus_connected = metrics.get("event_bus_connected", False)

            if reactor_available:
                transport = "EventBus" if event_bus_connected else "FileFallback"
                print(f"  {TerminalUI.GREEN}âœ… Experience Forwarder: Reactor Core connected via {transport}{TerminalUI.RESET}")
                self.logger.info(f"[v100.0] âœ… Experience Forwarder connected to Reactor Core via {transport}")

                # Connect to learning orchestrator for automatic forwarding
                if self._continuous_learning_orchestrator:
                    await self._connect_forwarder_to_learning()
            else:
                print(f"  {TerminalUI.YELLOW}âš ï¸ Experience Forwarder: Reactor Core not available (queuing enabled){TerminalUI.RESET}")
                self.logger.warning("[v100.0] âš ï¸ Experience Forwarder: Reactor Core not available")

        except ImportError as e:
            self.logger.warning(f"[v100.0] âš ï¸ Experience Forwarder import failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Experience Forwarder: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.warning(f"[v100.0] âš ï¸ Experience Forwarder initialization failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Experience Forwarder: Failed to initialize{TerminalUI.RESET}")

    async def _connect_forwarder_to_learning(self) -> None:
        """Connect the experience forwarder to the learning orchestrator."""
        try:
            if not self._continuous_learning_orchestrator or not self._experience_forwarder:
                return

            # Subscribe to experience collection events
            if hasattr(self._continuous_learning_orchestrator, 'on_experience_collected'):
                async def on_experience(exp_data: dict):
                    await self._experience_forwarder.forward_experience(
                        experience_type=exp_data.get("type", "interaction"),
                        input_data=exp_data.get("input", {}),
                        output_data=exp_data.get("output", {}),
                        quality_score=exp_data.get("quality_score", 0.5),
                        confidence=exp_data.get("confidence", 0.5),
                        success=exp_data.get("success", True),
                        component=exp_data.get("component", "learning_orchestrator"),
                        metadata=exp_data.get("metadata", {}),
                    )

                self._continuous_learning_orchestrator.on_experience_collected(on_experience)
                self.logger.info("[v100.0] Connected experience forwarder to learning orchestrator")

        except Exception as e:
            self.logger.warning(f"[v100.0] Failed to connect forwarder to learning: {e}")

    async def _initialize_unified_trinity_connector(self) -> None:
        """
        v101.0: Initialize the UnifiedTrinityConnector with Claude Code-like behaviors.

        This is the MASTER ORCHESTRATOR that connects:
        - Enhanced self-improvement with diff preview and approval
        - Cross-repo orchestration with Lamport clocks and dead letter queue
        - Real-time communication (voice + websocket + menu bar)
        - Session memory and atomic multi-file editing

        The connector bridges JARVIS Body, JARVIS Prime, and Reactor Core
        into a unified self-improving system with real-time user feedback.
        """
        enable_unified_connector = os.environ.get(
            "TRINITY_UNIFIED_CONNECTOR", "true"
        ).lower() in ("1", "true", "yes")

        if not enable_unified_connector:
            self.logger.info("[v101.0] UnifiedTrinityConnector disabled via TRINITY_UNIFIED_CONNECTOR=false")
            return

        try:
            self.logger.info("[v101.0] Initializing UnifiedTrinityConnector...")
            print(f"  {TerminalUI.CYAN}ğŸ”— Initializing Unified Trinity Connector...{TerminalUI.RESET}")

            # Get the Trinity connector
            connector = get_trinity_connector()

            # Get available communication channels
            websocket_manager = None
            voice_system = None
            menu_bar = None
            event_bus = None

            # Try to get WebSocket manager
            try:
                from backend.api.unified_websocket import get_websocket_manager
                websocket_manager = get_websocket_manager()
                self.logger.debug("[v101.0] WebSocket manager available")
            except Exception as e:
                self.logger.debug(f"[v101.0] WebSocket manager not available: {e}")

            # Try to get voice system from the narrator
            if hasattr(self, 'narrator') and self.narrator:
                voice_system = self.narrator
                self.logger.debug("[v101.0] Voice system available (narrator)")

            # Try to get menu bar indicator
            try:
                if hasattr(self, '_status_indicator') and self._status_indicator:
                    menu_bar = self._status_indicator
                    self.logger.debug("[v101.0] Menu bar indicator available")
            except Exception:
                pass

            # Try to get event bus
            try:
                if hasattr(self, '_event_bus') and self._event_bus:
                    event_bus = self._event_bus
                    self.logger.debug("[v101.0] Event bus available")
            except Exception:
                pass

            # Initialize the connector with all available channels
            success = await connector.initialize(
                websocket_manager=websocket_manager,
                voice_system=voice_system,
                menu_bar=menu_bar,
                event_bus=event_bus,
            )

            if success:
                self._unified_trinity_connector = connector
                status = connector.get_status()

                self.logger.info("[v101.0] âœ… UnifiedTrinityConnector initialized successfully")
                self.logger.info(f"[v101.0]    Session: {status.get('session_id', 'unknown')}")
                self.logger.info(f"[v101.0]    Repositories: JARVIS={status['repositories'].get('jarvis', False)}, "
                               f"Prime={status['repositories'].get('prime', False)}, "
                               f"Reactor={status['repositories'].get('reactor', False)}")

                # Log real-time communication status
                if 'cross_repo' in status:
                    enhanced = status['cross_repo'].get('enhanced', {})
                    if enhanced:
                        self.logger.info(f"[v101.0]    Lamport clock: {enhanced.get('lamport_clock', {}).get('node_id', 'unknown')}")
                        dlq = enhanced.get('dead_letter_queue', {})
                        self.logger.info(f"[v101.0]    Dead letter queue: {dlq.get('pending_events', 0)} pending")

                print(f"  {TerminalUI.GREEN}âœ“ UnifiedTrinityConnector: Claude Code-like behaviors enabled{TerminalUI.RESET}")
                print(f"  {TerminalUI.GREEN}  - Diff preview: enabled{TerminalUI.RESET}")
                print(f"  {TerminalUI.GREEN}  - Multi-file orchestration: enabled{TerminalUI.RESET}")
                print(f"  {TerminalUI.GREEN}  - Real-time voice: {'yes' if voice_system else 'no'}{TerminalUI.RESET}")
                print(f"  {TerminalUI.GREEN}  - WebSocket streaming: {'yes' if websocket_manager else 'no'}{TerminalUI.RESET}")

            else:
                self.logger.warning("[v101.0] UnifiedTrinityConnector initialization returned False")
                print(f"  {TerminalUI.YELLOW}âš ï¸ UnifiedTrinityConnector: Partial initialization{TerminalUI.RESET}")

        except Exception as e:
            self.logger.warning(f"[v101.0] UnifiedTrinityConnector initialization failed: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ UnifiedTrinityConnector: {e}{TerminalUI.RESET}")
            import traceback
            self.logger.debug(traceback.format_exc())

    async def _initialize_v80_cross_repo_system(
        self,
        jprime_online: bool = False,
        reactor_online: bool = False
    ) -> None:
        """
        v80.0: Initialize Advanced Cross-Repo Loading System.

        Features:
        - CrossRepoHealthMonitor: Circuit breakers, adaptive health check intervals,
          trend analysis, automatic recovery detection
        - TrinityStartupCoordinator: Parallel startup orchestration, dependency
          resolution, real-time progress broadcasting, graceful degradation
        - Dynamic repo discovery: Environment-based configuration, no hardcoding
        - Cross-repo event propagation: Real-time status updates across all components

        Architecture:
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  v80.0 Advanced Cross-Repo Loading System                       â”‚
            â”‚  â”œâ”€â”€ CrossRepoHealthMonitor (circuit breakers, adaptive)        â”‚
            â”‚  â”‚   â”œâ”€â”€ Parallel async health checks for all repos             â”‚
            â”‚  â”‚   â”œâ”€â”€ Circuit breaker: 3 failures â†’ open, 30s recovery       â”‚
            â”‚  â”‚   â”œâ”€â”€ Adaptive intervals: 10s healthy, 5s degraded, 2s crit  â”‚
            â”‚  â”‚   â””â”€â”€ Trend analysis: 5-sample window, slope detection       â”‚
            â”‚  â””â”€â”€ TrinityStartupCoordinator (parallel startup)               â”‚
            â”‚       â”œâ”€â”€ Phase 1: Infrastructure (dirs, Cloud SQL)             â”‚
            â”‚       â”œâ”€â”€ Phase 2: JARVIS Body (must complete first)            â”‚
            â”‚       â”œâ”€â”€ Phase 3: J-Prime + Reactor-Core (parallel)            â”‚
            â”‚       â”œâ”€â”€ Phase 4: Trinity Sync (after all online)              â”‚
            â”‚       â””â”€â”€ Phase 5: Finalization                                 â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        try:
            self.logger.info("[v80.0] Initializing Advanced Cross-Repo Loading System")
            print(f"  {TerminalUI.CYAN}ğŸ”— [v80.0] Starting Cross-Repo Health Monitor...{TerminalUI.RESET}")

            # Import the v80.0 components from advanced_startup_orchestrator
            try:
                from backend.core.advanced_startup_orchestrator import (
                    CrossRepoHealthMonitor,
                    TrinityStartupCoordinator,
                    get_health_monitor,
                    get_startup_coordinator,
                )
            except ImportError as e:
                self.logger.warning(f"[v80.0] Import failed (non-critical): {e}")
                return

            # Initialize CrossRepoHealthMonitor with circuit breakers
            try:
                self._cross_repo_health_monitor = await get_health_monitor()

                # Set initial component states based on current online status
                if jprime_online:
                    self._cross_repo_health_monitor._health_cache["j_prime"] = {
                        "status": "healthy",
                        "latency_ms": 0,
                        "last_check": asyncio.get_event_loop().time(),
                        "details": {"pre_verified": True},
                    }
                if reactor_online:
                    self._cross_repo_health_monitor._health_cache["reactor_core"] = {
                        "status": "healthy",
                        "latency_ms": 0,
                        "last_check": asyncio.get_event_loop().time(),
                        "details": {"pre_verified": True},
                    }

                self.logger.info("[v80.0] âœ… CrossRepoHealthMonitor started")
                print(f"  {TerminalUI.GREEN}âœ“ [v80.0] Health Monitor: Circuit breakers active{TerminalUI.RESET}")

                # Register health change callback for voice announcements
                async def on_health_change(component: str, old_status: str, new_status: str):
                    """Voice announce significant health changes."""
                    if old_status != new_status:
                        if new_status == "critical":
                            msg = f"{component.replace('_', ' ').title()} is experiencing issues. Checking automatically."
                            if self.narrator and self.config.voice_enabled:
                                await self.narrator.speak(msg, wait=False)
                        elif new_status == "healthy" and old_status in ("degraded", "critical"):
                            msg = f"{component.replace('_', ' ').title()} has recovered."
                            if self.narrator and self.config.voice_enabled:
                                await self.narrator.speak(msg, wait=False)

                self._cross_repo_health_monitor.register_health_callback(on_health_change)

            except Exception as e:
                self.logger.warning(f"[v80.0] Health monitor init failed: {e}")

            # Initialize TrinityStartupCoordinator (for future restarts/orchestration)
            try:
                self._trinity_startup_coordinator = await get_startup_coordinator()

                # Register progress callback for loading page updates
                async def on_progress_update(phase: str, progress: float, message: str, details: dict):
                    """Broadcast progress to loading page."""
                    await self._broadcast_startup_progress(
                        stage=f"trinity_{phase}",
                        message=message,
                        progress=int(progress),
                        metadata={
                            "trinity_phase": phase,
                            "v80_enabled": True,
                            **details,
                        },
                    )

                self._trinity_startup_coordinator.register_progress_callback(on_progress_update)

                self.logger.info("[v80.0] âœ… TrinityStartupCoordinator initialized")
                print(f"  {TerminalUI.GREEN}âœ“ [v80.0] Startup Coordinator: Parallel orchestration ready{TerminalUI.RESET}")

            except Exception as e:
                self.logger.warning(f"[v80.0] Startup coordinator init failed: {e}")

            # Aggregate stats for status display
            health_stats = {}
            if self._cross_repo_health_monitor:
                health_stats = await self._cross_repo_health_monitor.get_aggregate_health()

            self.logger.info(f"[v80.0] âœ… Cross-Repo Loading System initialized")
            self.logger.info(f"   â€¢ Health Monitor: Active")
            self.logger.info(f"   â€¢ Startup Coordinator: Ready")
            self.logger.info(f"   â€¢ Aggregate Health: {health_stats.get('overall_status', 'unknown')}")

            # Broadcast v80.0 status to loading server
            await self._broadcast_startup_progress(
                stage="v80_cross_repo_init",
                message="Advanced Cross-Repo Loading System initialized",
                progress=88,
                metadata={
                    "v80_enabled": True,
                    "health_monitor_active": self._cross_repo_health_monitor is not None,
                    "startup_coordinator_ready": self._trinity_startup_coordinator is not None,
                    "circuit_breakers": ["jarvis", "j_prime", "reactor_core"],
                    "aggregate_health": health_stats,
                },
            )

        except Exception as e:
            self.logger.warning(f"[v80.0] Cross-Repo Loading System init failed: {e}")
            self._cross_repo_health_monitor = None
            self._trinity_startup_coordinator = None

    async def _handle_trinity_voice_event(
        self,
        event_type: str,
        details: dict
    ) -> None:
        """
        v79.1: Handle voice events from Trinity components.

        This method receives voice events from J-Prime and Reactor Core
        and speaks them through the local voice system.
        """
        try:
            source = details.get('source_repo', 'unknown')
            message = details.get('message', '')
            priority = details.get('priority', 'medium')

            if message:
                self.logger.info(f"[v79.1] Trinity voice event: [{source}] {message[:50]}...")

                # Speak via narrator
                if self.narrator:
                    await self.narrator.speak(message, wait=False)

        except Exception as e:
            self.logger.debug(f"[v79.1] Trinity voice event error: {e}")

    async def _broadcast_trinity_status(self) -> bool:
        """
        Broadcast PROJECT TRINITY status to the loading server.

        This updates the frontend loading page with the current Trinity state,
        showing which components are online and the connection status.
        """
        if not self._loading_server_process:
            return False

        try:
            from pathlib import Path
            import json
            import time

            trinity_dir = Path.home() / ".jarvis" / "trinity"

            # Gather component states
            jprime_online = False
            reactor_online = False

            # v78.1: Support both naming conventions for backwards compatibility
            jprime_candidates = [
                trinity_dir / "components" / "jarvis_prime.json",
                trinity_dir / "components" / "j_prime.json",
            ]
            for jprime_file in jprime_candidates:
                if jprime_file.exists():
                    try:
                        with open(jprime_file) as f:
                            state = json.load(f)
                        if time.time() - state.get("timestamp", 0) < 30:
                            jprime_online = True
                            break  # Found valid heartbeat, no need to check other files
                    except Exception:
                        pass

            reactor_file = trinity_dir / "components" / "reactor_core.json"
            if reactor_file.exists():
                try:
                    with open(reactor_file) as f:
                        state = json.load(f)
                    if time.time() - state.get("timestamp", 0) < 30:
                        reactor_online = True
                except Exception:
                    pass

            # Build status
            components_online = 1 + (1 if jprime_online else 0) + (1 if reactor_online else 0)
            mode = "distributed" if components_online == 3 else "partial" if components_online > 1 else "standalone"

            trinity_status = {
                "initialized": self._trinity_initialized,
                "instance_id": self._trinity_instance_id,
                "components": {
                    "jarvis_body": {"online": True, "role": "Execution"},
                    "j_prime": {"online": jprime_online, "role": "Cognition"},
                    "reactor_core": {"online": reactor_online, "role": "Learning"},
                },
                "components_online": components_online,
                "total_components": 3,
                "mode": mode,
            }

            # v72.0: Enhanced voice narration based on Trinity mode
            if self.config.voice_enabled and self._trinity_initialized:
                if components_online == 3:
                    # Full distributed mode - all three components online
                    await self.narrator.speak(
                        "PROJECT TRINITY fully connected. Mind, Body, and Nerves synchronized.",
                        wait=False,
                    )
                elif components_online == 2:
                    # Partial mode - determine which component is missing
                    missing = []
                    if not jprime_online:
                        missing.append("Mind")
                    if not reactor_online:
                        missing.append("Nerves")
                    await self.narrator.speak(
                        f"PROJECT TRINITY partially connected. {components_online} of 3 components online. "
                        f"Missing: {', '.join(missing)}.",
                        wait=False,
                    )
                # Note: If only 1 component (standalone mode), the initial Trinity announcement is sufficient

            # Broadcast
            return await self._broadcast_startup_progress(
                stage="trinity_status",
                message=f"PROJECT TRINITY: {components_online}/3 components online",
                progress=90 if self._trinity_initialized else 85,
                metadata={"trinity": trinity_status},
            )

        except Exception as e:
            self.logger.debug(f"Trinity status broadcast failed: {e}")
            return False

    # =========================================================================
    # v72.0: UNIFIED TRINITY LAUNCH PROTOCOL
    # =========================================================================
    # These methods enable "one-command" startup of all three Trinity repos.
    # Running `python3 run_supervisor.py` automatically launches:
    #   - JARVIS Body (this process)
    #   - J-Prime Mind (subprocess)
    #   - Reactor-Core Nerves (subprocess)
    # =========================================================================

    async def _launch_trinity_components(self) -> None:
        """
        v100.0: Ultra-robust Trinity component launch system.

        This enables true "one-command" startup:
        python3 run_supervisor.py â†’ Launches all 3 repos automatically

        Architecture:
        - JARVIS Body: Already running (this process)
        - J-Prime Mind: Launched as subprocess (trinity_bridge.py or server.py)
        - Reactor-Core Nerves: Launched as subprocess (trinity_orchestrator.py)

        v100.0 Features:
        - Dynamic repo discovery (no hardcoded paths)
        - Robust venv detection (handles all Python environments)
        - Circuit breakers per component (prevent cascade failures)
        - Retry logic with exponential backoff and jitter
        - W3C distributed tracing (correlation IDs across repos)
        - Graceful degradation (continue if optional components fail)
        - Adaptive timeouts (based on system load)
        - Proper file handle management (context managers)
        - Health verification before declaring success
        """
        # Get ultra-robust Trinity config
        trinity_config = get_trinity_config()

        if not trinity_config.trinity_enabled:
            self.logger.info("â„¹ï¸ Trinity disabled - skipping component launch")
            return

        if not trinity_config.trinity_auto_launch:
            self.logger.info("â„¹ï¸ Trinity auto-launch disabled - components must be started manually")
            return

        # Create trace context for distributed tracing
        trace_ctx = TrinityTraceContext(
            component="supervisor",
            operation="trinity_launch",
            metadata={"instance_id": trinity_config.trinity_instance_id}
        )

        try:
            self.logger.info("=" * 60)
            self.logger.info(f"PROJECT TRINITY v100.0: Launching Distributed Components")
            self.logger.info(f"Trace ID: {trace_ctx.trace_id}")
            self.logger.info("=" * 60)

            print(f"  {TerminalUI.CYAN}ğŸš€ Launching Trinity components (v100.0)...{TerminalUI.RESET}")

            # Broadcast launch start
            await self._broadcast_startup_progress(
                stage="trinity_launch",
                message="Launching Trinity distributed components...",
                progress=88,
                metadata={
                    "trinity_launch": "starting",
                    "trace_id": trace_ctx.trace_id,
                    "version": "100.0"
                },
            )

            # v73.0: Pre-flight zombie reaper - kill any orphaned processes
            await self._reap_zombie_port_holders_v100(trinity_config)

            # v100.0: Initialize ultra-robust launch components
            repo_discovery = DynamicRepoDiscovery(trinity_config)
            venv_detector = RobustVenvDetector(trinity_config)
            retry_handler = RetryWithBackoff(trinity_config)

            # Initialize circuit breakers per component
            jprime_circuit_breaker = TrinityCircuitBreaker("jprime", trinity_config)
            reactor_circuit_breaker = TrinityCircuitBreaker("reactor_core", trinity_config)

            # v100.0: Discover repos dynamically
            jprime_path = await repo_discovery.discover_jprime()
            reactor_core_path = await repo_discovery.discover_reactor_core()

            self.logger.info(f"   ğŸ“ J-Prime path: {jprime_path or 'Not found'}")
            self.logger.info(f"   ğŸ“ Reactor-Core path: {reactor_core_path or 'Not found'}")

            # v100.0: Launch with retry logic and circuit breakers
            async def launch_jprime_with_retry():
                if not jprime_path:
                    if trinity_config.jprime_optional:
                        self.logger.info("   âš ï¸ J-Prime not found, but optional - continuing")
                        return None
                    raise FileNotFoundError(f"J-Prime repo not found in search paths")

                if not jprime_circuit_breaker.can_execute():
                    self.logger.warning("   âš ï¸ J-Prime circuit breaker OPEN - skipping")
                    return None

                child_trace = trace_ctx.create_child_span("jprime", "launch")

                success, result, exceptions = await retry_handler.execute(
                    operation=lambda: self._launch_jprime_orchestrator_v100(
                        jprime_path, venv_detector, trinity_config, child_trace
                    ),
                    operation_name="jprime_launch",
                )

                if success:
                    jprime_circuit_breaker.record_success()
                    return result
                else:
                    jprime_circuit_breaker.record_failure()
                    if trinity_config.jprime_optional and trinity_config.continue_on_partial_failure:
                        self.logger.warning(f"   âš ï¸ J-Prime failed but optional: {exceptions[-1] if exceptions else 'Unknown'}")
                        return None
                    raise exceptions[-1] if exceptions else Exception("J-Prime launch failed")

            async def launch_reactor_with_retry():
                if not reactor_core_path:
                    if trinity_config.reactor_core_optional:
                        self.logger.info("   âš ï¸ Reactor-Core not found, but optional - continuing")
                        return None
                    raise FileNotFoundError(f"Reactor-Core repo not found in search paths")

                if not reactor_circuit_breaker.can_execute():
                    self.logger.warning("   âš ï¸ Reactor-Core circuit breaker OPEN - skipping")
                    return None

                child_trace = trace_ctx.create_child_span("reactor_core", "launch")

                success, result, exceptions = await retry_handler.execute(
                    operation=lambda: self._launch_reactor_core_orchestrator_v100(
                        reactor_core_path, venv_detector, trinity_config, child_trace
                    ),
                    operation_name="reactor_core_launch",
                )

                if success:
                    reactor_circuit_breaker.record_success()
                    return result
                else:
                    reactor_circuit_breaker.record_failure()
                    if trinity_config.reactor_core_optional and trinity_config.continue_on_partial_failure:
                        self.logger.warning(f"   âš ï¸ Reactor-Core failed but optional: {exceptions[-1] if exceptions else 'Unknown'}")
                        return None
                    raise exceptions[-1] if exceptions else Exception("Reactor-Core launch failed")

            # v100.0: Launch in parallel with proper error handling
            jprime_task = asyncio.create_task(launch_jprime_with_retry())
            reactor_task = asyncio.create_task(launch_reactor_with_retry())

            # Wait for both with timeout
            try:
                results = await asyncio.wait_for(
                    asyncio.gather(jprime_task, reactor_task, return_exceptions=True),
                    timeout=trinity_config.launch_timeout_sec
                )
            except asyncio.TimeoutError:
                self.logger.warning(f"   âš ï¸ Trinity launch timeout ({trinity_config.launch_timeout_sec}s)")
                jprime_task.cancel()
                reactor_task.cancel()
                results = [None, None]

            # Log results with trace context
            for i, result in enumerate(results):
                component_name = "J-Prime" if i == 0 else "Reactor-Core"
                if isinstance(result, Exception):
                    self.logger.warning(f"   âŒ {component_name}: {result}")
                elif result is not None:
                    self.logger.info(f"   âœ… {component_name}: Launched successfully")

            # v16.0: Wait for J-Prime to be ready using ServiceReadinessChecker
            # This replaces the static sleep with a dynamic readiness check
            if self._jprime_orchestrator_process is not None:
                jprime_url = os.getenv("JARVIS_PRIME_URL", "http://localhost:8000")
                self.logger.info(f"   â³ Waiting for J-Prime to be ready at {jprime_url}...")
                print(f"  {TerminalUI.CYAN}â³ Waiting for J-Prime to be ready...{TerminalUI.RESET}")

                try:
                    from backend.core.ouroboros.integration import (
                        ServiceReadinessChecker,
                        ServiceReadinessLevel,
                    )

                    jprime_checker = ServiceReadinessChecker(
                        service_name="jarvis_prime",
                        base_url=jprime_url,
                        health_check_timeout=3.0,
                    )

                    jprime_ready = await jprime_checker.wait_for_ready(
                        timeout=float(trinity_config.registration_timeout_sec),
                        min_level=ServiceReadinessLevel.DEGRADED,
                    )

                    if jprime_ready:
                        snapshot = jprime_checker.last_health_snapshot
                        if snapshot and snapshot.available_models:
                            self.logger.info(f"   âœ… J-Prime ready with {len(snapshot.available_models)} models")
                            print(f"  {TerminalUI.GREEN}âœ… J-Prime ready ({len(snapshot.available_models)} models available){TerminalUI.RESET}")
                        else:
                            self.logger.info("   âœ… J-Prime ready")
                            print(f"  {TerminalUI.GREEN}âœ… J-Prime ready{TerminalUI.RESET}")
                    else:
                        self.logger.warning("   âš ï¸ J-Prime not ready - continuing with degraded mode")
                        print(f"  {TerminalUI.YELLOW}âš ï¸ J-Prime not ready - other components will use fallbacks{TerminalUI.RESET}")

                except ImportError:
                    # Fallback to static sleep if ServiceReadinessChecker not available
                    self.logger.debug("   ServiceReadinessChecker not available, using static wait")
                    await asyncio.sleep(min(trinity_config.registration_timeout_sec, 10.0))
                except Exception as e:
                    self.logger.warning(f"   J-Prime readiness check error: {e}, using static wait")
                    await asyncio.sleep(min(trinity_config.registration_timeout_sec, 10.0))
            else:
                # No J-Prime process - just wait briefly for other components
                await asyncio.sleep(min(trinity_config.registration_timeout_sec, 5.0))

            # v100.0: Verify health before declaring success
            if trinity_config.health_monitor_enabled:
                await self._verify_trinity_health_v100(trinity_config, trace_ctx)

            # Re-check Trinity status
            await self._broadcast_trinity_status()

            # Count launched components
            components_launched = 0
            if self._jprime_orchestrator_process is not None:
                components_launched += 1
            if self._reactor_core_orchestrator_process is not None:
                components_launched += 1

            # Voice announcement
            if self.config.voice_enabled and components_launched > 0:
                if components_launched == 2:
                    await self.narrator.speak(
                        "Trinity components launched. Mind, Body, and Nerves synchronizing.",
                        wait=False,
                    )
                else:
                    await self.narrator.speak(
                        f"Trinity launch complete. {components_launched + 1} of 3 components online.",
                        wait=False,
                    )

            self.logger.info(f"âœ… Trinity v100.0 launch complete ({components_launched} launched)")
            self.logger.info(f"   Trace duration: {trace_ctx.duration_ms():.2f}ms")

            # Broadcast launch complete
            await self._broadcast_startup_progress(
                stage="trinity_launch_complete",
                message=f"Trinity components launched: {components_launched + 1}/3 online",
                progress=89,
                metadata={
                    "trinity_launch": "complete",
                    "components_launched": components_launched,
                    "trace_id": trace_ctx.trace_id,
                    "duration_ms": trace_ctx.duration_ms()
                },
            )

            # v75.0: Start Trinity Health Monitor
            await self._start_trinity_health_monitor()

            # v95.0: Start Direct Subprocess Health Monitor (complements heartbeat monitoring)
            await self._start_subprocess_health_monitor()

            # v17.0: Start Service Supervisor for auto-restart of dead services
            await self._initialize_service_supervisor()

            # v100.0: Initialize Unified AGI Orchestrator
            await self._initialize_agi_orchestrator()

            # v100.0: Initialize Unified Model Serving (Prime + Claude fallback)
            if self._model_serving_enabled:
                await self._initialize_unified_model_serving()

            # v100.0: Initialize Unified Agent Registry (Redis-backed)
            if self._agent_registry_enabled:
                await self._initialize_unified_agent_registry()

            # v100.0: Initialize Distributed State Manager
            if self._state_manager_enabled:
                await self._initialize_distributed_state_manager()

            # v100.0: Initialize Continuous Learning Orchestrator
            if self._continuous_learning_enabled:
                await self._initialize_continuous_learning_orchestrator()

        except Exception as e:
            self.logger.warning(f"âš ï¸ Trinity component launch failed: {e}")
            self.logger.warning(f"   Trace ID: {trace_ctx.trace_id}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Some Trinity components failed to launch{TerminalUI.RESET}")

            # Graceful degradation: continue if configured
            if trinity_config.continue_on_partial_failure:
                self.logger.info("   Continuing in degraded mode...")
            else:
                raise

    async def _reap_zombie_port_holders(self) -> None:
        """
        v73.0: Pre-flight zombie reaper - kill orphaned processes holding Trinity ports.

        The Problem:
            If you hard-crash or force-kill JARVIS, the cleanup script won't run.
            J-Prime or Reactor-Core processes may stay alive as "zombies" holding ports.
            When you restart, Trinity launch fails because "Port 8000 already in use".

        The Solution:
            Before launching, scan for any process holding Trinity ports and terminate it
            if it belongs to a previous JARVIS session (not the current one).

        Protected Ports:
            - 8000: J-Prime default server port
            - 8002: J-Prime alternate port
            - 8090: Reactor-Core API port
        """
        import psutil

        # Trinity component ports to check
        zombie_ports = {
            8000: "J-Prime",
            8002: "J-Prime",
            8090: "Reactor-Core",
        }

        current_pid = os.getpid()
        reaped_count = 0

        for port, name in zombie_ports.items():
            try:
                # Find processes with connections on this port
                for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                    try:
                        # Skip current process and its children
                        if proc.pid == current_pid:
                            continue

                        # Check if this process has connections on the port
                        connections = proc.connections()
                        for conn in connections:
                            if hasattr(conn, 'laddr') and conn.laddr and conn.laddr.port == port:
                                # Found a zombie holding our port
                                proc_name = proc.name()
                                proc_cmdline = ' '.join(proc.cmdline()[:3]) if proc.cmdline() else ''

                                # Verify it's a Trinity-related process (not something else)
                                trinity_patterns = [
                                    'jarvis', 'jprime', 'j_prime', 'reactor',
                                    'trinity', 'python', 'uvicorn'
                                ]
                                is_trinity = any(
                                    p.lower() in proc_cmdline.lower() or p.lower() in proc_name.lower()
                                    for p in trinity_patterns
                                )

                                if is_trinity:
                                    self.logger.warning(
                                        f"ğŸ§Ÿ Found zombie {name} on port {port} "
                                        f"(PID: {proc.pid}, cmd: {proc_cmdline[:50]})"
                                    )
                                    print(
                                        f"  {TerminalUI.YELLOW}ğŸ§Ÿ Killing zombie {name} "
                                        f"(PID: {proc.pid}) on port {port}{TerminalUI.RESET}"
                                    )

                                    # Graceful termination first
                                    proc.terminate()
                                    await asyncio.sleep(1.0)

                                    # Force kill if still running
                                    if proc.is_running():
                                        self.logger.warning(f"   Force killing stubborn zombie PID {proc.pid}")
                                        proc.kill()

                                    reaped_count += 1
                                break  # One match per port is enough

                    except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                        continue

            except Exception as e:
                self.logger.debug(f"   Zombie check for port {port} failed: {e}")

        if reaped_count > 0:
            self.logger.info(f"   ğŸ§¹ Reaped {reaped_count} zombie process(es)")
            # Give the OS time to release the ports
            await asyncio.sleep(0.5)
        else:
            self.logger.debug("   No zombie processes found")

    async def _launch_jprime_orchestrator(self) -> None:
        """
        v72.0: Launch J-Prime orchestrator (Mind component).

        Attempts to launch in order of preference:
        1. jarvis_prime/server.py (full FastAPI server)
        2. jarvis_prime/core/trinity_bridge.py (Trinity bridge module)

        The launched process will:
        - Initialize Trinity connection
        - Start heartbeat broadcasting to ~/.jarvis/trinity/
        - Enable cognitive commands to JARVIS Body

        Features:
        - Skips launch if heartbeat detected (already running)
        - Uses repo's venv if available, falls back to system Python
        - Logs to ~/.jarvis/logs/services/jprime_*.log
        - Runs with PYTHONPATH set to repo root
        """
        if not self._jprime_repo_path.exists():
            self.logger.warning(f"âš ï¸ J-Prime repo not found: {self._jprime_repo_path}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ J-Prime: Repo not found{TerminalUI.RESET}")
            return

        # Check if already running (via heartbeat)
        # v78.1: Support both naming conventions for backwards compatibility
        trinity_dir = Path.home() / ".jarvis" / "trinity"
        components_dir = trinity_dir / "components"

        # Check for either jarvis_prime.json (new) or j_prime.json (legacy)
        jprime_state_files = [
            components_dir / "jarvis_prime.json",
            components_dir / "j_prime.json",
        ]

        for jprime_state_file in jprime_state_files:
            if jprime_state_file.exists():
                try:
                    import json
                    import time as time_module
                    with open(jprime_state_file) as f:
                        state = json.load(f)
                    heartbeat_age = time_module.time() - state.get("timestamp", 0)
                    if heartbeat_age < 30:
                        # v100.1: FIX RACE CONDITION - Validate process is actually running
                        pid = state.get("pid")
                        if pid:
                            try:
                                import psutil
                                proc = psutil.Process(pid)
                                if proc.is_running() and proc.status() != psutil.STATUS_ZOMBIE:
                                    # Process exists and is not zombie
                                    self.logger.info(f"   ğŸ§  J-Prime already running (heartbeat: {jprime_state_file.name}, PID: {pid})")
                                    print(f"  {TerminalUI.GREEN}âœ“ J-Prime: Already running (heartbeat: {heartbeat_age:.1f}s ago, PID: {pid}){TerminalUI.RESET}")
                                    return
                                else:
                                    self.logger.warning(f"   J-Prime PID {pid} exists but is zombie/stopped - relaunching")
                            except (ImportError, Exception) as e:
                                # psutil not available or process doesn't exist
                                # Fall back to os.kill check
                                try:
                                    os.kill(pid, 0)  # Just checks if process exists
                                    self.logger.info(f"   ğŸ§  J-Prime already running (heartbeat: {jprime_state_file.name}, PID: {pid})")
                                    print(f"  {TerminalUI.GREEN}âœ“ J-Prime: Already running (heartbeat: {heartbeat_age:.1f}s ago){TerminalUI.RESET}")
                                    return
                                except OSError:
                                    self.logger.warning(f"   J-Prime PID {pid} no longer exists - relaunching")
                        else:
                            # No PID in state but heartbeat recent - trust it for now
                            self.logger.info(f"   ğŸ§  J-Prime appears running (heartbeat: {jprime_state_file.name})")
                            print(f"  {TerminalUI.GREEN}âœ“ J-Prime: Already running (heartbeat: {heartbeat_age:.1f}s ago){TerminalUI.RESET}")
                            return
                except Exception as e:
                    self.logger.debug(f"   Could not read J-Prime state from {jprime_state_file}: {e}")

        # Find Python executable (prefer venv)
        venv_python = self._jprime_repo_path / "venv" / "bin" / "python3"
        if not venv_python.exists():
            venv_python = self._jprime_repo_path / "venv" / "bin" / "python"
        python_cmd = str(venv_python) if venv_python.exists() else sys.executable

        # Define launch scripts in order of preference
        launch_scripts = [
            ("jarvis_prime/server.py", "FastAPI Server"),
            ("run_server.py", "Server Runner"),
            ("jarvis_prime/core/trinity_bridge.py", "Trinity Bridge"),
        ]

        # Try each launch script
        launched = False
        for script_rel, description in launch_scripts:
            script_path = self._jprime_repo_path / script_rel
            if script_path.exists():
                self.logger.info(f"   ğŸš€ Launching J-Prime ({description})...")
                print(f"  {TerminalUI.CYAN}ğŸš€ Launching J-Prime ({description})...{TerminalUI.RESET}")

                # Create log directory
                log_dir = Path.home() / ".jarvis" / "logs" / "services"
                log_dir.mkdir(parents=True, exist_ok=True)
                stdout_log = log_dir / "jprime_stdout.log"
                stderr_log = log_dir / "jprime_stderr.log"

                try:
                    # Build environment with PYTHONPATH
                    env = os.environ.copy()
                    env["PYTHONPATH"] = str(self._jprime_repo_path)
                    env["TRINITY_ENABLED"] = "true"

                    # v100.1: FIX FILE DESCRIPTOR LEAK
                    # Store file handles as instance variables for proper cleanup
                    # These will be closed in _cleanup_trinity_components()
                    self._jprime_stdout_file = open(stdout_log, "w")
                    self._jprime_stderr_file = open(stderr_log, "w")

                    # Launch subprocess
                    self._jprime_orchestrator_process = await asyncio.create_subprocess_exec(
                        python_cmd,
                        str(script_path),
                        cwd=str(self._jprime_repo_path),
                        env=env,
                        stdout=self._jprime_stdout_file,
                        stderr=self._jprime_stderr_file,
                        start_new_session=True,  # Detach from parent process group
                    )

                    self.logger.info(f"   âœ… J-Prime launched (PID: {self._jprime_orchestrator_process.pid})")
                    self.logger.info(f"   ğŸ“„ Logs: {stdout_log}")
                    print(f"  {TerminalUI.GREEN}âœ“ J-Prime launched (PID: {self._jprime_orchestrator_process.pid}){TerminalUI.RESET}")

                    # v116.0: Register process in GlobalProcessRegistry for SIGHUP protection
                    try:
                        from backend.core.supervisor_singleton import GlobalProcessRegistry
                        GlobalProcessRegistry.register(
                            self._jprime_orchestrator_process.pid,
                            component="jarvis-prime",
                            port=8000
                        )
                        self.logger.debug(f"   [v116.0] J-Prime PID registered in GlobalProcessRegistry")
                    except Exception as reg_err:
                        self.logger.debug(f"   [v116.0] Could not register J-Prime PID: {reg_err}")

                    launched = True
                    break

                except Exception as e:
                    # v100.1: Clean up file handles on error
                    if hasattr(self, '_jprime_stdout_file') and self._jprime_stdout_file:
                        self._jprime_stdout_file.close()
                    if hasattr(self, '_jprime_stderr_file') and self._jprime_stderr_file:
                        self._jprime_stderr_file.close()
                    self.logger.warning(f"   Failed to launch J-Prime: {e}")
                    print(f"  {TerminalUI.YELLOW}âš ï¸ J-Prime launch failed: {e}{TerminalUI.RESET}")

        if not launched:
            self.logger.warning(f"   âš ï¸ No J-Prime launch script found in {self._jprime_repo_path}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ J-Prime: No launch script found{TerminalUI.RESET}")

    async def _launch_reactor_core_orchestrator(self) -> None:
        """
        v72.0: Launch Reactor-Core orchestrator (Nerves component).

        Attempts to launch in order of preference:
        1. reactor_core/orchestration/trinity_orchestrator.py (Trinity orchestrator)
        2. A standalone runner script

        The launched process will:
        - Initialize the Trinity Orchestrator singleton
        - Start health monitoring and command processing
        - Enable cross-repo command routing

        Features:
        - Skips launch if heartbeat detected (already running)
        - Uses repo's venv if available, falls back to system Python
        - Logs to ~/.jarvis/logs/services/reactor_core_*.log
        - Runs with PYTHONPATH set to repo root
        """
        if not self._reactor_core_repo_path.exists():
            self.logger.warning(f"âš ï¸ Reactor-Core repo not found: {self._reactor_core_repo_path}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Reactor-Core: Repo not found{TerminalUI.RESET}")
            return

        # Check if already running (via heartbeat)
        trinity_dir = Path.home() / ".jarvis" / "trinity"
        reactor_state_file = trinity_dir / "components" / "reactor_core.json"

        if reactor_state_file.exists():
            try:
                import json
                import time as time_module
                with open(reactor_state_file) as f:
                    state = json.load(f)
                heartbeat_age = time_module.time() - state.get("timestamp", 0)
                if heartbeat_age < 30:
                    # v100.1: FIX RACE CONDITION - Validate process is actually running
                    pid = state.get("pid")
                    if pid:
                        try:
                            import psutil
                            proc = psutil.Process(pid)
                            if proc.is_running() and proc.status() != psutil.STATUS_ZOMBIE:
                                self.logger.info(f"   âš¡ Reactor-Core already running (heartbeat detected, PID: {pid})")
                                print(f"  {TerminalUI.GREEN}âœ“ Reactor-Core: Already running (heartbeat: {heartbeat_age:.1f}s ago, PID: {pid}){TerminalUI.RESET}")
                                return
                            else:
                                self.logger.warning(f"   Reactor-Core PID {pid} exists but is zombie/stopped - relaunching")
                        except (ImportError, Exception):
                            # psutil not available, fall back to os.kill check
                            try:
                                os.kill(pid, 0)
                                self.logger.info(f"   âš¡ Reactor-Core already running (heartbeat detected, PID: {pid})")
                                print(f"  {TerminalUI.GREEN}âœ“ Reactor-Core: Already running (heartbeat: {heartbeat_age:.1f}s ago){TerminalUI.RESET}")
                                return
                            except OSError:
                                self.logger.warning(f"   Reactor-Core PID {pid} no longer exists - relaunching")
                    else:
                        # No PID but recent heartbeat - trust it
                        self.logger.info("   âš¡ Reactor-Core appears running (heartbeat detected)")
                        print(f"  {TerminalUI.GREEN}âœ“ Reactor-Core: Already running (heartbeat: {heartbeat_age:.1f}s ago){TerminalUI.RESET}")
                        return
            except Exception as e:
                self.logger.debug(f"   Could not read Reactor-Core state: {e}")

        # Find Python executable (prefer venv)
        venv_python = self._reactor_core_repo_path / "venv" / "bin" / "python3"
        if not venv_python.exists():
            venv_python = self._reactor_core_repo_path / "venv" / "bin" / "python"
        python_cmd = str(venv_python) if venv_python.exists() else sys.executable

        # Define launch approach - we need to create a runner script or launch module
        # Since trinity_orchestrator.py is a module, we need to run it properly
        orchestrator_module = self._reactor_core_repo_path / "reactor_core" / "orchestration" / "trinity_orchestrator.py"

        if orchestrator_module.exists():
            self.logger.info("   ğŸš€ Launching Reactor-Core orchestrator...")
            print(f"  {TerminalUI.CYAN}ğŸš€ Launching Reactor-Core orchestrator...{TerminalUI.RESET}")

            # Create log directory
            log_dir = Path.home() / ".jarvis" / "logs" / "services"
            log_dir.mkdir(parents=True, exist_ok=True)
            stdout_log = log_dir / "reactor_core_orchestrator_stdout.log"
            stderr_log = log_dir / "reactor_core_orchestrator_stderr.log"

            try:
                # Build environment with PYTHONPATH
                env = os.environ.copy()
                env["PYTHONPATH"] = str(self._reactor_core_repo_path)
                env["TRINITY_ENABLED"] = "true"

                # v100.1: FIX FILE DESCRIPTOR LEAK
                # Store file handles as instance variables for proper cleanup
                self._reactor_stdout_file = open(stdout_log, "w")
                self._reactor_stderr_file = open(stderr_log, "w")

                # v74.0: Direct execution - trinity_orchestrator.py now has __main__ block
                # No need for inline runner script anymore
                self._reactor_core_orchestrator_process = await asyncio.create_subprocess_exec(
                    python_cmd,
                    str(orchestrator_module),
                    cwd=str(self._reactor_core_repo_path),
                    env=env,
                    stdout=self._reactor_stdout_file,
                    stderr=self._reactor_stderr_file,
                    start_new_session=True,  # Detach from parent process group
                )

                # v101.0: Register with supervisor restart manager for automatic recovery
                self._supervisor_restart_manager.register(
                    name="reactor-core",
                    process=self._reactor_core_orchestrator_process,
                    restart_func=self._launch_reactor_core_orchestrator,
                    port=8090,  # Reactor-Core default port
                )

                self.logger.info(f"   âœ… Reactor-Core launched (PID: {self._reactor_core_orchestrator_process.pid})")
                self.logger.info(f"   ğŸ“„ Logs: {stdout_log}")
                print(f"  {TerminalUI.GREEN}âœ“ Reactor-Core launched (PID: {self._reactor_core_orchestrator_process.pid}){TerminalUI.RESET}")

                # v116.0: Register process in GlobalProcessRegistry for SIGHUP protection
                try:
                    from backend.core.supervisor_singleton import GlobalProcessRegistry
                    GlobalProcessRegistry.register(
                        self._reactor_core_orchestrator_process.pid,
                        component="reactor-core",
                        port=8090
                    )
                    self.logger.debug(f"   [v116.0] Reactor-Core PID registered in GlobalProcessRegistry")
                except Exception as reg_err:
                    self.logger.debug(f"   [v116.0] Could not register Reactor-Core PID: {reg_err}")

            except Exception as e:
                # v100.1: Clean up file handles on error
                if hasattr(self, '_reactor_stdout_file') and self._reactor_stdout_file:
                    self._reactor_stdout_file.close()
                if hasattr(self, '_reactor_stderr_file') and self._reactor_stderr_file:
                    self._reactor_stderr_file.close()
                self.logger.warning(f"   Failed to launch Reactor-Core: {e}")
                print(f"  {TerminalUI.YELLOW}âš ï¸ Reactor-Core launch failed: {e}{TerminalUI.RESET}")
        else:
            self.logger.warning(f"   âš ï¸ Reactor-Core orchestrator not found: {orchestrator_module}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Reactor-Core: Orchestrator module not found{TerminalUI.RESET}")

    async def _start_trinity_health_monitor(self) -> None:
        """
        v75.0: Start Trinity Health Monitor for crash detection and auto-recovery.

        Features:
            - Monitors heartbeat files for all Trinity components
            - Detects stale heartbeats (>15s = component dead)
            - Verifies process liveness via PID
            - Auto-restarts crashed components with circuit breaker
            - Dead letter queue for failed commands
        """
        try:
            from backend.system.trinity_initializer import (
                start_health_monitor,
                get_health_monitor,
            )

            # Start the global health monitor
            health_monitor = await start_health_monitor()

            # Register restart callbacks for J-Prime and Reactor-Core
            health_monitor.register_restart_callback(
                "j_prime",
                self._restart_jprime_on_crash,
            )
            health_monitor.register_restart_callback(
                "reactor_core",
                self._restart_reactor_core_on_crash,
            )

            self._trinity_health_monitor = health_monitor
            self.logger.info("âœ… Trinity Health Monitor v75.0 started")
            print(f"  {TerminalUI.GREEN}âœ“ Health Monitor: Crash detection active{TerminalUI.RESET}")

        except ImportError as e:
            self.logger.warning(f"âš ï¸ Trinity Health Monitor not available: {e}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ Failed to start Trinity Health Monitor: {e}")

    async def _stop_trinity_health_monitor(self) -> None:
        """v75.0: Stop Trinity Health Monitor."""
        try:
            from backend.system.trinity_initializer import stop_health_monitor
            await stop_health_monitor()
            self.logger.info("âœ… Trinity Health Monitor stopped")
        except Exception as e:
            self.logger.debug(f"Health Monitor stop error: {e}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v95.0: DIRECT SUBPROCESS HEALTH MONITORING
    # Complements heartbeat-based monitoring with immediate process state checks
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def _start_subprocess_health_monitor(self) -> None:
        """
        v95.0: Start direct subprocess health monitoring.

        Unlike heartbeat-based monitoring (which has a 15s timeout), this directly
        monitors the subprocess process handles for immediate crash detection.

        Features:
        - Immediate detection when subprocess exits (vs 15s heartbeat timeout)
        - Captures exit codes for debugging
        - Triggers restart callbacks via the trinity health monitor
        - Respects circuit breaker limits from trinity health monitor
        """
        if not self._subprocess_health_enabled:
            self.logger.info("[v95.0] Subprocess health monitoring disabled via env")
            return

        if self._subprocess_health_task is not None:
            self.logger.debug("[v95.0] Subprocess health monitor already running")
            return

        self._subprocess_health_task = asyncio.create_task(
            self._subprocess_health_monitor_loop(),
            name="subprocess_health_monitor"
        )
        self.logger.info("[v95.0] âœ… Subprocess health monitor started")
        print(f"  {TerminalUI.GREEN}âœ“ Subprocess Health: Direct process monitoring active{TerminalUI.RESET}")

    async def _stop_subprocess_health_monitor(self) -> None:
        """v95.0: Stop subprocess health monitoring."""
        if self._subprocess_health_task is not None:
            self._subprocess_health_task.cancel()
            try:
                await self._subprocess_health_task
            except asyncio.CancelledError:
                pass
            self._subprocess_health_task = None
            self.logger.info("[v95.0] âœ… Subprocess health monitor stopped")

    async def _subprocess_health_monitor_loop(self) -> None:
        """
        v95.0: Background loop monitoring subprocess processes directly.

        Checks returncode to detect when processes exit unexpectedly.
        """
        max_runtime = float(os.getenv("TIMEOUT_SUBPROCESS_HEALTH_SESSION", "86400.0"))  # 24 hours
        start = time.monotonic()
        cancelled = False

        # Track restart cooldowns to avoid rapid restart loops
        restart_cooldown: Dict[str, float] = {}
        cooldown_period = 30.0  # Don't restart same process within 30s

        while time.monotonic() - start < max_runtime:
            try:
                await asyncio.sleep(self._subprocess_health_interval)

                # Check J-Prime subprocess
                if self._jprime_orchestrator_process is not None:
                    returncode = self._jprime_orchestrator_process.returncode
                    if returncode is not None:
                        # Process has exited
                        pid = self._jprime_orchestrator_process.pid
                        self.logger.warning(
                            f"[v95.0] J-Prime subprocess (PID {pid}) exited with code {returncode}"
                        )

                        # Clear the process reference
                        self._jprime_orchestrator_process = None

                        # Check cooldown
                        last_restart = restart_cooldown.get("j_prime", 0)
                        if time.monotonic() - last_restart > cooldown_period:
                            restart_cooldown["j_prime"] = time.monotonic()

                            # Trigger restart via callback if registered
                            if self._trinity_health_monitor:
                                callback = self._trinity_health_monitor._restart_callbacks.get("j_prime")
                                if callback:
                                    self.logger.info("[v95.0] Triggering J-Prime restart via health monitor")
                                    try:
                                        await callback()
                                    except Exception as e:
                                        self.logger.error(f"[v95.0] J-Prime restart callback failed: {e}")
                            else:
                                # Direct restart if no health monitor
                                self.logger.info("[v95.0] Attempting direct J-Prime restart")
                                try:
                                    await self._restart_jprime_on_crash(f"exit_code_{returncode}")
                                except Exception as e:
                                    self.logger.error(f"[v95.0] J-Prime direct restart failed: {e}")
                        else:
                            self.logger.warning(
                                f"[v95.0] J-Prime restart skipped (cooldown: "
                                f"{cooldown_period - (time.monotonic() - last_restart):.1f}s remaining)"
                            )

                # Check Reactor-Core subprocess
                if self._reactor_core_orchestrator_process is not None:
                    returncode = self._reactor_core_orchestrator_process.returncode
                    if returncode is not None:
                        # Process has exited
                        pid = self._reactor_core_orchestrator_process.pid
                        self.logger.warning(
                            f"[v95.0] Reactor-Core subprocess (PID {pid}) exited with code {returncode}"
                        )

                        # Clear the process reference
                        self._reactor_core_orchestrator_process = None

                        # Check cooldown
                        last_restart = restart_cooldown.get("reactor_core", 0)
                        if time.monotonic() - last_restart > cooldown_period:
                            restart_cooldown["reactor_core"] = time.monotonic()

                            # Trigger restart via callback if registered
                            if self._trinity_health_monitor:
                                callback = self._trinity_health_monitor._restart_callbacks.get("reactor_core")
                                if callback:
                                    self.logger.info("[v95.0] Triggering Reactor-Core restart via health monitor")
                                    try:
                                        await callback()
                                    except Exception as e:
                                        self.logger.error(f"[v95.0] Reactor-Core restart callback failed: {e}")
                            else:
                                # Direct restart if no health monitor
                                self.logger.info("[v95.0] Attempting direct Reactor-Core restart")
                                try:
                                    await self._restart_reactor_core_on_crash(f"exit_code_{returncode}")
                                except Exception as e:
                                    self.logger.error(f"[v95.0] Reactor-Core direct restart failed: {e}")
                        else:
                            self.logger.warning(
                                f"[v95.0] Reactor-Core restart skipped (cooldown: "
                                f"{cooldown_period - (time.monotonic() - last_restart):.1f}s remaining)"
                            )

            except asyncio.CancelledError:
                cancelled = True
                break
            except Exception as e:
                self.logger.exception(f"[v95.0] Subprocess health monitor error: {e}")

        if cancelled:
            self.logger.info("[v95.0] Subprocess health monitor cancelled (shutdown)")
        else:
            self.logger.info("[v95.0] Subprocess health monitor reached max runtime, exiting")

    async def _initialize_service_supervisor(self) -> None:
        """
        v17.0: Initialize Service Supervisor for auto-restart of dead services.

        Features:
        - Monitors Trinity components (J-Prime, Reactor-Core)
        - Auto-restarts dead services with exponential backoff
        - Parallel launch support for fast startup
        - Cross-repo service management

        Architecture:
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                    Service Supervisor v17.0                      â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
            â”‚  â”‚   J-Prime       â”‚  â”‚  Reactor-Core   â”‚  â”‚  Other Services â”‚  â”‚
            â”‚  â”‚   (PID watch)   â”‚  â”‚  (PID watch)    â”‚  â”‚  (extensible)   â”‚  â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
            â”‚           â”‚                    â”‚                    â”‚           â”‚
            â”‚           â–¼                    â–¼                    â–¼           â”‚
            â”‚      Auto-restart        Auto-restart          Auto-restart     â”‚
            â”‚      on dead PID         on dead PID           on dead PID      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        """
        self.logger.info("=" * 60)
        self.logger.info("[v17.0] Initializing Service Supervisor")
        self.logger.info("=" * 60)

        print(f"  {TerminalUI.CYAN}ğŸ”§ Service Supervisor: Initializing auto-restart...{TerminalUI.RESET}")

        try:
            from backend.core.service_registry import (
                ServiceSupervisor,
                ServiceLaunchConfig,
                get_service_supervisor,
            )

            # Get the global supervisor instance
            self._service_supervisor = get_service_supervisor()

            # Register J-Prime launch config if repo exists
            if hasattr(self, '_jprime_repo_path') and self._jprime_repo_path.exists():
                # Find the launch script - run_server.py is the primary entry point
                launch_scripts = [
                    "run_server.py",  # Primary entry point with proper FastAPI server
                    "jarvis_prime/orchestration/trinity_orchestrator.py",  # Legacy fallback
                    "run_orchestrator.py",
                    "main.py",
                ]

                for script in launch_scripts:
                    script_path = self._jprime_repo_path / script
                    if script_path.exists():
                        # Find venv python
                        venv_python = None
                        venv_path = self._jprime_repo_path / "venv" / "bin" / "python"
                        if venv_path.exists():
                            venv_python = str(venv_path)

                        # Build command with port argument for run_server.py
                        command = ["python3", str(script_path)]
                        if "run_server.py" in str(script_path):
                            # run_server.py accepts --port and --host CLI args
                            jprime_port = int(os.getenv("JARVIS_PRIME_PORT", "8000"))
                            command.extend(["--port", str(jprime_port), "--host", "0.0.0.0"])

                        config = ServiceLaunchConfig(
                            service_name="jarvis-prime",
                            command=command,
                            working_dir=str(self._jprime_repo_path),
                            env_vars={
                                "TRINITY_ENABLED": "true",
                                "PYTHONPATH": str(self._jprime_repo_path),
                                "JARVIS_PRIME_PORT": os.getenv("JARVIS_PRIME_PORT", "8000"),
                            },
                            python_venv=venv_python,
                            restart_policy="always",
                            max_restarts=5,
                            restart_delay_sec=5.0,
                            health_check_timeout_sec=30.0,
                        )
                        self._service_supervisor.register_service_launch(config)
                        self.logger.info(f"[v17.0] Registered J-Prime auto-restart: {script_path.name}")
                        break

            # Register Reactor-Core launch config if repo exists
            if hasattr(self, '_reactor_core_repo_path') and self._reactor_core_repo_path.exists():
                # Find the launch script - run_reactor.py is the primary entry point
                launch_scripts = [
                    "run_reactor.py",  # Primary entry point with proper FastAPI server
                    "reactor_core/orchestration/trinity_orchestrator.py",  # Legacy fallback
                    "run_orchestrator.py",
                    "main.py",
                ]

                for script in launch_scripts:
                    script_path = self._reactor_core_repo_path / script
                    if script_path.exists():
                        # Find venv python
                        venv_python = None
                        venv_path = self._reactor_core_repo_path / "venv" / "bin" / "python"
                        if venv_path.exists():
                            venv_python = str(venv_path)

                        # Build command with port argument for run_reactor.py
                        command = ["python3", str(script_path)]
                        if "run_reactor.py" in str(script_path):
                            # run_reactor.py accepts --port CLI arg
                            reactor_port = int(os.getenv("REACTOR_CORE_PORT", "8090"))
                            command.extend(["--port", str(reactor_port)])

                        config = ServiceLaunchConfig(
                            service_name="reactor-core",
                            command=command,
                            working_dir=str(self._reactor_core_repo_path),
                            env_vars={
                                "TRINITY_ENABLED": "true",
                                "PYTHONPATH": str(self._reactor_core_repo_path),
                                "REACTOR_CORE_PORT": os.getenv("REACTOR_CORE_PORT", "8090"),
                            },
                            python_venv=venv_python,
                            restart_policy="always",
                            max_restarts=5,
                            restart_delay_sec=5.0,
                            health_check_timeout_sec=30.0,
                        )
                        self._service_supervisor.register_service_launch(config)
                        self.logger.info(f"[v17.0] Registered Reactor-Core auto-restart: {script_path.name}")
                        break

            # Register callback for restart notifications
            async def on_restart(service_name: str, success: bool, reason: str = None):
                if success:
                    self.logger.info(f"[v17.0] âœ… {service_name} auto-restarted successfully")
                    if hasattr(self, 'narrator') and self.narrator:
                        await self.narrator.speak(
                            f"{service_name} has been auto-restarted.",
                            wait=False
                        )
                else:
                    self.logger.warning(f"[v17.0] âš ï¸ {service_name} restart failed: {reason}")

            self._service_supervisor.on_restart(on_restart)

            # Start the supervisor monitoring loop
            await self._service_supervisor.start()

            stats = self._service_supervisor.get_stats()
            registered = stats.get("services_registered", 0)
            print(f"  {TerminalUI.GREEN}âœ… Service Supervisor: Active ({registered} services monitored){TerminalUI.RESET}")
            self.logger.info(f"[v17.0] âœ… Service Supervisor started - monitoring {registered} services")

        except ImportError as e:
            self.logger.warning(f"[v17.0] âš ï¸ Service Supervisor not available: {e}")
            print(f"  {TerminalUI.YELLOW}âš ï¸ Service Supervisor: Not available{TerminalUI.RESET}")
        except Exception as e:
            self.logger.error(f"[v17.0] âŒ Service Supervisor failed: {e}")
            print(f"  {TerminalUI.RED}âœ— Service Supervisor: Failed - {e}{TerminalUI.RESET}")

    async def _stop_service_supervisor(self) -> None:
        """v17.0: Stop Service Supervisor."""
        if hasattr(self, '_service_supervisor') and self._service_supervisor:
            try:
                await self._service_supervisor.stop()
                self.logger.info("[v17.0] âœ… Service Supervisor stopped")
            except Exception as e:
                self.logger.debug(f"[v17.0] Service Supervisor stop error: {e}")

    async def _shutdown_v80_cross_repo_system(self) -> None:
        """
        v80.0: Shutdown the Advanced Cross-Repo Loading System.

        Cleanup sequence:
        1. Stop CrossRepoHealthMonitor (cancels background health check task)
        2. Stop TrinityStartupCoordinator (cancels any in-progress startup)
        3. Clear references to allow garbage collection
        """
        self.logger.info("[v80.0] Shutting down Cross-Repo Loading System...")

        # Stop CrossRepoHealthMonitor
        if self._cross_repo_health_monitor is not None:
            try:
                from backend.core.advanced_startup_orchestrator import shutdown_health_monitor
                await shutdown_health_monitor()
                self.logger.info("   âœ… CrossRepoHealthMonitor stopped")
            except ImportError:
                # If import fails, try direct stop
                try:
                    await self._cross_repo_health_monitor.stop()
                except Exception as e:
                    self.logger.debug(f"   Health monitor stop error: {e}")
            except Exception as e:
                self.logger.debug(f"   Health monitor shutdown error: {e}")
            finally:
                self._cross_repo_health_monitor = None

        # Stop TrinityStartupCoordinator
        if self._trinity_startup_coordinator is not None:
            try:
                # Coordinator doesn't have a formal stop, just clear reference
                self._trinity_startup_coordinator = None
                self.logger.info("   âœ… TrinityStartupCoordinator stopped")
            except Exception as e:
                self.logger.debug(f"   Startup coordinator shutdown error: {e}")

        self.logger.info("[v80.0] âœ… Cross-Repo Loading System shutdown complete")

    async def _restart_jprime_on_crash(self, reason: str = "process_exited") -> None:
        """
        v95.16: Callback to restart J-Prime when crash detected.

        Called by TrinityHealthMonitor when J-Prime heartbeat goes stale
        or process is detected as dead.

        Args:
            reason: Why the restart is happening. Defaults to "process_exited"
                    to support callback invocation without arguments.
        """
        self.logger.warning(f"ğŸ”„ [Trinity] Restarting J-Prime (reason: {reason})")

        # Cleanup old process
        if self._jprime_orchestrator_process is not None:
            try:
                self._jprime_orchestrator_process.kill()
                await self._jprime_orchestrator_process.wait()
            except Exception:
                pass
            self._jprime_orchestrator_process = None

        # Wait a moment before restart
        await asyncio.sleep(2.0)

        # Relaunch
        await self._launch_jprime_orchestrator()

        if self._jprime_orchestrator_process is not None:
            self.logger.info(f"âœ… [Trinity] J-Prime restarted (PID: {self._jprime_orchestrator_process.pid})")

            # Voice announcement if enabled
            if hasattr(self, 'narrator') and self.config.voice_enabled:
                await self.narrator.speak(
                    "J-Prime recovered from crash. Mind component back online.",
                    wait=False,
                )

    async def _restart_reactor_core_on_crash(self, reason: str = "process_exited") -> None:
        """
        v95.16: Callback to restart Reactor-Core when crash detected.

        Called by TrinityHealthMonitor when Reactor-Core heartbeat goes stale
        or process is detected as dead.

        Args:
            reason: Why the restart is happening. Defaults to "process_exited"
                    to support callback invocation without arguments.
        """
        self.logger.warning(f"ğŸ”„ [Trinity] Restarting Reactor-Core (reason: {reason})")

        # Cleanup old process
        if self._reactor_core_orchestrator_process is not None:
            try:
                self._reactor_core_orchestrator_process.kill()
                await self._reactor_core_orchestrator_process.wait()
            except Exception:
                pass
            self._reactor_core_orchestrator_process = None

        # Wait a moment before restart
        await asyncio.sleep(2.0)

        # Relaunch
        await self._launch_reactor_core_orchestrator()

        if self._reactor_core_orchestrator_process is not None:
            self.logger.info(f"âœ… [Trinity] Reactor-Core restarted (PID: {self._reactor_core_orchestrator_process.pid})")

            # Voice announcement if enabled
            if hasattr(self, 'narrator') and self.config.voice_enabled:
                await self.narrator.speak(
                    "Reactor Core recovered from crash. Nerves component back online.",
                    wait=False,
                )

    # =========================================================================
    # v100.0: Ultra-Robust Trinity Launch Methods
    # =========================================================================

    async def _reap_zombie_port_holders_v100(self, config: 'TrinityLaunchConfig') -> None:
        """
        v100.0: Ultra-robust zombie port reaper with config-driven ports.

        Improvements over v73.0:
        - Config-driven port list (no hardcoding)
        - Parallel process scanning with psutil
        - Graceful shutdown with configurable timeout
        - Process ancestry checking to avoid killing current session children
        - Detailed logging with trace context support
        """
        import psutil

        current_pid = os.getpid()
        current_process = psutil.Process(current_pid)

        # Get children of current process (don't kill our own children)
        try:
            current_children = {p.pid for p in current_process.children(recursive=True)}
        except psutil.Error:
            current_children = set()

        reaped_count = 0

        # Build port-to-component mapping from config
        zombie_ports: Dict[int, str] = {}
        for port in config.jprime_ports:
            zombie_ports[port] = "J-Prime"
        for port in config.reactor_core_ports:
            zombie_ports[port] = "Reactor-Core"

        # Trinity identification patterns
        trinity_patterns = config.trinity_process_patterns

        for port, component_name in zombie_ports.items():
            try:
                # Find processes with connections on this port
                for proc in psutil.process_iter(['pid', 'name', 'cmdline', 'create_time']):
                    try:
                        pid = proc.pid

                        # Skip current process and its children
                        if pid == current_pid or pid in current_children:
                            continue

                        # Check if this process has connections on the port
                        connections = proc.connections()
                        for conn in connections:
                            if (hasattr(conn, 'laddr') and conn.laddr and
                                conn.laddr.port == port and
                                conn.status in ('LISTEN', 'ESTABLISHED')):

                                proc_name = proc.name()
                                proc_cmdline = ' '.join(proc.cmdline()[:5]) if proc.cmdline() else ''

                                # Verify it's a Trinity-related process
                                is_trinity = any(
                                    pattern.lower() in proc_cmdline.lower() or
                                    pattern.lower() in proc_name.lower()
                                    for pattern in trinity_patterns
                                )

                                if is_trinity:
                                    self.logger.warning(
                                        f"ğŸ§Ÿ [v100] Found zombie {component_name} on port {port} "
                                        f"(PID: {pid}, cmd: {proc_cmdline[:60]})"
                                    )
                                    print(
                                        f"  {TerminalUI.YELLOW}ğŸ§Ÿ Killing zombie {component_name} "
                                        f"(PID: {pid}) on port {port}{TerminalUI.RESET}"
                                    )

                                    # Graceful termination first
                                    proc.terminate()

                                    # Wait for graceful shutdown
                                    try:
                                        proc.wait(timeout=config.zombie_kill_timeout_sec)
                                    except psutil.TimeoutExpired:
                                        # Force kill if still running
                                        self.logger.warning(f"   Force killing stubborn zombie PID {pid}")
                                        proc.kill()
                                        try:
                                            proc.wait(timeout=2.0)
                                        except psutil.TimeoutExpired:
                                            pass

                                    reaped_count += 1
                                break  # One match per port is enough

                    except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                        continue

            except Exception as e:
                self.logger.debug(f"   Zombie check for port {port} failed: {e}")

        if reaped_count > 0:
            self.logger.info(f"   ğŸ§¹ [v100] Reaped {reaped_count} zombie process(es)")
            # Give the OS time to release the ports
            await asyncio.sleep(config.port_release_wait_sec)
        else:
            self.logger.debug("   [v100] No zombie processes found")

    async def _launch_jprime_orchestrator_v100(
        self,
        jprime_path: Path,
        venv_detector: 'RobustVenvDetector',
        config: 'TrinityLaunchConfig',
        trace_ctx: 'TrinityTraceContext',
    ) -> Optional[asyncio.subprocess.Process]:
        """
        v100.0: Ultra-robust J-Prime orchestrator launcher.

        Improvements over v72.0:
        - Robust venv detection (handles venv, poetry, pipenv, pyenv, conda)
        - Proper file handle management with context managers
        - W3C distributed tracing via environment variables
        - Config-driven timeouts and settings
        - Heartbeat-based detection with multiple file patterns
        - Atomic log file rotation
        - v90.0: PID-validated heartbeat checking via TrueHeartbeat
        - v109.5: Process-based double-launch prevention
        """
        # v109.5: Check if J-Prime process is already running (prevents double-launch during retry)
        # This is CRITICAL to avoid "address already in use" errors during retry logic
        if self._jprime_orchestrator_process is not None:
            try:
                # Check if the process is still running
                return_code = self._jprime_orchestrator_process.returncode
                if return_code is None:
                    # Process is still running - don't double-launch
                    self.logger.info(
                        f"   ğŸ§  [v109.5] J-Prime already launched by this session "
                        f"(PID: {self._jprime_orchestrator_process.pid})"
                    )
                    return self._jprime_orchestrator_process
                else:
                    # Process has exited - clear reference so we can relaunch
                    self.logger.info(
                        f"   ğŸ”„ [v109.5] Previous J-Prime exited (code={return_code}), "
                        f"preparing to relaunch"
                    )
                    self._jprime_orchestrator_process = None
            except Exception as e:
                self.logger.debug(f"   [v109.5] J-Prime process check error: {e}")
                # Clear reference on error - safer to allow relaunch
                self._jprime_orchestrator_process = None

        # v90.0: Use TrueHeartbeat for PID-validated heartbeat checking
        # This fixes Issues #3 (Heartbeat Staleness) and #4 (Missing PID Validation)
        if _SYSTEM_PRIMITIVES_AVAILABLE and TrueHeartbeat is not None:
            # Check both naming conventions
            for component_name in ["jarvis_prime", "j_prime"]:
                heartbeat = TrueHeartbeat(component_name)
                status, heartbeat_data = heartbeat.validate()

                if status == HeartbeatStatus.ALIVE:
                    self.logger.info(
                        f"   ğŸ§  [v90] J-Prime already running "
                        f"(PID: {heartbeat_data.pid}, age: {heartbeat_data.age_seconds:.1f}s)"
                    )
                    print(
                        f"  {TerminalUI.GREEN}âœ“ J-Prime: Already running "
                        f"(PID: {heartbeat_data.pid}, heartbeat: {heartbeat_data.age_seconds:.1f}s ago){TerminalUI.RESET}"
                    )
                    # v116.0: Register adopted J-Prime in GlobalProcessRegistry
                    try:
                        from backend.core.supervisor_singleton import GlobalProcessRegistry
                        GlobalProcessRegistry.register(pid=heartbeat_data.pid, component="J-Prime (adopted)", port=8000)
                        print(f"  [v116.0] âœ… Adopted J-Prime (PID {heartbeat_data.pid}) registered")
                    except Exception:
                        pass
                    return None  # Already running, not an error
                elif status == HeartbeatStatus.DEAD:
                    self.logger.info(
                        f"   ğŸ§Ÿ [v90] Found dead J-Prime heartbeat (PID {heartbeat_data.pid} is dead) - cleaning up"
                    )
                    # TrueHeartbeat already cleaned up the stale file
                elif status == HeartbeatStatus.STALE:
                    self.logger.warning(
                        f"   âš ï¸ [v90] J-Prime heartbeat is stale ({heartbeat_data.age_seconds:.1f}s old) - will restart"
                    )
                    heartbeat.cleanup()
        else:
            # Fallback to legacy heartbeat checking (without PID validation)
            trinity_dir = Path.home() / ".jarvis" / "trinity"
            components_dir = trinity_dir / "components"

            jprime_state_files = [
                components_dir / "jarvis_prime.json",
                components_dir / "j_prime.json",
            ]

            for jprime_state_file in jprime_state_files:
                if jprime_state_file.exists():
                    try:
                        import json
                        import time as time_module
                        with open(jprime_state_file) as f:
                            state = json.load(f)
                        heartbeat_age = time_module.time() - state.get("timestamp", 0)
                        if heartbeat_age < config.heartbeat_stale_threshold_sec:
                            self.logger.info(
                                f"   ğŸ§  [legacy] J-Prime already running "
                                f"(heartbeat: {jprime_state_file.name}, age: {heartbeat_age:.1f}s)"
                            )
                            print(
                                f"  {TerminalUI.GREEN}âœ“ J-Prime: Already running "
                                f"(heartbeat: {heartbeat_age:.1f}s ago){TerminalUI.RESET}"
                            )
                            return None  # Already running, not an error
                    except Exception as e:
                        self.logger.debug(f"   Could not read J-Prime state from {jprime_state_file}: {e}")

        # v100.0: Use RobustVenvDetector to find Python
        python_cmd = venv_detector.get_python_executable(jprime_path)
        self.logger.debug(f"   [v100] J-Prime Python: {python_cmd}")

        # Define launch scripts in order of preference
        launch_scripts = [
            ("jarvis_prime/server.py", "FastAPI Server"),
            ("run_server.py", "Server Runner"),
            ("jarvis_prime/core/trinity_bridge.py", "Trinity Bridge"),
            ("main.py", "Main Entry"),
        ]

        # Try each launch script
        for script_rel, description in launch_scripts:
            script_path = jprime_path / script_rel
            if script_path.exists():
                self.logger.info(f"   ğŸš€ [v100] Launching J-Prime ({description})...")
                print(f"  {TerminalUI.CYAN}ğŸš€ Launching J-Prime ({description})...{TerminalUI.RESET}")

                # Create log directory with atomic rotation
                log_dir = Path.home() / ".jarvis" / "logs" / "services"
                log_dir.mkdir(parents=True, exist_ok=True)

                # Rotate old logs if they exist and are large
                stdout_log = log_dir / "jprime_stdout.log"
                stderr_log = log_dir / "jprime_stderr.log"

                for log_file in [stdout_log, stderr_log]:
                    if log_file.exists() and log_file.stat().st_size > config.log_rotation_size_bytes:
                        rotated = log_file.with_suffix(f".log.{int(time.time())}")
                        try:
                            log_file.rename(rotated)
                        except Exception:
                            pass

                try:
                    # Build environment with PYTHONPATH and trace context
                    # v90.0: Use filtered environment to prevent pollution (Issue #15)
                    env = {
                        "PYTHONPATH": str(jprime_path),
                        "TRINITY_ENABLED": "true",
                        "TRINITY_COMPONENT": "jprime",
                        # W3C Distributed Tracing headers
                        "TRACEPARENT": trace_ctx.to_traceparent(),
                        "TRINITY_TRACE_ID": trace_ctx.trace_id,
                        "TRINITY_SPAN_ID": trace_ctx.span_id,
                        "TRINITY_PARENT_ID": trace_ctx.parent_span_id or "",
                        # Additional config-driven environment
                        "TRINITY_INSTANCE_ID": config.trinity_instance_id,
                        "TRINITY_LOG_LEVEL": config.log_level,
                    }

                    # v90.0: Use SafeProcess for guaranteed cleanup (fixes #1, #5, #6)
                    if _SYSTEM_PRIMITIVES_AVAILABLE and SafeProcess is not None:
                        # Create SafeProcess with process group management
                        safe_proc = SafeProcess(
                            name=f"jprime-{description.replace(' ', '-').lower()}",
                            cmd=[python_cmd, str(script_path)],
                            cwd=jprime_path,
                            env=env,
                            stdout_path=stdout_log,
                            stderr_path=stderr_log,
                            inherit_env=True,  # Inherit PATH, HOME, etc.
                            env_filter=["PATH", "HOME", "USER", "LANG", "LC_ALL", "SHELL",
                                       "PYTHONPATH", "VIRTUAL_ENV", "CONDA_PREFIX"],  # Filter pollution
                        )

                        # Start the process
                        proc_info = await safe_proc.start()

                        # Store both SafeProcess and raw process reference
                        self._jprime_safe_process = safe_proc
                        self._jprime_orchestrator_process = safe_proc._process

                        self.logger.info(
                            f"   âœ… [v90] J-Prime launched via SafeProcess "
                            f"(PID: {proc_info.pid}, PGID: {proc_info.pgid}, "
                            f"trace: {trace_ctx.trace_id[:8]}...)"
                        )
                        self.logger.info(f"   ğŸ“„ Logs: {stdout_log}")
                        print(
                            f"  {TerminalUI.GREEN}âœ“ J-Prime launched "
                            f"(PID: {proc_info.pid}, PGID: {proc_info.pgid}){TerminalUI.RESET}"
                        )

                        # v102.0: Register in process tree for proper shutdown
                        await self._register_trinity_process_in_tree(
                            pid=proc_info.pid,
                            name="J-Prime",
                            role_str="jprime",
                        )

                        return safe_proc._process

                    else:
                        # Fallback to legacy subprocess management
                        env_full = os.environ.copy()
                        env_full.update(env)

                        # Use proper file handle management (still fixes #1)
                        stdout_fd = os.open(str(stdout_log), os.O_WRONLY | os.O_CREAT | os.O_APPEND, 0o644)
                        stderr_fd = os.open(str(stderr_log), os.O_WRONLY | os.O_CREAT | os.O_APPEND, 0o644)

                        try:
                            # Write launch header
                            launch_header = f"\n{'='*60}\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] J-Prime Launch (v90.0-legacy)\n{'='*60}\n".encode()
                            os.write(stdout_fd, launch_header)
                            os.write(stderr_fd, launch_header)

                            # Launch subprocess with detached session
                            process = await asyncio.create_subprocess_exec(
                                python_cmd,
                                str(script_path),
                                cwd=str(jprime_path),
                                env=env_full,
                                stdout=stdout_fd,
                                stderr=stderr_fd,
                                start_new_session=True,
                            )

                            # Store reference
                            self._jprime_orchestrator_process = process

                        finally:
                            # Close file descriptors (process has its own copy)
                            # v109.3: Use safe_close to prevent EXC_GUARD crashes
                            safe_close(stdout_fd)
                            safe_close(stderr_fd)

                        self.logger.info(
                            f"   âœ… [legacy] J-Prime launched (PID: {process.pid}, "
                            f"trace: {trace_ctx.trace_id[:8]}...)"
                        )
                        self.logger.info(f"   ğŸ“„ Logs: {stdout_log}")
                        print(
                            f"  {TerminalUI.GREEN}âœ“ J-Prime launched "
                            f"(PID: {process.pid}){TerminalUI.RESET}"
                        )

                        # v102.0: Register in process tree for proper shutdown
                        await self._register_trinity_process_in_tree(
                            pid=process.pid,
                            name="J-Prime",
                            role_str="jprime",
                        )

                        return process

                except Exception as e:
                    self.logger.warning(f"   Failed to launch J-Prime: {e}")
                    # v90.0: Log full traceback for debugging (fixes #8)
                    if _SYSTEM_PRIMITIVES_AVAILABLE and EnhancedExceptionHandler is not None:
                        self.logger.debug(EnhancedExceptionHandler.format_exception(e))
                    print(f"  {TerminalUI.YELLOW}âš ï¸ J-Prime launch failed: {e}{TerminalUI.RESET}")
                    raise  # Re-raise for retry logic

        # No launch script found
        raise FileNotFoundError(f"No J-Prime launch script found in {jprime_path}")

    async def _launch_reactor_core_orchestrator_v100(
        self,
        reactor_core_path: Path,
        venv_detector: 'RobustVenvDetector',
        config: 'TrinityLaunchConfig',
        trace_ctx: 'TrinityTraceContext',
    ) -> Optional[asyncio.subprocess.Process]:
        """
        v100.0: Ultra-robust Reactor-Core orchestrator launcher.

        Improvements over v72.0:
        - Robust venv detection (handles all Python environments)
        - Proper file handle management with context managers
        - W3C distributed tracing via environment variables
        - Config-driven timeouts and settings
        - Heartbeat-based detection
        - Atomic log file rotation
        - v90.0: PID-validated heartbeat checking via TrueHeartbeat
        - v109.5: Process-based double-launch prevention
        """
        # v109.5: Check if Reactor-Core process is already running (prevents double-launch)
        if self._reactor_core_orchestrator_process is not None:
            try:
                return_code = self._reactor_core_orchestrator_process.returncode
                if return_code is None:
                    # Process is still running - don't double-launch
                    self.logger.info(
                        f"   âš¡ [v109.5] Reactor-Core already launched by this session "
                        f"(PID: {self._reactor_core_orchestrator_process.pid})"
                    )
                    return self._reactor_core_orchestrator_process
                else:
                    self.logger.info(
                        f"   ğŸ”„ [v109.5] Previous Reactor-Core exited (code={return_code}), "
                        f"preparing to relaunch"
                    )
                    self._reactor_core_orchestrator_process = None
            except Exception as e:
                self.logger.debug(f"   [v109.5] Reactor-Core process check error: {e}")
                self._reactor_core_orchestrator_process = None

        # v90.0: Use TrueHeartbeat for PID-validated heartbeat checking
        if _SYSTEM_PRIMITIVES_AVAILABLE and TrueHeartbeat is not None:
            heartbeat = TrueHeartbeat("reactor_core")
            status, heartbeat_data = heartbeat.validate()

            if status == HeartbeatStatus.ALIVE:
                self.logger.info(
                    f"   âš¡ [v90] Reactor-Core already running "
                    f"(PID: {heartbeat_data.pid}, age: {heartbeat_data.age_seconds:.1f}s)"
                )
                print(
                    f"  {TerminalUI.GREEN}âœ“ Reactor-Core: Already running "
                    f"(PID: {heartbeat_data.pid}, heartbeat: {heartbeat_data.age_seconds:.1f}s ago){TerminalUI.RESET}"
                )
                # v116.0: Register adopted Reactor-Core in GlobalProcessRegistry
                try:
                    from backend.core.supervisor_singleton import GlobalProcessRegistry
                    GlobalProcessRegistry.register(pid=heartbeat_data.pid, component="Reactor-Core (adopted)", port=8090)
                    print(f"  [v116.0] âœ… Adopted Reactor-Core (PID {heartbeat_data.pid}) registered")
                except Exception:
                    pass
                return None  # Already running, not an error
            elif status == HeartbeatStatus.DEAD:
                self.logger.info(
                    f"   ğŸ§Ÿ [v90] Found dead Reactor-Core heartbeat (PID {heartbeat_data.pid} is dead) - cleaning up"
                )
            elif status == HeartbeatStatus.STALE:
                self.logger.warning(
                    f"   âš ï¸ [v90] Reactor-Core heartbeat is stale ({heartbeat_data.age_seconds:.1f}s old) - will restart"
                )
                heartbeat.cleanup()
        else:
            # Fallback to legacy heartbeat checking
            trinity_dir = Path.home() / ".jarvis" / "trinity"
            reactor_state_file = trinity_dir / "components" / "reactor_core.json"

            if reactor_state_file.exists():
                try:
                    import json
                    import time as time_module
                    with open(reactor_state_file) as f:
                        state = json.load(f)
                    heartbeat_age = time_module.time() - state.get("timestamp", 0)
                    if heartbeat_age < config.heartbeat_stale_threshold_sec:
                        self.logger.info(
                            f"   âš¡ [legacy] Reactor-Core already running "
                            f"(heartbeat: {heartbeat_age:.1f}s)"
                        )
                        print(
                            f"  {TerminalUI.GREEN}âœ“ Reactor-Core: Already running "
                            f"(heartbeat: {heartbeat_age:.1f}s ago){TerminalUI.RESET}"
                        )
                        return None  # Already running, not an error
                except Exception as e:
                    self.logger.debug(f"   Could not read Reactor-Core state: {e}")

        # v100.0: Use RobustVenvDetector to find Python
        python_cmd = venv_detector.get_python_executable(reactor_core_path)
        self.logger.debug(f"   [v100] Reactor-Core Python: {python_cmd}")

        # Define launch scripts in order of preference
        launch_scripts = [
            ("reactor_core/orchestration/trinity_orchestrator.py", "Trinity Orchestrator"),
            ("run_orchestrator.py", "Orchestrator Runner"),
            ("main.py", "Main Entry"),
        ]

        # Try each launch script
        for script_rel, description in launch_scripts:
            script_path = reactor_core_path / script_rel
            if script_path.exists():
                self.logger.info(f"   ğŸš€ [v100] Launching Reactor-Core ({description})...")
                print(f"  {TerminalUI.CYAN}ğŸš€ Launching Reactor-Core ({description})...{TerminalUI.RESET}")

                # Create log directory with atomic rotation
                log_dir = Path.home() / ".jarvis" / "logs" / "services"
                log_dir.mkdir(parents=True, exist_ok=True)

                stdout_log = log_dir / "reactor_core_orchestrator_stdout.log"
                stderr_log = log_dir / "reactor_core_orchestrator_stderr.log"

                # Rotate old logs if they exist and are large
                for log_file in [stdout_log, stderr_log]:
                    if log_file.exists() and log_file.stat().st_size > config.log_rotation_size_bytes:
                        rotated = log_file.with_suffix(f".log.{int(time.time())}")
                        try:
                            log_file.rename(rotated)
                        except Exception:
                            pass

                try:
                    # Build environment with PYTHONPATH and trace context
                    # v90.0: Use filtered environment to prevent pollution (Issue #15)
                    env = {
                        "PYTHONPATH": str(reactor_core_path),
                        "TRINITY_ENABLED": "true",
                        "TRINITY_COMPONENT": "reactor_core",
                        # W3C Distributed Tracing headers
                        "TRACEPARENT": trace_ctx.to_traceparent(),
                        "TRINITY_TRACE_ID": trace_ctx.trace_id,
                        "TRINITY_SPAN_ID": trace_ctx.span_id,
                        "TRINITY_PARENT_ID": trace_ctx.parent_span_id or "",
                        # Additional config-driven environment
                        "TRINITY_INSTANCE_ID": config.trinity_instance_id,
                        "TRINITY_LOG_LEVEL": config.log_level,
                    }

                    # v90.0: Use SafeProcess for guaranteed cleanup (fixes #1, #5, #6)
                    if _SYSTEM_PRIMITIVES_AVAILABLE and SafeProcess is not None:
                        # Create SafeProcess with process group management
                        safe_proc = SafeProcess(
                            name=f"reactor-{description.replace(' ', '-').lower()}",
                            cmd=[python_cmd, str(script_path)],
                            cwd=reactor_core_path,
                            env=env,
                            stdout_path=stdout_log,
                            stderr_path=stderr_log,
                            inherit_env=True,
                            env_filter=["PATH", "HOME", "USER", "LANG", "LC_ALL", "SHELL",
                                       "PYTHONPATH", "VIRTUAL_ENV", "CONDA_PREFIX"],
                        )

                        # Start the process
                        proc_info = await safe_proc.start()

                        # Store both SafeProcess and raw process reference
                        self._reactor_safe_process = safe_proc
                        self._reactor_core_orchestrator_process = safe_proc._process

                        self.logger.info(
                            f"   âœ… [v90] Reactor-Core launched via SafeProcess "
                            f"(PID: {proc_info.pid}, PGID: {proc_info.pgid}, "
                            f"trace: {trace_ctx.trace_id[:8]}...)"
                        )
                        self.logger.info(f"   ğŸ“„ Logs: {stdout_log}")
                        print(
                            f"  {TerminalUI.GREEN}âœ“ Reactor-Core launched "
                            f"(PID: {proc_info.pid}, PGID: {proc_info.pgid}){TerminalUI.RESET}"
                        )

                        # v102.0: Register in process tree for proper shutdown
                        await self._register_trinity_process_in_tree(
                            pid=proc_info.pid,
                            name="Reactor-Core",
                            role_str="reactor",
                        )

                        return safe_proc._process

                    else:
                        # Fallback to legacy subprocess management
                        env_full = os.environ.copy()
                        env_full.update(env)

                        # Use proper file handle management
                        stdout_fd = os.open(str(stdout_log), os.O_WRONLY | os.O_CREAT | os.O_APPEND, 0o644)
                        stderr_fd = os.open(str(stderr_log), os.O_WRONLY | os.O_CREAT | os.O_APPEND, 0o644)

                        try:
                            # Write launch header
                            launch_header = f"\n{'='*60}\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] Reactor-Core Launch (v90.0-legacy)\n{'='*60}\n".encode()
                            os.write(stdout_fd, launch_header)
                            os.write(stderr_fd, launch_header)

                            # Launch subprocess with detached session
                            process = await asyncio.create_subprocess_exec(
                                python_cmd,
                                str(script_path),
                                cwd=str(reactor_core_path),
                                env=env_full,
                                stdout=stdout_fd,
                                stderr=stderr_fd,
                                start_new_session=True,
                            )

                            # Store reference
                            self._reactor_core_orchestrator_process = process

                        finally:
                            # Close file descriptors
                            # v109.3: Use safe_close to prevent EXC_GUARD crashes
                            safe_close(stdout_fd)
                            safe_close(stderr_fd)

                        self.logger.info(
                            f"   âœ… [legacy] Reactor-Core launched (PID: {process.pid}, "
                            f"trace: {trace_ctx.trace_id[:8]}...)"
                        )
                        self.logger.info(f"   ğŸ“„ Logs: {stdout_log}")
                        print(
                            f"  {TerminalUI.GREEN}âœ“ Reactor-Core launched "
                            f"(PID: {process.pid}){TerminalUI.RESET}"
                        )

                        # v102.0: Register in process tree for proper shutdown
                        await self._register_trinity_process_in_tree(
                            pid=process.pid,
                            name="Reactor-Core",
                            role_str="reactor",
                        )

                    return process

                except Exception as e:
                    self.logger.warning(f"   Failed to launch Reactor-Core: {e}")
                    print(f"  {TerminalUI.YELLOW}âš ï¸ Reactor-Core launch failed: {e}{TerminalUI.RESET}")
                    raise  # Re-raise for retry logic

        # No launch script found
        raise FileNotFoundError(f"No Reactor-Core launch script found in {reactor_core_path}")

    async def _verify_trinity_health_v100(
        self,
        config: 'TrinityLaunchConfig',
        trace_ctx: 'TrinityTraceContext',
    ) -> Dict[str, Any]:
        """
        v100.0: Verify Trinity component health before declaring launch success.

        Performs:
        1. Heartbeat file freshness check
        2. Process liveness verification (if PIDs available)
        3. HTTP health endpoint check (if configured)
        4. Cross-component communication test

        Returns:
            Dict with health status for each component
        """
        import json
        import time as time_module

        health_results: Dict[str, Any] = {
            "timestamp": time_module.time(),
            "trace_id": trace_ctx.trace_id,
            "components": {},
            "overall_healthy": True,
        }

        trinity_dir = Path.home() / ".jarvis" / "trinity"
        components_dir = trinity_dir / "components"

        # Component definitions with health check info
        components_to_check = [
            {
                "name": "jarvis_body",
                "heartbeat_files": [components_dir / "jarvis_body.json"],
                "process": None,  # This is us, always healthy
                "health_urls": [f"http://localhost:{config.jarvis_api_port}/health"],
            },
            {
                "name": "jprime",
                "heartbeat_files": [
                    components_dir / "jarvis_prime.json",
                    components_dir / "j_prime.json",
                ],
                "process": self._jprime_orchestrator_process,
                "health_urls": [f"http://localhost:{port}/health" for port in config.jprime_ports],
            },
            {
                "name": "reactor_core",
                "heartbeat_files": [components_dir / "reactor_core.json"],
                "process": self._reactor_core_orchestrator_process,
                "health_urls": [f"http://localhost:{port}/health" for port in config.reactor_core_ports],
            },
        ]

        for component in components_to_check:
            comp_name = component["name"]
            comp_health: Dict[str, Any] = {
                "heartbeat_ok": False,
                "process_ok": False,
                "http_ok": False,
                "healthy": False,
            }

            # Check 1: Heartbeat file freshness
            for heartbeat_file in component["heartbeat_files"]:
                if heartbeat_file.exists():
                    try:
                        with open(heartbeat_file) as f:
                            state = json.load(f)
                        heartbeat_age = time_module.time() - state.get("timestamp", 0)
                        if heartbeat_age < config.heartbeat_stale_threshold_sec:
                            comp_health["heartbeat_ok"] = True
                            comp_health["heartbeat_age_sec"] = heartbeat_age
                            break
                    except Exception:
                        pass

            # Check 2: Process liveness
            proc = component["process"]
            if proc is not None:
                try:
                    # Check if process is still running
                    if proc.returncode is None:
                        comp_health["process_ok"] = True
                        comp_health["pid"] = proc.pid
                    else:
                        comp_health["process_exit_code"] = proc.returncode
                except Exception:
                    pass
            elif comp_name == "jarvis_body":
                # We're always running
                comp_health["process_ok"] = True
                comp_health["pid"] = os.getpid()

            # Check 3: HTTP health endpoint (with timeout)
            if config.health_check_http_enabled:
                for health_url in component["health_urls"]:
                    try:
                        import aiohttp
                        async with aiohttp.ClientSession(
                            timeout=aiohttp.ClientTimeout(total=config.health_check_timeout_sec)
                        ) as session:
                            async with session.get(health_url) as resp:
                                if resp.status == 200:
                                    comp_health["http_ok"] = True
                                    comp_health["health_url"] = health_url
                                    break
                    except Exception:
                        continue

            # Determine overall component health
            # For now, heartbeat OR process is sufficient
            comp_health["healthy"] = comp_health["heartbeat_ok"] or comp_health["process_ok"]

            health_results["components"][comp_name] = comp_health

            if not comp_health["healthy"]:
                health_results["overall_healthy"] = False

        # Log health results
        healthy_count = sum(
            1 for c in health_results["components"].values() if c["healthy"]
        )
        total_count = len(health_results["components"])

        self.logger.info(
            f"   ğŸ¥ [v100] Trinity health check: {healthy_count}/{total_count} healthy"
        )

        for comp_name, comp_health in health_results["components"].items():
            status = "âœ…" if comp_health["healthy"] else "âŒ"
            self.logger.debug(
                f"      {status} {comp_name}: heartbeat={comp_health['heartbeat_ok']}, "
                f"process={comp_health['process_ok']}, http={comp_health['http_ok']}"
            )

        return health_results

    async def _initialize_agi_orchestrator(self) -> None:
        """
        v100.0: Initialize the Unified AGI Orchestrator.

        The AGI Orchestrator provides:
        - Unified event bus for cross-repo communication
        - Persistent state management (survives restarts)
        - Learning pipeline (JARVIS -> Reactor -> Prime)
        - Agent registry and activation
        - Cross-repo health aggregation
        """
        try:
            from backend.core.unified_agi_orchestrator import (
                get_agi_orchestrator,
                start_agi_orchestrator,
                AGIComponent,
                AGIEvent,
                EventPriority,
            )

            self.logger.info("ğŸ§  [v100.0] Initializing AGI Orchestrator...")
            print(f"  {TerminalUI.CYAN}ğŸ§  Initializing AGI Orchestrator...{TerminalUI.RESET}")

            # Start the orchestrator
            success = await start_agi_orchestrator()

            if success:
                self.logger.info("   âœ… AGI Orchestrator v100.0 started")
                print(f"  {TerminalUI.GREEN}âœ“ AGI Orchestrator ready{TerminalUI.RESET}")

                # Get orchestrator instance and publish startup event
                orchestrator = await get_agi_orchestrator()

                # Publish Trinity startup event
                await orchestrator.publish_event(AGIEvent(
                    event_type="trinity.startup.complete",
                    source=AGIComponent.JARVIS,
                    priority=EventPriority.HIGH,
                    payload={
                        "jarvis_running": True,
                        "prime_available": hasattr(self, '_jprime_orchestrator_process') and
                                           self._jprime_orchestrator_process is not None,
                        "reactor_available": hasattr(self, '_reactor_core_orchestrator_process') and
                                             self._reactor_core_orchestrator_process is not None,
                    }
                ))

                # Store reference for shutdown
                self._agi_orchestrator = orchestrator
            else:
                self.logger.warning("   âš ï¸ AGI Orchestrator failed to start")
                print(f"  {TerminalUI.YELLOW}âš ï¸ AGI Orchestrator unavailable{TerminalUI.RESET}")

        except ImportError as e:
            self.logger.debug(f"   AGI Orchestrator module not available: {e}")
        except Exception as e:
            self.logger.warning(f"   AGI Orchestrator init failed: {e}")
            # Non-fatal - continue without AGI orchestrator

    async def _shutdown_trinity_components(self) -> None:
        """
        v72.0: Gracefully shutdown Trinity component subprocesses.
        v81.0: Enhanced with TrinityIntegrator coordinated shutdown.

        This is called during cleanup_resources() to ensure all subprocess
        launched by this supervisor are properly terminated.

        Shutdown sequence:
        1. v81.0: Use TrinityIntegrator coordinated shutdown (if available)
        2. Send SIGTERM to each process
        3. Wait up to 5 seconds for graceful shutdown
        4. Send SIGKILL if process doesn't respond
        5. Clean up process references
        """
        self.logger.info("ğŸ”— Shutting down Trinity components...")

        # v104.0: Shutdown Trinity IPC Hub and Bridge v4.0 first
        if hasattr(self, '_trinity_ipc_hub') and self._trinity_ipc_hub is not None:
            try:
                self.logger.info("   [v104.0] Stopping Trinity IPC Hub...")
                await self._trinity_ipc_hub.stop()
                self.logger.info("   âœ… [v104.0] Trinity IPC Hub stopped")
            except Exception as e:
                self.logger.debug(f"   Trinity IPC Hub shutdown error: {e}")
            finally:
                self._trinity_ipc_hub = None

        if hasattr(self, '_trinity_bridge_v4') and self._trinity_bridge_v4 is not None:
            try:
                self.logger.info("   [v104.0] Stopping Trinity Bridge v4.0...")
                await self._trinity_bridge_v4.stop()
                self.logger.info("   âœ… [v104.0] Trinity Bridge v4.0 stopped")
            except Exception as e:
                self.logger.debug(f"   Trinity Bridge v4.0 shutdown error: {e}")
            finally:
                self._trinity_bridge_v4 = None

        # v105.0: Shutdown Trinity State Manager (creates final snapshot)
        if hasattr(self, '_trinity_state_manager') and self._trinity_state_manager is not None:
            try:
                self.logger.info("   [v105.0] Stopping Trinity State Manager...")
                await self._trinity_state_manager.stop()
                self.logger.info("   âœ… [v105.0] Trinity State Manager stopped (final snapshot created)")
            except Exception as e:
                self.logger.debug(f"   Trinity State Manager shutdown error: {e}")
            finally:
                self._trinity_state_manager = None

        # v106.0: Shutdown Trinity Observability (saves graphs and flushes data)
        if hasattr(self, '_trinity_observability') and self._trinity_observability is not None:
            try:
                self.logger.info("   [v106.0] Stopping Trinity Observability...")
                await self._trinity_observability.stop()
                self.logger.info("   âœ… [v106.0] Trinity Observability stopped (data flushed)")
            except Exception as e:
                self.logger.debug(f"   Trinity Observability shutdown error: {e}")
            finally:
                self._trinity_observability = None

        # v100.0: Shutdown AGI Orchestrator first
        if hasattr(self, '_agi_orchestrator') and self._agi_orchestrator is not None:
            try:
                self.logger.info("   [v100.0] Stopping AGI Orchestrator...")
                from backend.core.unified_agi_orchestrator import stop_agi_orchestrator
                await stop_agi_orchestrator()
                self.logger.info("   âœ… [v100.0] AGI Orchestrator stopped")
            except Exception as e:
                self.logger.debug(f"   AGI Orchestrator shutdown error: {e}")
            finally:
                self._agi_orchestrator = None

        # v81.0: Use TrinityIntegrator for coordinated phased shutdown
        if self._trinity_integrator is not None:
            try:
                self.logger.info("   [v81.0] Initiating TrinityIntegrator coordinated shutdown...")
                shutdown_success = await self._trinity_integrator.stop(
                    timeout=30.0,
                    force=False,
                )
                if shutdown_success:
                    self.logger.info("   âœ… [v81.0] TrinityIntegrator coordinated shutdown complete")
                else:
                    self.logger.warning("   âš ï¸ [v81.0] TrinityIntegrator shutdown incomplete - using fallback")
            except Exception as e:
                self.logger.warning(f"   âš ï¸ [v81.0] TrinityIntegrator shutdown error: {e} - using fallback")
            finally:
                self._trinity_integrator = None

        # v90.0: Shutdown J-Prime orchestrator using SafeProcess if available
        if hasattr(self, '_jprime_safe_process') and self._jprime_safe_process is not None:
            # Use SafeProcess for process group shutdown (kills all children too)
            try:
                self.logger.info("   [v90] Stopping J-Prime orchestrator via SafeProcess...")
                exit_code = await self._jprime_safe_process.stop(timeout=5.0)
                self.logger.info(f"   âœ… [v90] J-Prime stopped (PGID kill, exit={exit_code})")
            except Exception as e:
                self.logger.debug(f"   J-Prime SafeProcess shutdown error: {e}")
            finally:
                self._jprime_safe_process = None
                self._jprime_orchestrator_process = None
        elif self._jprime_orchestrator_process is not None:
            # Fallback to legacy shutdown
            try:
                self.logger.info("   Stopping J-Prime orchestrator...")
                self._jprime_orchestrator_process.terminate()
                try:
                    await asyncio.wait_for(
                        self._jprime_orchestrator_process.wait(),
                        timeout=5.0
                    )
                    self.logger.info("   âœ… J-Prime orchestrator stopped gracefully")
                except asyncio.TimeoutError:
                    self.logger.warning("   âš ï¸ J-Prime didn't stop gracefully, killing...")
                    self._jprime_orchestrator_process.kill()
                    await self._jprime_orchestrator_process.wait()
            except ProcessLookupError:
                self.logger.debug("   J-Prime process already terminated")
            except Exception as e:
                self.logger.debug(f"   J-Prime shutdown error: {e}")
            finally:
                self._jprime_orchestrator_process = None

        # v90.0: Shutdown Reactor-Core orchestrator using SafeProcess if available
        if hasattr(self, '_reactor_safe_process') and self._reactor_safe_process is not None:
            # Use SafeProcess for process group shutdown
            try:
                self.logger.info("   [v90] Stopping Reactor-Core orchestrator via SafeProcess...")
                exit_code = await self._reactor_safe_process.stop(timeout=5.0)
                self.logger.info(f"   âœ… [v90] Reactor-Core stopped (PGID kill, exit={exit_code})")
            except Exception as e:
                self.logger.debug(f"   Reactor-Core SafeProcess shutdown error: {e}")
            finally:
                self._reactor_safe_process = None
                self._reactor_core_orchestrator_process = None
        elif self._reactor_core_orchestrator_process is not None:
            # Fallback to legacy shutdown
            try:
                self.logger.info("   Stopping Reactor-Core orchestrator...")
                self._reactor_core_orchestrator_process.terminate()
                try:
                    await asyncio.wait_for(
                        self._reactor_core_orchestrator_process.wait(),
                        timeout=5.0
                    )
                    self.logger.info("   âœ… Reactor-Core orchestrator stopped gracefully")
                except asyncio.TimeoutError:
                    self.logger.warning("   âš ï¸ Reactor-Core didn't stop gracefully, killing...")
                    self._reactor_core_orchestrator_process.kill()
                    await self._reactor_core_orchestrator_process.wait()
            except ProcessLookupError:
                self.logger.debug("   Reactor-Core process already terminated")
            except Exception as e:
                self.logger.debug(f"   Reactor-Core shutdown error: {e}")
            finally:
                self._reactor_core_orchestrator_process = None

        # v100.1: Close file handles to prevent resource leaks
        file_handles_to_close = [
            ('_jprime_stdout_file', 'J-Prime stdout'),
            ('_jprime_stderr_file', 'J-Prime stderr'),
            ('_reactor_stdout_file', 'Reactor-Core stdout'),
            ('_reactor_stderr_file', 'Reactor-Core stderr'),
        ]
        for attr_name, description in file_handles_to_close:
            if hasattr(self, attr_name):
                try:
                    file_handle = getattr(self, attr_name)
                    if file_handle and not file_handle.closed:
                        file_handle.close()
                        self.logger.debug(f"   âœ… [v100.1] Closed {description} file handle")
                except Exception as e:
                    self.logger.debug(f"   Error closing {description} file handle: {e}")
                finally:
                    setattr(self, attr_name, None)

        # v95.0: Stop Subprocess Health Monitor (before trinity health monitor)
        await self._stop_subprocess_health_monitor()

        # v75.0: Stop Trinity Health Monitor
        await self._stop_trinity_health_monitor()

        # v80.0: Stop Cross-Repo Health Monitor and Startup Coordinator
        await self._shutdown_v80_cross_repo_system()

        # v78.0: Shutdown Advanced Orchestrator
        if hasattr(self, '_orchestrator_hooks') and self._orchestrator_hooks:
            try:
                self.logger.info("   Shutting down Advanced Orchestrator...")
                await self._orchestrator_hooks.shutdown()
                self.logger.info("   âœ… Advanced Orchestrator stopped")
            except Exception as e:
                self.logger.debug(f"   Orchestrator shutdown error: {e}")

        # v78.0/v102.0: Shutdown Process Management Components
        # Shutdown in reverse order of initialization
        if hasattr(self, '_process_tree') and self._process_tree:
            try:
                self.logger.info("   Shutting down Process Tree Manager...")
                await self._process_tree.stop_monitoring()
                # v102.0: Use cascading shutdown with exclude_self=True
                # This prevents the supervisor from trying to kill itself, which would
                # cause the confusing "0/1 succeeded" message
                from core.coding_council.advanced import ShutdownStrategy
                await self._process_tree.shutdown_tree(
                    strategy=ShutdownStrategy.CASCADING,
                    timeout=30.0,
                    force_after=10.0,
                    exclude_self=True,  # v102.0: Don't try to kill ourselves
                )
                self.logger.info("   âœ… Process Tree Manager stopped")
            except Exception as e:
                self.logger.debug(f"   Process tree shutdown error: {e}")

        if hasattr(self, '_command_buffer') and self._command_buffer:
            try:
                self.logger.info("   Flushing Command Buffer...")
                stats = self._command_buffer.get_stats()
                if stats.current_size > 0:
                    self.logger.warning(f"   âš ï¸ {stats.current_size} commands still buffered, flushing...")
                    await self._command_buffer.flush()
                self.logger.info("   âœ… Command Buffer shutdown complete")
            except Exception as e:
                self.logger.debug(f"   Command buffer shutdown error: {e}")

        if hasattr(self, '_timeout_manager') and self._timeout_manager:
            try:
                self.logger.info("   Persisting Timeout Manager stats...")
                await self._timeout_manager.persist()
                self.logger.info("   âœ… Timeout Manager stats persisted")
            except Exception as e:
                self.logger.debug(f"   Timeout manager persist error: {e}")

        if hasattr(self, '_retry_manager') and self._retry_manager:
            try:
                self.logger.info("   Persisting Retry Manager stats...")
                await self._retry_manager.persist()
                self.logger.info("   âœ… Retry Manager stats persisted")
            except Exception as e:
                self.logger.debug(f"   Retry manager persist error: {e}")

        # v85.0: Release state coordinator ownership (atomic lock cleanup)
        await self._release_v85_ownership()

        self.logger.info("âœ… Trinity component shutdown complete")

    async def _connect_training_status_hub(self) -> None:
        """
        v10.0: Connect Training Status Hub to Agentic Runner.

        This enables the Feedback Loop by:
        1. Setting the TTS callback on the training hub
        2. Registering training completion callbacks
        3. Enabling voice announcements during training

        Called after the AgenticTaskRunner is initialized.
        """
        try:
            from backend.api.reactor_core_api import get_training_hub

            hub = get_training_hub()

            # Connect TTS callback from agentic runner
            if self._agentic_runner and self._agentic_runner.tts_callback:
                hub.set_tts_callback(self._agentic_runner.tts_callback)
                self.logger.info("   âœ… Training Hub: TTS callback connected")

            # Register for training completion events
            # This triggers hot-swap when training completes
            if self._agentic_runner:
                hub.on_training_completed(self._agentic_runner._on_training_completed)
                hub.on_training_failed(self._agentic_runner._on_training_failed)
                self.logger.info("   âœ… Training Hub: Callbacks registered for hot-swap")

            self.logger.info("ğŸ”— Feedback Loop: Training Status Hub connected")

        except ImportError as e:
            self.logger.debug(f"   Training Hub not available: {e}")
        except Exception as e:
            self.logger.warning(f"   âš ï¸ Training Hub connection failed: {e}")

    async def _run_continuous_scraping(self) -> None:
        """
        v9.4: Enhanced background task for intelligent continuous web scraping.

        Uses IntelligentContinuousScraper for:
        - Adaptive scheduling based on system load and time of day
        - Topic priority queue with automatic discovery
        - Integration with JARVIS learning goals
        - Rate limiting and quality filtering
        - Safe Scout integration from reactor-core
        """
        self.logger.info("ğŸ“… v9.4 Intelligent Continuous Scraping starting...")

        try:
            # Try to use the new IntelligentContinuousScraper
            from backend.autonomy.intelligent_continuous_scraper import (
                get_continuous_scraper,
                ScrapingMode,
                TopicSource,
            )

            self._intelligent_scraper = get_continuous_scraper()

            # Register progress callback
            def on_scraping_progress(progress):
                if progress.pages_scraped_this_cycle > 0:
                    self.logger.debug(
                        f"ğŸ“Š Scraping: {progress.pages_scraped_this_cycle} pages, "
                        f"topic: {progress.current_topic}"
                    )

            self._intelligent_scraper.register_progress_callback(on_scraping_progress)

            # Add initial topics from config
            if self.config.continuous_scraping_topics:
                topics = [t.strip() for t in self.config.continuous_scraping_topics.split(",") if t.strip()]
                for topic in topics:
                    await self._intelligent_scraper.add_topic(
                        topic=topic,
                        priority=8,  # High priority for user-configured topics
                        source=TopicSource.MANUAL
                    )
                self.logger.info(f"ğŸ“š Added {len(topics)} configured topics")

            # Add topics from learning goals
            if hasattr(self, '_learning_goals_manager') and self._learning_goals_manager:
                pending_goals = self._learning_goals_manager.get_pending_goals()
                for goal in pending_goals[:10]:
                    await self._intelligent_scraper.add_topic(
                        topic=goal.topic,
                        priority=goal.priority if hasattr(goal, 'priority') else 5,
                        source=TopicSource.AUTO_DISCOVERED
                    )

            # Start the intelligent scraper
            await self._intelligent_scraper.start(mode=ScrapingMode.CONTINUOUS)

            # Announce startup
            if hasattr(self, 'narrator') and self.narrator:
                await self.narrator.speak(
                    "Intelligent continuous web research is now active.",
                    wait=False
                )

            # Wait until cancelled
            while True:
                await asyncio.sleep(60)

                # Periodically log stats
                stats = self._intelligent_scraper.get_stats()
                if stats["progress"]["cycles_completed"] > 0:
                    self.logger.debug(
                        f"ğŸ“ˆ Scraping stats: {stats['progress']['pages_scraped_total']} total pages, "
                        f"{stats['progress']['topics_completed']} topics completed"
                    )

        except ImportError:
            # Fall back to the legacy scraping mode
            self.logger.warning("IntelligentContinuousScraper not available, using legacy mode")
            await self._run_legacy_continuous_scraping()

        except asyncio.CancelledError:
            self.logger.info("Intelligent continuous scraping stopped")
            if hasattr(self, '_intelligent_scraper') and self._intelligent_scraper:
                await self._intelligent_scraper.stop()
            raise

        except Exception as e:
            self.logger.error(f"Intelligent continuous scraping error: {e}")
            # Fall back to legacy mode
            await self._run_legacy_continuous_scraping()

    async def _run_legacy_continuous_scraping(self) -> None:
        """Legacy continuous scraping fallback for compatibility."""
        interval_seconds = self.config.continuous_scraping_interval_hours * 3600
        self.logger.info(f"ğŸ“… Legacy scraping mode (interval: {self.config.continuous_scraping_interval_hours}h)")

        while True:
            try:
                await asyncio.sleep(interval_seconds)

                self.logger.info("ğŸŒ Starting legacy web scraping cycle...")

                if hasattr(self, '_data_flywheel') and self._data_flywheel:
                    if self._data_flywheel.is_running:
                        self.logger.debug("Flywheel busy, skipping scraping cycle")
                        continue

                    topics = []
                    if self.config.continuous_scraping_topics:
                        topics = [t.strip() for t in self.config.continuous_scraping_topics.split(",") if t.strip()]

                    if not topics and hasattr(self, '_learning_goals_manager') and self._learning_goals_manager:
                        pending_goals = self._learning_goals_manager.get_pending_goals()
                        topics = [g.topic for g in pending_goals[:5]]

                    if topics:
                        self.logger.info(f"ğŸ“š Scraping topics: {topics}")
                        result = await self._data_flywheel.run_full_cycle(
                            include_web_scraping=True,
                            include_training=False,
                        )
                        if result.success:
                            self.logger.info(f"âœ… Scraping complete: {result.progress.web_pages_scraped} pages")

            except asyncio.CancelledError:
                self.logger.info("Legacy scraping stopped")
                break
            except Exception as e:
                self.logger.error(f"Legacy scraping error: {e}")
                await asyncio.sleep(1800)

    async def _stop_intelligence_systems(self) -> None:
        """Stop all intelligence systems gracefully."""
        self.logger.info("ğŸ›‘ Stopping Intelligence Systems...")

        # v9.4: Stop intelligent continuous scraper
        if hasattr(self, '_intelligent_scraper') and self._intelligent_scraper:
            try:
                await self._intelligent_scraper.stop()
                self.logger.info("âœ… Intelligent Scraper stopped")
            except Exception as e:
                self.logger.warning(f"Intelligent Scraper shutdown error: {e}")
            self._intelligent_scraper = None

        # Stop continuous scraping task
        if hasattr(self, '_continuous_scraping_task') and self._continuous_scraping_task:
            self._continuous_scraping_task.cancel()
            try:
                await self._continuous_scraping_task
            except asyncio.CancelledError:
                pass
            self._continuous_scraping_task = None

        # Stop MAS
        if hasattr(self, '_mas') and self._mas:
            await self._mas.stop()
            self._mas = None

        # Stop Neural Mesh v9.4 (with production components)
        # 1. Cancel health monitoring task
        if hasattr(self, '_neural_mesh_health_task') and self._neural_mesh_health_task:
            self._neural_mesh_health_task.cancel()
            try:
                await self._neural_mesh_health_task
            except asyncio.CancelledError:
                pass
            self._neural_mesh_health_task = None

        # 2. Stop JARVIS Neural Mesh Bridge
        if hasattr(self, '_neural_mesh_bridge') and self._neural_mesh_bridge:
            try:
                await self._neural_mesh_bridge.stop()
            except Exception as e:
                self.logger.warning(f"Neural Mesh Bridge shutdown error: {e}")
            self._neural_mesh_bridge = None

        # 3. Stop Neural Mesh Coordinator (stops all agents)
        if hasattr(self, '_neural_mesh_coordinator') and self._neural_mesh_coordinator:
            try:
                await self._neural_mesh_coordinator.stop()
            except Exception as e:
                self.logger.warning(f"Neural Mesh Coordinator shutdown error: {e}")
            self._neural_mesh_coordinator = None

        # 4. Clear agent references
        if hasattr(self, '_neural_mesh_agents'):
            self._neural_mesh_agents = {}

        # 5. Stop basic Neural Mesh (fallback mode)
        if hasattr(self, '_neural_mesh') and self._neural_mesh:
            try:
                await self._neural_mesh.stop()
            except Exception as e:
                self.logger.warning(f"Neural Mesh shutdown error: {e}")
            self._neural_mesh = None

        # Shutdown UAE
        if hasattr(self, '_uae_engine') and self._uae_engine:
            try:
                from intelligence.uae_integration import shutdown_uae
                await shutdown_uae()
            except Exception as e:
                self.logger.warning(f"UAE shutdown error: {e}")
            self._uae_engine = None
            self._enhanced_uae = None

        # Clear CAI
        self._cai = None

        # Shutdown Prime Neural Mesh Bridge
        if hasattr(self, '_prime_neural_mesh_bridge') and self._prime_neural_mesh_bridge:
            try:
                await self._prime_neural_mesh_bridge.shutdown()
            except Exception as e:
                self.logger.warning(f"Prime Neural Mesh shutdown error: {e}")
            self._prime_neural_mesh_bridge = None

        # Shutdown Reactor-Core Integration
        if hasattr(self, '_reactor_core_integration') and self._reactor_core_integration:
            try:
                from autonomy.reactor_core_integration import shutdown_reactor_core
                await shutdown_reactor_core()
            except Exception as e:
                self.logger.warning(f"Reactor-Core shutdown error: {e}")
            self._reactor_core_integration = None

        self.logger.info("âœ… Intelligence Systems stopped")

    # =========================================================================
    # v9.2: Intelligent Training Orchestrator (Reactor-Core Pipeline)
    # =========================================================================
    # Multi-trigger training system that uses reactor-core's full 8-stage pipeline:
    # 1. SCOUTING - Safe Scout web documentation ingestion
    # 2. INGESTING - Parse JARVIS logs and experience events
    # 3. FORMATTING - Convert to training format
    # 4. DISTILLING - Improve examples with teacher models
    # 5. TRAINING - Fine-tune model with LoRA
    # 6. EVALUATING - Benchmark and gatekeeper approval
    # 7. QUANTIZING - Convert to GGUF format
    # 8. DEPLOYING - Update model registry and deploy to JARVIS-Prime
    # =========================================================================

    async def _init_training_orchestrator(self) -> None:
        """
        v9.2: Initialize the Intelligent Training Orchestrator.

        This system coordinates training across JARVIS-AI-Agent, reactor-core, and JARVIS-Prime
        using multiple intelligent triggers:
        - Time-based: Cron schedule (default: 3 AM daily)
        - Data-threshold: When enough new experiences accumulate (default: 100+)
        - Quality-degradation: When model performance drops below threshold
        - Manual: Via API, voice command, or console
        """
        if not self.config.training_scheduler_enabled:
            self.logger.info("â„¹ï¸ Training Orchestrator disabled via configuration")
            return

        self.logger.info("ğŸ§  Initializing Intelligent Training Orchestrator...")

        try:
            # Import reactor-core components
            reactor_core_path = Path(self.config.reactor_core_repo_path)
            if not reactor_core_path.exists():
                self.logger.warning(f"âš ï¸ Reactor-Core not found at {reactor_core_path}")
                return

            import sys
            if str(reactor_core_path) not in sys.path:
                sys.path.insert(0, str(reactor_core_path))

            # Try to import reactor-core scheduler
            try:
                from reactor_core.orchestration.scheduler import (
                    PipelineScheduler,
                    ScheduleConfig,
                    ScheduledRun,
                )
                from reactor_core.orchestration.pipeline import (
                    NightShiftPipeline,
                    PipelineConfig,
                    PipelineStage,
                )

                # Create pipeline config
                pipeline_config = PipelineConfig(
                    work_dir=Path.home() / ".jarvis" / "nightshift",
                    base_model=self.config.training_base_model,
                    lora_rank=self.config.training_lora_rank,
                    epochs=self.config.training_epochs,
                    quantize_method=self.config.training_quantization_method,
                    eval_threshold=self.config.training_eval_threshold,
                    skip_gatekeeper=self.config.training_skip_gatekeeper,
                    jarvis_repo=Path(__file__).parent,
                    prime_host=self.config.jarvis_prime_host,
                    prime_port=self.config.jarvis_prime_port,
                )

                # Create pipeline runner function
                async def run_reactor_core_pipeline() -> Dict[str, Any]:
                    """Execute the full reactor-core training pipeline."""
                    return await self._execute_reactor_core_pipeline(pipeline_config)

                # Create scheduler config
                schedule_config = ScheduleConfig(
                    cron_expression=self.config.training_cron_schedule,
                    timezone=self.config.training_timezone,
                    max_retries=self.config.training_max_retries,
                    retry_delay_minutes=self.config.training_retry_delay_minutes,
                    history_file=Path.home() / ".jarvis" / "training" / "scheduler_history.json",
                    enabled=True,
                    run_on_start=False,
                )

                # Create scheduler
                self._training_orchestrator = PipelineScheduler(
                    pipeline_runner=run_reactor_core_pipeline,
                    config=schedule_config,
                )

                self.logger.info("âœ… Reactor-Core PipelineScheduler initialized")
                self._reactor_core_pipeline_available = True

            except ImportError as e:
                self.logger.info(f"â„¹ï¸ Reactor-Core scheduler not available, using fallback: {e}")
                self._reactor_core_pipeline_available = False

            # Start the orchestrator tasks
            await self._start_training_orchestrator_tasks()

            self.logger.info("âœ… Intelligent Training Orchestrator initialized")
            print(f"  {TerminalUI.GREEN}âœ“ Training Orchestrator: Multi-trigger scheduling active{TerminalUI.RESET}")

            # Broadcast status
            await self._broadcast_training_status(
                status="ready",
                next_scheduled_run=self._get_next_training_time(),
                triggers_enabled={
                    "time_based": True,
                    "data_threshold": self.config.training_data_threshold_enabled,
                    "quality_trigger": self.config.training_quality_trigger_enabled,
                },
            )

        except Exception as e:
            self.logger.warning(f"âš ï¸ Training Orchestrator init error: {e}")

    async def _start_training_orchestrator_tasks(self) -> None:
        """Start all training orchestrator background tasks."""
        # 1. Time-based scheduler (cron)
        if hasattr(self, '_training_orchestrator') and self._training_orchestrator:
            await self._training_orchestrator.start()
            self.logger.info(f"ğŸ“… Cron scheduler started: {self.config.training_cron_schedule}")
        else:
            # Fallback to simple time-based scheduler
            self._training_orchestrator_task = asyncio.create_task(
                self._run_fallback_training_scheduler()
            )

        # 2. Data threshold monitor
        if self.config.training_data_threshold_enabled:
            self._data_threshold_monitor_task = asyncio.create_task(
                self._run_data_threshold_monitor()
            )
            self.logger.info(f"ğŸ“Š Data threshold monitor started (min: {self.config.training_min_new_experiences} experiences)")

        # 3. Quality degradation monitor
        if self.config.training_quality_trigger_enabled:
            self._quality_monitor_task = asyncio.create_task(
                self._run_quality_monitor()
            )
            self.logger.info(f"ğŸ“‰ Quality monitor started (threshold: {self.config.training_quality_threshold})")

    async def _execute_reactor_core_pipeline(self, pipeline_config: Any) -> Dict[str, Any]:
        """
        Execute the full reactor-core 8-stage training pipeline.

        This is the core training function that:
        1. Broadcasts progress to loading server
        2. Runs the full Night Shift pipeline
        3. Handles deployment to JARVIS-Prime
        4. Updates training history
        """
        run_id = f"train-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        self.logger.info(f"ğŸš€ Starting reactor-core pipeline: {run_id}")

        # Broadcast training started
        await self._broadcast_training_status(
            status="running",
            run_id=run_id,
            stage="initializing",
            progress=0,
        )

        # Announce via narrator
        if hasattr(self, 'narrator') and self.narrator:
            await self.narrator.speak(
                "Starting intelligent model training. Running the full reactor-core pipeline.",
                wait=False
            )

        try:
            # Import and run pipeline
            from reactor_core.orchestration.pipeline import NightShiftPipeline

            pipeline = NightShiftPipeline(pipeline_config)

            # Register progress callback
            def on_pipeline_progress(stage: str, progress: float, message: str):
                asyncio.create_task(self._broadcast_training_status(
                    status="running",
                    run_id=run_id,
                    stage=stage,
                    progress=int(progress * 100),
                    message=message,
                ))

            pipeline.add_progress_callback(on_pipeline_progress)

            # Run the pipeline
            result = await pipeline.run()

            if result.success:
                self._last_training_run = datetime.now()

                # Broadcast success
                await self._broadcast_training_status(
                    status="completed",
                    run_id=run_id,
                    stage="deployed",
                    progress=100,
                    result={
                        "model_path": str(result.artifacts.get("model_path", "")),
                        "gguf_path": str(result.artifacts.get("quantized_path", "")),
                        "examples_trained": result.metrics.get("training_examples", 0),
                        "final_loss": result.metrics.get("final_loss", 0),
                        "eval_score": result.metrics.get("eval_score", 0),
                    },
                )

                # Announce success
                if hasattr(self, 'narrator') and self.narrator:
                    await self.narrator.speak(
                        f"Training complete! Model trained on {result.metrics.get('training_examples', 0)} examples.",
                        wait=False
                    )

                # Auto-deploy to JARVIS-Prime if enabled
                if self.config.training_auto_deploy_to_prime:
                    await self._deploy_model_to_prime(result.artifacts)

                return {
                    "success": True,
                    "run_id": run_id,
                    "model_path": str(result.artifacts.get("model_path", "")),
                    "metrics": result.metrics,
                }

            else:
                # Broadcast failure
                await self._broadcast_training_status(
                    status="failed",
                    run_id=run_id,
                    stage=result.failed_stage or "unknown",
                    error=result.error,
                )

                return {
                    "success": False,
                    "run_id": run_id,
                    "error": result.error,
                }

        except Exception as e:
            self.logger.error(f"Pipeline execution error: {e}")

            await self._broadcast_training_status(
                status="failed",
                run_id=run_id,
                error=str(e),
            )

            return {
                "success": False,
                "run_id": run_id,
                "error": str(e),
            }

    async def _run_fallback_training_scheduler(self) -> None:
        """
        Fallback time-based scheduler when reactor-core PipelineScheduler is unavailable.
        Uses the data flywheel for training instead of the full pipeline.
        """
        self.logger.info(f"ğŸ“… Fallback training scheduler started (schedule: {self.config.training_cron_schedule})")

        while True:
            try:
                # Calculate next run time from cron expression
                next_run = self._get_next_training_time()
                if next_run:
                    sleep_seconds = (next_run - datetime.now()).total_seconds()
                    if sleep_seconds > 0:
                        self.logger.debug(f"Next training in {sleep_seconds / 3600:.1f} hours")
                        await asyncio.sleep(sleep_seconds)
                else:
                    # Fallback to simple daily schedule
                    await asyncio.sleep(86400)  # 24 hours

                # Check cooldown
                if not self._check_training_cooldown():
                    self.logger.debug("Training cooldown not elapsed, skipping")
                    continue

                # Run training via flywheel
                if self._data_flywheel and not self._data_flywheel.is_running:
                    await self._trigger_training("scheduled")

            except asyncio.CancelledError:
                self.logger.info("Fallback training scheduler stopped")
                break
            except Exception as e:
                self.logger.error(f"Fallback scheduler error: {e}")
                await asyncio.sleep(3600)

    async def _run_data_threshold_monitor(self) -> None:
        """
        Monitor experience accumulation and trigger training when threshold is reached.

        This provides adaptive training - when JARVIS learns a lot quickly,
        it trains more frequently. During slow periods, it waits for scheduled runs.
        """
        check_interval = self.config.training_data_check_interval_hours * 3600
        self.logger.info(f"ğŸ“Š Data threshold monitor started (interval: {check_interval/3600:.1f}h)")

        experiences_at_last_check = 0

        while True:
            try:
                await asyncio.sleep(check_interval)

                # Check cooldown first
                if not self._check_training_cooldown():
                    continue

                # Get current experience count
                current_experiences = await self._get_experience_count()
                new_experiences = current_experiences - experiences_at_last_check

                self.logger.debug(f"Data threshold check: {new_experiences} new experiences since last check")

                if new_experiences >= self.config.training_min_new_experiences:
                    self.logger.info(
                        f"ğŸ“ˆ Data threshold reached: {new_experiences} new experiences "
                        f"(threshold: {self.config.training_min_new_experiences})"
                    )

                    # Trigger training
                    await self._trigger_training("data_threshold")
                    experiences_at_last_check = current_experiences

            except asyncio.CancelledError:
                self.logger.info("Data threshold monitor stopped")
                break
            except Exception as e:
                self.logger.warning(f"Data threshold monitor error: {e}")
                await asyncio.sleep(1800)  # 30 min retry

    async def _run_quality_monitor(self) -> None:
        """
        Monitor model quality and trigger training when performance degrades.

        Uses JARVIS-Prime's evaluation endpoint to check response quality.
        If quality drops below threshold, triggers retraining.
        """
        check_interval = self.config.training_quality_check_interval_hours * 3600
        self.logger.info(f"ğŸ“‰ Quality monitor started (interval: {check_interval/3600:.1f}h)")

        while True:
            try:
                await asyncio.sleep(check_interval)

                # Check cooldown first
                if not self._check_training_cooldown():
                    continue

                # Get current quality score from JARVIS-Prime
                quality_score = await self._get_model_quality_score()

                if quality_score is not None:
                    self.logger.debug(f"Quality check: score={quality_score:.2f}, threshold={self.config.training_quality_threshold}")

                    if quality_score < self.config.training_quality_threshold:
                        self.logger.warning(
                            f"âš ï¸ Quality degradation detected: {quality_score:.2f} < {self.config.training_quality_threshold}"
                        )

                        # Trigger training
                        await self._trigger_training("quality_degradation")

            except asyncio.CancelledError:
                self.logger.info("Quality monitor stopped")
                break
            except Exception as e:
                self.logger.warning(f"Quality monitor error: {e}")
                await asyncio.sleep(3600)

    async def _trigger_training(self, trigger_source: str) -> bool:
        """
        Trigger a training run from any source.

        Args:
            trigger_source: What triggered this run (scheduled, data_threshold, quality_degradation, manual)

        Returns:
            True if training was successfully started
        """
        self.logger.info(f"ğŸ¯ Training triggered by: {trigger_source}")

        # Broadcast training triggered
        await self._broadcast_training_status(
            status="triggered",
            trigger_source=trigger_source,
        )

        # Announce
        if hasattr(self, 'narrator') and self.narrator:
            trigger_messages = {
                "scheduled": "Starting scheduled training run.",
                "data_threshold": "Starting training. Enough new experiences have accumulated.",
                "quality_degradation": "Model quality has dropped. Starting retraining.",
                "manual": "Manual training requested. Starting now.",
            }
            await self.narrator.speak(
                trigger_messages.get(trigger_source, "Starting training run."),
                wait=False
            )

        # Use reactor-core pipeline if available
        if hasattr(self, '_reactor_core_pipeline_available') and self._reactor_core_pipeline_available:
            if self._training_orchestrator:
                run = await self._training_orchestrator.trigger_now()
                return run.success

        # Fallback to data flywheel
        if self._data_flywheel and not self._data_flywheel.is_running:
            try:
                result = await self._data_flywheel.run_full_cycle(
                    include_web_scraping=True,
                    include_training=True,
                )

                if result.success:
                    self._last_training_run = datetime.now()
                    self.logger.info(f"âœ… Training completed via flywheel")
                    return True
                else:
                    self.logger.warning(f"âš ï¸ Training failed: {result.error}")
                    return False

            except Exception as e:
                self.logger.error(f"Training execution error: {e}")
                return False

        self.logger.warning("No training system available")
        return False

    def _check_training_cooldown(self) -> bool:
        """Check if enough time has passed since last training run."""
        if self._last_training_run is None:
            return True

        cooldown = timedelta(hours=self.config.training_cooldown_hours)
        elapsed = datetime.now() - self._last_training_run

        return elapsed >= cooldown

    def _get_next_training_time(self) -> Optional[datetime]:
        """Get the next scheduled training time based on cron expression."""
        try:
            from croniter import croniter
            cron = croniter(self.config.training_cron_schedule, datetime.now())
            return cron.get_next(datetime)
        except ImportError:
            # Fallback: parse simple HH:MM format
            try:
                hour, minute = map(int, self.config.data_flywheel_training_schedule.split(":"))
                now = datetime.now()
                target = now.replace(hour=hour, minute=minute, second=0, microsecond=0)
                if target <= now:
                    target += timedelta(days=1)
                return target
            except Exception:
                return None
        except Exception:
            return None

    async def _get_experience_count(self) -> int:
        """Get the current count of experiences in the training database."""
        try:
            if self._data_flywheel:
                stats = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self._data_flywheel.get_stats() if hasattr(self._data_flywheel, 'get_stats') else {}
                )
                return stats.get("total_experiences", 0)

            # Fallback: query training database directly
            db_path = Path.home() / ".jarvis" / "learning" / "jarvis_training.db"
            if db_path.exists():
                import sqlite3
                conn = sqlite3.connect(str(db_path))
                cursor = conn.execute("SELECT COUNT(*) FROM experiences WHERE used_in_training = 0")
                count = cursor.fetchone()[0]
                conn.close()
                return count

        except Exception as e:
            self.logger.debug(f"Experience count error: {e}")

        return 0

    async def _get_model_quality_score(self) -> Optional[float]:
        """Get the current model quality score from JARVIS-Prime."""
        try:
            import aiohttp
            prime_url = f"http://{self.config.jarvis_prime_host}:{self.config.jarvis_prime_port}"

            async with aiohttp.ClientSession() as session:
                async with session.get(f"{prime_url}/health/metrics", timeout=10) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        return data.get("model_quality_score", data.get("avg_confidence", None))

        except Exception as e:
            self.logger.debug(f"Quality score fetch error: {e}")

        return None

    async def _deploy_model_to_prime(self, artifacts: Dict[str, Any]) -> bool:
        """Deploy trained model to JARVIS-Prime."""
        try:
            model_path = artifacts.get("quantized_path") or artifacts.get("model_path")
            if not model_path:
                self.logger.warning("No model path in artifacts for deployment")
                return False

            self.logger.info(f"ğŸš€ Deploying model to JARVIS-Prime: {model_path}")

            # Use JARVIS-Prime API to swap model
            import aiohttp
            prime_url = f"http://{self.config.jarvis_prime_host}:{self.config.jarvis_prime_port}"

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{prime_url}/model/swap",
                    json={"model_path": str(model_path)},
                    timeout=60
                ) as resp:
                    if resp.status == 200:
                        self.logger.info("âœ… Model deployed to JARVIS-Prime")
                        return True
                    else:
                        error = await resp.text()
                        self.logger.warning(f"Model deployment failed: {error}")
                        return False

        except Exception as e:
            self.logger.error(f"Model deployment error: {e}")
            return False

    async def _broadcast_training_status(
        self,
        status: str,
        run_id: Optional[str] = None,
        stage: Optional[str] = None,
        progress: Optional[int] = None,
        message: Optional[str] = None,
        trigger_source: Optional[str] = None,
        next_scheduled_run: Optional[datetime] = None,
        triggers_enabled: Optional[Dict[str, bool]] = None,
        result: Optional[Dict[str, Any]] = None,
        error: Optional[str] = None,
    ) -> bool:
        """
        Broadcast training status to loading server.

        This enables the loading page to show real-time training progress
        and allows monitoring of the training pipeline.
        """
        try:
            import aiohttp

            payload = {
                "status": status,
                "timestamp": datetime.now().isoformat(),
            }

            if run_id:
                payload["run_id"] = run_id
            if stage:
                payload["stage"] = stage
            if progress is not None:
                payload["progress"] = progress
            if message:
                payload["message"] = message
            if trigger_source:
                payload["trigger_source"] = trigger_source
            if next_scheduled_run:
                payload["next_scheduled_run"] = next_scheduled_run.isoformat()
            if triggers_enabled:
                payload["triggers_enabled"] = triggers_enabled
            if result:
                payload["result"] = result
            if error:
                payload["error"] = error

            # Add scheduler statistics if available
            if hasattr(self, '_training_orchestrator') and self._training_orchestrator:
                payload["scheduler_stats"] = self._training_orchestrator.get_statistics()

            loading_server_url = f"http://localhost:{self.config.required_ports[2]}/api/training/update"

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    loading_server_url,
                    json=payload,
                    timeout=5
                ) as resp:
                    return resp.status == 200

        except Exception as e:
            self.logger.debug(f"Training status broadcast error: {e}")
            return False

    async def _stop_training_orchestrator(self) -> None:
        """Stop the training orchestrator and all its tasks."""
        self.logger.info("ğŸ›‘ Stopping Training Orchestrator...")

        # Stop reactor-core scheduler
        if hasattr(self, '_training_orchestrator') and self._training_orchestrator:
            try:
                await self._training_orchestrator.stop()
            except Exception as e:
                self.logger.warning(f"Scheduler stop error: {e}")

        # Cancel fallback scheduler task
        if hasattr(self, '_training_orchestrator_task') and self._training_orchestrator_task:
            self._training_orchestrator_task.cancel()
            try:
                await self._training_orchestrator_task
            except asyncio.CancelledError:
                pass
            self._training_orchestrator_task = None

        # Cancel data threshold monitor
        if hasattr(self, '_data_threshold_monitor_task') and self._data_threshold_monitor_task:
            self._data_threshold_monitor_task.cancel()
            try:
                await self._data_threshold_monitor_task
            except asyncio.CancelledError:
                pass
            self._data_threshold_monitor_task = None

        # Cancel quality monitor
        if hasattr(self, '_quality_monitor_task') and self._quality_monitor_task:
            self._quality_monitor_task.cancel()
            try:
                await self._quality_monitor_task
            except asyncio.CancelledError:
                pass
            self._quality_monitor_task = None

        self.logger.info("âœ… Training Orchestrator stopped")

    async def trigger_manual_training(self) -> Dict[str, Any]:
        """
        Public API to trigger manual training run.

        Can be called from voice commands, API endpoints, or console.

        Returns:
            Result dictionary with success status and details
        """
        self.logger.info("ğŸ¯ Manual training triggered")
        success = await self._trigger_training("manual")
        return {
            "success": success,
            "trigger": "manual",
            "timestamp": datetime.now().isoformat(),
        }

    def get_training_status(self) -> Dict[str, Any]:
        """
        Get current training status for API/UI.

        Returns comprehensive status including:
        - Current state (idle, running, etc.)
        - Last run details
        - Next scheduled run
        - Trigger statuses
        """
        status = {
            "orchestrator_enabled": self.config.training_scheduler_enabled,
            "last_training_run": self._last_training_run.isoformat() if self._last_training_run else None,
            "next_scheduled_run": None,
            "cooldown_remaining_hours": None,
            "triggers": {
                "time_based": {
                    "enabled": True,
                    "cron": self.config.training_cron_schedule,
                },
                "data_threshold": {
                    "enabled": self.config.training_data_threshold_enabled,
                    "threshold": self.config.training_min_new_experiences,
                },
                "quality": {
                    "enabled": self.config.training_quality_trigger_enabled,
                    "threshold": self.config.training_quality_threshold,
                },
            },
            "pipeline_config": {
                "base_model": self.config.training_base_model,
                "lora_rank": self.config.training_lora_rank,
                "epochs": self.config.training_epochs,
                "quantization": self.config.training_quantization_method,
            },
        }

        # Add next scheduled run
        next_run = self._get_next_training_time()
        if next_run:
            status["next_scheduled_run"] = next_run.isoformat()

        # Add cooldown info
        if self._last_training_run:
            cooldown = timedelta(hours=self.config.training_cooldown_hours)
            elapsed = datetime.now() - self._last_training_run
            remaining = cooldown - elapsed
            if remaining.total_seconds() > 0:
                status["cooldown_remaining_hours"] = remaining.total_seconds() / 3600

        # Add scheduler stats if available
        if hasattr(self, '_training_orchestrator') and self._training_orchestrator:
            status["scheduler_stats"] = self._training_orchestrator.get_statistics()
            current_run = self._training_orchestrator.get_current_run()
            if current_run:
                status["current_run"] = current_run.to_dict()

        return status

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v9.3: Learning Goals Discovery Methods
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def _run_learning_goals_discovery_loop(self) -> None:
        """
        v9.3: Periodic learning goals discovery loop.

        Runs discovery at configurable intervals (default: every 2 hours) to:
        - Analyze new experiences for learning opportunities
        - Extract topics from log files
        - Leverage reactor-core's TopicDiscovery when available
        - Queue new topics for Safe Scout scraping

        This loop ensures JARVIS continuously discovers what it needs to learn.
        """
        interval_hours = self.config.learning_goals_discovery_interval_hours
        interval_seconds = int(interval_hours * 3600)

        self.logger.info(
            f"ğŸ”„ Learning Goals Discovery loop started "
            f"(interval: {interval_hours}h)"
        )

        # Wait for initial startup to complete
        await asyncio.sleep(60)

        while True:
            try:
                # Wait for next discovery cycle
                await asyncio.sleep(interval_seconds)

                if not self._learning_goals_discovery:
                    continue

                self.logger.debug("ğŸ” Running scheduled learning goals discovery...")

                # Broadcast discovery starting
                await self._broadcast_learning_goals_status(
                    status="discovering",
                    message="Analyzing experiences for new learning topics",
                )

                # Run discovery from all sources in parallel
                from datetime import datetime
                discovery_start = datetime.now()

                tasks = []

                # Discover from experiences
                db_path = Path(__file__).parent / "data" / "jarvis_training.db"
                tasks.append(
                    self._learning_goals_discovery.discover_from_experiences(
                        db_path=db_path,
                        lookback_days=self.config.learning_goals_lookback_days,
                    )
                )

                # Discover from logs
                log_dir = Path(__file__).parent / "logs"
                tasks.append(
                    self._learning_goals_discovery.discover_from_logs(log_dir)
                )

                # Discover with reactor-core
                tasks.append(
                    self._learning_goals_discovery.discover_with_reactor_core()
                )

                # Run all discovery in parallel
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # Count new discoveries
                total_discovered = 0
                by_source = {"experiences": 0, "logs": 0, "reactor_core": 0}
                source_names = ["experiences", "logs", "reactor_core"]

                for i, result in enumerate(results):
                    if isinstance(result, list):
                        count = len(result)
                        total_discovered += count
                        by_source[source_names[i]] = count
                    elif isinstance(result, Exception):
                        self.logger.debug(f"Discovery source error: {result}")

                # Update discovery stats
                self._discovery_stats["total_discovered"] += total_discovered
                self._discovery_stats["last_sources"] = by_source
                self._last_discovery_run = datetime.now()
                self._learning_goals_discovery._last_discovery = datetime.now()

                # Save discovered topics
                self._learning_goals_discovery._save_topics()

                # Calculate elapsed time
                elapsed = (datetime.now() - discovery_start).total_seconds()

                # Get current stats
                stats = self._learning_goals_discovery.get_discovery_stats()

                if total_discovered > 0:
                    self.logger.info(
                        f"ğŸ¯ Discovery cycle complete: "
                        f"+{total_discovered} new topics "
                        f"({stats.get('pending_scrape', 0)} pending scrape, "
                        f"{elapsed:.1f}s)"
                    )
                else:
                    self.logger.debug(f"Discovery cycle: no new topics ({elapsed:.1f}s)")

                # Broadcast discovery complete
                await self._broadcast_learning_goals_status(
                    status="ready",
                    pending_topics=stats.get("pending_scrape", 0),
                    total_topics=stats.get("total_topics", 0),
                    new_discoveries=total_discovered,
                    by_source=by_source,
                )

            except asyncio.CancelledError:
                self.logger.info("Learning goals discovery loop cancelled")
                break
            except Exception as e:
                self.logger.warning(f"Discovery loop error: {e}")
                await asyncio.sleep(60)  # Brief pause before retry

    async def _run_discovery_queue_processor(self) -> None:
        """
        v9.3: Process discovered topics through Safe Scout for scraping.

        This queue processor:
        - Takes pending topics from the discovery system
        - Triggers Safe Scout to scrape documentation URLs
        - Tracks scraping progress and results
        - Marks topics as scraped when complete

        Runs continuously with configurable concurrency.
        """
        concurrency = self.config.learning_goals_scrape_concurrency
        max_pages = self.config.learning_goals_max_pages_per_topic

        self.logger.info(
            f"ğŸ“š Safe Scout queue processor started "
            f"(concurrency: {concurrency}, max_pages: {max_pages})"
        )

        # Wait for discovery to complete initial run
        await asyncio.sleep(120)

        while True:
            try:
                # Check for pending topics
                if not self._learning_goals_discovery:
                    await asyncio.sleep(60)
                    continue

                pending = self._learning_goals_discovery.get_pending_topics(limit=concurrency)

                if not pending:
                    # No topics to scrape, wait and check again
                    await asyncio.sleep(300)  # Check every 5 minutes
                    continue

                self.logger.info(f"ğŸ“š Processing {len(pending)} topics for scraping")

                # Broadcast scraping starting
                await self._broadcast_learning_goals_status(
                    status="scraping",
                    message=f"Scraping {len(pending)} topics",
                    topics_processing=[t.topic for t in pending],
                )

                # Process topics concurrently
                scrape_tasks = []
                for topic in pending:
                    scrape_tasks.append(self._scrape_topic(topic, max_pages))

                results = await asyncio.gather(*scrape_tasks, return_exceptions=True)

                # Count results
                successful = 0
                failed = 0
                total_pages = 0

                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        failed += 1
                        self.logger.debug(f"Scrape error for {pending[i].topic}: {result}")
                    elif isinstance(result, dict):
                        if result.get("success"):
                            successful += 1
                            total_pages += result.get("pages_scraped", 0)
                        else:
                            failed += 1

                # Update stats
                self._discovery_stats["topics_scraped"] += successful
                self._discovery_stats["failed_extractions"] += failed

                self.logger.info(
                    f"ğŸ“š Scraping complete: {successful} succeeded, "
                    f"{failed} failed, {total_pages} pages scraped"
                )

                # Broadcast scraping complete
                stats = self._learning_goals_discovery.get_discovery_stats()
                await self._broadcast_learning_goals_status(
                    status="ready",
                    pending_topics=stats.get("pending_scrape", 0),
                    total_topics=stats.get("total_topics", 0),
                    topics_scraped=successful,
                    pages_scraped=total_pages,
                )

                # Brief pause before next batch
                await asyncio.sleep(30)

            except asyncio.CancelledError:
                self.logger.info("Discovery queue processor cancelled")
                break
            except Exception as e:
                self.logger.warning(f"Queue processor error: {e}")
                await asyncio.sleep(60)

    async def _scrape_topic(self, topic, max_pages: int) -> Dict[str, Any]:
        """
        Scrape documentation for a single topic.

        Uses Safe Scout (reactor-core) if available, otherwise
        falls back to basic URL fetching.

        Args:
            topic: DiscoveredTopic object
            max_pages: Maximum pages to scrape per topic

        Returns:
            Result dictionary with success status and pages scraped
        """
        from datetime import datetime
        result = {
            "topic": topic.topic,
            "success": False,
            "pages_scraped": 0,
            "error": None,
        }

        try:
            # Mark topic as being scraped
            topic.scrape_started_at = datetime.now()

            # Try reactor-core Safe Scout first
            if self._learning_goals_discovery._safe_scout:
                try:
                    # Use SafeScoutOrchestrator for comprehensive scraping
                    scout = self._learning_goals_discovery._safe_scout
                    scrape_result = await scout.scrape_topic(
                        topic=topic.topic,
                        urls=topic.urls,
                        max_pages=max_pages,
                    )

                    if scrape_result and scrape_result.get("success"):
                        result["success"] = True
                        result["pages_scraped"] = scrape_result.get("pages_scraped", 0)
                        result["content_items"] = scrape_result.get("content_items", 0)
                except Exception as e:
                    self.logger.debug(f"SafeScout error: {e}")
                    # Fall through to basic scraping

            # Fallback: Basic URL fetching
            if not result["success"] and topic.urls:
                pages_scraped = 0
                for url in topic.urls[:max_pages]:
                    try:
                        # Use aiohttp for async fetching
                        import aiohttp
                        async with aiohttp.ClientSession() as session:
                            async with session.get(
                                url,
                                timeout=aiohttp.ClientTimeout(total=30),
                                headers={"User-Agent": "JARVIS-Scout/1.0"},
                            ) as response:
                                if response.status == 200:
                                    content = await response.text()
                                    if content and len(content) > 100:
                                        pages_scraped += 1

                                        # Store in training database
                                        await self._store_scraped_content(
                                            url=url,
                                            title=topic.topic,
                                            content=content,
                                            topic=topic.topic,
                                        )
                    except Exception:
                        continue

                if pages_scraped > 0:
                    result["success"] = True
                    result["pages_scraped"] = pages_scraped

            # Mark topic as scraped
            if result["success"]:
                self._learning_goals_discovery.mark_scraped(
                    topic.topic,
                    pages=result["pages_scraped"],
                )

        except Exception as e:
            result["error"] = str(e)
            self.logger.debug(f"Scrape error for {topic.topic}: {e}")

        return result

    async def _store_scraped_content(
        self,
        url: str,
        title: str,
        content: str,
        topic: str,
    ) -> Optional[int]:
        """Store scraped content in the training database."""
        try:
            import sqlite3
            from datetime import datetime

            # Extract text content (basic HTML stripping)
            import re
            text = re.sub(r'<[^>]+>', ' ', content)
            text = re.sub(r'\s+', ' ', text).strip()

            # Limit content size
            if len(text) > 50000:
                text = text[:50000]

            db_path = Path(__file__).parent / "data" / "jarvis_training.db"
            if not db_path.exists():
                return None

            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()

            cursor.execute("""
                INSERT OR REPLACE INTO scraped_content
                (url, title, content, topic, scraped_at, quality_score)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (url, title, text, topic, datetime.now().isoformat(), 0.5))

            conn.commit()
            content_id = cursor.lastrowid
            conn.close()

            return content_id

        except Exception as e:
            self.logger.debug(f"Failed to store scraped content: {e}")
            return None

    async def _broadcast_learning_goals_status(
        self,
        status: str,
        **kwargs,
    ) -> None:
        """
        Broadcast learning goals discovery status to loading server.

        Args:
            status: Current status (ready, discovering, scraping, error)
            **kwargs: Additional status fields
        """
        try:
            import aiohttp
            from datetime import datetime

            # Build payload
            payload = {
                "type": "learning_goals_update",
                "timestamp": datetime.now().isoformat(),
                "status": status,
                "stats": self._discovery_stats,
                "last_discovery": (
                    self._last_discovery_run.isoformat()
                    if self._last_discovery_run
                    else None
                ),
            }
            payload.update(kwargs)

            # Send to loading server
            loading_url = f"http://localhost:{self.config.loading_server_port}/api/learning-goals/update"

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    loading_url,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=5),
                ) as response:
                    if response.status != 200:
                        self.logger.debug(
                            f"Learning goals broadcast failed: {response.status}"
                        )

        except Exception as e:
            self.logger.debug(f"Learning goals broadcast error: {e}")

    async def _stop_learning_goals_discovery(self) -> None:
        """Stop the learning goals discovery system and all its tasks."""
        self.logger.info("ğŸ›‘ Stopping Learning Goals Discovery...")

        # Cancel discovery loop
        if hasattr(self, '_learning_goals_discovery_task') and self._learning_goals_discovery_task:
            self._learning_goals_discovery_task.cancel()
            try:
                await self._learning_goals_discovery_task
            except asyncio.CancelledError:
                pass
            self._learning_goals_discovery_task = None

        # Cancel queue processor
        if hasattr(self, '_discovery_queue_processor_task') and self._discovery_queue_processor_task:
            self._discovery_queue_processor_task.cancel()
            try:
                await self._discovery_queue_processor_task
            except asyncio.CancelledError:
                pass
            self._discovery_queue_processor_task = None

        # Save final state
        if self._learning_goals_discovery:
            self._learning_goals_discovery._save_topics()

        self.logger.info("âœ… Learning Goals Discovery stopped")

    def get_learning_goals_status(self) -> Dict[str, Any]:
        """
        Get current learning goals discovery status for API/UI.

        Returns comprehensive status including:
        - Discovery statistics
        - Pending topics
        - Scraping progress
        - Last discovery time
        """
        status = {
            "enabled": self.config.learning_goals_enabled,
            "auto_discover": self.config.learning_goals_auto_discover,
            "auto_scrape": self.config.learning_goals_auto_scrape,
            "discovery_interval_hours": self.config.learning_goals_discovery_interval_hours,
            "last_discovery": (
                self._last_discovery_run.isoformat()
                if self._last_discovery_run
                else None
            ),
            "stats": self._discovery_stats,
        }

        if self._learning_goals_discovery:
            discovery_stats = self._learning_goals_discovery.get_discovery_stats()
            status["discovery_stats"] = discovery_stats

            # Get top pending topics
            pending = self._learning_goals_discovery.get_pending_topics(limit=5)
            status["pending_topics"] = [
                {
                    "topic": t.topic,
                    "priority": t.priority,
                    "source": t.source.value,
                    "urls": t.urls[:2],
                }
                for t in pending
            ]

        return status

    async def add_learning_goal(self, topic: str, priority: float = 8.0) -> Dict[str, Any]:
        """
        Public API to add a manual learning goal.

        Can be called from voice commands, API endpoints, or console.

        Args:
            topic: Topic to learn about
            priority: Priority (0-10, default 8 for manual)

        Returns:
            Result with created topic details
        """
        if not self._learning_goals_discovery:
            return {"success": False, "error": "Discovery system not initialized"}

        try:
            new_topic = self._learning_goals_discovery.add_manual_topic(topic, priority)
            self.logger.info(f"ğŸ¯ Manual learning goal added: {topic}")

            return {
                "success": True,
                "topic": new_topic.to_dict(),
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # End of Learning Goals Discovery Methods
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v9.4: Intelligent Model Manager (Gap 6 Fix)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def _init_model_manager(self) -> None:
        """
        v9.4: Initialize the Intelligent Model Manager.

        This ensures JARVIS-Prime always has a model to load by:
        - Checking if models exist in JARVIS-Prime models directory
        - Auto-downloading base models if missing (memory-aware selection)
        - Watching for reactor-core trained model deployments
        - Supporting hot-swap without restart

        Model Selection Logic:
        - RAM >= 8GB: Can load production models (Mistral-7B, Llama-2-7B)
        - RAM 4-8GB: Use smaller models (TinyLlama, Phi-2)
        - RAM < 4GB: Minimal models or cloud fallback

        Integration Points:
        - JARVIS-Prime model_downloader.py for download
        - JARVIS-Prime model_registry.py for versioning
        - Reactor-core output dir for trained models
        - Loading server for status broadcasts
        """
        if not self.config.model_manager_enabled:
            self.logger.info("â„¹ï¸ Model Manager disabled via configuration")
            return

        self.logger.info("ğŸ§  Initializing Intelligent Model Manager...")

        try:
            from typing import Dict, Any, Optional, List
            from datetime import datetime
            from pathlib import Path
            import psutil
            import aiohttp

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Model Catalog (matches jarvis-prime/docker/model_downloader.py)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            MODEL_CATALOG = {
                "tinyllama-chat": {
                    "repo_id": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
                    "filename": "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
                    "size_mb": 670,
                    "min_ram_gb": 2.0,
                    "description": "TinyLlama 1.1B - Testing and simple chat",
                    "context_length": 2048,
                },
                "phi-2": {
                    "repo_id": "TheBloke/phi-2-GGUF",
                    "filename": "phi-2.Q4_K_M.gguf",
                    "size_mb": 1600,
                    "min_ram_gb": 4.0,
                    "description": "Phi-2 - Excellent for coding and reasoning",
                    "context_length": 2048,
                },
                "mistral-7b-instruct": {
                    "repo_id": "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
                    "filename": "mistral-7b-instruct-v0.2.Q4_K_M.gguf",
                    "size_mb": 4370,
                    "min_ram_gb": 8.0,
                    "description": "Mistral 7B Instruct - Production quality",
                    "context_length": 8192,
                },
                "llama-3-8b-instruct": {
                    "repo_id": "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF",
                    "filename": "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
                    "size_mb": 4920,
                    "min_ram_gb": 10.0,
                    "description": "Llama 3 8B Instruct - Latest and greatest",
                    "context_length": 8192,
                },
            }

            class IntelligentModelManager:
                """
                v9.4: Comprehensive model manager with auto-download and reactor-core integration.

                Features:
                - Memory-aware model selection
                - Auto-download from HuggingFace Hub
                - Reactor-core trained model deployment
                - Hot-swap capability
                - Version registry with rollback
                """

                def __init__(
                    self,
                    prime_path: Path,
                    models_dir: str = "models",
                    config: Optional[Any] = None,
                    logger: Optional[Any] = None,
                ):
                    self.prime_path = prime_path
                    self.models_dir = prime_path / models_dir
                    self.config = config
                    self.logger = logger

                    # Ensure models directory exists
                    self.models_dir.mkdir(parents=True, exist_ok=True)

                    # State tracking
                    self.current_model: Optional[str] = None
                    self.current_model_path: Optional[Path] = None
                    self.model_registry: Dict[str, Any] = {}
                    self.download_in_progress = False

                    # Reactor-core integration
                    self._reactor_core_watcher = None
                    self._watcher_task = None

                    # Load existing metadata
                    self._load_registry()

                def _load_registry(self) -> None:
                    """Load model registry from disk."""
                    import json
                    metadata_file = self.models_dir / "models_metadata.json"
                    if metadata_file.exists():
                        try:
                            # v92.0: Use safe file reading to avoid "Bad file descriptor" errors
                            self.model_registry = _safe_read_json(metadata_file, default={})
                            if self.logger:
                                self.logger.debug(f"Loaded model registry with {len(self.model_registry.get('models', {}))} models")
                        except Exception as e:
                            if self.logger:
                                self.logger.debug(f"Failed to load registry: {e}")
                            self.model_registry = {"models": {}, "current": None}
                    else:
                        self.model_registry = {"models": {}, "current": None}

                def _save_registry(self) -> None:
                    """Save model registry to disk."""
                    import json
                    metadata_file = self.models_dir / "models_metadata.json"
                    self.model_registry["last_updated"] = datetime.now().isoformat()
                    metadata_file.write_text(json.dumps(self.model_registry, indent=2, default=str))

                def get_available_memory_gb(self) -> float:
                    """Get available system memory in GB."""
                    mem = psutil.virtual_memory()
                    return mem.available / (1024 ** 3)

                def select_optimal_model(self) -> Optional[str]:
                    """
                    Select the best model based on available memory.

                    Returns model name from catalog or None if no suitable model.
                    """
                    available_gb = self.get_available_memory_gb()

                    if self.logger:
                        self.logger.debug(f"Available memory: {available_gb:.1f}GB")

                    # Sort models by min_ram_gb descending (prefer larger models)
                    suitable_models = [
                        (name, info)
                        for name, info in MODEL_CATALOG.items()
                        if info["min_ram_gb"] <= available_gb
                    ]

                    if not suitable_models:
                        if self.logger:
                            self.logger.warning("No models suitable for available memory")
                        return None

                    # Sort by min_ram_gb descending to get the best model we can run
                    suitable_models.sort(key=lambda x: x[1]["min_ram_gb"], reverse=True)
                    return suitable_models[0][0]

                def check_model_exists(self, model_name: str = None) -> Optional[Path]:
                    """
                    Check if a model exists in the models directory.

                    Returns path to model file if found, None otherwise.
                    """
                    # Check current.gguf symlink first
                    current_link = self.models_dir / "current.gguf"
                    if current_link.exists():
                        resolved = current_link.resolve()
                        if resolved.exists() and resolved.stat().st_size > 1000:
                            return resolved

                    # Check for specific model
                    if model_name and model_name in MODEL_CATALOG:
                        model_info = MODEL_CATALOG[model_name]
                        model_file = self.models_dir / model_info["filename"]
                        if model_file.exists() and model_file.stat().st_size > 1000:
                            return model_file

                    # Check for any .gguf files
                    gguf_files = list(self.models_dir.glob("*.gguf"))
                    for gguf in gguf_files:
                        if gguf.stat().st_size > 1000 and not gguf.is_symlink():
                            return gguf

                    return None

                async def ensure_model_available(self) -> Dict[str, Any]:
                    """
                    Ensure a model is available for JARVIS-Prime.

                    Returns status dict with:
                    - available: bool
                    - model_name: str
                    - model_path: Path
                    - source: str (existing, downloaded, reactor_core)
                    """
                    result = {
                        "available": False,
                        "model_name": None,
                        "model_path": None,
                        "source": None,
                        "error": None,
                    }

                    try:
                        # Step 1: Check for existing model
                        existing_path = self.check_model_exists()
                        if existing_path:
                            result["available"] = True
                            result["model_path"] = existing_path
                            result["model_name"] = existing_path.name
                            result["source"] = "existing"
                            self.current_model_path = existing_path
                            if self.logger:
                                self.logger.info(f"âœ“ Found existing model: {existing_path.name}")
                            return result

                        # Step 2: Check for reactor-core trained models
                        reactor_model = await self._check_reactor_core_models()
                        if reactor_model:
                            result["available"] = True
                            result["model_path"] = reactor_model
                            result["model_name"] = reactor_model.name
                            result["source"] = "reactor_core"
                            self.current_model_path = reactor_model
                            return result

                        # Step 3: Auto-download if enabled
                        if self.config and self.config.model_manager_auto_download:
                            # Select optimal model for available memory
                            if self.config.model_manager_auto_select:
                                model_name = self.select_optimal_model()
                            else:
                                model_name = self.config.model_manager_default_model

                            if model_name:
                                if self.logger:
                                    self.logger.info(f"ğŸ“¥ Auto-downloading model: {model_name}")
                                download_result = await self.download_model(model_name)
                                if download_result["success"]:
                                    result["available"] = True
                                    result["model_path"] = download_result["path"]
                                    result["model_name"] = model_name
                                    result["source"] = "downloaded"
                                    return result
                                else:
                                    result["error"] = download_result.get("error", "Download failed")

                    except Exception as e:
                        result["error"] = str(e)
                        if self.logger:
                            self.logger.error(f"Model availability check failed: {e}")

                    return result

                async def _check_reactor_core_models(self) -> Optional[Path]:
                    """Check for trained models from reactor-core."""
                    try:
                        # Check reactor-core output directories
                        reactor_paths = [
                            Path(__file__).parent.parent / "reactor-core" / "output" / "models",
                            Path(os.getenv("REACTOR_CORE_OUTPUT", "")) / "deployed",
                            self.prime_path / "reactor-core-output" / "deployed",
                        ]

                        for reactor_path in reactor_paths:
                            if reactor_path.exists():
                                gguf_files = list(reactor_path.glob("*.gguf"))
                                if gguf_files:
                                    # Sort by modification time, newest first
                                    gguf_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                                    newest = gguf_files[0]
                                    if newest.stat().st_size > 1000:
                                        if self.logger:
                                            self.logger.info(f"âœ“ Found reactor-core model: {newest.name}")
                                        return newest
                    except Exception as e:
                        if self.logger:
                            self.logger.debug(f"Reactor-core check error: {e}")
                    return None

                async def download_model(self, model_name: str) -> Dict[str, Any]:
                    """
                    Download a model from HuggingFace Hub.

                    Uses jarvis-prime's model_downloader if available,
                    otherwise falls back to direct huggingface_hub download.
                    """
                    result = {"success": False, "path": None, "error": None}

                    if model_name not in MODEL_CATALOG:
                        result["error"] = f"Unknown model: {model_name}"
                        return result

                    if self.download_in_progress:
                        result["error"] = "Download already in progress"
                        return result

                    self.download_in_progress = True
                    model_info = MODEL_CATALOG[model_name]

                    try:
                        # Try using jarvis-prime's downloader
                        try:
                            import sys
                            if str(self.prime_path) not in sys.path:
                                sys.path.insert(0, str(self.prime_path))

                            from jarvis_prime.docker.model_downloader import ModelDownloader
                            downloader = ModelDownloader(models_dir=str(self.models_dir))
                            download_result = await downloader.download_catalog_model(model_name)

                            if download_result.get("success"):
                                result["success"] = True
                                result["path"] = Path(download_result["path"])
                                self._update_registry(model_name, result["path"], "downloaded")
                                return result
                        except ImportError:
                            if self.logger:
                                self.logger.debug("jarvis-prime downloader not available, using fallback")

                        # Fallback: Direct huggingface_hub download
                        from huggingface_hub import hf_hub_download

                        if self.logger:
                            self.logger.info(
                                f"ğŸ“¥ Downloading {model_name} ({model_info['size_mb']}MB)..."
                            )

                        downloaded_path = await asyncio.get_event_loop().run_in_executor(
                            None,
                            lambda: hf_hub_download(
                                repo_id=model_info["repo_id"],
                                filename=model_info["filename"],
                                local_dir=str(self.models_dir),
                                local_dir_use_symlinks=False,
                            )
                        )

                        model_path = Path(downloaded_path)
                        if model_path.exists():
                            # Create current.gguf symlink
                            current_link = self.models_dir / "current.gguf"
                            if current_link.exists():
                                current_link.unlink()
                            current_link.symlink_to(model_path)

                            result["success"] = True
                            result["path"] = model_path
                            self._update_registry(model_name, model_path, "downloaded")

                            if self.logger:
                                self.logger.info(f"âœ… Downloaded {model_name} to {model_path}")

                    except Exception as e:
                        result["error"] = str(e)
                        if self.logger:
                            self.logger.error(f"Download failed: {e}")
                    finally:
                        self.download_in_progress = False

                    return result

                def _update_registry(self, model_name: str, path: Path, source: str) -> None:
                    """Update model registry with new model."""
                    self.model_registry["models"][model_name] = {
                        "path": str(path),
                        "source": source,
                        "downloaded_at": datetime.now().isoformat(),
                        "size_mb": path.stat().st_size / (1024 * 1024) if path.exists() else 0,
                    }
                    self.model_registry["current"] = model_name
                    self._save_registry()
                    self.current_model = model_name
                    self.current_model_path = path

                async def start_reactor_core_watcher(self) -> None:
                    """Start watching for reactor-core model deployments."""
                    if not self.config or not self.config.model_manager_reactor_core_watch:
                        return

                    try:
                        # Use jarvis-prime's reactor_core_watcher if available
                        import sys
                        if str(self.prime_path) not in sys.path:
                            sys.path.insert(0, str(self.prime_path))

                        from jarvis_prime.docker.reactor_core_watcher import ReactorCoreWatcher

                        watch_dirs = [
                            Path(os.getenv("REACTOR_CORE_OUTPUT", "")) / "pending",
                            self.prime_path / "reactor-core-output" / "pending",
                            Path(__file__).parent.parent / "reactor-core" / "output" / "pending",
                        ]

                        for watch_dir in watch_dirs:
                            if watch_dir.exists():
                                self._reactor_core_watcher = ReactorCoreWatcher(
                                    watch_dir=str(watch_dir),
                                    models_dir=str(self.models_dir),
                                    on_model_deployed=self._on_model_deployed,
                                )
                                await self._reactor_core_watcher.start()
                                if self.logger:
                                    self.logger.info(f"âœ“ Reactor-core watcher started: {watch_dir}")
                                break
                    except ImportError:
                        if self.logger:
                            self.logger.debug("Reactor-core watcher not available")
                    except Exception as e:
                        if self.logger:
                            self.logger.debug(f"Reactor-core watcher start error: {e}")

                async def _on_model_deployed(self, model_path: Path, manifest: Dict[str, Any]) -> None:
                    """Callback when reactor-core deploys a new model."""
                    if self.logger:
                        self.logger.info(f"ğŸš€ Reactor-core model deployed: {model_path.name}")

                    model_name = manifest.get("model_id", model_path.stem)
                    self._update_registry(model_name, model_path, "reactor_core")

                    # Trigger hot-swap if enabled
                    if self.config and self.config.model_manager_hot_swap_enabled:
                        if self.logger:
                            self.logger.info("ğŸ”„ Triggering hot-swap for new model...")
                        # Hot-swap would be handled by JARVIS-Prime's HotSwapManager

                def get_status(self) -> Dict[str, Any]:
                    """Get current model manager status."""
                    return {
                        "enabled": True,
                        "current_model": self.current_model,
                        "current_model_path": str(self.current_model_path) if self.current_model_path else None,
                        "available_memory_gb": self.get_available_memory_gb(),
                        "download_in_progress": self.download_in_progress,
                        "models_in_registry": len(self.model_registry.get("models", {})),
                        "reactor_watcher_active": self._reactor_core_watcher is not None,
                    }

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Create and Initialize Model Manager
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self._model_manager = IntelligentModelManager(
                prime_path=self.config.jarvis_prime_repo_path,
                models_dir=self.config.jarvis_prime_models_dir,
                config=self.config,
                logger=self.logger,
            )

            # Broadcast initialization start
            await self._broadcast_model_manager_status(
                status="initializing",
                message="Checking model availability...",
            )

            # Ensure a model is available
            model_result = await self._model_manager.ensure_model_available()

            if model_result["available"]:
                self._current_model_info = {
                    "name": model_result["model_name"],
                    "path": str(model_result["model_path"]),
                    "size_mb": model_result["model_path"].stat().st_size / (1024 * 1024) if model_result["model_path"] else 0,
                    "loaded": True,
                    "source": model_result["source"],
                }

                self.logger.info(
                    f"âœ… Model Manager ready: {model_result['model_name']} "
                    f"(source: {model_result['source']})"
                )
                print(f"  {TerminalUI.GREEN}âœ“ Model Manager: {model_result['model_name']} available{TerminalUI.RESET}")

                # Broadcast success
                await self._broadcast_model_manager_status(
                    status="ready",
                    model_name=model_result["model_name"],
                    model_path=str(model_result["model_path"]),
                    source=model_result["source"],
                )
            else:
                self.logger.warning(
                    f"âš ï¸ No model available: {model_result.get('error', 'Unknown error')}"
                )
                print(f"  {TerminalUI.YELLOW}âš ï¸ Model Manager: No model available{TerminalUI.RESET}")

                # Broadcast warning
                await self._broadcast_model_manager_status(
                    status="no_model",
                    error=model_result.get("error"),
                )

            # Start reactor-core watcher
            if self.config.model_manager_reactor_core_watch:
                await self._model_manager.start_reactor_core_watcher()

        except Exception as e:
            self.logger.warning(f"âš ï¸ Model Manager initialization failed: {e}")
            import traceback
            self.logger.debug(traceback.format_exc())

    async def _broadcast_model_manager_status(
        self,
        status: str,
        **kwargs,
    ) -> None:
        """Broadcast model manager status to loading server."""
        try:
            import aiohttp
            from datetime import datetime

            payload = {
                "type": "model_manager_update",
                "timestamp": datetime.now().isoformat(),
                "status": status,
                "model_info": self._current_model_info,
            }
            payload.update(kwargs)

            loading_url = f"http://localhost:{self.config.loading_server_port}/api/model/update"

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    loading_url,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=5),
                ) as response:
                    if response.status != 200:
                        self.logger.debug(f"Model status broadcast failed: {response.status}")

        except Exception as e:
            self.logger.debug(f"Model status broadcast error: {e}")

    def get_model_manager_status(self) -> Dict[str, Any]:
        """Get current model manager status for API/UI."""
        status = {
            "enabled": self.config.model_manager_enabled,
            "model_info": self._current_model_info,
            "download_in_progress": self._model_download_in_progress,
        }

        if self._model_manager:
            status["manager_status"] = self._model_manager.get_status()

        return status

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # End of Model Manager Methods
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    async def _run_training_scheduler(self) -> None:
        """
        Legacy method - redirects to new orchestrator.
        Kept for backward compatibility with existing code that creates this task.
        """
        # This method is now a no-op since training is handled by the orchestrator
        # The orchestrator is initialized separately in _init_training_orchestrator
        self.logger.debug("Legacy _run_training_scheduler called - training handled by orchestrator")

        # If orchestrator isn't initialized yet, run the fallback
        if not hasattr(self, '_training_orchestrator') or not self._training_orchestrator:
            await self._run_fallback_training_scheduler()

    async def _run_experience_collection_loop(self) -> None:
        """
        v9.1: Background task for continuous experience collection.

        This loop:
        - Periodically collects experiences from JARVIS interactions
        - Syncs with reactor-core JARVISConnector
        - Monitors experience quality and quantity
        - Triggers learning goal discovery when patterns emerge

        Runs every 5 minutes to ensure fresh training data.
        """
        collection_interval = 300  # 5 minutes
        self.logger.info(f"ğŸ”„ Experience collection loop started (interval: {collection_interval}s)")

        while True:
            try:
                await asyncio.sleep(collection_interval)

                if not self._data_flywheel or self._data_flywheel.is_running:
                    continue

                # Collect recent experiences
                try:
                    if self._data_flywheel._jarvis_connector:
                        experiences = await asyncio.get_event_loop().run_in_executor(
                            None,
                            self._data_flywheel._jarvis_connector.collect_recent_experiences,
                            1  # Last 1 hour
                        )

                        if experiences:
                            self.logger.debug(f"Collected {len(experiences)} recent experiences")

                            # Store in training database
                            if hasattr(self._data_flywheel, 'add_experience'):
                                for exp in experiences[:10]:  # Limit batch size
                                    self._data_flywheel.add_experience(
                                        source="auto_collection",
                                        input_text=exp.get("input", ""),
                                        output_text=exp.get("output", ""),
                                        context=exp.get("context", {}),
                                        quality_score=exp.get("quality", 0.5),
                                    )

                except Exception as collect_err:
                    self.logger.debug(f"Experience collection cycle: {collect_err}")

                # Check if we should trigger learning goal discovery
                try:
                    if hasattr(self, '_learning_goals_manager') and self._learning_goals_manager:
                        stats = self._data_flywheel.get_stats() if hasattr(self._data_flywheel, 'get_stats') else {}
                        total_experiences = stats.get("total_experiences", 0)

                        # Discover new goals every 100 experiences
                        if total_experiences > 0 and total_experiences % 100 == 0:
                            await self._learning_goals_manager.auto_discover_topics()
                            self.logger.info(f"Auto-discovered learning topics at {total_experiences} experiences")
                except Exception as goal_err:
                    self.logger.debug(f"Learning goal discovery: {goal_err}")

            except asyncio.CancelledError:
                self.logger.info("Experience collection loop stopped")
                break
            except Exception as e:
                self.logger.warning(f"Experience collection error: {e}")
                await asyncio.sleep(60)  # Wait a minute before retrying

    async def _stop_data_flywheel(self) -> None:
        """Stop the Data Flywheel and related tasks."""
        # Cancel experience collection loop
        if hasattr(self, '_experience_collection_task') and self._experience_collection_task:
            self._experience_collection_task.cancel()
            try:
                await self._experience_collection_task
            except asyncio.CancelledError:
                pass
            self._experience_collection_task = None

        # v9.2: Stop intelligent training orchestrator (replaces old scheduler)
        await self._stop_training_orchestrator()

        # v9.3: Stop learning goals discovery system
        await self._stop_learning_goals_discovery()

        # Cancel legacy training scheduler (if still running)
        if self._training_scheduler_task:
            self._training_scheduler_task.cancel()
            try:
                await self._training_scheduler_task
            except asyncio.CancelledError:
                pass
            self._training_scheduler_task = None

        # Cancel any running flywheel
        if self._data_flywheel:
            await self._data_flywheel.cancel()
            self._data_flywheel = None

        self._learning_goals_manager = None

    async def _stop_jarvis_prime(self) -> None:
        """Stop JARVIS-Prime subprocess or container."""
        # Stop subprocess
        if self._jarvis_prime_process:
            try:
                self._jarvis_prime_process.terminate()
                await asyncio.wait_for(self._jarvis_prime_process.wait(), timeout=5.0)
            except Exception:
                self._jarvis_prime_process.kill()
            self._jarvis_prime_process = None

        # Stop Docker container
        if self.config.jarvis_prime_use_docker:
            try:
                proc = await asyncio.create_subprocess_exec(
                    "docker", "stop", "jarvis-prime",
                    stdout=asyncio.subprocess.DEVNULL,
                    stderr=asyncio.subprocess.DEVNULL,
                )
                await asyncio.wait_for(proc.wait(), timeout=10.0)
            except Exception:
                pass

        # Stop Data Flywheel and training scheduler
        await self._stop_data_flywheel()

        # Stop reactor-core watcher
        if self._reactor_core_watcher:
            await self._reactor_core_watcher.stop()
            self._reactor_core_watcher = None

    def _wire_router_to_runner(self) -> None:
        """
        Wire the TieredCommandRouter's execute_tier2 to use the AgenticTaskRunner.

        This creates a seamless flow:
        Voice Command -> TieredRouter -> AgenticRunner -> Computer Use
        """
        if not self._tiered_router or not self._agentic_runner:
            return

        # Store reference for the closure
        runner = self._agentic_runner
        vbia_adapter = self._vbia_adapter

        async def execute_tier2_via_runner(command: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
            """Execute Tier 2 command via the unified AgenticTaskRunner."""
            from core.agentic_task_runner import RunnerMode

            try:
                # Execute via runner
                result = await runner.run(
                    goal=command,
                    mode=RunnerMode.AUTONOMOUS,
                    context=context,
                    narrate=True,
                )

                return {
                    "success": result.success,
                    "response": result.final_message,
                    "actions_count": result.actions_count,
                    "duration_ms": result.execution_time_ms,
                    "mode": result.mode,
                    "watchdog_status": result.watchdog_status,
                }

            except Exception as e:
                return {
                    "success": False,
                    "error": str(e),
                    "response": f"Task execution failed: {e}",
                }

        # Monkey-patch the router's execute_tier2 method
        self._tiered_router.execute_tier2 = execute_tier2_via_runner
        self.logger.debug("[Supervisor] Router execute_tier2 wired to AgenticRunner")

    async def _on_hot_reload_triggered(self, changed_files: List[str]) -> None:
        """
        v5.0: Handle hot reload trigger - restart JARVIS with new code.
        
        This is called by the HotReloadWatcher when file changes are detected.
        """
        self.logger.info(f"ğŸ”¥ Hot reload triggered by {len(changed_files)} file change(s)")
        restart_start_time = time.time()
        
        if not self._supervisor:
            self.logger.warning("No supervisor reference - cannot hot reload")
            return
        
        # Detect file types for announcement
        file_types = []
        for f in changed_files:
            ext = Path(f).suffix.lower()
            if ext in (".py", ".pyx"):
                if "Python" not in file_types:
                    file_types.append("Python")
            elif ext in (".rs",):
                if "Rust" not in file_types:
                    file_types.append("Rust")
            elif ext in (".swift",):
                if "Swift" not in file_types:
                    file_types.append("Swift")
            elif ext in (".js", ".jsx"):
                if "JavaScript" not in file_types:
                    file_types.append("JavaScript")
            elif ext in (".ts", ".tsx"):
                if "TypeScript" not in file_types:
                    file_types.append("TypeScript")
        
        # v5.0: Announce changes via voice coordinator
        try:
            from core.supervisor import get_startup_voice_coordinator
            voice_coordinator = get_startup_voice_coordinator()
            
            # Announce detection
            await voice_coordinator.announce_hot_reload_detected(
                file_count=len(changed_files),
                file_types=file_types,
                target="backend",
            )
            
            # Announce restart
            await voice_coordinator.announce_hot_reload_restarting(target="backend")
            
        except Exception as e:
            self.logger.debug(f"Voice announcement failed (non-fatal): {e}")
        
        try:
            # Clear Python cache for changed modules
            self._clear_python_cache(changed_files)
            
            # Request supervisor restart (this gracefully stops and restarts JARVIS)
            # The supervisor has built-in restart capability
            from core.supervisor import request_restart, RestartSource, RestartUrgency
            
            await request_restart(
                source=RestartSource.DEV_MODE,
                reason=f"Hot reload: {len(changed_files)} file(s) changed",
                urgency=RestartUrgency.NORMAL,
            )
            
            self.logger.info("âœ… Restart requested via supervisor")
            
            # v5.0: Announce completion (after restart completes)
            # Note: This runs before restart actually completes, but we still announce
            # The actual restart will trigger a full startup sequence with its own announcements
            
        except Exception as e:
            self.logger.error(f"Hot reload failed: {e}")
            # Fallback: Try direct supervisor restart
            try:
                if hasattr(self._supervisor, '_handle_restart_request'):
                    await self._supervisor._handle_restart_request()
            except Exception as e2:
                self.logger.error(f"Fallback restart also failed: {e2}")
    
    def _clear_python_cache(self, changed_files: List[str]) -> None:
        """Clear Python import cache for changed modules."""
        import shutil
        
        # Clear __pycache__ directories
        for cache_dir in (Path(__file__).parent / "backend").glob("**/__pycache__"):
            try:
                shutil.rmtree(cache_dir)
            except Exception:
                pass
        
        # Clear sys.modules for JARVIS modules
        modules_to_clear = []
        for module_name in list(sys.modules.keys()):
            if 'jarvis' in module_name.lower() or 'backend' in module_name.lower():
                modules_to_clear.append(module_name)
        
        for module_name in modules_to_clear:
            sys.modules.pop(module_name, None)
        
        self.logger.debug(f"Cleared {len(modules_to_clear)} cached modules")
    
    def _print_config_summary(self, supervisor) -> None:
        """Print supervisor configuration summary."""
        config = supervisor.config
        
        update_status = f"Enabled ({config.update.check.interval_seconds}s)" if config.update.check.enabled else "Disabled"
        idle_status = f"Enabled ({config.idle.threshold_seconds // 3600}h threshold)" if config.idle.enabled else "Disabled"
        rollback_status = "Enabled" if config.rollback.auto_on_boot_failure else "Disabled"
        
        # v3.0: Zero-Touch status
        zero_touch_status = "Enabled" if self.config.zero_touch_enabled else "Disabled"
        dms_status = f"Enabled ({self.config.dms_probation_seconds}s)" if self.config.dms_enabled else "Disabled"
        
        TerminalUI.print_info("Mode", config.mode.value.upper(), highlight=True)
        TerminalUI.print_info("Update Check", update_status)
        TerminalUI.print_info("Idle Updates", idle_status)
        TerminalUI.print_info("Auto-Rollback", rollback_status)
        TerminalUI.print_info("Max Retries", str(config.health.max_crash_retries))
        
        # v3.0: Zero-Touch info
        TerminalUI.print_info("Zero-Touch", zero_touch_status, highlight=self.config.zero_touch_enabled)
        TerminalUI.print_info("Dead Man's Switch", dms_status)
        TerminalUI.print_info("AGI OS Integration", "Enabled" if self.config.agi_os_enabled else "Disabled")
        
        TerminalUI.print_info("Loading Page", f"Enabled (port {self.config.required_ports[2]})", highlight=True)
        TerminalUI.print_info("Voice Narration", "Enabled" if self.config.voice_enabled else "Disabled", highlight=True)


# =============================================================================
# Entry Point
# =============================================================================

def parse_args():
    """Parse command-line arguments."""
    import argparse

    parser = argparse.ArgumentParser(
        description="JARVIS Supervisor - Unified System Orchestrator (v116.0)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Start supervisor (intelligent auto mode - handles existing supervisors)
  python run_supervisor.py

  # Check status of running supervisor
  python run_supervisor.py --status

  # Gracefully restart running supervisor (in-place restart)
  python run_supervisor.py --restart

  # Take over from existing supervisor (old shuts down, new starts)
  python run_supervisor.py --takeover

  # Shut down running supervisor
  python run_supervisor.py --shutdown

  # Force start (dangerous - use only when stuck)
  python run_supervisor.py --force

  # Execute single agentic task and exit
  python run_supervisor.py --task "Open Safari and check the weather"

  # Execute task with specific mode
  python run_supervisor.py --task "Organize desktop" --mode autonomous

  # Disable voice narration
  python run_supervisor.py --no-voice

  # Start without cross-repo coordination
  python run_supervisor.py --no-connect-repos

v116.0 Intelligent Startup Behavior:
  When a supervisor is already running, the default behavior is:
  1. Check if existing supervisor is healthy
  2. If healthy: Show status and available options (no duplicate startup)
  3. If unhealthy: Automatically take over from the zombie supervisor
  4. If unreachable: Clean up and start fresh
"""
    )

    parser.add_argument(
        "--task", "-t",
        help="Execute a single agentic task and exit"
    )

    parser.add_argument(
        "--mode", "-m",
        choices=["direct", "supervised", "autonomous"],
        default="autonomous",
        help="Execution mode for --task (default: autonomous)"
    )

    parser.add_argument(
        "--no-voice",
        action="store_true",
        help="Disable voice narration"
    )

    parser.add_argument(
        "--timeout",
        type=float,
        default=300.0,
        help="Task timeout in seconds (default: 300)"
    )

    # v116.0: Intelligent restart/takeover options
    parser.add_argument(
        "--restart",
        action="store_true",
        help="Gracefully restart existing supervisor (sends restart signal)"
    )

    parser.add_argument(
        "--takeover",
        action="store_true",
        help="Request takeover from existing supervisor (it shuts down, we start)"
    )

    parser.add_argument(
        "--force",
        action="store_true",
        help="Force start even if another supervisor is running (USE WITH CAUTION)"
    )

    parser.add_argument(
        "--status",
        action="store_true",
        help="Check supervisor status and exit (no startup)"
    )

    parser.add_argument(
        "--shutdown",
        action="store_true",
        help="Request graceful shutdown of running supervisor and exit"
    )

    parser.add_argument(
        "--cleanup",
        action="store_true",
        help="Run comprehensive zombie cleanup and exit (v109.7)"
    )

    # v119.0: Cross-repo coordination is ENABLED BY DEFAULT
    # Use --no-connect-repos to disable
    parser.add_argument(
        "--connect-repos",
        action="store_true",
        help="Enable cross-repo coordination (this is the default - flag is optional)"
    )

    parser.add_argument(
        "--no-connect-repos",
        action="store_true",
        help="Disable cross-repo coordination (repos connect by default without any flag)"
    )

    parser.add_argument(
        "--auto",
        action="store_true",
        default=True,
        help="Intelligent auto mode: auto-detect best action (default)"
    )

    return parser.parse_args()


async def run_single_task(
    bootstrapper: SupervisorBootstrapper,
    goal: str,
    mode: str,
    timeout: float,
) -> int:
    """
    Run a single agentic task and return.

    This is used when --task is provided. The supervisor will:
    1. Initialize just the essential components (watchdog, router, runner)
    2. Execute the task
    3. Shutdown and exit
    """
    from core.agentic_task_runner import RunnerMode, get_agentic_runner

    # Wait for agentic runner to be ready
    runner = get_agentic_runner()
    if not runner:
        bootstrapper.logger.error("Agentic runner not initialized")
        return 1

    if not runner.is_ready:
        bootstrapper.logger.info("Waiting for agentic runner to be ready...")
        for _ in range(30):  # Wait up to 30 seconds
            await asyncio.sleep(1)
            if runner.is_ready:
                break
        if not runner.is_ready:
            bootstrapper.logger.error("Agentic runner failed to initialize")
            return 1

    bootstrapper.logger.info(f"Executing task: {goal}")
    bootstrapper.logger.info(f"Mode: {mode}")

    try:
        result = await asyncio.wait_for(
            runner.run(
                goal=goal,
                mode=RunnerMode(mode),
                narrate=bootstrapper.config.voice_enabled,
            ),
            timeout=timeout,
        )

        print("\n" + "=" * 60)
        print("TASK RESULT")
        print("=" * 60)
        print(f"Success: {result.success}")
        print(f"Message: {result.final_message}")
        print(f"Time: {result.execution_time_ms:.0f}ms")
        print(f"Actions: {result.actions_count}")

        if result.learning_insights:
            print("\nInsights:")
            for insight in result.learning_insights:
                print(f"  - {insight}")

        if result.error:
            print(f"\nError: {result.error}")

        return 0 if result.success else 1

    except asyncio.TimeoutError:
        bootstrapper.logger.error(f"Task timed out after {timeout}s")
        return 1
    except Exception as e:
        bootstrapper.logger.error(f"Task execution failed: {e}")
        return 1


# =============================================================================
# UNIFIED TRINITY CONNECTOR v1.0
# =============================================================================
# Single command integration for JARVIS + JARVIS Prime + Reactor Core
#
# This is the MASTER ORCHESTRATOR that:
# 1. Initializes enhanced self-improvement with Claude Code-like behaviors
# 2. Connects to cross-repo event bus with Lamport clocks & dead letter queue
# 3. Establishes health consensus across all three repositories
# 4. Provides unified API for improvement requests, training, and deployment
# =============================================================================


class UnifiedTrinityConnector:
    """
    Master orchestrator that connects JARVIS, JARVIS Prime, and Reactor Core.

    This is the single point of coordination for the entire Trinity system,
    providing:
    - Cross-repo self-improvement with diff preview and approval
    - Atomic multi-repo transactions with 2PC
    - Distributed health consensus
    - Unified improvement request routing
    - Session memory across all repos
    """

    def __init__(self):
        self.logger = logging.getLogger("Trinity.Connector")
        self._running = False
        self._initialized = False

        # Components (lazy-loaded)
        self._enhanced_self_improvement = None
        self._enhanced_cross_repo = None
        self._session_id = f"trinity_{uuid.uuid4().hex[:12]}"

        # Repository paths (from environment or defaults)
        self._jarvis_path = Path(os.environ.get(
            "JARVIS_PATH",
            Path(__file__).parent
        ))
        self._prime_path = Path(os.environ.get(
            "JARVIS_PRIME_PATH",
            self._jarvis_path.parent / "JARVIS-Prime"
        ))
        self._reactor_path = Path(os.environ.get(
            "REACTOR_CORE_PATH",
            self._jarvis_path.parent / "reactor-core"
        ))

        # Health state
        self._health = {
            "jarvis": False,
            "prime": False,
            "reactor": False,
        }

        # Real-time communication
        self._realtime_broadcaster = None

    async def initialize(
        self,
        websocket_manager=None,
        voice_system=None,
        menu_bar=None,
        event_bus=None,
    ) -> bool:
        """
        Initialize the Trinity connector.

        This sets up all enhanced components and establishes
        connections to JARVIS Prime and Reactor Core.

        Args:
            websocket_manager: WebSocket manager for real-time UI updates
            voice_system: Voice system for real-time narration
            menu_bar: Menu bar for status indicators
            event_bus: Event bus for system events
        """
        if self._initialized:
            return True

        self.logger.info("=" * 60)
        self.logger.info("  UNIFIED TRINITY CONNECTOR v1.0")
        self.logger.info("=" * 60)
        self.logger.info(f"  Session: {self._session_id}")
        self.logger.info(f"  JARVIS: {self._jarvis_path}")
        self.logger.info(f"  Prime: {self._prime_path}")
        self.logger.info(f"  Reactor: {self._reactor_path}")
        self.logger.info("=" * 60)

        try:
            # Phase 1: Initialize enhanced self-improvement
            self.logger.info("[Trinity] Phase 1: Enhanced Self-Improvement...")
            from core.ouroboros.native_integration import (
                get_enhanced_self_improvement,
            )
            self._enhanced_self_improvement = get_enhanced_self_improvement()
            await self._enhanced_self_improvement.initialize()
            self.logger.info("[Trinity] âœ“ Enhanced self-improvement ready")
            self.logger.info(f"  - Session: {self._enhanced_self_improvement.session_memory.session_id}")
            self.logger.info(f"  - Diff preview: enabled")
            self.logger.info(f"  - Multi-file orchestration: enabled")

            # Phase 2: Initialize enhanced cross-repo orchestrator
            self.logger.info("[Trinity] Phase 2: Cross-Repo Orchestrator...")
            from core.ouroboros.cross_repo import (
                get_enhanced_cross_repo_orchestrator,
                initialize_enhanced_cross_repo,
            )
            await initialize_enhanced_cross_repo()
            self._enhanced_cross_repo = get_enhanced_cross_repo_orchestrator()
            self.logger.info("[Trinity] âœ“ Cross-repo orchestrator ready")
            self.logger.info(f"  - Lamport clock: {self._enhanced_cross_repo._lamport_clock.node_id}")
            self.logger.info(f"  - Dead letter queue: enabled")
            self.logger.info(f"  - Health consensus: enabled")

            # Phase 3: Validate repository connections
            self.logger.info("[Trinity] Phase 3: Repository Validation...")
            await self._validate_repositories()

            # Phase 4: Establish health consensus
            self.logger.info("[Trinity] Phase 4: Health Consensus...")
            health = self._enhanced_cross_repo._health_consensus.get_cluster_health()
            self.logger.info(f"  - Alive nodes: {health['alive_nodes']}/{health['total_nodes']}")
            self.logger.info(f"  - Quorum: {'yes' if health['quorum'] else 'NO'}")

            # Phase 5: Connect real-time broadcaster (if communication channels provided)
            if websocket_manager or voice_system or menu_bar or event_bus:
                self.logger.info("[Trinity] Phase 5: Real-Time Communication...")
                try:
                    from core.ouroboros.ui_integration import connect_realtime_broadcaster
                    self._realtime_broadcaster = await connect_realtime_broadcaster(
                        websocket_manager=websocket_manager,
                        voice_system=voice_system,
                        menu_bar=menu_bar,
                        event_bus=event_bus,
                    )
                    self.logger.info("[Trinity] âœ“ Real-time communication enabled")
                    self.logger.info(f"  - Voice narration: {'yes' if voice_system else 'no'}")
                    self.logger.info(f"  - WebSocket streaming: {'yes' if websocket_manager else 'no'}")
                    self.logger.info(f"  - Menu bar updates: {'yes' if menu_bar else 'no'}")
                except Exception as e:
                    self.logger.warning(f"[Trinity] Real-time broadcaster not available: {e}")
                    self._realtime_broadcaster = None
            else:
                self.logger.info("[Trinity] Phase 5: Skipped (no communication channels provided)")
                self._realtime_broadcaster = None

            self._initialized = True
            self._running = True

            self.logger.info("=" * 60)
            self.logger.info("  TRINITY CONNECTOR INITIALIZED SUCCESSFULLY")
            self.logger.info("=" * 60)

            return True

        except Exception as e:
            self.logger.error(f"[Trinity] Initialization failed: {e}")
            import traceback
            self.logger.debug(traceback.format_exc())
            return False

    async def _validate_repositories(self) -> None:
        """Validate all repository connections."""
        # JARVIS (always available - we're in it)
        self._health["jarvis"] = True
        self.logger.info(f"  - JARVIS: âœ“ (local)")

        # JARVIS Prime
        if self._prime_path.exists():
            prime_git = self._prime_path / ".git"
            if prime_git.exists():
                self._health["prime"] = True
                self.logger.info(f"  - JARVIS Prime: âœ“ ({self._prime_path})")
            else:
                self.logger.warning(f"  - JARVIS Prime: âš  not a git repo")
        else:
            self.logger.warning(f"  - JARVIS Prime: âš  not found ({self._prime_path})")

        # Reactor Core
        if self._reactor_path.exists():
            reactor_git = self._reactor_path / ".git"
            if reactor_git.exists():
                self._health["reactor"] = True
                self.logger.info(f"  - Reactor Core: âœ“ ({self._reactor_path})")
            else:
                self.logger.warning(f"  - Reactor Core: âš  not a git repo")
        else:
            self.logger.warning(f"  - Reactor Core: âš  not found ({self._reactor_path})")

    async def shutdown(self) -> None:
        """Shutdown the Trinity connector."""
        if not self._running:
            return

        self.logger.info("[Trinity] Shutting down...")

        try:
            # Disconnect real-time broadcaster first
            if self._realtime_broadcaster:
                try:
                    from core.ouroboros.ui_integration import disconnect_realtime_broadcaster
                    await disconnect_realtime_broadcaster()
                except Exception as e:
                    self.logger.warning(f"[Trinity] Realtime broadcaster disconnect error: {e}")
                self._realtime_broadcaster = None

            if self._enhanced_cross_repo:
                from core.ouroboros.cross_repo import shutdown_enhanced_cross_repo
                await shutdown_enhanced_cross_repo()

            if self._enhanced_self_improvement:
                await self._enhanced_self_improvement.shutdown()

        except Exception as e:
            self.logger.warning(f"[Trinity] Shutdown error: {e}")

        self._running = False
        self._initialized = False
        self.logger.info("[Trinity] Shutdown complete")

    async def execute_improvement_with_preview(
        self,
        target: str,
        goal: str,
        require_approval: bool = True,
    ):
        """
        Execute improvement with diff preview and approval workflow.

        This is the main interface for Claude Code-like self-improvement.
        """
        if not self._initialized:
            await self.initialize()

        return await self._enhanced_self_improvement.execute_with_preview(
            target=target,
            goal=goal,
            require_approval=require_approval,
        )

    async def execute_multi_file_improvement(
        self,
        files_and_goals: list,
        shared_context: str = None,
    ):
        """Execute atomic multi-file improvement."""
        if not self._initialized:
            await self.initialize()

        return await self._enhanced_self_improvement.execute_multi_file_improvement(
            files_and_goals=files_and_goals,
            shared_context=shared_context,
        )

    async def request_cross_repo_improvement(
        self,
        file_path: str,
        goal: str,
    ) -> str:
        """
        Request improvement across repositories with proper ordering.

        Uses Lamport clocks for causal ordering.
        """
        if not self._initialized:
            await self.initialize()

        return await self._enhanced_cross_repo.request_improvement_with_ordering(
            file_path=file_path,
            goal=goal,
        )

    def get_status(self) -> dict:
        """Get comprehensive Trinity status."""
        status = {
            "session_id": self._session_id,
            "running": self._running,
            "initialized": self._initialized,
            "repositories": self._health,
        }

        if self._enhanced_self_improvement:
            status["self_improvement"] = self._enhanced_self_improvement.get_status()

        if self._enhanced_cross_repo:
            status["cross_repo"] = self._enhanced_cross_repo.get_status()

        return status


# Global Trinity connector
_trinity_connector: Optional[UnifiedTrinityConnector] = None


def get_trinity_connector() -> UnifiedTrinityConnector:
    """Get the global Trinity connector."""
    global _trinity_connector
    if _trinity_connector is None:
        _trinity_connector = UnifiedTrinityConnector()
    return _trinity_connector


async def initialize_trinity() -> bool:
    """Initialize the Trinity connector (call from run_supervisor.py)."""
    connector = get_trinity_connector()
    return await connector.initialize()


async def shutdown_trinity() -> None:
    """Shutdown the Trinity connector."""
    global _trinity_connector
    if _trinity_connector:
        await _trinity_connector.shutdown()
        _trinity_connector = None


async def main() -> int:
    """
    Main entry point.

    v111.0: Integrated unified signal handling for graceful shutdown.
    Signal escalation: 1st=graceful, 2nd=faster, 3rd=immediate exit.

    v121.0: Signal handler only installed for supervisor mode, not CLI commands.
    """
    # v121.0: Parse args FIRST to determine if we're in CLI command mode
    args = parse_args()

    # v121.0: Check if this is a CLI command that should NOT install signal handlers
    # These commands make IPC calls and exit - they don't need signal handling
    is_cli_command = any([
        args.status,
        args.restart,
        args.shutdown,
        getattr(args, 'cleanup', False),
    ])

    # =========================================================================
    # v111.0: UNIFIED SIGNAL HANDLING - Only for supervisor mode
    # =========================================================================
    # v121.0: Don't install for CLI commands - they need to survive signals
    # during restart verification. The restart command was getting killed
    # by SIGTERM because it had the signal handler installed.
    # =========================================================================
    if not is_cli_command:
        signal_handler = get_unified_signal_handler()
        loop = asyncio.get_running_loop()
        signal_handler.install(loop)
    else:
        # v121.0: CLI mode - IMMEDIATELY protect from signals
        # The supervisor restart process sends signals that can kill the client
        # We need protection BEFORE any IPC calls
        import signal as _sig
        import os as _os
        _sig.signal(_sig.SIGINT, _sig.SIG_IGN)
        _sig.signal(_sig.SIGTERM, _sig.SIG_IGN)
        _sig.signal(_sig.SIGHUP, _sig.SIG_IGN)
        # Try to create own process group to isolate from supervisor's signal group
        try:
            _os.setpgrp()
        except Exception:
            pass

    # =========================================================================
    # v116.0: INTELLIGENT SINGLETON MANAGEMENT - Smart startup with takeover
    # =========================================================================
    # Enhanced from v110.0 to support intelligent restart/takeover modes:
    # - --status: Just check status and exit
    # - --shutdown: Request graceful shutdown of running supervisor
    # - --restart: Request restart of running supervisor (no new instance)
    # - --takeover: Gracefully take over from running supervisor
    # - --force: Force start even if another is running (dangerous)
    # - --auto (default): Intelligent decision based on situation
    # =========================================================================
    if _SINGLETON_AVAILABLE:
        # v116.0: Use async version since we're in async context
        from backend.core.supervisor_singleton import send_supervisor_command

        async def send_ipc_command(cmd: str, timeout: float = 5.0) -> dict:
            """Helper to send IPC command with error handling."""
            try:
                return await send_supervisor_command(cmd, timeout=timeout)
            except Exception as e:
                return {"success": False, "error": str(e)}

        is_running, existing_state = is_supervisor_running()

        # =====================================================================
        # Handle --status: Just show status and exit
        # =====================================================================
        if args.status:
            if is_running and existing_state:
                print(f"\n{'='*70}")
                print(f"âœ… JARVIS SUPERVISOR STATUS")
                print(f"{'='*70}")
                print(f"   Status:      RUNNING")
                print(f"   Entry Point: {existing_state.get('entry_point', 'unknown')}")
                print(f"   PID:         {existing_state.get('pid', 'unknown')}")
                print(f"   Started:     {existing_state.get('started_at', 'unknown')}")
                print(f"   Working Dir: {existing_state.get('working_dir', 'unknown')}")

                # Get detailed health via IPC
                try:
                    health_result = await send_ipc_command('health', timeout=5.0)
                    if health_result.get('success'):
                        # v116.0: Health data is in 'result' key from IPC wrapper
                        health_data = health_result.get('result', {})
                        print(f"\n   Health Level: {health_data.get('health_level', 'unknown')}")
                        print(f"   Healthy:      {health_data.get('healthy', False)}")
                        checks = health_data.get('checks', {})
                        if checks:
                            for check_name, check_result in checks.items():
                                status = "âœ…" if check_result.get('healthy') else "âŒ"
                                print(f"   {status} {check_name}: {check_result.get('level', 'unknown')}")
                except Exception as e:
                    print(f"\n   âš ï¸  Could not get health details: {e}")

                print(f"{'='*70}\n")
                return 0
            else:
                print(f"\n{'='*70}")
                print(f"âšª JARVIS SUPERVISOR STATUS")
                print(f"{'='*70}")
                print(f"   Status: NOT RUNNING")
                print(f"{'='*70}\n")
                return 1

        # =====================================================================
        # Handle --cleanup: Run comprehensive zombie cleanup and exit (v109.7)
        # =====================================================================
        if args.cleanup:
            print(f"\n{'='*70}")
            print(f"ğŸ§¹ JARVIS COMPREHENSIVE ZOMBIE CLEANUP v109.7")
            print(f"{'='*70}")
            print(f"   Cleaning up zombie processes across JARVIS ecosystem...")
            print(f"   Repos: JARVIS, J-Prime, Reactor-Core")
            print(f"   Ports: 8000 (J-Prime), 8010 (JARVIS), 8090 (Reactor-Core)")
            print(f"{'='*70}\n")

            try:
                # Create a minimal config for cleanup
                cleanup_config = BootstrapConfig()
                cleanup_logger = logging.getLogger("jarvis.cleanup")
                cleanup_logger.setLevel(logging.INFO)

                zombie_cleaner = ComprehensiveZombieCleanup(
                    config=cleanup_config,
                    logger=cleanup_logger,
                    enable_cross_repo=True,
                    enable_memory_aware=True,
                    enable_circuit_breaker=False,  # Don't use circuit breaker for manual run
                )

                result = await zombie_cleaner.run_comprehensive_cleanup()

                print(f"\n{'='*70}")
                print(f"ğŸ“Š CLEANUP RESULTS")
                print(f"{'='*70}")
                print(f"   Zombies found:  {result['zombies_found']}")
                print(f"   Zombies killed: {result['zombies_killed']}")
                print(f"   Ports freed:    {result['ports_freed']}")
                print(f"   Duration:       {result['duration_ms']}ms")
                print(f"   Phases:         {', '.join(result['phases_completed'])}")

                if result['errors']:
                    print(f"   Errors:         {', '.join(result['errors'])}")

                stats = zombie_cleaner.get_stats()
                print(f"\n   Cross-repo orphans cleaned: {stats['orphans_cleaned']}")
                print(f"   Cross-repo resources cleaned: {stats['cross_repo_cleaned']}")
                print(f"{'='*70}\n")

                return 0 if result['success'] else 1

            except Exception as e:
                print(f"âŒ Cleanup failed: {e}")
                return 1

        # =====================================================================
        # Handle --shutdown: Request graceful shutdown of running supervisor
        # =====================================================================
        if args.shutdown:
            if is_running and existing_state:
                print(f"\nğŸ”„ Requesting graceful shutdown of supervisor (PID {existing_state.get('pid')})...")
                result = await send_ipc_command('shutdown', timeout=10.0)
                if result.get('success') or result.get('shutdown_initiated'):
                    print(f"âœ… Shutdown initiated. Supervisor will exit gracefully.")
                    return 0
                else:
                    print(f"âŒ Failed to initiate shutdown: {result.get('error', 'unknown')}")
                    return 1
            else:
                print(f"âšª No supervisor running to shut down.")
                return 0

        # =====================================================================
        # Handle --restart: Request restart of running supervisor (no new instance)
        # =====================================================================
        if args.restart and not args.takeover:
            if is_running and existing_state:
                print(f"\nğŸ”„ Requesting restart of supervisor (PID {existing_state.get('pid')})...")

                # v121.0: CRITICAL - Protect restart client from signals BEFORE making IPC call
                # The supervisor restart process can send signals that kill the client
                # Install signal protection EARLY to survive the entire restart process
                import signal as _signal
                import os as _os

                # Save original handlers
                _orig_sigint = _signal.signal(_signal.SIGINT, _signal.SIG_IGN)
                _orig_sigterm = _signal.signal(_signal.SIGTERM, _signal.SIG_IGN)
                _orig_sighup = _signal.signal(_signal.SIGHUP, _signal.SIG_IGN)

                # Also detach from process group to avoid group signals
                try:
                    # Create new process group to isolate from supervisor's signals
                    _os.setpgrp()
                except Exception:
                    pass  # May not be possible in all contexts

                try:
                    # v117.1: Enhanced restart handling with connection drop tolerance
                    # The server sends SIGHUP 100ms after responding, which may cause
                    # connection drops during response transmission. We handle this gracefully.
                    result = await send_ipc_command('restart', timeout=10.0)

                    # v122.0: Success cases: explicit success, restart_initiated, or connection issues
                    # (connection issues during restart are EXPECTED - supervisor is replacing itself)
                    error = result.get('error', '')
                    connection_dropped = (
                        'Expecting value' in error or  # Empty response (server restarted)
                        'Connection' in error or       # Connection reset/closed
                        'EOF' in error or              # End of stream
                        'BrokenPipe' in error or       # Pipe broken
                        'timeout' in error.lower() or  # IPC timeout (supervisor restarting)
                        'IPC' in error                 # Any IPC-related error
                    )

                    if result.get('success') or result.get('restart_initiated') or connection_dropped:
                        if connection_dropped:
                            print(f"âœ… Restart initiated (connection closed - server is restarting)")
                        else:
                            print(f"âœ… Restart initiated. Supervisor will restart in-place.")
                            marker_id = result.get('marker_id')
                            if marker_id:
                                print(f"   Restart marker: {marker_id}")

                        # v121.0: Signal protection already active from outer scope
                        from pathlib import Path as _Path
                        print(f"   Waiting for supervisor to restart...")
                        original_pid = existing_state.get('pid')
                        original_started = existing_state.get('started_at', '')

                        # v121.0: Use state file as primary restart detection
                        # This works even when IPC is down during restart transition
                        state_file = _Path.home() / ".jarvis" / "locks" / "supervisor.state"
                        max_wait = 90.0  # Increased timeout for heavy init
                        poll_interval = 0.5
                        waited = 0.0
                        ipc_available = False
                        last_status = "waiting for os.execv()..."

                        # Initial wait for os.execv() to trigger (150ms delay + some buffer)
                        await asyncio.sleep(0.5)
                        waited += 0.5

                        while waited < max_wait:
                            # v121.0: PRIMARY METHOD - Check state file timestamp
                            # This is reliable even when IPC is down
                            try:
                                if state_file.exists():
                                    import json as _json
                                    state_data = _json.loads(state_file.read_text())
                                    new_started = state_data.get('started_at', '')
                                    new_pid = state_data.get('pid')

                                    # os.execv keeps same PID but started_at changes
                                    if new_started and new_started != original_started:
                                        # State file has new timestamp - restart happened!
                                        # Now verify IPC is responding
                                        try:
                                            ping_result, _ = await send_supervisor_command('ping', timeout=2.0)
                                            if ping_result and ping_result.get('success'):
                                                print(f"âœ… Supervisor restarted successfully!")
                                                print(f"   PID: {new_pid} | Started: {new_started}")
                                                return 0
                                            else:
                                                if last_status != "ipc_starting":
                                                    print(f"   â†’ New process started, IPC initializing...")
                                                    last_status = "ipc_starting"
                                        except Exception:
                                            if last_status != "ipc_starting":
                                                print(f"   â†’ New process started (started_at changed)")
                                                last_status = "ipc_starting"
                            except Exception as e:
                                # State file not ready yet - this is expected during restart
                                if last_status != "state_unavailable" and waited > 3.0:
                                    print(f"   â†’ Supervisor restarting (state file updating...)")
                                    last_status = "state_unavailable"

                            # v121.0: SECONDARY METHOD - Try IPC if state file check failed
                            # This handles edge cases where state file might be stale
                            try:
                                status_result, _ = await send_supervisor_command('restart-status', timeout=2.0)
                                if status_result:
                                    phase = status_result.get('phase', 'unknown')
                                    is_new = status_result.get('is_new_instance', False)

                                    if is_new or phase == "completed":
                                        new_pid = status_result.get('current_pid', original_pid)
                                        print(f"âœ… Supervisor restarted successfully (PID {new_pid})")
                                        return 0
                                    elif phase and phase != last_status:
                                        print(f"   â†’ Phase: {phase}")
                                        last_status = phase
                            except Exception:
                                pass  # IPC down during restart - expected

                            await asyncio.sleep(poll_interval)
                            waited += poll_interval

                            # Progress indicator every 10 seconds
                            if int(waited) % 10 == 0 and waited > 0:
                                print(f"   ... still waiting ({int(waited)}s / {int(max_wait)}s)")

                        # v121.0: Final status check after timeout
                        error_log = "/tmp/jarvis_execv_error.log"
                        if _os.path.exists(error_log):
                            print(f"âŒ Restart failed! os.execv() error detected:")
                            try:
                                with open(error_log, 'r') as f:
                                    print(f"   {f.read().strip()}")
                            except Exception:
                                pass
                            return 1

                        # Check final state
                        is_back, new_state = is_supervisor_running()
                        if is_back and new_state:
                            new_started = new_state.get('started_at', '')
                            if new_started != original_started:
                                new_pid = new_state.get('pid', original_pid)
                                print(f"âœ… Supervisor restarted (PID {new_pid})")
                                print(f"   Note: IPC may still be initializing")
                                return 0
                            else:
                                print(f"âš ï¸ Supervisor running but may not have restarted yet.")
                                print(f"   Check with --status in a few seconds.")
                        else:
                            print(f"âŒ Supervisor does not appear to be running after restart.")
                            print(f"   Check logs: tail -100 /tmp/supervisor_*.log")
                        return 0
                    else:
                        print(f"âŒ Failed to initiate restart: {error}")
                        return 1
                finally:
                    # v121.0: ALWAYS restore signal handlers when leaving restart block
                    _signal.signal(_signal.SIGINT, _orig_sigint)
                    _signal.signal(_signal.SIGTERM, _orig_sigterm)
                    _signal.signal(_signal.SIGHUP, _orig_sighup)
            else:
                print(f"âšª No supervisor running. Starting fresh instance...")
                # Fall through to normal startup

        # =====================================================================
        # Handle supervisor already running
        # =====================================================================
        if is_running and existing_state:
            existing_pid = existing_state.get('pid', 'unknown')

            # --force: Skip all checks and force start (dangerous!)
            if args.force:
                print(f"\nâš ï¸  WARNING: --force specified. Forcefully starting despite existing supervisor.")
                print(f"   This may cause conflicts! Existing PID: {existing_pid}")
                print(f"   Attempting to force-stop existing instance...")

                result = await send_ipc_command('force-stop', timeout=5.0)
                await asyncio.sleep(2.0)  # Give it time to die

            # --takeover: Graceful takeover from existing supervisor
            elif args.takeover:
                print(f"\nğŸ”„ Requesting graceful takeover from existing supervisor (PID {existing_pid})...")
                print(f"   Sending takeover request...")

                result = await send_ipc_command('takeover', timeout=10.0)
                if result.get('success') or result.get('takeover_accepted'):
                    print(f"   âœ… Takeover accepted. Waiting for existing supervisor to shutdown...")

                    # Wait for existing supervisor to shutdown
                    max_wait = 15.0
                    wait_interval = 0.5
                    waited = 0.0

                    while waited < max_wait:
                        await asyncio.sleep(wait_interval)
                        waited += wait_interval

                        still_running, _ = is_supervisor_running()
                        if not still_running:
                            print(f"   âœ… Previous supervisor has exited. Starting new instance...")
                            break

                        if waited % 2.0 < wait_interval:  # Print every 2 seconds
                            print(f"   ... waiting ({waited:.0f}s)")

                    if waited >= max_wait:
                        print(f"   âš ï¸  Timeout waiting for previous supervisor. Attempting to proceed...")
                else:
                    print(f"   âš ï¸  Takeover request failed: {result.get('error', 'unknown')}")
                    print(f"   Attempting to proceed anyway...")

            # --auto (default): Intelligent decision
            else:
                print(f"\n{'='*70}")
                print(f"ğŸ¤– JARVIS SUPERVISOR - INTELLIGENT STARTUP")
                print(f"{'='*70}")
                print(f"   Detected: Supervisor already running (PID {existing_pid})")
                print(f"   Entry Point: {existing_state.get('entry_point', 'unknown')}")
                print(f"   Started:     {existing_state.get('started_at', 'unknown')}")
                print(f"{'='*70}")

                # Check if existing supervisor is healthy
                print(f"\n   Checking health of existing supervisor...")
                health_result = await send_ipc_command('health', timeout=5.0)

                if health_result.get('success'):
                    # v116.0: Health data is in 'result' key from IPC wrapper
                    health_data = health_result.get('result', {})
                    health_level = health_data.get('health_level', 'UNKNOWN')
                    print(f"   Health Level: {health_level}")

                    if health_level in ('FULLY_READY', 'HTTP_HEALTHY', 'IPC_RESPONSIVE'):
                        # v119.1: Existing supervisor is healthy - use it automatically (no menu)
                        # This makes `python3 run_supervisor.py` a true single command that just works
                        components = health_data.get('components', {})
                        repos_status = health_data.get('cross_repo_health', {})

                        print(f"\n   âœ… JARVIS Supervisor is already running and healthy!")
                        print(f"   ")

                        # Show component status if available
                        if components:
                            healthy_count = sum(1 for c in components.values() if c.get('status') == 'healthy')
                            total_count = len(components)
                            print(f"   Components: {healthy_count}/{total_count} healthy")

                        # Show cross-repo status if available
                        if repos_status:
                            healthy_repos = sum(1 for r in repos_status.values() if r.get('healthy', False))
                            total_repos = len(repos_status)
                            print(f"   Cross-Repo: {healthy_repos}/{total_repos} connected")

                        print(f"   ")
                        print(f"   No action needed - your supervisor is ready to use.")
                        print(f"   ")
                        print(f"   Quick commands:")
                        print(f"   â€¢ --restart   Restart the supervisor")
                        print(f"   â€¢ --shutdown  Stop the supervisor")
                        print(f"   â€¢ --status    Check detailed status")
                        print(f"{'='*70}\n")
                        return 0  # Exit cleanly - supervisor is running and healthy
                    else:
                        # Existing supervisor is unhealthy - auto-takeover
                        print(f"\n   âš ï¸  Existing supervisor appears unhealthy (level: {health_level})")
                        print(f"   Initiating automatic takeover...")

                        result = await send_ipc_command('takeover', timeout=5.0)
                        await asyncio.sleep(3.0)  # Give it time
                else:
                    # Can't reach existing supervisor - it may be zombie
                    print(f"   âš ï¸  Cannot reach existing supervisor via IPC.")
                    print(f"   It may be a zombie process. Attempting cleanup...")

                    # Try force-stop
                    result = await send_ipc_command('force-stop', timeout=2.0)
                    await asyncio.sleep(2.0)

        # =====================================================================
        # Acquire lock and start new supervisor
        # =====================================================================
        if not acquire_supervisor_lock("run_supervisor"):
            # One more check - maybe previous instance just died
            await asyncio.sleep(1.0)
            if not acquire_supervisor_lock("run_supervisor"):
                print("\nâŒ Could not acquire supervisor lock. Another instance may be starting.")
                print("   Try: python3 run_supervisor.py --force")
                return 1

        # Start heartbeat to keep lock fresh
        await start_supervisor_heartbeat()

        # v113.0: Start IPC server for remote commands (status, restart, shutdown, takeover)
        try:
            from backend.core.supervisor_singleton import start_supervisor_ipc_server
            await start_supervisor_ipc_server()
            print("[v116.0] Supervisor IPC server started")
        except Exception as e:
            print(f"[v116.0] IPC server warning: {e}")  # Non-fatal, continue startup

        # v119.0: Cross-repo coordination is ENABLED BY DEFAULT
        # Use --no-connect-repos to disable
        # Note: args.connect_repos with action="store_true" ignores default=True,
        # so we only check if --no-connect-repos was passed
        if not args.no_connect_repos:
            try:
                from backend.core.cross_repo_orchestrator import CrossRepoOrchestrator
                orchestrator = CrossRepoOrchestrator()
                asyncio.create_task(
                    orchestrator.start_coordination(),
                    name="cross_repo_coordinator"
                )
                print("[v119.0] Cross-repo coordination initialized (default: enabled)")
            except Exception as e:
                print(f"[v119.0] Cross-repo coordination warning: {e}")  # Non-fatal
        else:
            print("[v119.0] Cross-repo coordination disabled via --no-connect-repos")

    # =========================================================================
    # v95.17: Clean up orphaned semaphores from previous crashes
    # This prevents semaphore accumulation across restarts
    try:
        from backend.scripts.shutdown_hook import cleanup_orphaned_semaphores_on_startup
        orphan_result = cleanup_orphaned_semaphores_on_startup()
        if orphan_result.get("semaphores_found", 0) > 0:
            print(f"[v95.17] Startup: Found {orphan_result['semaphores_found']} system semaphores")
    except ImportError:
        pass  # Module not available
    except Exception as e:
        print(f"[v95.17] Startup semaphore check warning: {e}")

    # Apply command-line settings to environment
    if args.no_voice:
        os.environ["JARVIS_VOICE_ENABLED"] = "false"

    bootstrapper = SupervisorBootstrapper()

    if args.task:
        # Single task mode: Initialize, run task, shutdown
        bootstrapper.logger.info("Running in single-task mode")

        # Initialize agentic security (minimal startup)
        try:
            await bootstrapper._initialize_agentic_security()
        except Exception as e:
            bootstrapper.logger.error(f"Failed to initialize agentic security: {e}")
            return 1

        # Run the task
        exit_code = await run_single_task(
            bootstrapper=bootstrapper,
            goal=args.task,
            mode=args.mode,
            timeout=args.timeout,
        )

        # Cleanup
        try:
            if bootstrapper._agentic_runner:
                await bootstrapper._agentic_runner.shutdown()
            if bootstrapper._watchdog:
                from core.agentic_watchdog import stop_watchdog
                await stop_watchdog()
        except Exception as e:
            bootstrapper.logger.warning(f"Cleanup error: {e}")

        return exit_code
    else:
        # =====================================================================
        # v111.0: Normal supervisor mode with unified signal handling
        # =====================================================================
        # Run the bootstrapper with shutdown signal monitoring.
        # If a shutdown signal is received, we gracefully stop the supervisor.
        # =====================================================================
        supervisor_task = asyncio.create_task(
            bootstrapper.run(),
            name="supervisor_main"
        )
        shutdown_task = asyncio.create_task(
            signal_handler.wait_for_shutdown(),
            name="shutdown_monitor"
        )

        try:
            # Wait for either supervisor completion or shutdown signal
            done, pending = await asyncio.wait(
                [supervisor_task, shutdown_task],
                return_when=asyncio.FIRST_COMPLETED
            )

            if shutdown_task in done:
                # Shutdown signal received - stop supervisor gracefully
                print("[v111.0] Shutdown signal received, stopping supervisor...")

                # v123.4: Start a deadline timer - force exit after 30 seconds
                # This prevents hangs during cleanup
                def force_exit_after_timeout():
                    import time
                    time.sleep(30)
                    print("[v123.4] Force exit deadline reached after 30s - terminating now")
                    import os as _os
                    _os._exit(143 if signal_handler.shutdown_reason == "SIGTERM" else 130)

                deadline_thread = threading.Thread(target=force_exit_after_timeout, daemon=True)
                deadline_thread.start()

                # Cancel the supervisor task with timeout
                supervisor_task.cancel()
                try:
                    # v123.4: Only wait 15 seconds for cleanup
                    await asyncio.wait_for(
                        asyncio.shield(supervisor_task),
                        timeout=15.0
                    )
                except (asyncio.CancelledError, asyncio.TimeoutError):
                    print("[v123.4] Supervisor task cleanup timed out or cancelled")
                except Exception as e:
                    print(f"[v123.4] Supervisor cleanup error: {e}")

                # Return appropriate exit code based on signal
                if signal_handler.shutdown_reason == "SIGINT":
                    return 130  # 128 + SIGINT(2)
                elif signal_handler.shutdown_reason == "SIGTERM":
                    return 143  # 128 + SIGTERM(15)
                else:
                    return 0

            # Supervisor completed on its own
            if supervisor_task in done:
                # Cancel the shutdown monitor
                shutdown_task.cancel()
                with suppress(asyncio.CancelledError):
                    await shutdown_task

                # Get the supervisor's exit code
                try:
                    return supervisor_task.result()
                except Exception as e:
                    print(f"[v111.0] Supervisor failed with error: {e}")
                    return 1

        except asyncio.CancelledError:
            # Handle cancellation during wait
            print("[v111.0] Main task cancelled, cleaning up...")
            for task in [supervisor_task, shutdown_task]:
                if not task.done():
                    task.cancel()
                    with suppress(asyncio.CancelledError):
                        await task
            return 130

        return 0


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
    except KeyboardInterrupt:
        # v111.0: This should rarely fire now since signals are handled in main()
        print("\n[Supervisor] Interrupted by user (unhandled KeyboardInterrupt)")
        exit_code = 130  # 128 + SIGINT(2)
    except Exception as e:
        print(f"\n[Supervisor] Fatal error: {e}")
        exit_code = 1
    finally:
        # v110.0: Release singleton lock on exit
        if _SINGLETON_AVAILABLE:
            release_supervisor_lock()

        # v111.0: Log final shutdown state
        handler = _unified_signal_handler
        if handler and handler.shutdown_count > 0:
            print(f"[v111.0] Shutdown completed (signals received: {handler.shutdown_count}, reason: {handler.shutdown_reason})")

    # v123.3: Force exit - sys.exit() may hang if non-daemon threads are running.
    # Use os._exit() as a guaranteed termination after attempting clean exit.
    try:
        sys.exit(exit_code)
    finally:
        # If sys.exit() raised but didn't actually exit (e.g., caught by atexit handlers
        # that hang on non-daemon threads), force immediate termination.
        import os as _os
        import threading as _threading

        # Count non-daemon threads
        non_daemon_threads = [t for t in _threading.enumerate()
                              if t.is_alive() and not t.daemon and t.name != 'MainThread']
        if non_daemon_threads:
            print(f"[v123.3] Warning: {len(non_daemon_threads)} non-daemon threads blocking exit: "
                  f"{[t.name for t in non_daemon_threads]}")

        # v123.3: Force termination if we got here (sys.exit didn't work)
        print(f"[v123.3] Forcing immediate exit with os._exit({exit_code})")
        _os._exit(exit_code)
