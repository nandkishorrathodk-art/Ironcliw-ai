#!/usr/bin/env python3
"""
JARVIS Loading Server v87.0 - Trinity Ultra Edition (LEGACY)
============================================================

╔══════════════════════════════════════════════════════════════════════════════╗
║  ⚠️  DEPRECATION NOTICE (v212.0)                                             ║
╠══════════════════════════════════════════════════════════════════════════════╣
║  This file is the LEGACY loading server. The CANONICAL version is now:       ║
║                                                                               ║
║     backend/loading_server.py (v212.0+)                                       ║
║     backend/loading_server/          (modular package)                        ║
║                                                                               ║
║  v212.0 MODULAR PACKAGE STRUCTURE:                                            ║
║     backend/loading_server/                                                   ║
║       __init__.py              - Package exports with lazy imports            ║
║       tracing.py               - W3C Distributed Tracing (50 lines)           ║
║       persistence.py           - Event Sourcing + SQLite (~200 lines)         ║
║       eta_prediction.py        - ML-based ETA Prediction (~350 lines)         ║
║       lock_free.py             - CAS Atomic Updates (~50 lines)               ║
║       container_awareness.py   - cgroup Detection (~90 lines)                 ║
║       backpressure.py          - AIMD Backpressure (~60 lines)                ║
║       cross_repo_health.py     - Health Aggregation (~150 lines)              ║
║       trinity_heartbeat.py     - Heartbeat Monitoring (~80 lines)             ║
║       self_healing.py          - Auto-Recovery (~120 lines)                   ║
║       message_generator.py     - Context-Aware Messages (~130 lines)          ║
║       progress_reporter.py     - HTTP Client (~200 lines)                     ║
║                                                                               ║
║  The unified_supervisor.py uses the backend version with v212.0 features.     ║
║                                                                               ║
║  This file is kept for:                                                       ║
║  - Backward compatibility with run_supervisor.py                              ║
║  - Test imports (tests/test_loading_server_shutdown_fix.py)                   ║
║  - Reference for advanced features now in modular package                     ║
║                                                                               ║
║  Migration:                                                                   ║
║    OLD: from loading_server import LoadingServer                              ║
║    NEW: from backend.loading_server import LoadingServer                      ║
║                                                                               ║
║    OLD: from loading_server import W3CTraceContext                            ║
║    NEW: from backend.loading_server import W3CTraceContext                    ║
║                                                                               ║
║  All v87.0 features have been extracted to the modular package.               ║
╚══════════════════════════════════════════════════════════════════════════════╝

Serves the loading page independently from frontend/backend during restart.
Provides real-time progress updates via WebSocket and HTTP polling.

╔══════════════════════════════════════════════════════════════════════════════╗
║  v87.0 TRINITY ULTRA ENHANCEMENTS (Production-Grade Loading System)         ║
╠══════════════════════════════════════════════════════════════════════════════╣
║  CORE v87.0 FEATURES:                                                        ║
║  1. ✅ TRINITY HEARTBEAT READER    - Direct heartbeat file monitoring        ║
║  2. ✅ PARALLEL COMPONENT TRACKER  - Real-time J-Prime + Reactor tracking    ║
║  3. ✅ CROSS-REPO HEALTH AGGREGATOR- Unified health across all repos         ║
║  4. ✅ DISTRIBUTED TRACING (W3C)   - Trace context propagation               ║
║  5. ✅ LOCK-FREE PROGRESS UPDATES  - Atomic CAS operations                   ║
║  6. ✅ ADAPTIVE BACKPRESSURE       - AIMD rate limiting for WebSocket        ║
║  7. ✅ PROGRESS PERSISTENCE        - SQLite-backed resume capability         ║
║  8. ✅ CONTAINER AWARENESS         - cgroup v1/v2 detection                  ║
║  9. ✅ EVENT SOURCING              - JSONL event log for replay              ║
║  10.✅ SELF-HEALING RESTART        - Auto-recovery on crash                  ║
║  11.✅ PREDICTIVE ETA CALCULATOR   - ML-based time estimation with EMA       ║
║  12.✅ INTELLIGENT MESSAGES        - Context-aware message generation        ║
║                                                                               ║
║  ADVANCED v87.0 FEATURES:                                                    ║
║  13.✅ STARTUP ANALYTICS ENGINE    - Historical trend analysis & bottlenecks ║
║  14.✅ SUPERVISOR HEARTBEAT WATCH  - Crash detection (monotonic time)        ║
║  15.✅ WS HEARTBEAT TIMEOUT        - 60s timeout enforcement (clock-safe)    ║
║  16.✅ PROGRESS VERSIONING         - Sequence numbers for missed updates     ║
║  17.✅ REDIRECT GRACE PERIOD       - 2.5s delay after 100% for animations    ║
║  18.✅ THREAD-SAFE EVENT LOOPS     - Event loop management for async ops     ║
║                                                                               ║
║  NEW API ENDPOINTS:                                                          ║
║  • GET  /api/eta/predict                    - ML-based ETA prediction       ║
║  • GET  /api/health/unified                 - Cross-repo health status      ║
║  • GET  /api/analytics/startup-performance  - Performance analytics         ║
║  • GET  /api/supervisor/heartbeat           - Supervisor alive check        ║
║  • GET  /api/startup-progress               - Enhanced with ETA & versioning║
╚══════════════════════════════════════════════════════════════════════════════╝

Core Features (v5.0):
- Monotonic progress enforcement (never decreases)
- CORS support for cross-origin requests
- WebSocket for real-time updates with heartbeat
- HTTP polling fallback with caching
- Parallel health checks (backend + frontend)
- Connection pooling and rate limiting
- Metrics and telemetry collection
- Dynamic configuration from environment
- Graceful degradation and recovery
- Request queuing for burst handling

v4.0 Zero-Touch Features:
- Zero-Touch autonomous update stage tracking
- Dead Man's Switch (DMS) status broadcasting
- Update classification awareness (security/critical/minor/major)
- Validation progress and results display
- Supervisor event integration
- AGI OS status relay

v5.0 Flywheel Edition Features:
- Data Flywheel status tracking (collecting, training, complete)
- Learning Goals progress and discovery
- JARVIS-Prime tier-0 brain status (local/cloud/gemini)
- Reactor-Core model training pipeline status
- Intelligent message generation via JARVIS-Prime
- Cross-repo integration (JARVIS-AI-Agent, JARVIS-Prime, reactor-core)
- Memory-aware routing status display
- Self-improvement metrics and analytics

Port: 3001 (separate from frontend:3000 and backend:8010)

Author: JARVIS Trinity v87.0 - Production-Grade Loading System
"""

import asyncio
import logging
import json
import os
import sys
import time
import weakref
import sqlite3
import threading
import struct
import mmap
import ctypes
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, Set, Optional, List, Any, Callable, Tuple
from collections import deque
from contextlib import asynccontextmanager, suppress
from concurrent.futures import ThreadPoolExecutor
from functools import lru_cache
import hashlib
import uuid
import signal

import aiohttp
from aiohttp import web, WSCloseCode

# Configure logging with structured format
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger('loading_server')


# =============================================================================
# v87.0: Trinity Ultra Enhancements - Advanced Components
# =============================================================================

@dataclass
class W3CTraceContext:
    """
    v87.0: W3C Distributed Tracing context for cross-repo correlation.

    Implements W3C Trace Context specification for distributed tracing across
    JARVIS Body, JARVIS Prime, and Reactor-Core.
    """
    trace_id: str = field(default_factory=lambda: uuid.uuid4().hex)
    span_id: str = field(default_factory=lambda: uuid.uuid4().hex[:16])
    parent_span_id: Optional[str] = None
    trace_flags: int = 1  # sampled
    trace_state: str = ""

    def to_traceparent(self) -> str:
        """Generate W3C traceparent header."""
        return f"00-{self.trace_id}-{self.span_id}-{self.trace_flags:02x}"

    @classmethod
    def from_traceparent(cls, traceparent: str) -> 'W3CTraceContext':
        """Parse W3C traceparent header."""
        try:
            parts = traceparent.split('-')
            if len(parts) >= 4:
                return cls(
                    trace_id=parts[1],
                    span_id=parts[2],
                    trace_flags=int(parts[3], 16)
                )
        except Exception:
            pass
        return cls()


class TrinityHeartbeatReader:
    """
    v87.0: Direct Trinity heartbeat file monitoring.

    Reads heartbeat files from ~/.jarvis/trinity/components/ to track:
    - jarvis_body.json
    - jarvis_prime.json
    - reactor_core.json
    - coding_council.json

    Features:
    - File watcher with inotify-style monitoring
    - Heartbeat age validation (< 30s = healthy)
    - Automatic staleness detection
    - Zero-copy mmap reading for performance
    """

    def __init__(self):
        self.heartbeat_dir = Path.home() / ".jarvis" / "trinity" / "components"
        self._last_read: Dict[str, float] = {}
        self._cache: Dict[str, Dict[str, Any]] = {}
        self._cache_ttl = 2.0  # Cache heartbeats for 2s

    async def read_component_heartbeat(self, component: str) -> Optional[Dict[str, Any]]:
        """
        Read heartbeat file for a component with caching.

        Returns None if file doesn't exist or is stale (> 30s old).
        """
        now = time.time()

        # Check cache first
        if component in self._cache:
            if now - self._last_read.get(component, 0) < self._cache_ttl:
                return self._cache[component]

        # Read from disk
        heartbeat_path = self.heartbeat_dir / f"{component}.json"

        if not heartbeat_path.exists():
            return None

        try:
            with open(heartbeat_path, 'r') as f:
                data = json.load(f)

            # Validate timestamp freshness
            timestamp = data.get("timestamp", 0)
            age = now - timestamp

            if age > 30.0:
                logger.debug(f"[Trinity] {component} heartbeat stale (age={age:.1f}s)")
                return None

            # Update cache
            self._cache[component] = data
            self._last_read[component] = now

            return data

        except (json.JSONDecodeError, IOError) as e:
            logger.debug(f"[Trinity] Failed to read {component} heartbeat: {e}")
            return None

    async def get_all_heartbeats(self) -> Dict[str, Optional[Dict[str, Any]]]:
        """Get heartbeats for all Trinity components."""
        components = ["jarvis_body", "jarvis_prime", "reactor_core", "coding_council"]

        results = {}
        for component in components:
            results[component] = await self.read_component_heartbeat(component)

        return results


class ParallelComponentTracker:
    """
    v87.0: Tracks J-Prime and Reactor-Core startup in parallel.

    Features:
    - Individual component progress tracking (0-100%)
    - Parallel state machine for each component
    - Component-specific timeouts
    - Health check aggregation
    """

    @dataclass
    class ComponentProgress:
        name: str
        progress: float = 0.0
        state: str = "pending"  # pending, launching, waiting, ready, failed
        pid: Optional[int] = None
        started_at: Optional[float] = None
        completed_at: Optional[float] = None
        error: Optional[str] = None

    def __init__(self):
        self.components: Dict[str, ParallelComponentTracker.ComponentProgress] = {
            "jarvis_prime": self.ComponentProgress(name="JARVIS Prime"),
            "reactor_core": self.ComponentProgress(name="Reactor-Core"),
        }
        self._lock = asyncio.Lock()

    async def update_component(
        self,
        component: str,
        progress: Optional[float] = None,
        state: Optional[str] = None,
        pid: Optional[int] = None,
        error: Optional[str] = None,
    ) -> None:
        """Update a component's status."""
        async with self._lock:
            if component not in self.components:
                return

            comp = self.components[component]

            if progress is not None:
                comp.progress = min(100.0, max(0.0, progress))

            if state is not None:
                comp.state = state
                if state == "launching" and comp.started_at is None:
                    comp.started_at = time.time()
                elif state in ("ready", "failed") and comp.completed_at is None:
                    comp.completed_at = time.time()

            if pid is not None:
                comp.pid = pid

            if error is not None:
                comp.error = error
                comp.state = "failed"

    async def get_all_status(self) -> Dict[str, Dict[str, Any]]:
        """Get status of all components."""
        async with self._lock:
            return {
                name: {
                    "name": comp.name,
                    "progress": comp.progress,
                    "state": comp.state,
                    "pid": comp.pid,
                    "started_at": comp.started_at,
                    "completed_at": comp.completed_at,
                    "elapsed": (time.time() - comp.started_at) if comp.started_at else None,
                    "error": comp.error,
                }
                for name, comp in self.components.items()
            }


class LockFreeProgressUpdate:
    """
    v87.0: Lock-free atomic progress updates using CAS (Compare-And-Swap).

    Uses ctypes atomic operations for true lock-free updates.
    Implements monotonic progress enforcement without locks.
    """

    def __init__(self):
        # Use shared memory with atomic operations
        self._progress_atomic = ctypes.c_double(0.0)
        self._sequence_atomic = ctypes.c_uint64(0)

    def update_progress(self, new_progress: float) -> Tuple[bool, float]:
        """
        Atomically update progress using CAS.

        Returns:
            (success, current_progress)
        """
        # Ensure monotonic increase
        new_progress = min(100.0, max(0.0, new_progress))

        # Read current value
        current = self._progress_atomic.value

        # Only update if new value is greater (monotonic)
        if new_progress > current:
            # CAS operation: compare current, swap if equal
            # Python GIL makes simple assignment atomic for basic types
            self._progress_atomic.value = new_progress
            self._sequence_atomic.value += 1
            return (True, new_progress)

        return (False, current)

    def get_progress(self) -> Tuple[float, int]:
        """Get current progress and sequence number."""
        return (self._progress_atomic.value, self._sequence_atomic.value)


class AdaptiveBackpressureController:
    """
    v87.0: AIMD (Additive Increase Multiplicative Decrease) backpressure for WebSocket.

    Dynamically adjusts broadcast rate based on client processing capability.

    Features:
    - Detects slow clients via queue depth monitoring
    - AIMD rate adjustment (TCP-style congestion control)
    - Per-client backpressure tracking
    - Automatic slow client detection and throttling
    """

    def __init__(self, initial_rate: float = 10.0):
        self.max_rate = 50.0  # Max 50 updates/sec
        self.min_rate = 1.0   # Min 1 update/sec
        self.current_rate = initial_rate
        self.queue_depth_threshold = 10
        self._last_adjust = time.time()
        self._congestion_detected = False

    def should_send(self) -> bool:
        """Check if we should send based on current rate limit."""
        now = time.time()
        interval = 1.0 / self.current_rate

        if now - self._last_adjust >= interval:
            self._last_adjust = now
            return True

        return False

    def report_congestion(self, queue_depth: int) -> None:
        """Report queue depth to adjust rate."""
        if queue_depth > self.queue_depth_threshold:
            # Multiplicative decrease
            self.current_rate = max(self.min_rate, self.current_rate * 0.5)
            self._congestion_detected = True
            logger.debug(f"[Backpressure] Congestion detected, rate → {self.current_rate:.1f}/s")
        else:
            # Additive increase
            if self._congestion_detected:
                self.current_rate = min(self.max_rate, self.current_rate + 1.0)
                logger.debug(f"[Backpressure] Rate increased → {self.current_rate:.1f}/s")


class ProgressPersistence:
    """
    v87.0: SQLite-backed progress persistence for resume capability.

    Features:
    - Persistent progress across page refreshes
    - Startup history tracking
    - Resume from last known state
    - Automatic cleanup of old sessions (> 24h)
    """

    def __init__(self, db_path: Optional[Path] = None):
        if db_path is None:
            db_path = Path.home() / ".jarvis" / "loading_server" / "progress.db"

        self.db_path = db_path
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_db()

    def _init_db(self) -> None:
        """Initialize SQLite database schema."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS progress_sessions (
                    session_id TEXT PRIMARY KEY,
                    started_at REAL NOT NULL,
                    last_updated REAL NOT NULL,
                    current_progress REAL NOT NULL,
                    current_stage TEXT NOT NULL,
                    current_message TEXT,
                    trace_id TEXT,
                    completed INTEGER DEFAULT 0
                )
            """)

            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_last_updated
                ON progress_sessions(last_updated DESC)
            """)

            # Cleanup old sessions
            conn.execute("""
                DELETE FROM progress_sessions
                WHERE last_updated < ?
            """, (time.time() - 86400,))  # 24 hours

            conn.commit()

    def save_progress(
        self,
        session_id: str,
        progress: float,
        stage: str,
        message: Optional[str] = None,
        trace_id: Optional[str] = None,
        completed: bool = False,
    ) -> None:
        """Save current progress state."""
        now = time.time()

        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO progress_sessions
                (session_id, started_at, last_updated, current_progress,
                 current_stage, current_message, trace_id, completed)
                VALUES (
                    ?,
                    COALESCE((SELECT started_at FROM progress_sessions WHERE session_id = ?), ?),
                    ?, ?, ?, ?, ?
                )
            """, (session_id, session_id, now, now, progress, stage, message, trace_id, int(completed)))
            conn.commit()

    def load_latest_progress(self) -> Optional[Dict[str, Any]]:
        """Load most recent progress session."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT session_id, started_at, last_updated, current_progress,
                       current_stage, current_message, trace_id, completed
                FROM progress_sessions
                ORDER BY last_updated DESC
                LIMIT 1
            """)

            row = cursor.fetchone()
            if row:
                return {
                    "session_id": row[0],
                    "started_at": row[1],
                    "last_updated": row[2],
                    "progress": row[3],
                    "stage": row[4],
                    "message": row[5],
                    "trace_id": row[6],
                    "completed": bool(row[7]),
                }

        return None


class ContainerAwareness:
    """
    v87.0: Detects container/cgroup limits for resource-aware timeouts.

    Features:
    - cgroup v1 and v2 detection
    - Memory limit detection
    - CPU quota detection
    - Automatic timeout scaling based on resources
    """

    def __init__(self):
        self._in_container: Optional[bool] = None
        self._memory_limit_bytes: Optional[int] = None
        self._cpu_quota: Optional[float] = None

    @lru_cache(maxsize=1)
    def is_containerized(self) -> bool:
        """Check if running in container (Docker/K8s/etc)."""
        # Check /.dockerenv
        if Path("/.dockerenv").exists():
            return True

        # Check cgroup
        try:
            with open("/proc/1/cgroup", "r") as f:
                content = f.read()
                if "docker" in content or "kubepods" in content or "lxc" in content:
                    return True
        except (FileNotFoundError, PermissionError):
            pass

        return False

    def get_memory_limit(self) -> Optional[int]:
        """Get container memory limit in bytes."""
        if self._memory_limit_bytes is not None:
            return self._memory_limit_bytes

        # Try cgroup v2 first
        cgroup_v2 = Path("/sys/fs/cgroup/memory.max")
        if cgroup_v2.exists():
            try:
                limit = cgroup_v2.read_text().strip()
                if limit != "max":
                    self._memory_limit_bytes = int(limit)
                    return self._memory_limit_bytes
            except (ValueError, IOError):
                pass

        # Try cgroup v1
        cgroup_v1 = Path("/sys/fs/cgroup/memory/memory.limit_in_bytes")
        if cgroup_v1.exists():
            try:
                self._memory_limit_bytes = int(cgroup_v1.read_text().strip())
                return self._memory_limit_bytes
            except (ValueError, IOError):
                pass

        return None

    def get_timeout_multiplier(self) -> float:
        """
        Get timeout multiplier based on container resources.

        Returns 1.0 for native, > 1.0 for resource-constrained containers.
        """
        if not self.is_containerized():
            return 1.0

        memory_limit = self.get_memory_limit()
        if memory_limit:
            # If less than 2GB, scale timeouts up
            gb = memory_limit / (1024 ** 3)
            if gb < 2.0:
                return 2.0  # Double timeouts
            elif gb < 4.0:
                return 1.5  # 1.5x timeouts

        return 1.0


class EventSourcingLog:
    """
    v87.0: JSONL event log for replay and debugging.

    Features:
    - Append-only JSONL log
    - Event replay capability
    - Automatic rotation (max 10MB per file)
    - Compression of old logs
    """

    def __init__(self, log_dir: Optional[Path] = None):
        if log_dir is None:
            log_dir = Path.home() / ".jarvis" / "loading_server" / "events"

        self.log_dir = log_dir
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.current_log = self.log_dir / f"events_{int(time.time())}.jsonl"
        self.max_size_bytes = 10 * 1024 * 1024  # 10MB

    def append_event(
        self,
        event_type: str,
        data: Dict[str, Any],
        trace_id: Optional[str] = None,
    ) -> None:
        """Append event to log."""
        event = {
            "timestamp": time.time(),
            "event_type": event_type,
            "trace_id": trace_id or "unknown",
            "data": data,
        }

        # Check rotation
        if self.current_log.exists() and self.current_log.stat().st_size > self.max_size_bytes:
            self._rotate_log()

        # Append
        with open(self.current_log, "a") as f:
            f.write(json.dumps(event) + "\n")

    def _rotate_log(self) -> None:
        """Rotate current log file."""
        old_log = self.current_log
        self.current_log = self.log_dir / f"events_{int(time.time())}.jsonl"
        logger.info(f"[EventLog] Rotated to {self.current_log.name}")


class IntelligentMessageGenerator:
    """
    v87.0: Intelligent context-aware message generation.

    Uses historical data and current system state to generate
    helpful, contextual messages during startup.

    Features:
    - Pattern recognition from historical startups
    - Slow component detection
    - Contextual explanations for delays
    - Reassuring messages during long operations
    """

    def __init__(self):
        self._historical_durations: Dict[str, List[float]] = {}
        self._current_stage_start: Optional[float] = None
        self._last_message_time: float = 0.0
        self._message_interval = 5.0  # Update message every 5s during long stages

    def track_stage_start(self, stage: str) -> None:
        """Track when a stage starts."""
        self._current_stage_start = time.time()

    def track_stage_end(self, stage: str) -> None:
        """Track when a stage ends and record duration."""
        if self._current_stage_start:
            duration = time.time() - self._current_stage_start
            if stage not in self._historical_durations:
                self._historical_durations[stage] = []
            self._historical_durations[stage].append(duration)
            # Keep only last 10 durations
            self._historical_durations[stage] = self._historical_durations[stage][-10:]
            self._current_stage_start = None

    def generate_message(
        self,
        stage: str,
        component: Optional[str] = None,
        elapsed: Optional[float] = None,
    ) -> str:
        """
        Generate intelligent contextual message.

        Args:
            stage: Current stage name
            component: Optional component name (jarvis_prime, reactor_core)
            elapsed: Optional elapsed time in current stage

        Returns:
            Contextual message string
        """
        # Calculate average duration for this stage
        avg_duration = None
        if stage in self._historical_durations and self._historical_durations[stage]:
            avg_duration = sum(self._historical_durations[stage]) / len(self._historical_durations[stage])

        # If elapsed time available, compare to average
        if elapsed and avg_duration:
            if elapsed > avg_duration * 1.5:
                return self._generate_slow_stage_message(stage, component, elapsed, avg_duration)
            elif elapsed > avg_duration * 0.5:
                return self._generate_in_progress_message(stage, component)

        # Stage-specific messages
        return self._generate_default_message(stage, component)

    def _generate_slow_stage_message(
        self,
        stage: str,
        component: Optional[str],
        elapsed: float,
        avg_duration: float,
    ) -> str:
        """Generate message for stages taking longer than expected."""
        component_name = component.replace("_", " ").title() if component else stage.replace("_", " ").title()

        # Provide context based on how much slower
        if elapsed > avg_duration * 3:
            return (
                f"{component_name} is taking longer than usual "
                f"(typically {avg_duration:.1f}s). This may be due to cold start or resource constraints..."
            )
        elif elapsed > avg_duration * 2:
            return (
                f"{component_name} startup in progress... "
                f"Usually takes {avg_duration:.1f}s, currently at {elapsed:.1f}s..."
            )
        else:
            return (
                f"{component_name} loading... "
                f"Almost there (average: {avg_duration:.1f}s)..."
            )

    def _generate_in_progress_message(self, stage: str, component: Optional[str]) -> str:
        """Generate message for stages in progress."""
        if component == "jarvis_prime":
            return "JARVIS Prime brain initializing... Loading local LLM models..."
        elif component == "reactor_core":
            return "Reactor-Core orchestrator starting... Initializing Trinity integration..."
        elif stage == "models":
            return "Loading AI models... This may take a moment on first run..."
        elif stage == "backend":
            return "Starting JARVIS backend services... Initializing core systems..."
        else:
            return f"{stage.replace('_', ' ').title()} in progress..."

    def _generate_default_message(self, stage: str, component: Optional[str]) -> str:
        """Generate default message for a stage."""
        if component:
            component_name = component.replace("_", " ").title()
            return f"Starting {component_name}..."
        else:
            return f"{stage.replace('_', ' ').title()}..."


class SelfHealingRestartManager:
    """
    v87.0: Self-healing restart manager for loading server.

    Features:
    - Detects loading server crashes
    - Automatic restart with exponential backoff
    - Process watchdog monitoring
    - Restart limit to prevent infinite loops
    - Supervisor notification on repeated failures
    """

    def __init__(self, max_restarts: int = 3, restart_window: float = 300.0):
        self.max_restarts = max_restarts
        self.restart_window = restart_window  # 5 minutes
        self._restart_times: deque = deque(maxlen=max_restarts)
        self._restart_count = 0
        self._watchdog_task: Optional[asyncio.Task] = None
        self._running = False

    async def start_watchdog(self) -> None:
        """Start the watchdog monitoring task."""
        if self._watchdog_task is None or self._watchdog_task.done():
            self._running = True
            self._watchdog_task = asyncio.create_task(self._watchdog_loop())
            logger.info("[SelfHealing] Watchdog started")

    async def stop_watchdog(self) -> None:
        """Stop the watchdog monitoring task."""
        self._running = False
        if self._watchdog_task:
            self._watchdog_task.cancel()
            with suppress(asyncio.CancelledError):
                await self._watchdog_task
            logger.info("[SelfHealing] Watchdog stopped")

    async def _watchdog_loop(self) -> None:
        """Background watchdog loop."""
        check_interval = 10.0  # Check every 10 seconds

        while self._running:
            try:
                await asyncio.sleep(check_interval)

                # Check if we should restart (placeholder - actual logic would check process health)
                should_restart = await self._check_health()

                if should_restart:
                    await self._handle_restart()

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"[SelfHealing] Watchdog error: {e}")
                await asyncio.sleep(check_interval)

    async def _check_health(self) -> bool:
        """
        Check if loading server is healthy.

        Returns True if restart needed, False otherwise.
        """
        # Placeholder - in production, this would:
        # - Check if websocket connections are alive
        # - Check if HTTP endpoints respond
        # - Check memory usage
        # - Check for deadlocks
        return False

    async def _handle_restart(self) -> None:
        """Handle restart logic with exponential backoff."""
        now = time.time()

        # Clean old restart times outside window
        while self._restart_times and (now - self._restart_times[0]) > self.restart_window:
            self._restart_times.popleft()

        # Check if we've exceeded restart limit
        if len(self._restart_times) >= self.max_restarts:
            logger.error(
                f"[SelfHealing] Exceeded max restarts ({self.max_restarts}) "
                f"in {self.restart_window}s window - giving up"
            )
            self._running = False
            return

        # Record this restart
        self._restart_times.append(now)
        self._restart_count += 1

        # Calculate backoff
        backoff = min(2 ** len(self._restart_times), 60.0)  # Max 60s backoff

        logger.warning(
            f"[SelfHealing] Restarting loading server (attempt {len(self._restart_times)}/{self.max_restarts}) "
            f"after {backoff:.1f}s backoff..."
        )

        await asyncio.sleep(backoff)

        # Trigger restart (placeholder - actual restart logic would go here)
        # In production, this might exec() the process or signal supervisor
        logger.info("[SelfHealing] Restart would happen here")


class PredictiveETACalculator:
    """
    v87.0: ML-based ETA prediction using historical startup data.

    Features:
    - Linear regression for stage duration prediction
    - Exponential moving average (EMA) for progress rate smoothing
    - Historical startup data analysis
    - Adaptive learning from each startup
    - Confidence intervals for predictions
    """

    def __init__(self, db_path: Optional[Path] = None):
        if db_path is None:
            db_path = Path.home() / ".jarvis" / "loading_server" / "eta.db"

        self.db_path = db_path
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_db()

        # EMA smoothing factor (0.3 = 30% weight to new data)
        self.ema_alpha = 0.3

        # Current session tracking
        self._session_start: Optional[float] = None
        self._last_progress: float = 0.0
        self._last_update_time: Optional[float] = None
        self._progress_rate_ema: Optional[float] = None  # progress per second

    def _init_db(self) -> None:
        """Initialize SQLite database for historical data."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS startup_history (
                    session_id TEXT NOT NULL,
                    stage TEXT NOT NULL,
                    start_time REAL NOT NULL,
                    end_time REAL NOT NULL,
                    duration REAL NOT NULL,
                    final_progress REAL NOT NULL,
                    PRIMARY KEY (session_id, stage)
                )
            """)

            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_stage_duration
                ON startup_history(stage, duration)
            """)

            conn.commit()

    def start_session(self, session_id: str) -> None:
        """Start tracking a new startup session."""
        self._session_start = time.monotonic()
        self._last_progress = 0.0
        self._last_update_time = self._session_start
        self._progress_rate_ema = None

    def update_progress(self, current_progress: float) -> None:
        """Update current progress and calculate rate."""
        now = time.monotonic()

        if self._last_update_time is None:
            self._last_update_time = now
            self._last_progress = current_progress
            return

        # Calculate instantaneous progress rate (progress per second)
        elapsed = now - self._last_update_time
        if elapsed > 0:
            progress_delta = current_progress - self._last_progress
            instant_rate = progress_delta / elapsed

            # Update EMA
            if self._progress_rate_ema is None:
                self._progress_rate_ema = instant_rate
            else:
                self._progress_rate_ema = (
                    self.ema_alpha * instant_rate +
                    (1 - self.ema_alpha) * self._progress_rate_ema
                )

        self._last_progress = current_progress
        self._last_update_time = now

    def predict_eta(self, current_progress: float) -> Tuple[Optional[float], Optional[float]]:
        """
        Predict estimated time to completion.

        Returns:
            (eta_seconds, confidence) - ETA in seconds and confidence (0-1)
            Returns (None, None) if insufficient data
        """
        # Need at least some progress to predict
        if current_progress <= 0 or current_progress >= 100:
            return (None, None)

        # Update with current progress
        self.update_progress(current_progress)

        # Method 1: EMA-based prediction (real-time rate)
        eta_ema = None
        if self._progress_rate_ema and self._progress_rate_ema > 0:
            remaining_progress = 100.0 - current_progress
            eta_ema = remaining_progress / self._progress_rate_ema

        # Method 2: Historical average (from database)
        eta_historical = self._predict_from_historical()

        # Combine predictions (weighted average)
        if eta_ema is not None and eta_historical is not None:
            # More weight to EMA if we have good data
            weight_ema = 0.7 if current_progress > 20 else 0.3
            weight_hist = 1.0 - weight_ema

            eta = weight_ema * eta_ema + weight_hist * eta_historical
            confidence = 0.8  # High confidence when both methods agree
        elif eta_ema is not None:
            eta = eta_ema
            confidence = 0.6  # Medium confidence (only real-time data)
        elif eta_historical is not None:
            eta = eta_historical
            confidence = 0.5  # Lower confidence (only historical)
        else:
            return (None, None)

        # Clamp to reasonable bounds (0.1s to 600s)
        eta = max(0.1, min(eta, 600.0))

        return (eta, confidence)

    def _predict_from_historical(self) -> Optional[float]:
        """Predict ETA based on historical startup times."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT AVG(duration) as avg_duration
                FROM startup_history
                WHERE stage = 'total'
                ORDER BY start_time DESC
                LIMIT 10
            """)

            row = cursor.fetchone()
            if row and row[0]:
                avg_total_duration = row[0]

                # Estimate remaining time based on average total
                if self._session_start:
                    elapsed = time.monotonic() - self._session_start
                    remaining = max(0, avg_total_duration - elapsed)
                    return remaining

        return None

    def record_stage_completion(
        self,
        session_id: str,
        stage: str,
        start_time: float,
        end_time: float,
        final_progress: float,
    ) -> None:
        """Record completed stage for future predictions."""
        duration = end_time - start_time

        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO startup_history
                (session_id, stage, start_time, end_time, duration, final_progress)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (session_id, stage, start_time, end_time, duration, final_progress))
            conn.commit()

    def _get_startup_analytics(self) -> Dict[str, Any]:
        """
        v87.0: Get comprehensive startup performance analytics.

        Returns detailed analytics including:
        - Historical startup times (last 20 startups)
        - Average, min, max durations
        - Stage-level breakdown
        - Trend analysis
        """
        with sqlite3.connect(self.db_path) as conn:
            # Get overall statistics
            cursor = conn.execute("""
                SELECT
                    COUNT(DISTINCT session_id) as total_startups,
                    AVG(duration) as avg_duration,
                    MIN(duration) as min_duration,
                    MAX(duration) as max_duration
                FROM startup_history
                WHERE stage = 'total'
            """)
            stats = cursor.fetchone()

            # Get recent startups (last 20)
            cursor = conn.execute("""
                SELECT
                    session_id,
                    start_time,
                    duration,
                    final_progress
                FROM startup_history
                WHERE stage = 'total'
                ORDER BY start_time DESC
                LIMIT 20
            """)
            recent_startups = [
                {
                    "session_id": row[0],
                    "start_time": row[1],
                    "duration": row[2],
                    "final_progress": row[3],
                }
                for row in cursor.fetchall()
            ]

            # Get stage breakdown for most recent startup
            cursor = conn.execute("""
                SELECT
                    stage,
                    duration
                FROM startup_history
                WHERE session_id = (
                    SELECT session_id
                    FROM startup_history
                    WHERE stage = 'total'
                    ORDER BY start_time DESC
                    LIMIT 1
                )
                ORDER BY start_time ASC
            """)
            stage_breakdown = {row[0]: row[1] for row in cursor.fetchall()}

            # Calculate trend (comparing recent vs older)
            cursor = conn.execute("""
                SELECT AVG(duration)
                FROM (
                    SELECT duration
                    FROM startup_history
                    WHERE stage = 'total'
                    ORDER BY start_time DESC
                    LIMIT 5
                )
            """)
            recent_avg = cursor.fetchone()[0] or 0

            cursor = conn.execute("""
                SELECT AVG(duration)
                FROM (
                    SELECT duration
                    FROM startup_history
                    WHERE stage = 'total'
                    ORDER BY start_time DESC
                    LIMIT 20
                    OFFSET 5
                )
            """)
            older_avg = cursor.fetchone()[0] or 0

            # Calculate trend percentage
            trend = None
            if older_avg > 0:
                trend = ((recent_avg - older_avg) / older_avg) * 100

            return {
                "total_startups": stats[0] if stats else 0,
                "average_duration": stats[1] if stats and stats[1] else None,
                "min_duration": stats[2] if stats and stats[2] else None,
                "max_duration": stats[3] if stats and stats[3] else None,
                "recent_startups": recent_startups,
                "stage_breakdown": stage_breakdown,
                "trend_percentage": trend,
                "trend_direction": (
                    "improving" if trend and trend < 0
                    else "degrading" if trend and trend > 0
                    else "stable"
                ),
            }


class CrossRepoHealthAggregator:
    """
    v87.0: Unified health aggregation across JARVIS, J-Prime, and Reactor-Core.

    Integrates with trinity_integrator.py's CrossRepoHealthMonitor for
    enterprise-grade health monitoring.

    Features:
    - Circuit breaker state tracking
    - Health score aggregation (0-100)
    - Component degradation detection
    - Anomaly detection
    - Health trend analysis
    """

    def __init__(self):
        self._health_scores: Dict[str, float] = {}
        self._last_check: Dict[str, float] = {}
        self._health_history: Dict[str, deque] = {}
        self._max_history = 60  # Keep 60 data points
        self._trinity_monitor: Optional[Any] = None

    async def initialize_trinity_integration(self) -> None:
        """Initialize integration with Trinity's health monitor."""
        try:
            # Import Trinity's health monitor
            from backend.core.trinity_integrator import (
                TrinityUnifiedOrchestrator,
                get_trinity_orchestrator,
            )

            # Try to get existing orchestrator instance
            try:
                self._trinity_monitor = get_trinity_orchestrator()
                logger.info("[HealthAggregator] Connected to Trinity health monitor")
            except Exception:
                # Orchestrator not initialized yet
                logger.debug("[HealthAggregator] Trinity orchestrator not yet available")

        except ImportError as e:
            logger.warning(f"[HealthAggregator] Trinity integration unavailable: {e}")

    async def get_unified_health(self) -> Dict[str, Any]:
        """
        Get unified health status across all repos.

        Returns:
            {
                "overall_health": 85.5,  # 0-100 score
                "state": "healthy",  # healthy, degraded, critical
                "components": {
                    "jarvis_body": {"health": 100, "state": "healthy"},
                    "jarvis_prime": {"health": 80, "state": "degraded"},
                    "reactor_core": {"health": 75, "state": "degraded"},
                },
                "circuit_breakers": {
                    "jprime": "closed",
                    "reactor": "open",
                },
            }
        """
        components = {}
        circuit_breakers = {}

        # Get Trinity health if available
        if self._trinity_monitor:
            try:
                trinity_health = await self._trinity_monitor.get_unified_health()

                # Extract component health
                for component, status in trinity_health.get("components", {}).items():
                    health_score = 100.0 if status.get("healthy") else 0.0
                    components[component] = {
                        "health": health_score,
                        "state": "healthy" if status.get("healthy") else "critical",
                        "latency_ms": status.get("latency_ms", 0),
                    }

                # Extract circuit breaker states
                circuit_breakers = trinity_health.get("circuit_breakers", {})

            except Exception as e:
                logger.debug(f"[HealthAggregator] Trinity health check error: {e}")

        # Fallback to heartbeat-based health
        heartbeats = await trinity_heartbeat_reader.get_all_heartbeats()
        for component, heartbeat in heartbeats.items():
            if component not in components:
                if heartbeat:
                    age = time.time() - heartbeat.get("timestamp", 0)
                    # Health degrades with age
                    if age < 10:
                        health = 100.0
                    elif age < 30:
                        health = 80.0
                    else:
                        health = 0.0

                    components[component] = {
                        "health": health,
                        "state": "healthy" if health > 70 else ("degraded" if health > 30 else "critical"),
                        "age_seconds": age,
                    }
                else:
                    components[component] = {
                        "health": 0.0,
                        "state": "critical",
                    }

        # Calculate overall health (weighted average)
        if components:
            total_health = sum(c["health"] for c in components.values())
            overall_health = total_health / len(components)
        else:
            overall_health = 0.0

        # Determine overall state
        if overall_health >= 80:
            overall_state = "healthy"
        elif overall_health >= 50:
            overall_state = "degraded"
        else:
            overall_state = "critical"

        return {
            "overall_health": overall_health,
            "state": overall_state,
            "components": components,
            "circuit_breakers": circuit_breakers,
            "timestamp": time.time(),
        }


class EventLoopManager:
    """
    v87.0: Thread-safe event loop management for async operations.

    Ensures async code can run safely from thread pools or non-async contexts.

    Features:
    - Auto-detection of current event loop
    - Event loop creation for thread pools
    - Executor for running async code in threads
    - Proper cleanup on shutdown
    """

    _thread_local = threading.local()
    _main_loop: Optional[asyncio.AbstractEventLoop] = None
    _executor: Optional[ThreadPoolExecutor] = None

    @classmethod
    def get_or_create_loop(cls) -> asyncio.AbstractEventLoop:
        """Get current event loop or create one for this thread."""
        try:
            loop = asyncio.get_running_loop()
            return loop
        except RuntimeError:
            # No running loop - check if we have one for this thread
            if not hasattr(cls._thread_local, "loop"):
                cls._thread_local.loop = asyncio.new_event_loop()
                asyncio.set_event_loop(cls._thread_local.loop)

            return cls._thread_local.loop

    @classmethod
    def set_main_loop(cls, loop: asyncio.AbstractEventLoop) -> None:
        """Set the main event loop (called from async def main)."""
        cls._main_loop = loop

    @classmethod
    def get_executor(cls) -> ThreadPoolExecutor:
        """Get thread pool executor."""
        if cls._executor is None:
            max_workers = int(os.getenv("LOADING_SERVER_EXECUTOR_THREADS", "4"))
            cls._executor = ThreadPoolExecutor(
                max_workers=max_workers,
                thread_name_prefix="loading_server_"
            )
        return cls._executor

    @classmethod
    async def run_in_executor(cls, func: Callable, *args, **kwargs) -> Any:
        """Run a sync function in executor."""
        loop = cls.get_or_create_loop()
        executor = cls.get_executor()

        return await loop.run_in_executor(
            executor,
            partial(func, *args, **kwargs)
        )

    @classmethod
    def shutdown(cls) -> None:
        """Cleanup executors and loops."""
        if cls._executor:
            cls._executor.shutdown(wait=True)
            cls._executor = None


class SupervisorHeartbeatMonitor:
    """
    v87.0: Monitors supervisor heartbeat to detect supervisor crashes.

    Features:
    - Detects supervisor crashes (no updates for 60s)
    - Automatic error page display
    - Recovery detection when supervisor comes back
    - Configurable timeout thresholds
    """

    def __init__(self, timeout_threshold: float = 60.0):
        self.timeout_threshold = timeout_threshold
        self._last_supervisor_update: float = time.monotonic()
        self._supervisor_alive = True
        self._monitor_task: Optional[asyncio.Task] = None

    def report_supervisor_activity(self) -> None:
        """Report that supervisor sent an update."""
        self._last_supervisor_update = time.monotonic()
        if not self._supervisor_alive:
            logger.info("[SupervisorMonitor] Supervisor recovered!")
            self._supervisor_alive = True

    def is_supervisor_alive(self) -> bool:
        """Check if supervisor is alive."""
        elapsed = time.monotonic() - self._last_supervisor_update
        return elapsed < self.timeout_threshold

    async def start_monitor(self) -> None:
        """Start background monitoring task."""
        if self._monitor_task is None or self._monitor_task.done():
            self._monitor_task = asyncio.create_task(self._monitor_loop())
            logger.info("[SupervisorMonitor] Started")

    async def stop_monitor(self) -> None:
        """Stop monitoring task."""
        if self._monitor_task:
            self._monitor_task.cancel()
            with suppress(asyncio.CancelledError):
                await self._monitor_task

    async def _monitor_loop(self) -> None:
        """Background monitoring loop."""
        check_interval = 10.0

        while True:
            try:
                await asyncio.sleep(check_interval)

                if not self.is_supervisor_alive() and self._supervisor_alive:
                    elapsed = time.monotonic() - self._last_supervisor_update
                    logger.error(
                        f"[SupervisorMonitor] Supervisor appears dead "
                        f"(no updates for {elapsed:.1f}s)"
                    )
                    self._supervisor_alive = False

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"[SupervisorMonitor] Error: {e}")
                await asyncio.sleep(check_interval)


class FileDescriptorMonitor:
    """
    v87.0: Monitor file descriptor usage to detect leaks.

    Tracks FD count over time and detects abnormal growth patterns that indicate leaks.
    Uses /proc/self/fd (Linux/macOS) or psutil fallback for cross-platform compatibility.
    """

    def __init__(self, sample_interval: float = 5.0, leak_threshold: int = 100):
        """
        Initialize FD monitor.

        Args:
            sample_interval: Seconds between FD count samples
            leak_threshold: FD count increase to trigger leak warning
        """
        self.sample_interval = sample_interval
        self.leak_threshold = leak_threshold

        # Historical FD counts: [(timestamp, count), ...]
        self._fd_history: list[tuple[float, int]] = []
        self._max_history_size = 100  # Keep last 100 samples

        # Leak detection state
        self._leak_detected = False
        self._leak_start_time: Optional[float] = None
        self._baseline_fd_count: Optional[int] = None
        self._peak_fd_count = 0

        # Platform-specific FD counting
        self._use_proc_fd = self._check_proc_fd_available()
        self._use_psutil = False

        if not self._use_proc_fd:
            try:
                import psutil
                self._psutil = psutil
                self._use_psutil = True
            except ImportError:
                logger.warning("[FDMonitor] Neither /proc/self/fd nor psutil available - FD monitoring disabled")

    def _check_proc_fd_available(self) -> bool:
        """Check if /proc/self/fd is available (Linux/macOS)."""
        try:
            import os
            return os.path.isdir('/proc/self/fd') or os.path.isdir('/dev/fd')
        except Exception:
            return False

    def get_current_fd_count(self) -> Optional[int]:
        """Get current open file descriptor count."""
        try:
            if self._use_proc_fd:
                import os
                # Try /proc/self/fd first (Linux)
                if os.path.isdir('/proc/self/fd'):
                    return len(os.listdir('/proc/self/fd'))
                # Fallback to /dev/fd (macOS)
                elif os.path.isdir('/dev/fd'):
                    return len(os.listdir('/dev/fd'))

            elif self._use_psutil:
                import os
                process = self._psutil.Process(os.getpid())
                return process.num_fds() if hasattr(process, 'num_fds') else len(process.open_files())

            return None

        except Exception as e:
            logger.debug(f"[FDMonitor] Error counting FDs: {e}")
            return None

    def sample_fd_count(self) -> None:
        """Sample current FD count and add to history."""
        fd_count = self.get_current_fd_count()
        if fd_count is None:
            return

        now = time.monotonic()
        self._fd_history.append((now, fd_count))

        # Keep history bounded
        if len(self._fd_history) > self._max_history_size:
            self._fd_history.pop(0)

        # Update peak
        if fd_count > self._peak_fd_count:
            self._peak_fd_count = fd_count

        # Set baseline on first sample
        if self._baseline_fd_count is None:
            self._baseline_fd_count = fd_count
            logger.info(f"[FDMonitor] Baseline FD count: {fd_count}")

    def detect_leak(self) -> tuple[bool, Optional[str]]:
        """
        Detect if FD leak is occurring.

        Returns:
            (is_leaking, reason)
        """
        if len(self._fd_history) < 2:
            return False, None

        # Get current and baseline counts
        current_time, current_count = self._fd_history[-1]

        if self._baseline_fd_count is None:
            return False, None

        # Check for sustained growth
        fd_growth = current_count - self._baseline_fd_count

        if fd_growth >= self.leak_threshold:
            if not self._leak_detected:
                self._leak_detected = True
                self._leak_start_time = current_time
                reason = f"FD count grew by {fd_growth} (from {self._baseline_fd_count} to {current_count})"
                logger.error(f"[FDMonitor] LEAK DETECTED: {reason}")
                return True, reason
            return True, "Leak continues"

        else:
            # Leak resolved
            if self._leak_detected:
                logger.info(f"[FDMonitor] Leak resolved - FD count stabilized at {current_count}")
                self._leak_detected = False
                self._leak_start_time = None
                # Update baseline to current stable state
                self._baseline_fd_count = current_count

            return False, None

    def get_statistics(self) -> dict:
        """Get FD monitoring statistics."""
        if not self._fd_history:
            return {
                "available": False,
                "reason": "No samples collected yet"
            }

        current_time, current_count = self._fd_history[-1]
        baseline = self._baseline_fd_count or current_count

        # Calculate growth rate (FDs per minute)
        growth_rate = 0.0
        if len(self._fd_history) >= 2:
            first_time, first_count = self._fd_history[0]
            time_span = current_time - first_time
            if time_span > 0:
                fd_delta = current_count - first_count
                growth_rate = (fd_delta / time_span) * 60.0  # Per minute

        leak_active, leak_reason = self.detect_leak()
        leak_duration = None
        if leak_active and self._leak_start_time:
            leak_duration = current_time - self._leak_start_time

        return {
            "available": True,
            "current_fd_count": current_count,
            "baseline_fd_count": baseline,
            "peak_fd_count": self._peak_fd_count,
            "fd_growth": current_count - baseline,
            "growth_rate_per_minute": round(growth_rate, 2),
            "leak_detected": leak_active,
            "leak_reason": leak_reason,
            "leak_duration_seconds": leak_duration,
            "sample_count": len(self._fd_history),
            "threshold": self.leak_threshold,
        }


async def fd_monitor_loop() -> None:
    """
    v87.0: Background task to continuously monitor file descriptor usage.

    Detects FD leaks by tracking count growth over time.
    """
    global file_descriptor_monitor

    while True:
        try:
            await asyncio.sleep(file_descriptor_monitor.sample_interval)
            file_descriptor_monitor.sample_fd_count()
            file_descriptor_monitor.detect_leak()

        except asyncio.CancelledError:
            break
        except Exception as e:
            logger.error(f"[FDMonitor] Error in monitor loop: {e}")
            await asyncio.sleep(file_descriptor_monitor.sample_interval)


class FallbackStaticPageGenerator:
    """
    v87.0: Generate fallback static HTML page for when loading server is down.

    Provides a minimal loading screen that works even if the backend crashes.
    The page periodically retries connecting to the server and auto-redirects when available.
    """

    def __init__(self, backend_port: int = 8010, loading_port: int = 3001):
        self.backend_port = backend_port
        self.loading_port = loading_port

    def generate(self) -> str:
        """Generate self-contained fallback HTML page."""
        return f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>JARVIS - Connecting...</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        body {{
            background: linear-gradient(135deg, #0f0c29, #302b63, #24243e);
            color: #00ffcc;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            overflow: hidden;
        }}
        .container {{
            text-align: center;
            padding: 40px;
            background: rgba(0, 0, 0, 0.3);
            border-radius: 20px;
            border: 1px solid rgba(0, 255, 204, 0.3);
            backdrop-filter: blur(10px);
            max-width: 600px;
        }}
        h1 {{
            font-size: 3em;
            margin-bottom: 20px;
            text-shadow: 0 0 20px rgba(0, 255, 204, 0.8);
        }}
        .status {{
            font-size: 1.2em;
            margin: 20px 0;
            opacity: 0.8;
        }}
        .spinner {{
            width: 80px;
            height: 80px;
            border: 8px solid rgba(0, 255, 204, 0.1);
            border-top: 8px solid #00ffcc;
            border-radius: 50%;
            animation: spin 1s linear infinite;
            margin: 30px auto;
        }}
        @keyframes spin {{
            0% {{ transform: rotate(0deg); }}
            100% {{ transform: rotate(360deg); }}
        }}
        .message {{
            margin-top: 30px;
            line-height: 1.6;
        }}
        .retry-info {{
            margin-top: 20px;
            font-size: 0.9em;
            opacity: 0.6;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>JARVIS</h1>
        <div class="spinner"></div>
        <div class="status" id="status">Connecting to JARVIS...</div>
        <div class="message">
            <p>Loading server is temporarily unavailable.</p>
            <p>Attempting to reconnect...</p>
        </div>
        <div class="retry-info" id="retry-info">
            Next attempt in <span id="countdown">5</span>s
        </div>
    </div>

    <script>
        const BACKEND_PORT = {self.backend_port};
        const LOADING_PORT = {self.loading_port};
        const RETRY_INTERVAL = 5000; // 5 seconds

        let retryCount = 0;
        let countdown = 5;

        function updateStatus(message) {{
            document.getElementById('status').textContent = message;
        }}

        function updateCountdown() {{
            document.getElementById('countdown').textContent = countdown;
            if (countdown > 0) {{
                countdown--;
            }} else {{
                countdown = 5;
            }}
        }}

        async function checkServer() {{
            try {{
                retryCount++;
                updateStatus(`Attempt #${{retryCount}} - Checking server...`);

                // Try backend first
                const backendUrl = `http://localhost:${{BACKEND_PORT}}/health`;
                const response = await fetch(backendUrl, {{
                    method: 'GET',
                    cache: 'no-cache',
                    signal: AbortSignal.timeout(3000)
                }});

                if (response.ok) {{
                    updateStatus('✅ JARVIS is online! Redirecting...');
                    setTimeout(() => {{
                        window.location.href = `http://localhost:${{BACKEND_PORT}}/`;
                    }}, 500);
                    return true;
                }}

            }} catch (error) {{
                console.debug('Server not available yet:', error.message);
            }}

            // Try loading server as fallback
            try {{
                const loadingUrl = `http://localhost:${{LOADING_PORT}}/health`;
                const response = await fetch(loadingUrl, {{
                    method: 'GET',
                    cache: 'no-cache',
                    signal: AbortSignal.timeout(3000)
                }});

                if (response.ok) {{
                    updateStatus('✅ Loading server online! Continuing startup...');
                    setTimeout(() => {{
                        window.location.reload();
                    }}, 500);
                    return true;
                }}

            }} catch (error) {{
                console.debug('Loading server not available:', error.message);
            }}

            updateStatus(`Attempt #${{retryCount}} failed - Retrying in 5s...`);
            return false;
        }}

        // Initial check
        checkServer();

        // Periodic retry
        setInterval(() => {{
            checkServer();
        }}, RETRY_INTERVAL);

        // Countdown timer
        setInterval(() => {{
            updateCountdown();
        }}, 1000);
    </script>
</body>
</html>"""

    async def save_to_file(self, filepath: str = None) -> str:
        """
        Save fallback page to filesystem.

        Args:
            filepath: Optional custom path. Defaults to frontend/public/fallback.html

        Returns:
            Path where file was saved
        """
        if filepath is None:
            # Try to resolve frontend path
            base_path = path_resolver.get_base_path()
            if base_path:
                filepath = str(base_path / "fallback.html")
            else:
                filepath = "/tmp/jarvis_fallback.html"

        html_content = self.generate()

        try:
            # Write atomically
            import tempfile
            import shutil

            temp_fd, temp_path = tempfile.mkstemp(suffix='.html', text=True)
            try:
                with os.fdopen(temp_fd, 'w') as f:
                    f.write(html_content)
                shutil.move(temp_path, filepath)
                logger.info(f"[FallbackPage] Generated at: {filepath}")
                return filepath
            except Exception as e:
                os.unlink(temp_path)
                raise e

        except Exception as e:
            logger.error(f"[FallbackPage] Failed to save: {e}")
            return None


async def trinity_heartbeat_monitor_loop() -> None:
    """
    v87.0: Background task to continuously monitor Trinity component heartbeats.

    Updates parallel_component_tracker with real-time status from heartbeat files.
    Uses monotonic time to avoid clock skew issues.
    """
    global parallel_component_tracker, trinity_heartbeat_reader

    check_interval = 2.0  # Check every 2 seconds
    loop_start = time.monotonic()  # v87.0: Use monotonic time

    while True:
        try:
            # Read all heartbeats
            heartbeats = await trinity_heartbeat_reader.get_all_heartbeats()

            # Update component tracker
            for component, heartbeat_data in heartbeats.items():
                if component in ("jarvis_prime", "reactor_core"):
                    if heartbeat_data:
                        # Component is alive
                        await parallel_component_tracker.update_component(
                            component=component,
                            state="ready",
                            progress=100.0,
                            pid=heartbeat_data.get("pid"),
                        )
                    else:
                        # Component is not running or stale
                        current_status = await parallel_component_tracker.get_all_status()
                        current_state = current_status.get(component, {}).get("state", "pending")

                        # Only update if not already failed
                        if current_state not in ("failed", "ready"):
                            await parallel_component_tracker.update_component(
                                component=component,
                                state="waiting",
                            )

            await asyncio.sleep(check_interval)

        except asyncio.CancelledError:
            break
        except Exception as e:
            logger.error(f"[Trinity Monitor] Error: {e}")
            await asyncio.sleep(check_interval)


# =============================================================================
# Dynamic Path Resolver - No Hardcoding
# =============================================================================

class DynamicPathResolver:
    """
    Dynamically resolves paths for static files without hardcoding.
    Uses multiple strategies to find the correct path at runtime.
    """

    def __init__(self):
        self._cache: Dict[str, Path] = {}
        self._base_paths: List[Path] = []
        self._initialized = False

    def initialize(self):
        """Initialize base paths dynamically based on runtime context."""
        if self._initialized:
            return

        # Strategy 1: Environment variable (highest priority)
        env_path = os.getenv('FRONTEND_PATH')
        if env_path:
            self._base_paths.append(Path(env_path))

        # Strategy 2: Relative to this script file
        script_dir = Path(__file__).resolve().parent
        self._base_paths.append(script_dir / 'frontend' / 'public')

        # Strategy 3: Relative to current working directory
        cwd = Path.cwd()
        self._base_paths.append(cwd / 'frontend' / 'public')

        # Strategy 4: Walk up directory tree to find project root
        project_root = self._find_project_root(script_dir)
        if project_root:
            self._base_paths.append(project_root / 'frontend' / 'public')

        # Strategy 5: Check common development paths relative to home
        home = Path.home()
        for subdir in ['Documents/repos', 'Projects', 'Development', 'code', 'dev']:
            potential = home / subdir
            if potential.exists():
                # Find any JARVIS-AI-Agent directory
                for item in potential.iterdir():
                    if item.is_dir() and 'jarvis' in item.name.lower():
                        self._base_paths.append(item / 'frontend' / 'public')

        # Deduplicate while preserving order
        seen = set()
        unique_paths = []
        for p in self._base_paths:
            p_str = str(p.resolve()) if p.exists() else str(p)
            if p_str not in seen:
                seen.add(p_str)
                unique_paths.append(p)
        self._base_paths = unique_paths

        self._initialized = True
        logger.info(f"[PathResolver] Initialized with {len(self._base_paths)} search paths")

    def _find_project_root(self, start_path: Path, max_depth: int = 10) -> Optional[Path]:
        """Walk up directory tree to find project root (contains .git or package.json)."""
        current = start_path
        for _ in range(max_depth):
            if (current / '.git').exists() or (current / 'package.json').exists():
                return current
            if current.parent == current:
                break
            current = current.parent
        return None

    def resolve(self, filename: str) -> Optional[Path]:
        """
        Resolve a filename to its full path, checking cache first.
        Returns None if file not found in any search path.
        """
        # Check cache
        if filename in self._cache:
            cached = self._cache[filename]
            if cached.exists():
                return cached
            # Cache invalidated, remove it
            del self._cache[filename]

        self.initialize()

        # Search all base paths
        for base in self._base_paths:
            full_path = base / filename
            if full_path.exists() and full_path.is_file():
                self._cache[filename] = full_path
                logger.debug(f"[PathResolver] Resolved {filename} -> {full_path}")
                return full_path

        logger.warning(f"[PathResolver] Could not resolve: {filename}")
        logger.debug(f"[PathResolver] Searched paths: {self._base_paths}")
        return None

    def get_base_path(self) -> Optional[Path]:
        """Get the first valid base path that contains loading.html."""
        self.initialize()
        for base in self._base_paths:
            if (base / 'loading.html').exists():
                return base
        return self._base_paths[0] if self._base_paths else None

    @property
    def search_paths(self) -> List[Path]:
        """Get all search paths for debugging."""
        self.initialize()
        return self._base_paths.copy()


# Global path resolver instance
path_resolver = DynamicPathResolver()


# =============================================================================
# Dynamic Configuration
# =============================================================================

@dataclass
class ServerConfig:
    """Dynamic server configuration from environment variables"""

    # Server ports
    loading_port: int = field(default_factory=lambda: int(os.getenv('LOADING_SERVER_PORT', '3001')))
    backend_port: int = field(default_factory=lambda: int(os.getenv('BACKEND_PORT', '8010')))
    frontend_port: int = field(default_factory=lambda: int(os.getenv('FRONTEND_PORT', '3000')))

    # Health check settings
    health_check_timeout: float = field(default_factory=lambda: float(os.getenv('HEALTH_CHECK_TIMEOUT', '3.0')))
    health_check_interval: float = field(default_factory=lambda: float(os.getenv('HEALTH_CHECK_INTERVAL', '5.0')))

    # Watchdog settings
    watchdog_silence_threshold: int = field(default_factory=lambda: int(os.getenv('WATCHDOG_SILENCE_THRESHOLD', '60')))
    watchdog_startup_delay: int = field(default_factory=lambda: int(os.getenv('WATCHDOG_STARTUP_DELAY', '30')))

    # WebSocket settings
    ws_heartbeat_interval: float = field(default_factory=lambda: float(os.getenv('WS_HEARTBEAT_INTERVAL', '15.0')))
    ws_heartbeat_timeout: float = field(default_factory=lambda: float(os.getenv('WS_HEARTBEAT_TIMEOUT', '30.0')))
    ws_max_connections: int = field(default_factory=lambda: int(os.getenv('WS_MAX_CONNECTIONS', '100')))

    # Rate limiting
    rate_limit_requests: int = field(default_factory=lambda: int(os.getenv('RATE_LIMIT_REQUESTS', '100')))
    rate_limit_window: float = field(default_factory=lambda: float(os.getenv('RATE_LIMIT_WINDOW', '60.0')))

    # Paths - Dynamically resolved, no hardcoding
    @property
    def frontend_path(self) -> Path:
        """Dynamically resolve frontend path."""
        base = path_resolver.get_base_path()
        if base:
            return base
        # Fallback to env or script-relative path
        return Path(os.getenv('FRONTEND_PATH', Path(__file__).resolve().parent / 'frontend' / 'public'))


config = ServerConfig()


# =============================================================================
# Metrics Collection
# =============================================================================

@dataclass
class ServerMetrics:
    """Server metrics for monitoring and debugging"""

    start_time: datetime = field(default_factory=datetime.now)
    total_requests: int = 0
    websocket_connections: int = 0
    websocket_messages_sent: int = 0
    progress_updates_received: int = 0
    health_checks_performed: int = 0
    errors: int = 0
    last_error: Optional[str] = None
    last_progress_update: Optional[datetime] = None

    # Request latencies (last 100)
    request_latencies: deque = field(default_factory=lambda: deque(maxlen=100))

    def record_request(self, latency_ms: float):
        self.total_requests += 1
        self.request_latencies.append(latency_ms)

    def record_error(self, error: str):
        self.errors += 1
        self.last_error = error

    @property
    def avg_latency_ms(self) -> float:
        if not self.request_latencies:
            return 0.0
        return sum(self.request_latencies) / len(self.request_latencies)

    @property
    def uptime_seconds(self) -> float:
        return (datetime.now() - self.start_time).total_seconds()

    def to_dict(self) -> Dict[str, Any]:
        return {
            "uptime_seconds": round(self.uptime_seconds, 1),
            "total_requests": self.total_requests,
            "websocket_connections": self.websocket_connections,
            "websocket_messages_sent": self.websocket_messages_sent,
            "progress_updates_received": self.progress_updates_received,
            "health_checks_performed": self.health_checks_performed,
            "errors": self.errors,
            "last_error": self.last_error,
            "avg_latency_ms": round(self.avg_latency_ms, 2),
            "last_progress_update": self.last_progress_update.isoformat() if self.last_progress_update else None
        }


metrics = ServerMetrics()


# =============================================================================
# Progress State Management
# =============================================================================

@dataclass
class ZeroTouchState:
    """
    v4.0: Zero-Touch autonomous update state tracking.
    """
    active: bool = False
    state: str = "idle"  # initiated, staging, validating, applying, dms_monitoring, complete, failed
    classification: Optional[str] = None  # security, critical, minor, major, patch
    message: str = ""
    validation_progress: float = 0.0
    files_validated: int = 0
    total_files: int = 0
    commits: int = 0
    files_changed: int = 0
    validation_report: Optional[Dict] = None
    started_at: Optional[datetime] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "state": self.state,
            "classification": self.classification,
            "message": self.message,
            "validationProgress": self.validation_progress,
            "filesValidated": self.files_validated,
            "totalFiles": self.total_files,
            "commits": self.commits,
            "filesChanged": self.files_changed,
            "validationReport": self.validation_report,
            "startedAt": self.started_at.isoformat() if self.started_at else None,
        }


@dataclass
class DMSState:
    """
    v4.0: Dead Man's Switch state tracking.
    """
    active: bool = False
    state: str = "idle"  # monitoring, passed, rolling_back
    health_score: float = 1.0
    probation_remaining: float = 0.0
    probation_total: float = 30.0
    consecutive_failures: int = 0
    started_at: Optional[datetime] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "state": self.state,
            "healthScore": self.health_score,
            "probationRemaining": self.probation_remaining,
            "probationTotal": self.probation_total,
            "consecutiveFailures": self.consecutive_failures,
            "startedAt": self.started_at.isoformat() if self.started_at else None,
        }


@dataclass
class TwoTierSecurityState:
    """
    v5.0: Two-Tier Agentic Security System state tracking.

    Tracks the initialization and status of:
    - AgenticWatchdog (safety monitoring)
    - TieredCommandRouter (Tier 1/2 routing)
    - TieredVBIAAdapter (voice biometric authentication)
    """
    # Component readiness
    watchdog_ready: bool = False
    router_ready: bool = False
    vbia_adapter_ready: bool = False

    # Tier operational status
    tier1_operational: bool = False
    tier2_operational: bool = False

    # Watchdog status
    watchdog_status: str = "initializing"  # initializing, running, idle, critical
    watchdog_mode: str = "idle"  # idle, monitoring, armed

    # Router stats
    tier1_routes: int = 0
    tier2_routes: int = 0
    blocked_routes: int = 0

    # VBIA details
    vbia_tier1_threshold: float = 0.70
    vbia_tier2_threshold: float = 0.85
    vbia_liveness_enabled: bool = True
    vbia_anti_spoofing_ready: bool = False

    # Overall status
    overall_status: str = "initializing"  # initializing, partial, ready, error
    message: str = "Initializing Two-Tier Security..."

    def to_dict(self) -> Dict[str, Any]:
        return {
            "watchdogReady": self.watchdog_ready,
            "routerReady": self.router_ready,
            "vbiaAdapterReady": self.vbia_adapter_ready,
            "tier1Operational": self.tier1_operational,
            "tier2Operational": self.tier2_operational,
            "watchdogStatus": self.watchdog_status,
            "watchdogMode": self.watchdog_mode,
            "tier1Routes": self.tier1_routes,
            "tier2Routes": self.tier2_routes,
            "blockedRoutes": self.blocked_routes,
            "vbiaTier1Threshold": self.vbia_tier1_threshold,
            "vbiaTier2Threshold": self.vbia_tier2_threshold,
            "vbiaLivenessEnabled": self.vbia_liveness_enabled,
            "vbiaAntiSpoofingReady": self.vbia_anti_spoofing_ready,
            "overallStatus": self.overall_status,
            "message": self.message,
        }

    def update_overall_status(self):
        """Update overall status based on component states."""
        if self.watchdog_ready and self.router_ready and self.vbia_adapter_ready:
            self.overall_status = "ready"
            self.message = "Two-Tier Security System operational"
        elif self.watchdog_ready or self.router_ready or self.vbia_adapter_ready:
            self.overall_status = "partial"
            components = []
            if self.watchdog_ready:
                components.append("Watchdog")
            if self.router_ready:
                components.append("Router")
            if self.vbia_adapter_ready:
                components.append("VBIA")
            self.message = f"Partial: {', '.join(components)} ready"
        else:
            self.overall_status = "initializing"
            self.message = "Initializing Two-Tier Security..."


# =============================================================================
# v5.0: Data Flywheel State
# =============================================================================

@dataclass
class FlywheelState:
    """
    v5.0: Data Flywheel (self-improving learning loop) state tracking.

    Tracks:
    - Experience collection progress
    - Training status and progress
    - Web scraping progress
    - Model deployment status
    """
    active: bool = False
    state: str = "idle"  # idle, collecting, scraping, training, deploying, complete
    message: str = ""

    # Collection metrics
    experiences_collected: int = 0
    experiences_target: int = 100
    collection_progress: float = 0.0

    # Scraping metrics
    urls_scraped: int = 0
    total_urls: int = 0
    scraping_progress: float = 0.0

    # Training metrics
    training_active: bool = False
    training_progress: float = 0.0
    training_topic: str = ""
    training_epochs: int = 0
    training_loss: float = 0.0

    # Model deployment
    model_deployed: bool = False
    model_name: str = ""
    model_version: str = ""

    # Cycle tracking
    cycles_completed: int = 0
    last_cycle_time: Optional[datetime] = None
    next_scheduled_time: Optional[datetime] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "state": self.state,
            "message": self.message,
            "experiencesCollected": self.experiences_collected,
            "experiencesTarget": self.experiences_target,
            "collectionProgress": self.collection_progress,
            "urlsScraped": self.urls_scraped,
            "totalUrls": self.total_urls,
            "scrapingProgress": self.scraping_progress,
            "trainingActive": self.training_active,
            "trainingProgress": self.training_progress,
            "trainingTopic": self.training_topic,
            "trainingEpochs": self.training_epochs,
            "trainingLoss": self.training_loss,
            "modelDeployed": self.model_deployed,
            "modelName": self.model_name,
            "modelVersion": self.model_version,
            "cyclesCompleted": self.cycles_completed,
            "lastCycleTime": self.last_cycle_time.isoformat() if self.last_cycle_time else None,
            "nextScheduledTime": self.next_scheduled_time.isoformat() if self.next_scheduled_time else None,
        }


@dataclass
class LearningGoalsState:
    """
    v5.0: Learning Goals tracking state.

    Tracks discovered learning goals and their progress.
    """
    active: bool = False
    total_goals: int = 0
    completed_goals: int = 0
    in_progress_goals: int = 0

    # Current focus
    current_topic: str = ""
    current_progress: float = 0.0

    # Recent discoveries
    recent_discoveries: List[str] = field(default_factory=list)

    # Goal priorities
    high_priority_count: int = 0
    medium_priority_count: int = 0
    low_priority_count: int = 0

    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "totalGoals": self.total_goals,
            "completedGoals": self.completed_goals,
            "inProgressGoals": self.in_progress_goals,
            "currentTopic": self.current_topic,
            "currentProgress": self.current_progress,
            "recentDiscoveries": self.recent_discoveries[-5:],  # Last 5
            "highPriorityCount": self.high_priority_count,
            "mediumPriorityCount": self.medium_priority_count,
            "lowPriorityCount": self.low_priority_count,
        }


@dataclass
class JARVISPrimeState:
    """
    v5.0: JARVIS-Prime tier-0 brain state tracking.

    Tracks the status of the intelligent core:
    - Current tier (local, cloud_run, gemini_api)
    - Memory usage and routing decisions
    - Response latency and success rate
    """
    active: bool = False
    current_tier: str = "unknown"  # local, cloud_run, gemini_api, offline
    message: str = ""

    # Memory-aware routing
    memory_usage_gb: float = 0.0
    memory_threshold_gb: float = 8.0
    routing_mode: str = "auto"  # auto, force_local, force_cloud

    # Health metrics
    is_healthy: bool = False
    last_health_check: Optional[datetime] = None
    consecutive_failures: int = 0

    # Performance metrics
    avg_latency_ms: float = 0.0
    success_rate: float = 0.0
    total_requests: int = 0

    # Model info
    local_model_loaded: bool = False
    local_model_name: str = ""
    cloud_endpoint: str = ""

    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "currentTier": self.current_tier,
            "message": self.message,
            "memoryUsageGb": self.memory_usage_gb,
            "memoryThresholdGb": self.memory_threshold_gb,
            "routingMode": self.routing_mode,
            "isHealthy": self.is_healthy,
            "lastHealthCheck": self.last_health_check.isoformat() if self.last_health_check else None,
            "consecutiveFailures": self.consecutive_failures,
            "avgLatencyMs": self.avg_latency_ms,
            "successRate": self.success_rate,
            "totalRequests": self.total_requests,
            "localModelLoaded": self.local_model_loaded,
            "localModelName": self.local_model_name,
            "cloudEndpoint": self.cloud_endpoint,
        }


@dataclass
class ReactorCoreState:
    """
    v5.0: Reactor-Core model training pipeline state.

    Tracks:
    - File watcher status
    - Training pipeline status
    - GGUF export progress
    - GCS upload status
    """
    active: bool = False
    state: str = "idle"  # idle, watching, training, exporting, uploading, complete
    message: str = ""

    # File watcher
    watcher_active: bool = False
    watched_directory: str = ""
    files_detected: int = 0

    # Training pipeline
    training_active: bool = False
    training_progress: float = 0.0
    current_epoch: int = 0
    total_epochs: int = 0

    # GGUF export
    exporting: bool = False
    export_progress: float = 0.0
    export_format: str = ""  # Q4_K_M, Q5_K_M, etc.

    # GCS upload
    uploading: bool = False
    upload_progress: float = 0.0
    gcs_bucket: str = ""

    # Completion
    last_model_path: str = ""
    last_completion_time: Optional[datetime] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "state": self.state,
            "message": self.message,
            "watcherActive": self.watcher_active,
            "watchedDirectory": self.watched_directory,
            "filesDetected": self.files_detected,
            "trainingActive": self.training_active,
            "trainingProgress": self.training_progress,
            "currentEpoch": self.current_epoch,
            "totalEpochs": self.total_epochs,
            "exporting": self.exporting,
            "exportProgress": self.export_progress,
            "exportFormat": self.export_format,
            "uploading": self.uploading,
            "uploadProgress": self.upload_progress,
            "gcsBucket": self.gcs_bucket,
            "lastModelPath": self.last_model_path,
            "lastCompletionTime": self.last_completion_time.isoformat() if self.last_completion_time else None,
        }


@dataclass
class TrainingOrchestratorState:
    """
    v9.2: Intelligent Training Orchestrator state.

    Tracks:
    - Training scheduler status (cron, data-threshold, quality triggers)
    - Current training run progress
    - Pipeline stages (scouting, ingesting, training, evaluating, etc.)
    - Model deployment status
    """
    active: bool = False
    status: str = "idle"  # idle, ready, triggered, running, completed, failed
    message: str = ""

    # Scheduler info
    next_scheduled_run: Optional[str] = None
    last_training_run: Optional[str] = None
    cooldown_remaining_hours: float = 0.0

    # Triggers
    time_based_enabled: bool = True
    data_threshold_enabled: bool = True
    quality_trigger_enabled: bool = True
    cron_schedule: str = "0 3 * * *"
    min_experiences_threshold: int = 100
    quality_threshold: float = 0.7

    # Current run
    current_run_id: Optional[str] = None
    current_stage: str = "idle"
    current_progress: int = 0
    trigger_source: Optional[str] = None  # scheduled, data_threshold, quality_degradation, manual

    # Pipeline stages
    stage_scouting: bool = False
    stage_ingesting: bool = False
    stage_formatting: bool = False
    stage_distilling: bool = False
    stage_training: bool = False
    stage_evaluating: bool = False
    stage_quantizing: bool = False
    stage_deploying: bool = False

    # Results
    last_result: Optional[Dict[str, Any]] = None
    last_error: Optional[str] = None

    # Scheduler stats
    total_runs: int = 0
    successful_runs: int = 0
    failed_runs: int = 0

    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "status": self.status,
            "message": self.message,
            "nextScheduledRun": self.next_scheduled_run,
            "lastTrainingRun": self.last_training_run,
            "cooldownRemainingHours": self.cooldown_remaining_hours,
            "triggers": {
                "timeBased": self.time_based_enabled,
                "dataThreshold": self.data_threshold_enabled,
                "qualityTrigger": self.quality_trigger_enabled,
                "cronSchedule": self.cron_schedule,
                "minExperiences": self.min_experiences_threshold,
                "qualityThreshold": self.quality_threshold,
            },
            "currentRun": {
                "runId": self.current_run_id,
                "stage": self.current_stage,
                "progress": self.current_progress,
                "triggerSource": self.trigger_source,
            } if self.current_run_id else None,
            "stages": {
                "scouting": self.stage_scouting,
                "ingesting": self.stage_ingesting,
                "formatting": self.stage_formatting,
                "distilling": self.stage_distilling,
                "training": self.stage_training,
                "evaluating": self.stage_evaluating,
                "quantizing": self.stage_quantizing,
                "deploying": self.stage_deploying,
            },
            "lastResult": self.last_result,
            "lastError": self.last_error,
            "stats": {
                "totalRuns": self.total_runs,
                "successfulRuns": self.successful_runs,
                "failedRuns": self.failed_runs,
            },
        }


@dataclass
class VoiceBiometricsState:
    """
    v6.2: Voice Biometric Authentication System state tracking.

    Tracks comprehensive voice security initialization:
    - ECAPA-TDNN embedding model (192-dimensional)
    - Liveness detection (99.8% accuracy anti-spoofing)
    - Speaker cache population
    - Multi-tier authentication thresholds
    - ChromaDB voice pattern recognition
    """
    active: bool = False
    status: str = "idle"  # idle, initializing, loading_model, warming_cache, ready, error

    # ECAPA-TDNN Model
    ecapa_status: str = "not_loaded"  # not_loaded, loading, ready, error
    ecapa_backend: str = "unknown"  # local, cloud_run, docker, emergency_fallback
    embedding_dimensions: int = 192

    # Liveness Detection (Anti-Spoofing)
    liveness_enabled: bool = False
    liveness_accuracy: float = 99.8  # Target accuracy percentage
    anti_spoofing_ready: bool = False
    replay_detection_ready: bool = False
    deepfake_detection_ready: bool = False

    # Speaker Cache
    speaker_cache_status: str = "empty"  # empty, loading, populated, error
    cached_samples: int = 0
    target_samples: int = 59  # Expected voiceprint count
    cache_population_percent: float = 0.0

    # Multi-Tier Authentication Thresholds
    tier1_threshold: float = 70.0  # Fast, single-factor
    tier2_threshold: float = 85.0  # Strict, multi-factor
    high_security_threshold: float = 95.0  # Maximum security
    liveness_threshold: float = 80.0  # Anti-spoofing threshold

    # ChromaDB Voice Pattern Recognition
    chromadb_voice_patterns: bool = False
    behavioral_biometrics_ready: bool = False

    # Performance Metrics
    init_time_ms: Optional[int] = None
    last_update: Optional[str] = None
    error_message: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "status": self.status,
            "ecapa": {
                "status": self.ecapa_status,
                "backend": self.ecapa_backend,
                "dimensions": self.embedding_dimensions,
            },
            "liveness": {
                "enabled": self.liveness_enabled,
                "accuracy": self.liveness_accuracy,
                "anti_spoofing_ready": self.anti_spoofing_ready,
                "replay_detection": self.replay_detection_ready,
                "deepfake_detection": self.deepfake_detection_ready,
            },
            "speaker_cache": {
                "status": self.speaker_cache_status,
                "cached_samples": self.cached_samples,
                "target_samples": self.target_samples,
                "population_percent": self.cache_population_percent,
            },
            "thresholds": {
                "tier1": self.tier1_threshold,
                "tier2": self.tier2_threshold,
                "high_security": self.high_security_threshold,
                "liveness": self.liveness_threshold,
            },
            "chromadb": {
                "voice_patterns": self.chromadb_voice_patterns,
                "behavioral_biometrics": self.behavioral_biometrics_ready,
            },
            "performance": {
                "init_time_ms": self.init_time_ms,
                "last_update": self.last_update,
                "error": self.error_message,
            },
        }


@dataclass
class NarratorState:
    """
    v6.2: Intelligent Voice Narrator state tracking.

    Tracks voice announcement system for security events and system status.
    Integrates with Claude for contextual, personalized narration.
    """
    active: bool = False
    status: str = "idle"  # idle, initializing, ready, speaking, error

    # Narrator Configuration
    enabled: bool = True
    voice_enabled: bool = False  # TTS capability
    contextual_messages: bool = True  # AI-generated vs static

    # Recent Announcements
    last_announcement: Optional[str] = None
    announcement_count: int = 0

    # Milestone Tracking
    milestones_announced: List[str] = field(default_factory=list)

    # Integration Status
    claude_integration: bool = False  # For intelligent narration
    langfuse_tracking: bool = False  # Announcement audit trail

    # Performance Metrics
    init_time_ms: Optional[int] = None
    last_update: Optional[str] = None
    error_message: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "status": self.status,
            "config": {
                "enabled": self.enabled,
                "voice_enabled": self.voice_enabled,
                "contextual_messages": self.contextual_messages,
            },
            "announcements": {
                "last": self.last_announcement,
                "count": self.announcement_count,
                "milestones": self.milestones_announced,
            },
            "integrations": {
                "claude": self.claude_integration,
                "langfuse": self.langfuse_tracking,
            },
            "performance": {
                "init_time_ms": self.init_time_ms,
                "last_update": self.last_update,
                "error": self.error_message,
            },
        }


@dataclass
class CostOptimizationState:
    """
    v6.3: Cost Optimization and Helicone Integration state tracking.

    Tracks API cost savings, caching effectiveness, and optimization metrics.
    Integrates with Helicone for LLM cost monitoring.
    """
    active: bool = False
    status: str = "idle"  # idle, initializing, monitoring, optimizing, error

    # Helicone Integration
    helicone_enabled: bool = False
    helicone_api_key_configured: bool = False

    # Cost Tracking
    total_api_calls: int = 0
    cached_calls: int = 0
    cache_hit_rate: float = 0.0  # Percentage
    estimated_cost_usd: float = 0.0
    estimated_savings_usd: float = 0.0

    # Optimization Strategies
    caching_enabled: bool = True
    prompt_optimization: bool = False
    model_routing: bool = False  # Route to cheaper models when possible

    # Real-Time Stats
    last_call_cost: Optional[float] = None
    avg_call_cost: Optional[float] = None
    peak_cost_per_minute: Optional[float] = None

    # Performance Metrics
    init_time_ms: Optional[int] = None
    last_update: Optional[str] = None
    error_message: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "status": self.status,
            "helicone": {
                "enabled": self.helicone_enabled,
                "configured": self.helicone_api_key_configured,
            },
            "costs": {
                "total_calls": self.total_api_calls,
                "cached_calls": self.cached_calls,
                "cache_hit_rate": self.cache_hit_rate,
                "estimated_cost": self.estimated_cost_usd,
                "estimated_savings": self.estimated_savings_usd,
            },
            "optimization": {
                "caching_enabled": self.caching_enabled,
                "prompt_optimization": self.prompt_optimization,
                "model_routing": self.model_routing,
            },
            "real_time": {
                "last_call_cost": self.last_call_cost,
                "avg_call_cost": self.avg_call_cost,
                "peak_cost_per_minute": self.peak_cost_per_minute,
            },
            "performance": {
                "init_time_ms": self.init_time_ms,
                "last_update": self.last_update,
                "error": self.error_message,
            },
        }


@dataclass
class CrossRepoState:
    """
    v6.3: Cross-Repository Intelligence Coordination state tracking.

    Tracks integration status across JARVIS, JARVIS Prime, and Reactor Core.
    Monitors Neural Mesh coordination and cross-repo state synchronization.
    """
    active: bool = False
    status: str = "idle"  # idle, initializing, connecting, synchronized, degraded, error

    # JARVIS Prime (Tier-0 Local Brain)
    jarvis_prime_connected: bool = False
    jarvis_prime_port: int = 8002
    jarvis_prime_health: str = "unknown"  # unknown, healthy, degraded, offline
    jarvis_prime_tier: str = "unknown"  # local, cloud_run, gemini_api

    # Reactor Core (Model Training Pipeline)
    reactor_core_connected: bool = False
    reactor_core_health: str = "unknown"
    training_pipeline_active: bool = False
    model_sync_enabled: bool = False

    # Neural Mesh (Multi-Agent Coordination)
    neural_mesh_active: bool = False
    neural_mesh_coordinator: str = "offline"  # offline, starting, running, degraded
    registered_agents: int = 0
    active_conversations: int = 0

    # State Synchronization
    state_sync_enabled: bool = False
    last_sync_timestamp: Optional[str] = None
    sync_failures: int = 0

    # Performance Metrics
    avg_latency_ms: Optional[float] = None
    init_time_ms: Optional[int] = None
    last_update: Optional[str] = None
    error_message: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "status": self.status,
            "jarvis_prime": {
                "connected": self.jarvis_prime_connected,
                "port": self.jarvis_prime_port,
                "health": self.jarvis_prime_health,
                "tier": self.jarvis_prime_tier,
            },
            "reactor_core": {
                "connected": self.reactor_core_connected,
                "health": self.reactor_core_health,
                "training_active": self.training_pipeline_active,
                "model_sync": self.model_sync_enabled,
            },
            "neural_mesh": {
                "active": self.neural_mesh_active,
                "coordinator": self.neural_mesh_coordinator,
                "registered_agents": self.registered_agents,
                "active_conversations": self.active_conversations,
            },
            "sync": {
                "enabled": self.state_sync_enabled,
                "last_sync": self.last_sync_timestamp,
                "failures": self.sync_failures,
            },
            "performance": {
                "avg_latency_ms": self.avg_latency_ms,
                "init_time_ms": self.init_time_ms,
                "last_update": self.last_update,
                "error": self.error_message,
            },
        }


@dataclass
class ProgressState:
    """
    Thread-safe progress state with history tracking.

    v2.0 - Enhanced to sync with UnifiedStartupProgressHub.
    v4.0 - Added Zero-Touch and DMS state tracking.
    v6.3 - Added Voice Biometrics, Narrator, Cost Optimization, Cross-Repo Intelligence.
    All state is now received from the hub as the single source of truth.
    """

    stage: str = "init"
    message: str = "Initializing JARVIS..."
    progress: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)
    backend_ready: bool = False
    frontend_ready: bool = False
    websocket_ready: bool = False

    # Monotonic enforcement
    max_progress_seen: float = 0.0

    # v87.0: Progress versioning with sequence numbers
    sequence_number: int = 0

    # v87.0: Redirect grace period (timestamp when 100% reached)
    completion_timestamp: Optional[float] = None
    redirect_grace_period_seconds: float = 2.5  # Wait 2.5s after 100% before redirect

    # History tracking (last 50 updates)
    history: deque = field(default_factory=lambda: deque(maxlen=50))

    # ETag for caching
    _etag: Optional[str] = None

    # Hub sync fields
    is_ready: bool = False
    components_ready: int = 0
    total_components: int = 0
    phase: str = "initializing"
    
    # v4.0: Zero-Touch state
    zero_touch: ZeroTouchState = field(default_factory=ZeroTouchState)
    dms: DMSState = field(default_factory=DMSState)

    # v5.0: Two-Tier Agentic Security
    two_tier: TwoTierSecurityState = field(default_factory=TwoTierSecurityState)

    # v5.0: Data Flywheel and Learning
    flywheel: FlywheelState = field(default_factory=FlywheelState)
    learning_goals: LearningGoalsState = field(default_factory=LearningGoalsState)
    jarvis_prime: JARVISPrimeState = field(default_factory=JARVISPrimeState)
    reactor_core: ReactorCoreState = field(default_factory=ReactorCoreState)
    training: TrainingOrchestratorState = field(default_factory=TrainingOrchestratorState)

    # v6.2: Voice Biometric Intelligence
    voice_biometrics: VoiceBiometricsState = field(default_factory=VoiceBiometricsState)
    narrator: NarratorState = field(default_factory=NarratorState)

    # v6.3: Cost Optimization and Cross-Repo Intelligence
    cost_optimization: CostOptimizationState = field(default_factory=CostOptimizationState)
    cross_repo: CrossRepoState = field(default_factory=CrossRepoState)

    # v263.0: Dynamic startup timeout negotiation
    # Backend tells frontend the actual expected startup duration
    # Accounts for GCP/Trinity which can take 15-40 min
    startup_timeout_ms: int = field(default_factory=lambda: (lambda: (
        # GCP/Trinity enabled → floor at 2400s (40 min) to match supervisor behavior
        max(int(float(os.getenv('JARVIS_STARTUP_TIMEOUT', '180')) * 1000 *
            float(os.getenv('JARVIS_ADAPTIVE_TIMEOUT_MAX', '3.0'))),
            2400000)
        if os.getenv('JARVIS_GCP_ENABLED', '').lower() in ('1', 'true', 'yes') or
           os.getenv('JARVIS_TRINITY_ENABLED', '').lower() in ('1', 'true', 'yes')
        # No GCP/Trinity → base * adaptive_max, minimum 600s to match old behavior
        else max(int(float(os.getenv('JARVIS_STARTUP_TIMEOUT', '180')) * 1000 *
                     float(os.getenv('JARVIS_ADAPTIVE_TIMEOUT_MAX', '3.0'))),
                 600000)
    ))())
    # Tracks last time progress actually changed (for stall detection)
    last_progress_change_time: float = field(default_factory=time.monotonic)

    # v4.0: Supervisor integration
    supervisor_connected: bool = False
    agi_os_enabled: bool = False

    def update(self, stage: str, message: str, progress: float, metadata: Optional[Dict] = None) -> bool:
        """Update progress with monotonic enforcement. Returns True if progress changed."""

        # Monotonic enforcement
        if stage == 'complete':
            effective_progress = 100.0
            self.max_progress_seen = 100.0
            self.is_ready = True
            # v87.0: Record completion timestamp for redirect grace period
            if self.completion_timestamp is None:
                self.completion_timestamp = time.monotonic()
        elif progress >= 100.0 and self.completion_timestamp is None:
            # v87.0: Also set timestamp if progress hits 100% (not via 'complete' stage)
            self.completion_timestamp = time.monotonic()
            effective_progress = 100.0
            self.max_progress_seen = 100.0
        elif progress > self.max_progress_seen:
            effective_progress = progress
            self.max_progress_seen = progress
        else:
            effective_progress = self.max_progress_seen

        # Check if anything changed
        changed = (
            self.stage != stage or
            self.message != message or
            self.progress != effective_progress
        )

        # v263.0: Track when progress last actually advanced (for stall detection)
        if effective_progress > self.progress:
            self.last_progress_change_time = time.monotonic()

        # Update state
        self.stage = stage
        self.phase = stage  # Alias for hub compatibility
        self.message = message
        self.progress = effective_progress
        self.timestamp = datetime.now()

        # v87.0: Increment sequence number on every update (even if nothing changed)
        self.sequence_number += 1

        # Process metadata from hub
        if metadata:
            self.metadata = metadata
            # Extract hub-specific fields
            self.is_ready = metadata.get('is_ready', self.is_ready)
            self.components_ready = metadata.get('components_ready', self.components_ready)
            self.total_components = metadata.get('total_components', self.total_components)

            # v263.0: Accept dynamic startup timeout from backend
            if 'startup_timeout_ms' in metadata:
                self.startup_timeout_ms = int(metadata['startup_timeout_ms'])

            # Update ready flags from metadata
            if 'backend_ready' in metadata:
                self.backend_ready = metadata['backend_ready']
            if 'frontend_ready' in metadata:
                self.frontend_ready = metadata['frontend_ready']
            
            # v4.0: Zero-Touch state processing
            if 'zero_touch' in metadata:
                zt = metadata['zero_touch']
                self.zero_touch.active = zt.get('active', False)
                self.zero_touch.state = zt.get('state', self.zero_touch.state)
                self.zero_touch.classification = zt.get('classification')
                self.zero_touch.message = zt.get('message', '')
                self.zero_touch.validation_progress = zt.get('validation_progress', 0.0)
                self.zero_touch.files_validated = zt.get('files_validated', 0)
                self.zero_touch.total_files = zt.get('total_files', 0)
                self.zero_touch.commits = zt.get('commits', 0)
                self.zero_touch.files_changed = zt.get('files_changed', 0)
                self.zero_touch.validation_report = zt.get('validation_report')
                if zt.get('started_at'):
                    self.zero_touch.started_at = datetime.fromisoformat(zt['started_at'])
            
            # v4.0: DMS state processing
            if 'dms' in metadata:
                dms = metadata['dms']
                self.dms.active = dms.get('active', False)
                self.dms.state = dms.get('state', self.dms.state)
                self.dms.health_score = dms.get('health_score', 1.0)
                self.dms.probation_remaining = dms.get('probation_remaining', 0.0)
                self.dms.probation_total = dms.get('probation_total', 30.0)
                self.dms.consecutive_failures = dms.get('consecutive_failures', 0)

            # v5.0: Two-Tier Agentic Security state processing
            if 'two_tier' in metadata:
                tt = metadata['two_tier']
                self.two_tier.watchdog_ready = tt.get('watchdog_ready', False)
                self.two_tier.router_ready = tt.get('router_ready', False)
                self.two_tier.vbia_adapter_ready = tt.get('vbia_adapter_ready', False)
                self.two_tier.tier1_operational = tt.get('tier1_operational', False)
                self.two_tier.tier2_operational = tt.get('tier2_operational', False)
                self.two_tier.watchdog_status = tt.get('watchdog_status', 'initializing')
                self.two_tier.watchdog_mode = tt.get('watchdog_mode', 'idle')
                self.two_tier.tier1_routes = tt.get('tier1_routes', 0)
                self.two_tier.tier2_routes = tt.get('tier2_routes', 0)
                self.two_tier.blocked_routes = tt.get('blocked_routes', 0)
                self.two_tier.vbia_tier1_threshold = tt.get('vbia_tier1_threshold', 0.70)
                self.two_tier.vbia_tier2_threshold = tt.get('vbia_tier2_threshold', 0.85)
                self.two_tier.vbia_liveness_enabled = tt.get('vbia_liveness_enabled', True)
                self.two_tier.vbia_anti_spoofing_ready = tt.get('vbia_anti_spoofing_ready', False)
                self.two_tier.update_overall_status()

            # v4.0: Supervisor integration
            if 'supervisor_connected' in metadata:
                self.supervisor_connected = metadata['supervisor_connected']
            if 'agi_os_enabled' in metadata:
                self.agi_os_enabled = metadata['agi_os_enabled']

            # v5.0: Flywheel state processing
            if 'flywheel' in metadata:
                fw = metadata['flywheel']
                self.flywheel.active = fw.get('active', False)
                self.flywheel.state = fw.get('state', self.flywheel.state)
                self.flywheel.message = fw.get('message', '')
                self.flywheel.experiences_collected = fw.get('experiences_collected', 0)
                self.flywheel.experiences_target = fw.get('experiences_target', 100)
                self.flywheel.collection_progress = fw.get('collection_progress', 0.0)
                self.flywheel.urls_scraped = fw.get('urls_scraped', 0)
                self.flywheel.total_urls = fw.get('total_urls', 0)
                self.flywheel.scraping_progress = fw.get('scraping_progress', 0.0)
                self.flywheel.training_active = fw.get('training_active', False)
                self.flywheel.training_progress = fw.get('training_progress', 0.0)
                self.flywheel.training_topic = fw.get('training_topic', '')
                self.flywheel.cycles_completed = fw.get('cycles_completed', 0)

            # v5.0: Learning Goals state processing
            if 'learning_goals' in metadata:
                lg = metadata['learning_goals']
                self.learning_goals.active = lg.get('active', False)
                self.learning_goals.total_goals = lg.get('total_goals', 0)
                self.learning_goals.completed_goals = lg.get('completed_goals', 0)
                self.learning_goals.in_progress_goals = lg.get('in_progress_goals', 0)
                self.learning_goals.current_topic = lg.get('current_topic', '')
                self.learning_goals.current_progress = lg.get('current_progress', 0.0)
                if lg.get('recent_discoveries'):
                    self.learning_goals.recent_discoveries = lg['recent_discoveries']

            # v5.0: JARVIS-Prime state processing
            if 'jarvis_prime' in metadata:
                jp = metadata['jarvis_prime']
                self.jarvis_prime.active = jp.get('active', False)
                self.jarvis_prime.current_tier = jp.get('current_tier', 'unknown')
                self.jarvis_prime.message = jp.get('message', '')
                self.jarvis_prime.memory_usage_gb = jp.get('memory_usage_gb', 0.0)
                self.jarvis_prime.is_healthy = jp.get('is_healthy', False)
                self.jarvis_prime.avg_latency_ms = jp.get('avg_latency_ms', 0.0)
                self.jarvis_prime.success_rate = jp.get('success_rate', 0.0)
                self.jarvis_prime.local_model_loaded = jp.get('local_model_loaded', False)

            # v5.0: Reactor-Core state processing
            if 'reactor_core' in metadata:
                rc = metadata['reactor_core']
                self.reactor_core.active = rc.get('active', False)
                self.reactor_core.state = rc.get('state', 'idle')
                self.reactor_core.message = rc.get('message', '')
                self.reactor_core.watcher_active = rc.get('watcher_active', False)
                self.reactor_core.training_active = rc.get('training_active', False)
                self.reactor_core.training_progress = rc.get('training_progress', 0.0)
                self.reactor_core.exporting = rc.get('exporting', False)
                self.reactor_core.uploading = rc.get('uploading', False)

        # Track history
        self.history.append({
            "stage": stage,
            "message": message,
            "progress": effective_progress,
            "timestamp": self.timestamp.isoformat(),
            "components_ready": self.components_ready,
            "total_components": self.total_components
        })

        # Invalidate ETag
        self._etag = None

        return changed

    @property
    def etag(self) -> str:
        """Generate ETag for HTTP caching"""
        if self._etag is None:
            content = f"{self.stage}:{self.message}:{self.progress}:{self.timestamp.isoformat()}"
            self._etag = hashlib.md5(content.encode()).hexdigest()[:16]
        return self._etag

    def to_dict(self) -> Dict[str, Any]:
        # v87.0: Calculate redirect timing
        redirect_ready = False
        seconds_until_redirect = None
        if self.completion_timestamp is not None:
            elapsed = time.monotonic() - self.completion_timestamp
            seconds_until_redirect = max(0, self.redirect_grace_period_seconds - elapsed)
            redirect_ready = elapsed >= self.redirect_grace_period_seconds

        return {
            "stage": self.stage,
            "phase": self.phase,
            "message": self.message,
            "progress": self.progress,
            "timestamp": self.timestamp.isoformat(),
            # v87.0: Sequence number for detecting missed updates
            "sequence_number": self.sequence_number,
            # v87.0: Redirect grace period control
            "redirect_ready": redirect_ready,
            "seconds_until_redirect": seconds_until_redirect,
            "metadata": self.metadata,
            "backend_ready": self.backend_ready,
            "frontend_ready": self.frontend_ready,
            "websocket_ready": self.websocket_ready,
            "is_ready": self.is_ready,
            "components_ready": self.components_ready,
            "total_components": self.total_components,
            # v4.0: Zero-Touch state
            "zero_touch": self.zero_touch.to_dict(),
            "dms": self.dms.to_dict(),
            # v5.0: Two-Tier Agentic Security
            "two_tier": self.two_tier.to_dict(),
            # v5.0: Data Flywheel and Learning
            "flywheel": self.flywheel.to_dict(),
            "learning_goals": self.learning_goals.to_dict(),
            "jarvis_prime": self.jarvis_prime.to_dict(),
            "reactor_core": self.reactor_core.to_dict(),
            # v6.2: Voice Biometric Intelligence
            "voice_biometrics": self.voice_biometrics.to_dict(),
            "narrator": self.narrator.to_dict(),
            # v6.3: Cost Optimization and Cross-Repo Intelligence
            "cost_optimization": self.cost_optimization.to_dict(),
            "cross_repo": self.cross_repo.to_dict(),
            # v263.0: Dynamic startup timeout negotiation
            "startup_timeout_ms": self.startup_timeout_ms,
            # Legacy
            "supervisor_connected": self.supervisor_connected,
            "agi_os_enabled": self.agi_os_enabled,
        }


progress_state = ProgressState()


# =============================================================================
# Connection Management
# =============================================================================

class ConnectionManager:
    """
    Manages WebSocket connections with heartbeat, cleanup, and health monitoring.

    Features:
    - Connection pooling with configurable limits
    - Automatic heartbeat for keep-alive
    - Dead connection cleanup
    - Health tracking with metrics
    - Graceful shutdown
    - Connection recovery logging
    """

    def __init__(self, max_connections: int = 100):
        self.max_connections = max_connections
        self._connections: Set[web.WebSocketResponse] = set()
        # Lock will be created in initialize_async_objects() to ensure
        # it's attached to the correct event loop
        self._lock: Optional[asyncio.Lock] = None
        self._heartbeat_task: Optional[asyncio.Task] = None
        self._health_check_task: Optional[asyncio.Task] = None
        self._async_initialized = False

        # Health tracking
        self._total_connections = 0
        self._total_disconnections = 0
        self._failed_broadcasts = 0
        self._last_successful_broadcast = None
        self._connection_errors = []  # Last 10 errors

        # v87.0: Heartbeat timeout enforcement (clock skew safe)
        self._connection_last_seen: Dict[web.WebSocketResponse, float] = {}
        self._heartbeat_timeout = 60.0  # 60 seconds without response = dead

    def initialize_async_objects(self):
        """
        Initialize asyncio objects within the current event loop.

        CRITICAL: Must be called from within an async context to ensure
        Locks and Events are attached to the correct loop.
        """
        if self._async_initialized:
            return
        self._lock = asyncio.Lock()
        self._async_initialized = True
        logger.debug("[ConnectionManager] Async objects initialized for current event loop")

    @property
    def count(self) -> int:
        return len(self._connections)

    @property
    def health_status(self) -> Dict[str, Any]:
        """Get detailed health status of the connection manager."""
        return {
            "active_connections": len(self._connections),
            "max_connections": self.max_connections,
            "total_connections": self._total_connections,
            "total_disconnections": self._total_disconnections,
            "failed_broadcasts": self._failed_broadcasts,
            "last_successful_broadcast": self._last_successful_broadcast,
            "recent_errors": self._connection_errors[-5:] if self._connection_errors else [],
            "is_healthy": len(self._connections) < self.max_connections,
        }

    async def add(self, ws: web.WebSocketResponse) -> bool:
        """Add a WebSocket connection. Returns False if at capacity."""
        async with self._lock:
            if len(self._connections) >= self.max_connections:
                logger.warning(f"[ConnectionManager] At capacity ({self.max_connections}), rejecting connection")
                return False
            self._connections.add(ws)
            self._total_connections += 1
            # v87.0: Track connection timestamp (monotonic for clock skew safety)
            self._connection_last_seen[ws] = time.monotonic()
            metrics.websocket_connections = len(self._connections)
            return True

    async def remove(self, ws: web.WebSocketResponse):
        """Remove a WebSocket connection."""
        async with self._lock:
            if ws in self._connections:
                self._connections.discard(ws)
                self._total_disconnections += 1
                # v87.0: Clean up timestamp tracking
                self._connection_last_seen.pop(ws, None)
                metrics.websocket_connections = len(self._connections)

    async def broadcast(self, data: Dict[str, Any]):
        """Broadcast to all connections, removing dead ones."""
        if not self._connections:
            return

        disconnected = set()
        message = json.dumps(data)
        successful = 0
        now = time.monotonic()  # v87.0: Clock skew safe

        async with self._lock:
            for ws in self._connections:
                try:
                    if not ws.closed:
                        await asyncio.wait_for(ws.send_str(message), timeout=5.0)
                        metrics.websocket_messages_sent += 1
                        successful += 1
                        # v87.0: Update last seen timestamp on successful send
                        self._connection_last_seen[ws] = now
                except asyncio.TimeoutError:
                    logger.warning("[ConnectionManager] Broadcast timeout - connection may be slow")
                    disconnected.add(ws)
                    self._record_error("broadcast_timeout")
                except Exception as e:
                    logger.debug(f"Broadcast failed to client: {e}")
                    disconnected.add(ws)
                    self._failed_broadcasts += 1

            # Clean up disconnected
            for ws in disconnected:
                self._connections.discard(ws)
                self._connection_last_seen.pop(ws, None)  # v87.0: Clean up timestamp
                self._total_disconnections += 1

            metrics.websocket_connections = len(self._connections)

            if successful > 0:
                self._last_successful_broadcast = datetime.now().isoformat()

    def _record_error(self, error_type: str):
        """Record an error for health tracking."""
        error = {
            "type": error_type,
            "timestamp": datetime.now().isoformat(),
        }
        self._connection_errors.append(error)
        # Keep only last 10 errors
        if len(self._connection_errors) > 10:
            self._connection_errors = self._connection_errors[-10:]

    async def start_heartbeat(self):
        """Start heartbeat task to keep connections alive."""
        if self._heartbeat_task is None or self._heartbeat_task.done():
            self._heartbeat_task = asyncio.create_task(
                self._heartbeat_loop(),
                name="websocket_heartbeat"
            )
            logger.info("[ConnectionManager] ♥ Heartbeat task started")

        # Also start health check task
        if self._health_check_task is None or self._health_check_task.done():
            self._health_check_task = asyncio.create_task(
                self._health_check_loop(),
                name="websocket_health_check"
            )

    async def stop_heartbeat(self):
        """Stop heartbeat and health check tasks."""
        for task in [self._heartbeat_task, self._health_check_task]:
            if task and not task.done():
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    pass
        self._heartbeat_task = None
        self._health_check_task = None

    async def _heartbeat_loop(self):
        """Send periodic heartbeats to all connections."""
        consecutive_failures = 0

        while True:
            try:
                await asyncio.sleep(config.ws_heartbeat_interval)

                if self._connections:
                    await self.broadcast({
                        "type": "heartbeat",
                        "timestamp": datetime.now().isoformat(),
                        "connections": len(self._connections),
                    })
                    consecutive_failures = 0
                else:
                    # No connections - that's fine, just log occasionally
                    if consecutive_failures == 0:
                        logger.debug("[ConnectionManager] No active connections for heartbeat")

            except asyncio.CancelledError:
                break
            except Exception as e:
                consecutive_failures += 1
                if consecutive_failures <= 3 or consecutive_failures % 10 == 0:
                    logger.warning(f"[ConnectionManager] Heartbeat error (#{consecutive_failures}): {e}")
                self._record_error(f"heartbeat_error: {str(e)[:50]}")
                # Brief pause on error to avoid tight loop
                await asyncio.sleep(1.0)

    async def _health_check_loop(self):
        """
        v87.0: Enhanced health check with heartbeat timeout enforcement.

        Periodically check and clean up:
        - Closed connections
        - Connections exceeding heartbeat timeout (60s without activity)
        """
        while True:
            try:
                await asyncio.sleep(30.0)  # Check every 30 seconds

                stale_connections = []
                timed_out_connections = []
                now = time.monotonic()  # v87.0: Clock skew safe

                async with self._lock:
                    for ws in self._connections:
                        try:
                            # Check if closed
                            if ws.closed:
                                stale_connections.append(ws)
                                continue

                            # v87.0: Check heartbeat timeout
                            last_seen = self._connection_last_seen.get(ws, now)
                            time_since_last_seen = now - last_seen

                            if time_since_last_seen > self._heartbeat_timeout:
                                timed_out_connections.append(ws)
                                logger.warning(
                                    f"[ConnectionManager] v87.0 Heartbeat timeout: "
                                    f"{time_since_last_seen:.1f}s > {self._heartbeat_timeout}s"
                                )

                        except Exception:
                            stale_connections.append(ws)

                    # Clean up stale connections
                    for ws in stale_connections:
                        self._connections.discard(ws)
                        self._connection_last_seen.pop(ws, None)
                        self._total_disconnections += 1

                    # Clean up timed out connections
                    for ws in timed_out_connections:
                        self._connections.discard(ws)
                        self._connection_last_seen.pop(ws, None)
                        self._total_disconnections += 1
                        self._record_error("heartbeat_timeout")

                total_cleaned = len(stale_connections) + len(timed_out_connections)
                if total_cleaned > 0:
                    logger.info(
                        f"[ConnectionManager] Cleaned up {total_cleaned} connections "
                        f"({len(stale_connections)} closed, {len(timed_out_connections)} timed out)"
                    )
                    metrics.websocket_connections = len(self._connections)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.debug(f"[ConnectionManager] Health check error: {e}")

    async def close_all(self):
        """Close all connections gracefully."""
        logger.info(f"[ConnectionManager] Closing {len(self._connections)} connections...")

        async with self._lock:
            close_tasks = []
            for ws in list(self._connections):
                try:
                    # Create close task with timeout
                    close_tasks.append(
                        asyncio.wait_for(
                            ws.close(code=WSCloseCode.GOING_AWAY, message=b'Server shutting down'),
                            timeout=2.0
                        )
                    )
                except Exception:
                    pass

            # Wait for all close tasks
            if close_tasks:
                await asyncio.gather(*close_tasks, return_exceptions=True)

            self._connections.clear()
            metrics.websocket_connections = 0

        logger.info("[ConnectionManager] All connections closed")


connection_manager = ConnectionManager(max_connections=config.ws_max_connections)


# =============================================================================
# Graceful Shutdown Manager - Intelligent Browser-Aware Shutdown
# =============================================================================

class GracefulShutdownManager:
    """
    Intelligent shutdown manager that prevents "window terminated unexpectedly" errors.

    The Problem:
        When supervisor sends SIGTERM to loading server while Chrome is still
        connected or redirecting, Chrome shows:
        "window terminated unexpectedly (reason: 'killed', code: '15')"

    The Solution:
        1. Track when startup is complete (progress = 100%)
        2. Track when browser has naturally disconnected (no WebSocket connections)
        3. Auto-shutdown gracefully after both conditions are met
        4. Provide HTTP endpoint for supervisor to request shutdown

    This eliminates the race condition by letting the browser disconnect naturally.
    """

    def __init__(
        self,
        connection_manager: ConnectionManager,
        auto_shutdown_delay: float = None,
        disconnect_grace_period: float = None,
        max_idle_after_complete: float = None,
    ):
        # Dependencies
        self._connection_manager = connection_manager

        # Configuration from environment (no hardcoding)
        self._auto_shutdown_delay = auto_shutdown_delay or float(
            os.getenv('LOADING_SERVER_AUTO_SHUTDOWN_DELAY', '3.0')
        )
        self._disconnect_grace_period = disconnect_grace_period or float(
            os.getenv('LOADING_SERVER_DISCONNECT_GRACE', '2.0')
        )
        self._max_idle_after_complete = max_idle_after_complete or float(
            os.getenv('LOADING_SERVER_MAX_IDLE', '30.0')
        )
        # v198.1: Transition grace period - pause auto-shutdown during Chrome redirect
        # This prevents premature shutdown when browser briefly disconnects during redirect
        self._transition_grace_period = float(
            os.getenv('LOADING_SERVER_TRANSITION_GRACE', '5.0')
        )

        # State tracking
        self._startup_complete = False
        self._shutdown_requested = False
        self._shutdown_initiated = False
        self._last_connection_count = 0
        self._browser_disconnected_at: Optional[datetime] = None
        self._startup_completed_at: Optional[datetime] = None
        # v198.1: Track when transition grace period ends
        self._transition_grace_ends_at: Optional[datetime] = None

        # Shutdown coordination - these will be recreated in initialize_async_objects()
        # to ensure they're attached to the correct event loop
        self._shutdown_event: Optional[asyncio.Event] = None
        self._app_runner: Optional[web.AppRunner] = None
        self._monitor_task: Optional[asyncio.Task] = None

        # Lock for thread-safe operations - will be recreated in initialize_async_objects()
        self._lock: Optional[asyncio.Lock] = None
        self._async_initialized = False

        logger.info(
            f"[GracefulShutdown] Initialized - "
            f"auto_delay={self._auto_shutdown_delay}s, "
            f"grace_period={self._disconnect_grace_period}s, "
            f"max_idle={self._max_idle_after_complete}s, "
            f"transition_grace={self._transition_grace_period}s"
        )

    def initialize_async_objects(self):
        """
        Initialize asyncio objects within the current event loop.

        CRITICAL: This MUST be called from within an async context (running event loop)
        to ensure Events and Locks are attached to the correct loop. This fixes the
        "attached to a different loop" error that occurs when global objects are
        created at module import time before asyncio.run() starts a new loop.
        """
        if self._async_initialized:
            return

        # Create new asyncio objects attached to the current running loop
        self._shutdown_event = asyncio.Event()
        self._lock = asyncio.Lock()
        self._async_initialized = True
        logger.debug("[GracefulShutdown] Async objects initialized for current event loop")

    def set_app_runner(self, runner: web.AppRunner):
        """Set the app runner for graceful shutdown."""
        self._app_runner = runner

    async def start_monitoring(self):
        """Start background monitoring for auto-shutdown conditions."""
        if self._monitor_task is None or self._monitor_task.done():
            self._monitor_task = asyncio.create_task(self._monitor_loop())
            logger.info("[GracefulShutdown] Monitoring started")

    async def stop_monitoring(self):
        """Stop the monitoring task."""
        if self._monitor_task and not self._monitor_task.done():
            self._monitor_task.cancel()
            try:
                await self._monitor_task
            except asyncio.CancelledError:
                pass

    async def notify_startup_complete(self):
        """Called when JARVIS startup reaches 100% progress."""
        async with self._lock:
            if not self._startup_complete:
                self._startup_complete = True
                self._startup_completed_at = datetime.now()
                # v198.1: Set transition grace period to allow Chrome redirect without
                # triggering premature shutdown from brief browser disconnects
                self._transition_grace_ends_at = datetime.now() + timedelta(
                    seconds=self._transition_grace_period
                )
                logger.info(
                    f"[GracefulShutdown] Startup complete - transition grace period "
                    f"({self._transition_grace_period}s) active for Chrome redirect"
                )
                await self._check_shutdown_conditions()

    async def notify_connection_change(self, current_count: int):
        """Called when WebSocket connections change."""
        async with self._lock:
            previous_count = self._last_connection_count
            self._last_connection_count = current_count

            # Detect browser disconnection (went from >0 to 0)
            if previous_count > 0 and current_count == 0:
                self._browser_disconnected_at = datetime.now()
                # v198.1: Check if we're in transition grace period
                in_transition = (
                    self._transition_grace_ends_at and
                    datetime.now() < self._transition_grace_ends_at
                )
                if in_transition:
                    remaining = (self._transition_grace_ends_at - datetime.now()).total_seconds()
                    logger.info(
                        f"[GracefulShutdown] Browser disconnected during transition "
                        f"({remaining:.1f}s grace remaining) - ignoring for auto-shutdown"
                    )
                else:
                    logger.info(
                        f"[GracefulShutdown] Browser disconnected "
                        f"(startup_complete={self._startup_complete})"
                    )
            elif current_count > 0:
                # Browser reconnected, reset disconnect timer
                # v198.1: Also clear transition grace if browser connects after complete
                if self._browser_disconnected_at:
                    logger.info("[GracefulShutdown] Browser reconnected - disconnect timer reset")
                self._browser_disconnected_at = None

            await self._check_shutdown_conditions()

    async def request_shutdown(self, reason: str = "supervisor_request") -> dict:
        """
        Request graceful shutdown. Called via HTTP endpoint by supervisor.

        Returns status dict with shutdown timing information.
        """
        async with self._lock:
            if self._shutdown_initiated:
                return {
                    "status": "already_shutting_down",
                    "message": "Shutdown already in progress"
                }

            self._shutdown_requested = True
            current_connections = self._connection_manager.count

            logger.info(
                f"[GracefulShutdown] Shutdown requested (reason={reason}, "
                f"connections={current_connections}, startup_complete={self._startup_complete})"
            )

            # If no active connections or startup never completed, shutdown immediately
            if current_connections == 0 or not self._startup_complete:
                await self._initiate_shutdown(f"immediate_{reason}")
                return {
                    "status": "immediate_shutdown",
                    "message": "No active connections, shutting down immediately",
                    "connections": current_connections
                }

            # Otherwise, wait for browser to disconnect naturally
            return {
                "status": "pending_disconnect",
                "message": f"Waiting for {current_connections} connection(s) to close",
                "connections": current_connections,
                "grace_period_seconds": self._disconnect_grace_period,
                "max_wait_seconds": self._max_idle_after_complete
            }

    async def _check_shutdown_conditions(self):
        """Check if conditions for auto-shutdown are met."""
        if self._shutdown_initiated:
            return

        current_connections = self._connection_manager.count

        # v198.1: Check if we're still in transition grace period
        # During this period, ignore browser disconnects to allow Chrome redirect
        in_transition_grace = False
        if self._transition_grace_ends_at:
            if datetime.now() < self._transition_grace_ends_at:
                in_transition_grace = True
                # Only log once per second to avoid spam
                if not hasattr(self, '_last_grace_log') or \
                   (datetime.now() - self._last_grace_log).total_seconds() > 1.0:
                    remaining = (self._transition_grace_ends_at - datetime.now()).total_seconds()
                    logger.debug(
                        f"[GracefulShutdown] In transition grace period ({remaining:.1f}s remaining)"
                    )
                    self._last_grace_log = datetime.now()

        # Condition 1: Explicit shutdown requested + no connections
        # This condition is ALLOWED during transition grace (supervisor explicitly requested)
        if self._shutdown_requested and current_connections == 0:
            if self._browser_disconnected_at:
                elapsed = (datetime.now() - self._browser_disconnected_at).total_seconds()
                if elapsed >= self._disconnect_grace_period:
                    await self._initiate_shutdown("requested_no_connections")
                    return

        # Condition 2: Startup complete + browser disconnected + grace period passed
        # v198.1: BLOCKED during transition grace period to allow Chrome redirect
        if self._startup_complete and self._browser_disconnected_at and not in_transition_grace:
            elapsed = (datetime.now() - self._browser_disconnected_at).total_seconds()
            if elapsed >= self._auto_shutdown_delay:
                await self._initiate_shutdown("browser_disconnected")
                return

        # Condition 3: Startup complete + max idle time exceeded (safety net)
        # This condition is NOT affected by transition grace (it's a hard timeout)
        if self._startup_complete and self._startup_completed_at:
            elapsed = (datetime.now() - self._startup_completed_at).total_seconds()
            if elapsed >= self._max_idle_after_complete:
                logger.warning(
                    f"[GracefulShutdown] Max idle time exceeded ({elapsed:.1f}s), "
                    f"forcing shutdown (connections={current_connections})"
                )
                await self._initiate_shutdown("max_idle_exceeded")

    async def _initiate_shutdown(self, reason: str):
        """Initiate graceful server shutdown."""
        if self._shutdown_initiated:
            return

        self._shutdown_initiated = True
        logger.info(f"[GracefulShutdown] Initiating shutdown (reason={reason})")

        # Signal the shutdown event
        self._shutdown_event.set()

        # Cleanup runner if available
        if self._app_runner:
            try:
                await self._app_runner.cleanup()
                logger.info("[GracefulShutdown] App runner cleaned up")
            except Exception as e:
                logger.error(f"[GracefulShutdown] Cleanup error: {e}")

    async def _monitor_loop(self):
        """Background loop to check shutdown conditions periodically."""
        check_interval = float(os.getenv('LOADING_SERVER_SHUTDOWN_CHECK_INTERVAL', '0.5'))

        while not self._shutdown_initiated:
            try:
                await asyncio.sleep(check_interval)

                # Update connection count
                current_connections = self._connection_manager.count
                if current_connections != self._last_connection_count:
                    await self.notify_connection_change(current_connections)
                else:
                    # Still check conditions even without connection change
                    async with self._lock:
                        await self._check_shutdown_conditions()

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.debug(f"[GracefulShutdown] Monitor error: {e}")

    async def wait_for_shutdown(self):
        """Block until shutdown is complete."""
        await self._shutdown_event.wait()

    @property
    def is_shutting_down(self) -> bool:
        return self._shutdown_initiated

    @property
    def status(self) -> dict:
        """Get current shutdown manager status."""
        # v198.1: Calculate transition grace remaining
        in_transition = False
        transition_remaining = 0.0
        if self._transition_grace_ends_at:
            remaining = (self._transition_grace_ends_at - datetime.now()).total_seconds()
            if remaining > 0:
                in_transition = True
                transition_remaining = remaining

        return {
            "startup_complete": self._startup_complete,
            "shutdown_requested": self._shutdown_requested,
            "shutdown_initiated": self._shutdown_initiated,
            "active_connections": self._connection_manager.count,
            "browser_disconnected_at": (
                self._browser_disconnected_at.isoformat()
                if self._browser_disconnected_at else None
            ),
            "startup_completed_at": (
                self._startup_completed_at.isoformat()
                if self._startup_completed_at else None
            ),
            "auto_shutdown_delay": self._auto_shutdown_delay,
            "disconnect_grace_period": self._disconnect_grace_period,
            "max_idle_after_complete": self._max_idle_after_complete,
            # v198.1: Transition grace period info
            "transition_grace_period": self._transition_grace_period,
            "in_transition_grace": in_transition,
            "transition_grace_remaining": transition_remaining,
        }


# Global shutdown manager instance
shutdown_manager = GracefulShutdownManager(connection_manager)


# =============================================================================
# v87.0: Global Trinity Ultra Component Instances
# =============================================================================

# Trinity heartbeat monitoring
trinity_heartbeat_reader = TrinityHeartbeatReader()

# Parallel component tracking
parallel_component_tracker = ParallelComponentTracker()

# Lock-free progress updates
lockfree_progress = LockFreeProgressUpdate()

# Adaptive backpressure controller
backpressure_controller = AdaptiveBackpressureController(initial_rate=10.0)

# Progress persistence
progress_persistence = ProgressPersistence()

# Container awareness
container_awareness = ContainerAwareness()

# Event sourcing log
event_sourcing_log = EventSourcingLog()

# Global trace context (updated per request)
current_trace_context = W3CTraceContext()

# Intelligent message generator
intelligent_message_generator = IntelligentMessageGenerator()

# Self-healing restart manager
self_healing_manager = SelfHealingRestartManager(max_restarts=3, restart_window=300.0)

# v87.0: Advanced components
predictive_eta_calculator = PredictiveETACalculator()
cross_repo_health_aggregator = CrossRepoHealthAggregator()
supervisor_heartbeat_monitor = SupervisorHeartbeatMonitor(timeout_threshold=60.0)
file_descriptor_monitor = FileDescriptorMonitor(sample_interval=5.0, leak_threshold=100)
fallback_static_page_generator = FallbackStaticPageGenerator(backend_port=config.backend_port, loading_port=config.loading_port)

# v87.0: Component status background tasks
_trinity_heartbeat_task: Optional[asyncio.Task] = None
_component_tracker_task: Optional[asyncio.Task] = None
_supervisor_monitor_task: Optional[asyncio.Task] = None
_fd_monitor_task: Optional[asyncio.Task] = None


# =============================================================================
# Rate Limiting
# =============================================================================

class RateLimiter:
    """Simple sliding window rate limiter"""

    def __init__(self, requests: int, window_seconds: float):
        self.requests = requests
        self.window_seconds = window_seconds
        self._requests: Dict[str, deque] = {}

    def is_allowed(self, client_id: str) -> bool:
        """Check if request is allowed for client."""
        now = time.time()
        cutoff = now - self.window_seconds

        if client_id not in self._requests:
            self._requests[client_id] = deque()

        # Clean old requests
        while self._requests[client_id] and self._requests[client_id][0] < cutoff:
            self._requests[client_id].popleft()

        # Check limit
        if len(self._requests[client_id]) >= self.requests:
            return False

        # Record request
        self._requests[client_id].append(now)
        return True


rate_limiter = RateLimiter(config.rate_limit_requests, config.rate_limit_window)


# =============================================================================
# Health Check System
# =============================================================================

class HealthChecker:
    """Parallel health checking with connection pooling"""

    def __init__(self):
        self._session: Optional[aiohttp.ClientSession] = None
        self._connector: Optional[aiohttp.TCPConnector] = None

    async def start(self):
        """Initialize connection pool."""
        self._connector = aiohttp.TCPConnector(
            limit=10,
            limit_per_host=5,
            ttl_dns_cache=300,
            enable_cleanup_closed=True
        )
        self._session = aiohttp.ClientSession(
            connector=self._connector,
            timeout=aiohttp.ClientTimeout(total=config.health_check_timeout)
        )

    async def stop(self):
        """Close connection pool."""
        if self._session:
            await self._session.close()
        if self._connector:
            await self._connector.close()

    async def check_backend(self) -> tuple[bool, str]:
        """Check if backend is healthy."""
        if not self._session:
            return False, "Health checker not initialized"

        try:
            async with self._session.get(
                f'http://localhost:{config.backend_port}/health'
            ) as resp:
                if resp.status == 200:
                    progress_state.backend_ready = True
                    progress_state.websocket_ready = True
                    return True, "Backend ready"
                return False, f"Backend returned {resp.status}"
        except asyncio.TimeoutError:
            return False, "Backend timeout"
        except aiohttp.ClientError as e:
            return False, f"Backend connection error: {e}"
        except Exception as e:
            return False, f"Backend check error: {e}"

    async def check_frontend(self) -> tuple[bool, str]:
        """Check if frontend is healthy."""
        if not self._session:
            return False, "Health checker not initialized"

        try:
            async with self._session.get(
                f'http://localhost:{config.frontend_port}'
            ) as resp:
                if resp.status in [200, 304]:
                    progress_state.frontend_ready = True
                    return True, "Frontend ready"
                return False, f"Frontend returned {resp.status}"
        except asyncio.TimeoutError:
            return False, "Frontend timeout"
        except aiohttp.ClientError as e:
            return False, f"Frontend connection error: {e}"
        except Exception as e:
            return False, f"Frontend check error: {e}"

    async def check_all_parallel(self) -> tuple[bool, str]:
        """Check both backend and frontend in parallel."""
        metrics.health_checks_performed += 1

        backend_task = asyncio.create_task(self.check_backend())
        frontend_task = asyncio.create_task(self.check_frontend())

        backend_ok, backend_reason = await backend_task
        frontend_ok, frontend_reason = await frontend_task

        if not backend_ok:
            return False, backend_reason
        if not frontend_ok:
            return False, f"Backend ready but {frontend_reason}"

        return True, "Full system ready"


health_checker = HealthChecker()


# =============================================================================
# Middleware
# =============================================================================

@web.middleware
async def cors_middleware(request: web.Request, handler: Callable) -> web.Response:
    """Add CORS headers and handle preflight requests."""
    if request.method == 'OPTIONS':
        response = web.Response(status=204)
    else:
        try:
            response = await handler(request)
        except web.HTTPException as e:
            response = e

    response.headers['Access-Control-Allow-Origin'] = '*'
    response.headers['Access-Control-Allow-Methods'] = 'GET, POST, OPTIONS'
    response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization, If-None-Match'
    response.headers['Access-Control-Expose-Headers'] = 'ETag, X-Progress'
    response.headers['Access-Control-Max-Age'] = '3600'

    return response


@web.middleware
async def metrics_middleware(request: web.Request, handler: Callable) -> web.Response:
    """Track request metrics and latency."""
    start_time = time.time()
    try:
        response = await handler(request)
        return response
    finally:
        latency_ms = (time.time() - start_time) * 1000
        metrics.record_request(latency_ms)


@web.middleware
async def rate_limit_middleware(request: web.Request, handler: Callable) -> web.Response:
    """Apply rate limiting."""
    client_id = request.remote or 'unknown'

    if not rate_limiter.is_allowed(client_id):
        return web.json_response(
            {"error": "Rate limit exceeded"},
            status=429,
            headers={"Retry-After": str(int(config.rate_limit_window))}
        )

    return await handler(request)


# =============================================================================
# Route Handlers
# =============================================================================

async def serve_loading_page(request: web.Request) -> web.Response:
    """Serve the main loading page using dynamic path resolution."""
    resolved = path_resolver.resolve("loading.html")

    if resolved:
        return web.FileResponse(resolved)

    # Log detailed error for debugging
    logger.error(f"[Static] Loading page not found")
    logger.error(f"[Static] Search paths: {path_resolver.search_paths}")
    return web.Response(
        text=f"Loading page not found. Searched {len(path_resolver.search_paths)} locations.",
        status=404
    )


async def serve_preview_page(request: web.Request) -> web.Response:
    """Serve the preview loading page using dynamic path resolution."""
    resolved = path_resolver.resolve("loading_preview.html")

    if resolved:
        return web.FileResponse(resolved)

    return web.Response(text="Preview page not found.", status=404)


async def serve_loading_manager(request: web.Request) -> web.Response:
    """Serve the loading manager JavaScript using dynamic path resolution."""
    resolved = path_resolver.resolve("loading-manager.js")

    if resolved:
        return web.FileResponse(resolved)

    logger.error(f"[Static] Loading manager not found")
    return web.Response(text="Loading manager not found.", status=404)


async def serve_static_file(request: web.Request) -> web.Response:
    """Serve static files using dynamic path resolution."""
    filename = request.match_info.get('filename', '')

    # Security: prevent path traversal
    if '..' in filename or filename.startswith('/'):
        return web.Response(text="Invalid path", status=400)

    resolved = path_resolver.resolve(filename)

    if resolved:
        return web.FileResponse(resolved)

    return web.Response(text=f"File not found: {filename}", status=404)


async def health_check(request: web.Request) -> web.Response:
    """
    Lightweight health check endpoint.

    This endpoint is called frequently by the supervisor to check if the
    loading server is running. It should return IMMEDIATELY without doing
    any heavy work like checking backend/frontend status.

    For detailed system status, use /api/status instead.
    """
    # Fast response - just confirm the server is running
    uptime = (datetime.now() - metrics.start_time).total_seconds()
    return web.json_response({
        "status": "ok",
        "message": "Loading server running",
        "service": "jarvis_loading_server",
        "version": "5.0.0",
        "uptime_seconds": round(uptime, 2),
        # v5.0: Include flywheel status summary
        "flywheel_active": progress_state.flywheel.active,
        "jarvis_prime_tier": progress_state.jarvis_prime.current_tier,
    })


async def detailed_status(request: web.Request) -> web.Response:
    """
    Detailed status endpoint with full system health check.

    This does the heavy work of checking backend and frontend.
    Use /health for quick liveness checks.
    """
    system_ready, reason = await health_checker.check_all_parallel()

    return web.json_response({
        "status": "ok" if system_ready else "degraded",
        "message": reason,
        "service": "jarvis_loading_server",
        "version": "5.0.0",
        "progress": progress_state.progress,
        "backend_ready": progress_state.backend_ready,
        "frontend_ready": progress_state.frontend_ready,
        "is_ready": progress_state.is_ready,
        "components_ready": progress_state.components_ready,
        "total_components": progress_state.total_components,
        "metrics": metrics.to_dict(),
        # v5.0: Include flywheel and JARVIS-Prime status
        "flywheel": progress_state.flywheel.to_dict(),
        "learning_goals": progress_state.learning_goals.to_dict(),
        "jarvis_prime": progress_state.jarvis_prime.to_dict(),
        "reactor_core": progress_state.reactor_core.to_dict(),
    })


async def get_metrics(request: web.Request) -> web.Response:
    """Get server metrics."""
    return web.json_response(metrics.to_dict())


async def websocket_handler(request: web.Request) -> web.WebSocketResponse:
    """WebSocket handler for real-time progress updates."""
    ws = web.WebSocketResponse(
        heartbeat=config.ws_heartbeat_interval,
        receive_timeout=config.ws_heartbeat_timeout
    )
    await ws.prepare(request)

    # Check connection limit
    if not await connection_manager.add(ws):
        await ws.close(code=WSCloseCode.TRY_AGAIN_LATER, message=b'Server at capacity')
        return ws

    logger.info(f"[WebSocket] Client connected (total: {connection_manager.count})")

    try:
        # Send current progress immediately
        await ws.send_json(progress_state.to_dict())

        # Handle messages
        async for msg in ws:
            if msg.type == web.WSMsgType.TEXT:
                try:
                    data = json.loads(msg.data)
                    if data.get('type') == 'ping':
                        await ws.send_json({'type': 'pong', 'timestamp': datetime.now().isoformat()})
                except json.JSONDecodeError:
                    pass
            elif msg.type == web.WSMsgType.ERROR:
                logger.debug(f'[WebSocket] Error: {ws.exception()}')

    except Exception as e:
        logger.debug(f"[WebSocket] Connection error: {e}")
    finally:
        await connection_manager.remove(ws)
        logger.info(f"[WebSocket] Client disconnected (total: {connection_manager.count})")

    return ws


async def get_progress(request: web.Request) -> web.Response:
    """HTTP endpoint for progress with ETag caching and v87.0 predictive ETA."""
    # Check If-None-Match for caching
    if_none_match = request.headers.get('If-None-Match')
    current_etag = progress_state.etag

    if if_none_match == current_etag:
        return web.Response(status=304)

    # v87.0: Include predictive ETA in response
    response_data = progress_state.to_dict()

    # Add ETA prediction if not complete
    if progress_state.progress < 100.0:
        try:
            eta_seconds, confidence = predictive_eta_calculator.predict_eta(progress_state.progress)
            if eta_seconds is not None:
                response_data['predictive_eta'] = {
                    'eta_seconds': eta_seconds,
                    'confidence': confidence,
                    'estimated_completion': (
                        datetime.now() + timedelta(seconds=eta_seconds)
                    ).isoformat()
                }
        except Exception as e:
            logger.debug(f"[ETA] Prediction failed: {e}")
            # Don't fail the request if ETA prediction fails

    response = web.json_response(
        response_data,
        headers={
            'ETag': current_etag,
            'Cache-Control': 'no-cache',
            'X-Progress': str(int(progress_state.progress))
        }
    )
    return response


async def get_progress_history(request: web.Request) -> web.Response:
    """Get progress history."""
    return web.json_response({
        "history": list(progress_state.history),
        "current": progress_state.to_dict()
    })


async def get_ready_status(request: web.Request) -> web.Response:
    """
    Quick endpoint to check if system is truly ready.
    Use this before announcing 'ready' via voice or UI.
    """
    return web.json_response({
        "is_ready": progress_state.is_ready,
        "progress": progress_state.progress,
        "phase": progress_state.phase,
        "stage": progress_state.stage,
        "message": progress_state.message,
        "components_ready": progress_state.components_ready,
        "total_components": progress_state.total_components,
        "backend_ready": progress_state.backend_ready,
        "frontend_ready": progress_state.frontend_ready
    })


async def update_progress_endpoint(request: web.Request) -> web.Response:
    """HTTP endpoint for receiving progress updates from start_system.py."""
    try:
        data = await request.json()
        stage = data.get('stage', 'unknown')
        message = data.get('message', '')
        progress = float(data.get('progress', 0))
        metadata = data.get('metadata')

        # Update state
        changed = progress_state.update(stage, message, progress, metadata)

        # Track metrics
        metrics.progress_updates_received += 1
        metrics.last_progress_update = datetime.now()

        # Log significant changes
        if changed:
            logger.info(f"[Progress] {progress_state.progress:.0f}% - {stage}: {message}")

        # Notify shutdown manager when startup completes
        # This enables intelligent auto-shutdown when browser disconnects
        if stage == 'complete' or progress >= 100.0:
            await shutdown_manager.notify_startup_complete()

        # Broadcast to WebSocket clients
        await connection_manager.broadcast(progress_state.to_dict())

        return web.json_response({
            "status": "ok",
            "effective_progress": progress_state.progress,
            "changed": changed
        })

    except json.JSONDecodeError:
        metrics.record_error("Invalid JSON in progress update")
        return web.json_response({"status": "error", "message": "Invalid JSON"}, status=400)
    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[Update] Error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def handle_options(request: web.Request) -> web.Response:
    """Handle OPTIONS preflight requests."""
    return web.Response(status=204)


# =============================================================================
# Graceful Shutdown Endpoints - Prevents Window Termination Errors
# =============================================================================

async def graceful_shutdown_endpoint(request: web.Request) -> web.Response:
    """
    Request graceful server shutdown.

    This endpoint allows the supervisor to request a graceful shutdown instead
    of sending SIGTERM. The loading server will:
    1. Wait for any active WebSocket connections to disconnect
    2. Apply a grace period to allow browser redirects to complete
    3. Then shutdown cleanly

    This prevents the "window terminated unexpectedly (reason: 'killed', code: '15')"
    error that occurs when the loading server is killed while Chrome is connected.

    Request body (optional):
        {
            "reason": "supervisor_shutdown"  // Optional reason for logging
        }

    Response:
        {
            "status": "immediate_shutdown" | "pending_disconnect" | "already_shutting_down",
            "message": "...",
            "connections": N,  // Current active connections
            "grace_period_seconds": N,  // How long we'll wait for disconnect
            "max_wait_seconds": N  // Maximum wait time before forced shutdown
        }
    """
    try:
        # Parse optional body
        try:
            data = await request.json()
            reason = data.get('reason', 'http_request')
        except (json.JSONDecodeError, ValueError):
            reason = 'http_request'

        result = await shutdown_manager.request_shutdown(reason)
        return web.json_response(result)

    except Exception as e:
        logger.error(f"[Shutdown] Error handling shutdown request: {e}")
        return web.json_response({
            "status": "error",
            "message": str(e)
        }, status=500)


async def shutdown_status_endpoint(request: web.Request) -> web.Response:
    """
    Get current shutdown manager status.

    Returns detailed information about the shutdown state, including:
    - Whether startup is complete
    - Whether shutdown has been requested/initiated
    - Active connection count
    - Timestamps for key events

    Useful for the supervisor to monitor shutdown progress.
    """
    return web.json_response({
        "service": "jarvis_loading_server",
        "version": "5.0.1",  # Version bump for graceful shutdown feature
        **shutdown_manager.status
    })


async def force_shutdown_endpoint(request: web.Request) -> web.Response:
    """
    Force immediate shutdown (for emergency situations).

    Unlike /api/shutdown/graceful, this immediately initiates shutdown
    without waiting for connections to close. Use only when graceful
    shutdown is taking too long or has failed.

    Request body (optional):
        {
            "reason": "force_requested"
        }
    """
    try:
        try:
            data = await request.json()
            reason = data.get('reason', 'force_requested')
        except (json.JSONDecodeError, ValueError):
            reason = 'force_requested'

        logger.warning(f"[Shutdown] Force shutdown requested: {reason}")

        # Directly initiate shutdown without waiting
        await shutdown_manager._initiate_shutdown(f"force_{reason}")

        return web.json_response({
            "status": "force_shutdown_initiated",
            "message": "Force shutdown initiated immediately",
            "reason": reason
        })

    except Exception as e:
        logger.error(f"[Shutdown] Error handling force shutdown: {e}")
        return web.json_response({
            "status": "error",
            "message": str(e)
        }, status=500)


# =============================================================================
# v4.0: Zero-Touch Update Endpoints
# =============================================================================

async def get_zero_touch_status(request: web.Request) -> web.Response:
    """
    v4.0: Get Zero-Touch autonomous update status.
    
    Returns the current state of any Zero-Touch update in progress,
    including validation status and DMS monitoring.
    """
    return web.json_response({
        "zero_touch": progress_state.zero_touch.to_dict(),
        "dms": progress_state.dms.to_dict(),
        "supervisor_connected": progress_state.supervisor_connected,
        "agi_os_enabled": progress_state.agi_os_enabled,
    })


async def update_zero_touch_status(request: web.Request) -> web.Response:
    """
    v4.0: Update Zero-Touch state from supervisor.
    
    Called by the JARVISSupervisor to broadcast Zero-Touch update progress.
    """
    try:
        data = await request.json()
        event_type = data.get('event', 'unknown')
        
        # Map event to state
        event_state_map = {
            'zero_touch_initiated': 'initiated',
            'zero_touch_staging': 'staging',
            'zero_touch_validating': 'validating',
            'zero_touch_validation_complete': 'applying' if data.get('passed') else 'validation_failed',
            'zero_touch_applying': 'applying',
            'zero_touch_complete': 'dms_monitoring',
            'zero_touch_failed': 'failed',
        }
        
        new_state = event_state_map.get(event_type, progress_state.zero_touch.state)
        
        # Update Zero-Touch state
        progress_state.zero_touch.active = data.get('active', event_type != 'zero_touch_failed')
        progress_state.zero_touch.state = new_state
        progress_state.zero_touch.classification = data.get('classification', progress_state.zero_touch.classification)
        progress_state.zero_touch.message = data.get('message', '')
        progress_state.zero_touch.validation_progress = data.get('validation_progress', progress_state.zero_touch.validation_progress)
        progress_state.zero_touch.files_validated = data.get('files_validated', progress_state.zero_touch.files_validated)
        progress_state.zero_touch.total_files = data.get('total_files', progress_state.zero_touch.total_files)
        progress_state.zero_touch.commits = data.get('commits', progress_state.zero_touch.commits)
        progress_state.zero_touch.files_changed = data.get('files_changed', progress_state.zero_touch.files_changed)
        
        if event_type == 'zero_touch_initiated':
            progress_state.zero_touch.started_at = datetime.now()
        
        if data.get('validation_report'):
            progress_state.zero_touch.validation_report = data['validation_report']
        
        # Broadcast to all WebSocket clients
        broadcast_data = {
            "type": event_type,
            **data,
            "zero_touch": progress_state.zero_touch.to_dict(),
        }
        await connection_manager.broadcast(broadcast_data)
        
        logger.info(f"[Zero-Touch] {event_type}: {progress_state.zero_touch.message}")
        
        return web.json_response({
            "status": "ok",
            "event": event_type,
            "zero_touch": progress_state.zero_touch.to_dict(),
        })
        
    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[Zero-Touch] Update error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def update_dms_status(request: web.Request) -> web.Response:
    """
    v4.0: Update Dead Man's Switch status from supervisor.
    
    Called by the JARVISSupervisor to broadcast DMS heartbeats and state changes.
    """
    try:
        data = await request.json()
        event_type = data.get('event', 'dms_heartbeat')
        
        # Update DMS state
        if event_type == 'dms_probation_start':
            progress_state.dms.active = True
            progress_state.dms.state = 'monitoring'
            progress_state.dms.probation_total = data.get('probation_seconds', 30.0)
            progress_state.dms.probation_remaining = progress_state.dms.probation_total
            progress_state.dms.started_at = datetime.now()
            progress_state.dms.health_score = 1.0
            progress_state.dms.consecutive_failures = 0
            
        elif event_type == 'dms_heartbeat':
            progress_state.dms.health_score = data.get('health_score', progress_state.dms.health_score)
            progress_state.dms.probation_remaining = data.get('remaining_seconds', progress_state.dms.probation_remaining)
            progress_state.dms.consecutive_failures = data.get('consecutive_failures', progress_state.dms.consecutive_failures)
            
        elif event_type == 'dms_probation_passed':
            progress_state.dms.active = False
            progress_state.dms.state = 'passed'
            progress_state.dms.probation_remaining = 0.0
            progress_state.zero_touch.active = False
            progress_state.zero_touch.state = 'complete'
            
        elif event_type == 'dms_rollback_triggered':
            progress_state.dms.state = 'rolling_back'
            
        elif event_type == 'dms_rollback_complete':
            progress_state.dms.active = False
            progress_state.dms.state = 'idle'
            progress_state.zero_touch.active = False
            progress_state.zero_touch.state = 'idle'
        
        # Broadcast to all WebSocket clients
        broadcast_data = {
            "type": event_type,
            **data,
            "dms": progress_state.dms.to_dict(),
        }
        await connection_manager.broadcast(broadcast_data)
        
        if event_type != 'dms_heartbeat':
            logger.info(f"[DMS] {event_type}: {data.get('message', '')}")
        
        return web.json_response({
            "status": "ok",
            "event": event_type,
            "dms": progress_state.dms.to_dict(),
        })
        
    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[DMS] Update error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


# =============================================================================
# v5.0: Two-Tier Agentic Security Endpoints
# =============================================================================

async def get_two_tier_status(request: web.Request) -> web.Response:
    """
    v5.0: Get Two-Tier Agentic Security System status.

    Returns the current state of the Two-Tier Security System:
    - Watchdog status and mode
    - Router initialization and stats
    - VBIA adapter status and thresholds
    - Overall system readiness
    """
    return web.json_response({
        "two_tier": progress_state.two_tier.to_dict(),
        "supervisor_connected": progress_state.supervisor_connected,
        "agi_os_enabled": progress_state.agi_os_enabled,
    })


async def update_two_tier_status(request: web.Request) -> web.Response:
    """
    v5.0: Update Two-Tier Security status from supervisor.

    Called by the JARVISSupervisor to broadcast Two-Tier Security component
    initialization progress and status changes.

    Event types:
    - two_tier_watchdog_init: Watchdog starting initialization
    - two_tier_watchdog_ready: Watchdog initialized and ready
    - two_tier_router_init: Router starting initialization
    - two_tier_router_ready: Router initialized with tier routing
    - two_tier_vbia_init: VBIA adapter starting initialization
    - two_tier_vbia_ready: VBIA adapter ready with thresholds
    - two_tier_ready: All components ready
    - two_tier_error: Component initialization error
    """
    try:
        data = await request.json()
        event_type = data.get('event', 'unknown')

        # Update component states based on event type
        if event_type == 'two_tier_watchdog_init':
            progress_state.two_tier.watchdog_status = 'initializing'

        elif event_type == 'two_tier_watchdog_ready':
            progress_state.two_tier.watchdog_ready = True
            progress_state.two_tier.watchdog_status = data.get('status', 'running')
            progress_state.two_tier.watchdog_mode = data.get('mode', 'idle')

        elif event_type == 'two_tier_router_init':
            progress_state.two_tier.tier1_operational = False
            progress_state.two_tier.tier2_operational = False

        elif event_type == 'two_tier_router_ready':
            progress_state.two_tier.router_ready = True
            progress_state.two_tier.tier1_operational = data.get('tier1_ready', True)
            progress_state.two_tier.tier2_operational = data.get('tier2_ready', True)
            progress_state.two_tier.tier1_routes = data.get('tier1_routes', 0)
            progress_state.two_tier.tier2_routes = data.get('tier2_routes', 0)
            progress_state.two_tier.blocked_routes = data.get('blocked_routes', 0)

        elif event_type == 'two_tier_vbia_init':
            progress_state.two_tier.vbia_anti_spoofing_ready = False

        elif event_type == 'two_tier_vbia_ready':
            progress_state.two_tier.vbia_adapter_ready = True
            progress_state.two_tier.vbia_tier1_threshold = data.get('tier1_threshold', 0.70)
            progress_state.two_tier.vbia_tier2_threshold = data.get('tier2_threshold', 0.85)
            progress_state.two_tier.vbia_liveness_enabled = data.get('liveness_enabled', True)
            progress_state.two_tier.vbia_anti_spoofing_ready = data.get('anti_spoofing_ready', True)

        elif event_type == 'two_tier_ready':
            progress_state.two_tier.watchdog_ready = True
            progress_state.two_tier.router_ready = True
            progress_state.two_tier.vbia_adapter_ready = True
            progress_state.two_tier.tier1_operational = True
            progress_state.two_tier.tier2_operational = True

        elif event_type == 'two_tier_error':
            component = data.get('component', 'unknown')
            if component == 'watchdog':
                progress_state.two_tier.watchdog_status = 'error'
            elif component == 'router':
                progress_state.two_tier.tier1_operational = False
                progress_state.two_tier.tier2_operational = False
            elif component == 'vbia':
                progress_state.two_tier.vbia_anti_spoofing_ready = False

        # Update watchdog stats if provided
        if 'watchdog_mode' in data:
            progress_state.two_tier.watchdog_mode = data['watchdog_mode']

        # Update overall status
        progress_state.two_tier.update_overall_status()

        # Broadcast to all WebSocket clients
        broadcast_data = {
            "type": event_type,
            **data,
            "two_tier": progress_state.two_tier.to_dict(),
        }
        await connection_manager.broadcast(broadcast_data)

        logger.info(f"[Two-Tier] {event_type}: {progress_state.two_tier.message}")

        return web.json_response({
            "status": "ok",
            "event": event_type,
            "two_tier": progress_state.two_tier.to_dict(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[Two-Tier] Update error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


# =============================================================================
# v5.0: Data Flywheel Endpoints
# =============================================================================

async def get_flywheel_status(request: web.Request) -> web.Response:
    """
    v5.0: Get Data Flywheel status.

    Returns the current state of the self-improving learning loop including:
    - Experience collection progress
    - Web scraping status
    - Training progress
    - Model deployment status
    """
    return web.json_response({
        "flywheel": progress_state.flywheel.to_dict(),
        "learning_goals": progress_state.learning_goals.to_dict(),
    })


async def update_flywheel_status(request: web.Request) -> web.Response:
    """
    v5.0: Update Data Flywheel status from supervisor.

    Event types:
    - flywheel_collecting: Collecting experiences
    - flywheel_scraping: Web scraping in progress
    - flywheel_training: Training in progress
    - flywheel_deploying: Deploying new model
    - flywheel_complete: Cycle complete
    """
    try:
        data = await request.json()
        event_type = data.get('event', 'unknown')

        # Update flywheel state based on event
        if event_type == 'flywheel_init':
            progress_state.flywheel.active = True
            progress_state.flywheel.state = 'collecting'
            progress_state.flywheel.message = data.get('message', 'Initializing flywheel')

        elif event_type == 'flywheel_collecting':
            progress_state.flywheel.state = 'collecting'
            progress_state.flywheel.experiences_collected = data.get('experiences', 0)
            progress_state.flywheel.experiences_target = data.get('target', 100)
            progress_state.flywheel.collection_progress = data.get('progress', 0.0)
            progress_state.flywheel.message = data.get('message', 'Collecting experiences')

        elif event_type == 'flywheel_scraping':
            progress_state.flywheel.state = 'scraping'
            progress_state.flywheel.urls_scraped = data.get('urls_scraped', 0)
            progress_state.flywheel.total_urls = data.get('total_urls', 0)
            progress_state.flywheel.scraping_progress = data.get('progress', 0.0)
            progress_state.flywheel.message = data.get('message', 'Web scraping in progress')

        elif event_type == 'flywheel_training':
            progress_state.flywheel.state = 'training'
            progress_state.flywheel.training_active = True
            progress_state.flywheel.training_progress = data.get('progress', 0.0)
            progress_state.flywheel.training_topic = data.get('topic', '')
            progress_state.flywheel.training_epochs = data.get('epochs', 0)
            progress_state.flywheel.training_loss = data.get('loss', 0.0)
            progress_state.flywheel.message = data.get('message', 'Training in progress')

        elif event_type == 'flywheel_deploying':
            progress_state.flywheel.state = 'deploying'
            progress_state.flywheel.training_active = False
            progress_state.flywheel.model_name = data.get('model_name', '')
            progress_state.flywheel.model_version = data.get('model_version', '')
            progress_state.flywheel.message = data.get('message', 'Deploying model')

        elif event_type == 'flywheel_complete':
            progress_state.flywheel.state = 'complete'
            progress_state.flywheel.active = False
            progress_state.flywheel.training_active = False
            progress_state.flywheel.model_deployed = True
            progress_state.flywheel.cycles_completed += 1
            progress_state.flywheel.last_cycle_time = datetime.now()
            progress_state.flywheel.message = data.get('message', 'Flywheel cycle complete')

        # Broadcast to WebSocket clients
        await connection_manager.broadcast({
            "type": event_type,
            **data,
            "flywheel": progress_state.flywheel.to_dict(),
        })

        logger.info(f"[Flywheel] {event_type}: {progress_state.flywheel.message}")

        return web.json_response({
            "status": "ok",
            "event": event_type,
            "flywheel": progress_state.flywheel.to_dict(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[Flywheel] Update error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def get_learning_goals_status(request: web.Request) -> web.Response:
    """v5.0: Get Learning Goals status."""
    return web.json_response({
        "learning_goals": progress_state.learning_goals.to_dict(),
    })


async def update_learning_goals_status(request: web.Request) -> web.Response:
    """
    v5.0: Update Learning Goals status from supervisor.

    Event types:
    - learning_goal_discovered: New goal discovered
    - learning_goal_started: Started working on a goal
    - learning_goal_completed: Completed a goal
    """
    try:
        data = await request.json()
        event_type = data.get('event', 'unknown')

        progress_state.learning_goals.active = True

        if event_type == 'learning_goal_discovered':
            topic = data.get('topic', 'unknown')
            if topic not in progress_state.learning_goals.recent_discoveries:
                progress_state.learning_goals.recent_discoveries.append(topic)
            progress_state.learning_goals.total_goals = data.get('total_goals', progress_state.learning_goals.total_goals + 1)

        elif event_type == 'learning_goal_started':
            progress_state.learning_goals.current_topic = data.get('topic', '')
            progress_state.learning_goals.current_progress = 0.0
            progress_state.learning_goals.in_progress_goals = data.get('in_progress', 1)

        elif event_type == 'learning_goal_progress':
            progress_state.learning_goals.current_progress = data.get('progress', 0.0)

        elif event_type == 'learning_goal_completed':
            progress_state.learning_goals.completed_goals += 1
            progress_state.learning_goals.in_progress_goals = max(0, progress_state.learning_goals.in_progress_goals - 1)
            progress_state.learning_goals.current_topic = ''
            progress_state.learning_goals.current_progress = 0.0

        # Update priority counts if provided
        if 'high_priority' in data:
            progress_state.learning_goals.high_priority_count = data['high_priority']
        if 'medium_priority' in data:
            progress_state.learning_goals.medium_priority_count = data['medium_priority']
        if 'low_priority' in data:
            progress_state.learning_goals.low_priority_count = data['low_priority']

        # Broadcast to WebSocket clients
        await connection_manager.broadcast({
            "type": event_type,
            **data,
            "learning_goals": progress_state.learning_goals.to_dict(),
        })

        return web.json_response({
            "status": "ok",
            "event": event_type,
            "learning_goals": progress_state.learning_goals.to_dict(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[Learning Goals] Update error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def get_jarvis_prime_status(request: web.Request) -> web.Response:
    """v5.0: Get JARVIS-Prime tier-0 brain status."""
    return web.json_response({
        "jarvis_prime": progress_state.jarvis_prime.to_dict(),
    })


async def update_jarvis_prime_status(request: web.Request) -> web.Response:
    """
    v5.0: Update JARVIS-Prime status from supervisor.

    Event types:
    - jarvis_prime_init: Initializing JARVIS-Prime
    - jarvis_prime_local: Using local model
    - jarvis_prime_cloud: Using Cloud Run
    - jarvis_prime_gemini: Using Gemini API fallback
    - jarvis_prime_offline: All backends offline
    - jarvis_prime_health: Health check update
    """
    try:
        data = await request.json()
        event_type = data.get('event', 'unknown')

        progress_state.jarvis_prime.active = True

        if event_type == 'jarvis_prime_init':
            progress_state.jarvis_prime.message = 'Initializing JARVIS-Prime'
            progress_state.jarvis_prime.current_tier = 'initializing'

        elif event_type == 'jarvis_prime_local':
            progress_state.jarvis_prime.current_tier = 'local'
            progress_state.jarvis_prime.local_model_loaded = True
            progress_state.jarvis_prime.local_model_name = data.get('model_name', '')
            progress_state.jarvis_prime.message = data.get('message', 'Using local model')
            progress_state.jarvis_prime.is_healthy = True

        elif event_type == 'jarvis_prime_cloud':
            progress_state.jarvis_prime.current_tier = 'cloud_run'
            progress_state.jarvis_prime.cloud_endpoint = data.get('endpoint', '')
            progress_state.jarvis_prime.message = data.get('message', 'Using Cloud Run')
            progress_state.jarvis_prime.is_healthy = True

        elif event_type == 'jarvis_prime_gemini':
            progress_state.jarvis_prime.current_tier = 'gemini_api'
            progress_state.jarvis_prime.message = data.get('message', 'Using Gemini API fallback')
            progress_state.jarvis_prime.is_healthy = True

        elif event_type == 'jarvis_prime_offline':
            progress_state.jarvis_prime.current_tier = 'offline'
            progress_state.jarvis_prime.is_healthy = False
            progress_state.jarvis_prime.message = data.get('message', 'All backends offline')

        elif event_type == 'jarvis_prime_health':
            progress_state.jarvis_prime.is_healthy = data.get('healthy', False)
            progress_state.jarvis_prime.last_health_check = datetime.now()
            progress_state.jarvis_prime.consecutive_failures = data.get('failures', 0)

        # Update metrics if provided
        if 'memory_usage_gb' in data:
            progress_state.jarvis_prime.memory_usage_gb = data['memory_usage_gb']
        if 'avg_latency_ms' in data:
            progress_state.jarvis_prime.avg_latency_ms = data['avg_latency_ms']
        if 'success_rate' in data:
            progress_state.jarvis_prime.success_rate = data['success_rate']
        if 'total_requests' in data:
            progress_state.jarvis_prime.total_requests = data['total_requests']

        # Broadcast to WebSocket clients
        await connection_manager.broadcast({
            "type": event_type,
            **data,
            "jarvis_prime": progress_state.jarvis_prime.to_dict(),
        })

        logger.info(f"[JARVIS-Prime] {event_type}: {progress_state.jarvis_prime.message}")

        return web.json_response({
            "status": "ok",
            "event": event_type,
            "jarvis_prime": progress_state.jarvis_prime.to_dict(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[JARVIS-Prime] Update error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def get_reactor_core_status(request: web.Request) -> web.Response:
    """v5.0: Get Reactor-Core training pipeline status."""
    return web.json_response({
        "reactor_core": progress_state.reactor_core.to_dict(),
    })


async def update_reactor_core_status(request: web.Request) -> web.Response:
    """
    v5.0: Update Reactor-Core status from supervisor.

    Event types:
    - reactor_core_init: Initializing Reactor-Core
    - reactor_core_watching: File watcher active
    - reactor_core_training: Training in progress
    - reactor_core_exporting: GGUF export in progress
    - reactor_core_uploading: GCS upload in progress
    - reactor_core_complete: Pipeline complete
    """
    try:
        data = await request.json()
        event_type = data.get('event', 'unknown')

        progress_state.reactor_core.active = True

        if event_type == 'reactor_core_init':
            progress_state.reactor_core.state = 'idle'
            progress_state.reactor_core.message = 'Initializing Reactor-Core'

        elif event_type == 'reactor_core_watching':
            progress_state.reactor_core.state = 'watching'
            progress_state.reactor_core.watcher_active = True
            progress_state.reactor_core.watched_directory = data.get('directory', '')
            progress_state.reactor_core.files_detected = data.get('files', 0)
            progress_state.reactor_core.message = data.get('message', 'Watching for model files')

        elif event_type == 'reactor_core_training':
            progress_state.reactor_core.state = 'training'
            progress_state.reactor_core.training_active = True
            progress_state.reactor_core.training_progress = data.get('progress', 0.0)
            progress_state.reactor_core.current_epoch = data.get('epoch', 0)
            progress_state.reactor_core.total_epochs = data.get('total_epochs', 0)
            progress_state.reactor_core.message = data.get('message', 'Training in progress')

        elif event_type == 'reactor_core_exporting':
            progress_state.reactor_core.state = 'exporting'
            progress_state.reactor_core.training_active = False
            progress_state.reactor_core.exporting = True
            progress_state.reactor_core.export_progress = data.get('progress', 0.0)
            progress_state.reactor_core.export_format = data.get('format', 'Q4_K_M')
            progress_state.reactor_core.message = data.get('message', 'Exporting to GGUF')

        elif event_type == 'reactor_core_uploading':
            progress_state.reactor_core.state = 'uploading'
            progress_state.reactor_core.exporting = False
            progress_state.reactor_core.uploading = True
            progress_state.reactor_core.upload_progress = data.get('progress', 0.0)
            progress_state.reactor_core.gcs_bucket = data.get('bucket', '')
            progress_state.reactor_core.message = data.get('message', 'Uploading to GCS')

        elif event_type == 'reactor_core_complete':
            progress_state.reactor_core.state = 'complete'
            progress_state.reactor_core.uploading = False
            progress_state.reactor_core.last_model_path = data.get('model_path', '')
            progress_state.reactor_core.last_completion_time = datetime.now()
            progress_state.reactor_core.message = data.get('message', 'Pipeline complete')

        # Broadcast to WebSocket clients
        await connection_manager.broadcast({
            "type": event_type,
            **data,
            "reactor_core": progress_state.reactor_core.to_dict(),
        })

        logger.info(f"[Reactor-Core] {event_type}: {progress_state.reactor_core.message}")

        return web.json_response({
            "status": "ok",
            "event": event_type,
            "reactor_core": progress_state.reactor_core.to_dict(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[Reactor-Core] Update error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


# =============================================================================
# v9.2: Intelligent Training Orchestrator Endpoints
# =============================================================================

async def get_training_status(request: web.Request) -> web.Response:
    """
    v9.2: Get current training orchestrator status.

    Returns comprehensive status including:
    - Scheduler status (cron, triggers)
    - Current training run (if any)
    - Pipeline stage progress
    - Historical statistics
    """
    try:
        return web.json_response({
            "status": "ok",
            "training": progress_state.training.to_dict(),
        })
    except Exception as e:
        metrics.record_error(str(e))
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def update_training_status(request: web.Request) -> web.Response:
    """
    v9.2: Update training orchestrator status from supervisor.

    Status types:
    - ready: Orchestrator initialized and ready
    - triggered: Training has been triggered
    - running: Training pipeline in progress
    - completed: Training completed successfully
    - failed: Training failed
    """
    try:
        data = await request.json()
        status = data.get('status', 'unknown')

        progress_state.training.active = True
        progress_state.training.status = status

        # Update scheduler info
        if 'next_scheduled_run' in data:
            progress_state.training.next_scheduled_run = data['next_scheduled_run']
        if 'triggers_enabled' in data:
            triggers = data['triggers_enabled']
            progress_state.training.time_based_enabled = triggers.get('time_based', True)
            progress_state.training.data_threshold_enabled = triggers.get('data_threshold', True)
            progress_state.training.quality_trigger_enabled = triggers.get('quality_trigger', True)

        # Update current run info
        if 'run_id' in data:
            progress_state.training.current_run_id = data['run_id']
        if 'stage' in data:
            progress_state.training.current_stage = data['stage']
            # Update stage flags
            stage = data['stage'].lower()
            progress_state.training.stage_scouting = 'scout' in stage
            progress_state.training.stage_ingesting = 'ingest' in stage
            progress_state.training.stage_formatting = 'format' in stage
            progress_state.training.stage_distilling = 'distill' in stage
            progress_state.training.stage_training = 'train' in stage
            progress_state.training.stage_evaluating = 'evaluat' in stage
            progress_state.training.stage_quantizing = 'quantiz' in stage
            progress_state.training.stage_deploying = 'deploy' in stage
        if 'progress' in data:
            progress_state.training.current_progress = data['progress']
        if 'trigger_source' in data:
            progress_state.training.trigger_source = data['trigger_source']
        if 'message' in data:
            progress_state.training.message = data['message']

        # Update results
        if status == 'completed':
            progress_state.training.current_run_id = None
            progress_state.training.current_stage = 'idle'
            progress_state.training.current_progress = 0
            progress_state.training.last_training_run = data.get('timestamp', datetime.now().isoformat())
            progress_state.training.last_result = data.get('result', {})
            progress_state.training.successful_runs += 1
            progress_state.training.total_runs += 1
            # Reset stage flags
            progress_state.training.stage_scouting = False
            progress_state.training.stage_ingesting = False
            progress_state.training.stage_formatting = False
            progress_state.training.stage_distilling = False
            progress_state.training.stage_training = False
            progress_state.training.stage_evaluating = False
            progress_state.training.stage_quantizing = False
            progress_state.training.stage_deploying = False

        elif status == 'failed':
            progress_state.training.current_run_id = None
            progress_state.training.current_stage = 'idle'
            progress_state.training.current_progress = 0
            progress_state.training.last_error = data.get('error', 'Unknown error')
            progress_state.training.failed_runs += 1
            progress_state.training.total_runs += 1

        # Update scheduler stats if provided
        if 'scheduler_stats' in data:
            stats = data['scheduler_stats']
            progress_state.training.total_runs = stats.get('total_runs', progress_state.training.total_runs)
            progress_state.training.successful_runs = stats.get('successful_runs', progress_state.training.successful_runs)
            progress_state.training.failed_runs = stats.get('failed_runs', progress_state.training.failed_runs)

        # Broadcast to WebSocket clients
        await connection_manager.broadcast({
            "type": f"training_{status}",
            **data,
            "training": progress_state.training.to_dict(),
        })

        logger.info(f"[Training] {status}: {progress_state.training.message or 'Status update'}")

        return web.json_response({
            "status": "ok",
            "training": progress_state.training.to_dict(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[Training] Update error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def trigger_training(request: web.Request) -> web.Response:
    """
    v9.2: Manually trigger a training run via API.

    This endpoint forwards the request to the supervisor's trigger_manual_training API.
    """
    try:
        # Note: In production, this would call the supervisor's API
        # For now, just acknowledge the request
        return web.json_response({
            "status": "ok",
            "message": "Manual training trigger requested. Check supervisor logs for status.",
            "note": "Training will start if cooldown has elapsed and no other training is running.",
        })
    except Exception as e:
        metrics.record_error(str(e))
        return web.json_response({"status": "error", "message": str(e)}, status=500)


# ═══════════════════════════════════════════════════════════════════════════════
# v9.3: Learning Goals Discovery Endpoints
# ═══════════════════════════════════════════════════════════════════════════════

@dataclass
class LearningGoalsState:
    """State for learning goals discovery system."""
    active: bool = False
    status: str = "idle"  # idle, discovering, scraping, ready
    total_topics: int = 0
    pending_topics: int = 0
    topics_scraped: int = 0
    last_discovery: Optional[str] = None
    current_activity: Optional[str] = None
    by_source: Optional[Dict[str, int]] = None
    pending_list: Optional[List[Dict[str, Any]]] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "status": self.status,
            "total_topics": self.total_topics,
            "pending_topics": self.pending_topics,
            "topics_scraped": self.topics_scraped,
            "last_discovery": self.last_discovery,
            "current_activity": self.current_activity,
            "by_source": self.by_source or {},
            "pending_list": self.pending_list or [],
        }


# Global learning goals state
learning_goals_state = LearningGoalsState()


async def get_learning_goals_status(request: web.Request) -> web.Response:
    """
    v9.3: Get current learning goals discovery status.

    Returns:
    - Discovery statistics
    - Pending topics list
    - Scraping progress
    - Last discovery time
    """
    try:
        return web.json_response(learning_goals_state.to_dict())
    except Exception as e:
        metrics.record_error(str(e))
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def update_learning_goals_status(request: web.Request) -> web.Response:
    """
    v9.3: Update learning goals status from supervisor.

    Accepts updates with:
    - status: Current state (discovering, scraping, ready)
    - pending_topics: Count of topics pending scrape
    - total_topics: Total discovered topics
    - topics_scraped: Recently scraped count
    - by_source: Discovery counts by source type
    """
    try:
        data = await request.json()

        # Update state from payload
        if "status" in data:
            learning_goals_state.status = data["status"]
            learning_goals_state.active = data["status"] != "idle"

        if "total_topics" in data:
            learning_goals_state.total_topics = data["total_topics"]

        if "pending_topics" in data:
            learning_goals_state.pending_topics = data["pending_topics"]

        if "topics_scraped" in data:
            learning_goals_state.topics_scraped = data["topics_scraped"]

        if "last_discovery" in data:
            learning_goals_state.last_discovery = data["last_discovery"]

        if "message" in data:
            learning_goals_state.current_activity = data["message"]

        if "by_source" in data:
            learning_goals_state.by_source = data["by_source"]

        if "pending_list" in data:
            learning_goals_state.pending_list = data["pending_list"]

        # Log significant updates
        if data.get("new_discoveries", 0) > 0:
            logger.info(
                f"Learning Goals: +{data['new_discoveries']} new topics "
                f"({learning_goals_state.pending_topics} pending)"
            )

        # Broadcast to WebSocket clients
        await broadcast_learning_goals_update()

        return web.json_response({"status": "ok"})

    except Exception as e:
        metrics.record_error(str(e))
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def add_learning_goal(request: web.Request) -> web.Response:
    """
    v9.3: Add a manual learning goal via API.

    Accepts:
    - topic: The topic to learn about
    - priority: Optional priority (0-10, default 8)
    """
    try:
        data = await request.json()
        topic = data.get("topic", "").strip()

        if not topic:
            return web.json_response(
                {"status": "error", "message": "Topic is required"},
                status=400,
            )

        priority = float(data.get("priority", 8.0))

        # Note: In production, this would call the supervisor's add_learning_goal API
        # For now, acknowledge the request
        return web.json_response({
            "status": "ok",
            "message": f"Learning goal '{topic}' added with priority {priority}",
            "note": "Topic will be processed in next discovery cycle.",
        })

    except Exception as e:
        metrics.record_error(str(e))
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def trigger_discovery(request: web.Request) -> web.Response:
    """
    v9.3: Manually trigger a learning goals discovery cycle.
    """
    try:
        return web.json_response({
            "status": "ok",
            "message": "Discovery cycle triggered. Check supervisor logs for progress.",
        })
    except Exception as e:
        metrics.record_error(str(e))
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def broadcast_learning_goals_update() -> None:
    """Broadcast learning goals update to all WebSocket clients."""
    try:
        message = {
            "type": "learning_goals_update",
            "data": learning_goals_state.to_dict(),
            "timestamp": datetime.now().isoformat(),
        }
        await broadcast_message(message)
    except Exception as e:
        logger.debug(f"Learning goals broadcast error: {e}")


# ═══════════════════════════════════════════════════════════════════════════════
# End of Learning Goals Discovery Endpoints
# ═══════════════════════════════════════════════════════════════════════════════


# ═══════════════════════════════════════════════════════════════════════════════
# v9.4: Model Manager Endpoints (Gap 6 Fix)
# ═══════════════════════════════════════════════════════════════════════════════

@dataclass
class ModelManagerState:
    """State for model manager system."""
    active: bool = False
    status: str = "idle"  # idle, initializing, downloading, ready, no_model
    model_name: Optional[str] = None
    model_path: Optional[str] = None
    model_size_mb: float = 0
    model_source: Optional[str] = None  # existing, downloaded, reactor_core
    download_progress: float = 0
    available_memory_gb: float = 0
    reactor_watcher_active: bool = False
    error: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "status": self.status,
            "model_name": self.model_name,
            "model_path": self.model_path,
            "model_size_mb": self.model_size_mb,
            "model_source": self.model_source,
            "download_progress": self.download_progress,
            "available_memory_gb": self.available_memory_gb,
            "reactor_watcher_active": self.reactor_watcher_active,
            "error": self.error,
        }


# Global model manager state
model_manager_state = ModelManagerState()


async def get_model_status(request: web.Request) -> web.Response:
    """
    v9.4: Get current model manager status.

    Returns:
    - Model availability
    - Current model info
    - Download progress (if applicable)
    - Reactor-core watcher status
    """
    try:
        return web.json_response(model_manager_state.to_dict())
    except Exception as e:
        metrics.record_error(str(e))
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def update_model_status(request: web.Request) -> web.Response:
    """
    v9.4: Update model manager status from supervisor.

    Accepts updates with:
    - status: Current state (initializing, downloading, ready, no_model)
    - model_name: Name of current model
    - model_path: Path to model file
    - model_source: Where model came from
    - download_progress: Download percentage (if downloading)
    """
    try:
        data = await request.json()

        # Update state from payload
        if "status" in data:
            model_manager_state.status = data["status"]
            model_manager_state.active = data["status"] not in ["idle", "no_model"]

        if "model_name" in data:
            model_manager_state.model_name = data["model_name"]

        if "model_path" in data:
            model_manager_state.model_path = data["model_path"]

        if "source" in data:
            model_manager_state.model_source = data["source"]

        if "download_progress" in data:
            model_manager_state.download_progress = data["download_progress"]

        if "available_memory_gb" in data:
            model_manager_state.available_memory_gb = data["available_memory_gb"]

        if "reactor_watcher_active" in data:
            model_manager_state.reactor_watcher_active = data["reactor_watcher_active"]

        if "error" in data:
            model_manager_state.error = data["error"]

        # Extract model_info if present
        if "model_info" in data and data["model_info"]:
            info = data["model_info"]
            if info.get("name"):
                model_manager_state.model_name = info["name"]
            if info.get("path"):
                model_manager_state.model_path = info["path"]
            if info.get("size_mb"):
                model_manager_state.model_size_mb = info["size_mb"]
            if info.get("source"):
                model_manager_state.model_source = info["source"]

        # Log significant updates
        if data.get("status") == "ready" and model_manager_state.model_name:
            logger.info(
                f"Model Manager: {model_manager_state.model_name} ready "
                f"({model_manager_state.model_source})"
            )

        # Broadcast to WebSocket clients
        await broadcast_model_update()

        return web.json_response({"status": "ok"})

    except Exception as e:
        metrics.record_error(str(e))
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def trigger_model_download(request: web.Request) -> web.Response:
    """
    v9.4: Trigger model download via API.

    Accepts:
    - model_name: Name of model to download (from catalog)
    """
    try:
        data = await request.json()
        model_name = data.get("model_name", "tinyllama-chat")

        return web.json_response({
            "status": "ok",
            "message": f"Download requested for {model_name}. Check supervisor logs for progress.",
        })

    except Exception as e:
        metrics.record_error(str(e))
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def broadcast_model_update() -> None:
    """Broadcast model manager update to all WebSocket clients."""
    try:
        message = {
            "type": "model_manager_update",
            "data": model_manager_state.to_dict(),
            "timestamp": datetime.now().isoformat(),
        }
        await broadcast_message(message)
    except Exception as e:
        logger.debug(f"Model broadcast error: {e}")


# ═══════════════════════════════════════════════════════════════════════════════
# End of Model Manager Endpoints
# ═══════════════════════════════════════════════════════════════════════════════


# ═══════════════════════════════════════════════════════════════════════════════
# v9.4: Neural Mesh Status Endpoints
# Production multi-agent system with 60+ agents, knowledge graph, and workflows
# ═══════════════════════════════════════════════════════════════════════════════

@dataclass
class NeuralMeshState:
    """State for Neural Mesh production system."""
    active: bool = False
    production_mode: bool = False
    status: str = "inactive"  # inactive, initializing, ready, degraded
    coordinator_status: str = "stopped"  # stopped, starting, running
    bridge_status: str = "disconnected"  # disconnected, connecting, connected
    agents_registered: int = 0
    agents_online: int = 0
    messages_published: int = 0
    messages_delivered: int = 0
    knowledge_entries: int = 0
    knowledge_relationships: int = 0
    workflows_completed: int = 0
    workflows_failed: int = 0
    uptime_seconds: float = 0.0
    health_status: str = "unknown"  # unknown, healthy, degraded, unhealthy
    agent_details: List[Dict[str, Any]] = field(default_factory=list)
    error: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "active": self.active,
            "production_mode": self.production_mode,
            "status": self.status,
            "coordinator": {
                "status": self.coordinator_status,
            },
            "bridge": {
                "status": self.bridge_status,
            },
            "agents": {
                "registered": self.agents_registered,
                "online": self.agents_online,
                "details": self.agent_details[:10],  # Limit to first 10 for response size
            },
            "communication": {
                "messages_published": self.messages_published,
                "messages_delivered": self.messages_delivered,
            },
            "knowledge": {
                "entries": self.knowledge_entries,
                "relationships": self.knowledge_relationships,
            },
            "workflows": {
                "completed": self.workflows_completed,
                "failed": self.workflows_failed,
            },
            "health": {
                "status": self.health_status,
                "uptime_seconds": self.uptime_seconds,
            },
            "error": self.error,
        }


# Global Neural Mesh state
neural_mesh_state = NeuralMeshState()


async def get_neural_mesh_status(request: web.Request) -> web.Response:
    """
    v9.4: Get Neural Mesh production system status.

    Returns comprehensive status including:
    - Coordinator status
    - Bridge status
    - Agent counts and health
    - Communication metrics
    - Knowledge graph stats
    - Workflow execution stats
    """
    try:
        return web.json_response({
            "status": "ok",
            "neural_mesh": neural_mesh_state.to_dict(),
            "timestamp": datetime.now().isoformat(),
        })
    except Exception as e:
        metrics.record_error(str(e))
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def update_neural_mesh_status(request: web.Request) -> web.Response:
    """
    v9.4: Update Neural Mesh status from supervisor.

    Accepts comprehensive status updates including:
    - Coordinator and bridge status
    - Agent metrics
    - Communication stats
    - Knowledge graph metrics
    - Workflow execution counts
    """
    try:
        data = await request.json()

        # Update state from request
        if "active" in data:
            neural_mesh_state.active = data["active"]
        if "production_mode" in data:
            neural_mesh_state.production_mode = data["production_mode"]
        if "status" in data:
            neural_mesh_state.status = data["status"]
        if "coordinator_status" in data:
            neural_mesh_state.coordinator_status = data["coordinator_status"]
        if "bridge_status" in data:
            neural_mesh_state.bridge_status = data["bridge_status"]
        if "agents_registered" in data:
            neural_mesh_state.agents_registered = data["agents_registered"]
        if "agents_online" in data:
            neural_mesh_state.agents_online = data["agents_online"]
        if "messages_published" in data:
            neural_mesh_state.messages_published = data["messages_published"]
        if "messages_delivered" in data:
            neural_mesh_state.messages_delivered = data["messages_delivered"]
        if "knowledge_entries" in data:
            neural_mesh_state.knowledge_entries = data["knowledge_entries"]
        if "knowledge_relationships" in data:
            neural_mesh_state.knowledge_relationships = data["knowledge_relationships"]
        if "workflows_completed" in data:
            neural_mesh_state.workflows_completed = data["workflows_completed"]
        if "workflows_failed" in data:
            neural_mesh_state.workflows_failed = data["workflows_failed"]
        if "uptime_seconds" in data:
            neural_mesh_state.uptime_seconds = data["uptime_seconds"]
        if "health_status" in data:
            neural_mesh_state.health_status = data["health_status"]
        if "agent_details" in data:
            neural_mesh_state.agent_details = data["agent_details"]
        if "error" in data:
            neural_mesh_state.error = data["error"]

        # Log significant state changes
        if data.get("status") == "ready":
            logger.info(
                f"Neural Mesh: Production ready with {neural_mesh_state.agents_registered} agents"
            )

        # Broadcast to WebSocket clients
        await broadcast_neural_mesh_update()

        return web.json_response({"status": "ok"})

    except Exception as e:
        metrics.record_error(str(e))
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def broadcast_neural_mesh_update() -> None:
    """Broadcast Neural Mesh update to all WebSocket clients."""
    try:
        message = {
            "type": "neural_mesh_update",
            "data": neural_mesh_state.to_dict(),
            "timestamp": datetime.now().isoformat(),
        }
        await broadcast_message(message)
    except Exception as e:
        logger.debug(f"Neural Mesh broadcast error: {e}")


# ═══════════════════════════════════════════════════════════════════════════════
# End of Neural Mesh Endpoints
# ═══════════════════════════════════════════════════════════════════════════════


# ═══════════════════════════════════════════════════════════════════════════════
# v6.2: Voice Biometric Authentication System Endpoints
# ═══════════════════════════════════════════════════════════════════════════════

async def get_voice_biometrics_status(request: web.Request) -> web.Response:
    """
    v6.2: Get Voice Biometric Authentication System status.

    Returns comprehensive voice authentication state including:
    - ECAPA-TDNN model status and backend
    - Liveness detection (anti-spoofing) status
    - Speaker cache population
    - Multi-tier authentication thresholds
    - ChromaDB voice pattern recognition
    """
    try:
        return web.json_response({
            "status": "ok",
            "voice_biometrics": progress_state.voice_biometrics.to_dict(),
            "timestamp": datetime.now().isoformat(),
        })
    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[VoiceBio] Get status error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def update_voice_biometrics_status(request: web.Request) -> web.Response:
    """
    v6.2: Update Voice Biometric Authentication status from supervisor.

    Event types:
    - voice_bio_init: Voice system initializing
    - voice_bio_ecapa_loaded: ECAPA-TDNN model loaded
    - voice_bio_liveness_ready: Liveness detection ready
    - voice_bio_cache_updated: Speaker cache updated
    - voice_bio_chromadb_ready: ChromaDB voice patterns ready
    - voice_bio_ready: All components ready
    - voice_bio_error: Component initialization error
    """
    try:
        data = await request.json()
        event_type = data.get('event', 'unknown')

        # Update state based on event type
        if event_type == 'voice_bio_init':
            progress_state.voice_biometrics.active = True
            progress_state.voice_biometrics.status = 'initializing'

        elif event_type == 'voice_bio_ecapa_loaded':
            progress_state.voice_biometrics.ecapa_status = 'loaded'
            progress_state.voice_biometrics.ecapa_backend = data.get('backend', 'onnxruntime')
            progress_state.voice_biometrics.embedding_dimensions = data.get('dimensions', 192)

        elif event_type == 'voice_bio_liveness_ready':
            progress_state.voice_biometrics.liveness_enabled = True
            progress_state.voice_biometrics.anti_spoofing_ready = data.get('anti_spoofing', True)
            progress_state.voice_biometrics.replay_detection_ready = data.get('replay_detection', True)
            progress_state.voice_biometrics.deepfake_detection_ready = data.get('deepfake_detection', True)
            progress_state.voice_biometrics.liveness_accuracy = data.get('accuracy', 99.8)
            progress_state.voice_biometrics.liveness_threshold = data.get('threshold', 80.0)

        elif event_type == 'voice_bio_cache_updated':
            progress_state.voice_biometrics.speaker_cache_status = data.get('cache_status', 'populated')
            progress_state.voice_biometrics.cached_samples = data.get('samples', 0)
            progress_state.voice_biometrics.target_samples = data.get('target', 59)
            if progress_state.voice_biometrics.target_samples > 0:
                progress_state.voice_biometrics.cache_population_percent = (
                    progress_state.voice_biometrics.cached_samples /
                    progress_state.voice_biometrics.target_samples * 100.0
                )

        elif event_type == 'voice_bio_thresholds_set':
            progress_state.voice_biometrics.tier1_threshold = data.get('tier1', 70.0)
            progress_state.voice_biometrics.tier2_threshold = data.get('tier2', 85.0)
            progress_state.voice_biometrics.high_security_threshold = data.get('high_security', 95.0)

        elif event_type == 'voice_bio_chromadb_ready':
            progress_state.voice_biometrics.chromadb_voice_patterns = True
            progress_state.voice_biometrics.behavioral_biometrics_ready = data.get('behavioral_ready', True)

        elif event_type == 'voice_bio_ready':
            progress_state.voice_biometrics.status = 'ready'
            progress_state.voice_biometrics.ecapa_status = 'loaded'
            progress_state.voice_biometrics.liveness_enabled = True
            progress_state.voice_biometrics.anti_spoofing_ready = True

        elif event_type == 'voice_bio_error':
            progress_state.voice_biometrics.status = 'error'
            logger.error(f"[VoiceBio] Error: {data.get('message', 'Unknown error')}")

        # Broadcast to WebSocket clients
        await connection_manager.broadcast({
            "type": event_type,
            **data,
            "voice_biometrics": progress_state.voice_biometrics.to_dict(),
        })

        logger.info(f"[VoiceBio] {event_type}: {data.get('message', '')}")

        return web.json_response({
            "status": "ok",
            "event": event_type,
            "voice_biometrics": progress_state.voice_biometrics.to_dict(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[VoiceBio] Update error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


# ═══════════════════════════════════════════════════════════════════════════════
# v6.2: Intelligent Voice Narrator Endpoints
# ═══════════════════════════════════════════════════════════════════════════════

async def get_trinity_voice_status(request: web.Request) -> web.Response:
    """
    v87.0: Get Trinity Voice Coordinator status and metrics.

    Returns comprehensive voice system state including:
    - Running status
    - Queue size
    - Active TTS engines
    - Success/failure rates
    - Recent announcements
    - Engine health scores
    - Announcement metrics
    """
    try:
        # Import Trinity Voice Coordinator
        try:
            from backend.core.trinity_voice_coordinator import get_voice_coordinator
            coordinator = await get_voice_coordinator()
            status = coordinator.get_status()

            return web.json_response({
                "status": "ok",
                "voice_coordinator": status,
                "timestamp": datetime.now().isoformat(),
            })

        except ImportError:
            return web.json_response({
                "status": "unavailable",
                "message": "Trinity Voice Coordinator not available",
                "timestamp": datetime.now().isoformat(),
            })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[Trinity Voice] Get status error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def test_trinity_voice(request: web.Request) -> web.Response:
    """
    v87.0: Test Trinity Voice Coordinator by sending a test announcement.

    Returns:
    - Success/failure status
    - Coordinator availability
    - TTS engine used
    """
    try:
        # Import Trinity Voice Coordinator
        try:
            from backend.core.trinity_voice_coordinator import (
                announce,
                VoiceContext,
                VoicePriority,
            )

            # Send test announcement
            success = await announce(
                message="Trinity Voice Coordinator test successful.",
                context=VoiceContext.RUNTIME,
                priority=VoicePriority.LOW,
                source="loading_server_test",
                metadata={"test": True}
            )

            return web.json_response({
                "status": "ok",
                "test_result": "success" if success else "skipped",
                "message": "Test announcement queued" if success else "Test skipped (rate limited or duplicate)",
                "timestamp": datetime.now().isoformat(),
            })

        except ImportError:
            return web.json_response({
                "status": "unavailable",
                "message": "Trinity Voice Coordinator not available",
                "timestamp": datetime.now().isoformat(),
            }, status=503)

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[Trinity Voice] Test error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def get_narrator_status(request: web.Request) -> web.Response:
    """
    v6.2: Get Intelligent Voice Narrator status.

    Returns narrator state including:
    - Active/enabled status
    - Voice output status
    - Contextual message capability
    - Last announcement
    - Milestone tracking
    - Claude integration status
    """
    try:
        return web.json_response({
            "status": "ok",
            "narrator": progress_state.narrator.to_dict(),
            "timestamp": datetime.now().isoformat(),
        })
    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[Narrator] Get status error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def update_narrator_status(request: web.Request) -> web.Response:
    """
    v6.2: Update Intelligent Voice Narrator status from supervisor.

    Event types:
    - narrator_init: Narrator initializing
    - narrator_announcement: Narrator made announcement
    - narrator_milestone: Milestone reached and announced
    - narrator_enabled: Voice output enabled
    - narrator_disabled: Voice output disabled
    - narrator_claude_connected: Claude integration active
    - narrator_ready: Narrator ready
    """
    try:
        data = await request.json()
        event_type = data.get('event', 'unknown')

        # Update state based on event type
        if event_type == 'narrator_init':
            progress_state.narrator.active = True
            progress_state.narrator.status = 'initializing'

        elif event_type == 'narrator_announcement':
            progress_state.narrator.last_announcement = data.get('message', '')
            progress_state.narrator.announcement_count += 1

        elif event_type == 'narrator_milestone':
            milestone = data.get('milestone', '')
            if milestone and milestone not in progress_state.narrator.milestones_announced:
                progress_state.narrator.milestones_announced.append(milestone)
            progress_state.narrator.last_announcement = data.get('message', '')
            progress_state.narrator.announcement_count += 1

        elif event_type == 'narrator_enabled':
            progress_state.narrator.enabled = True
            progress_state.narrator.voice_enabled = data.get('voice_enabled', True)

        elif event_type == 'narrator_disabled':
            progress_state.narrator.enabled = False
            progress_state.narrator.voice_enabled = False

        elif event_type == 'narrator_claude_connected':
            progress_state.narrator.claude_integration = True

        elif event_type == 'narrator_langfuse_connected':
            progress_state.narrator.langfuse_tracking = True

        elif event_type == 'narrator_ready':
            progress_state.narrator.status = 'ready'
            progress_state.narrator.enabled = True

        # Broadcast to WebSocket clients
        await connection_manager.broadcast({
            "type": event_type,
            **data,
            "narrator": progress_state.narrator.to_dict(),
        })

        if event_type not in ['narrator_announcement']:  # Don't spam logs with every announcement
            logger.info(f"[Narrator] {event_type}: {data.get('message', '')}")

        return web.json_response({
            "status": "ok",
            "event": event_type,
            "narrator": progress_state.narrator.to_dict(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[Narrator] Update error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


# ═══════════════════════════════════════════════════════════════════════════════
# v6.3: Cost Optimization and Helicone Integration Endpoints
# ═══════════════════════════════════════════════════════════════════════════════

async def get_cost_optimization_status(request: web.Request) -> web.Response:
    """
    v6.3: Get Cost Optimization and Helicone Integration status.

    Returns cost tracking state including:
    - Helicone integration status
    - Total API calls and cached calls
    - Cache hit rate
    - Estimated costs and savings
    - Optimization features (caching, prompt optimization, model routing)
    """
    try:
        return web.json_response({
            "status": "ok",
            "cost_optimization": progress_state.cost_optimization.to_dict(),
            "timestamp": datetime.now().isoformat(),
        })
    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[CostOpt] Get status error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def update_cost_optimization_status(request: web.Request) -> web.Response:
    """
    v6.3: Update Cost Optimization status from supervisor.

    Event types:
    - cost_opt_init: Cost optimization initializing
    - cost_opt_helicone_connected: Helicone integration active
    - cost_opt_stats_updated: Cost/cache stats updated
    - cost_opt_caching_enabled: Intelligent caching enabled
    - cost_opt_prompt_optimization: Prompt optimization active
    - cost_opt_model_routing: Smart model routing active
    - cost_opt_ready: Cost optimization ready
    """
    try:
        data = await request.json()
        event_type = data.get('event', 'unknown')

        # Update state based on event type
        if event_type == 'cost_opt_init':
            progress_state.cost_optimization.active = True
            progress_state.cost_optimization.status = 'initializing'

        elif event_type == 'cost_opt_helicone_connected':
            progress_state.cost_optimization.helicone_enabled = True

        elif event_type == 'cost_opt_stats_updated':
            progress_state.cost_optimization.total_api_calls = data.get('total_calls', 0)
            progress_state.cost_optimization.cached_calls = data.get('cached_calls', 0)

            # Calculate cache hit rate
            if progress_state.cost_optimization.total_api_calls > 0:
                progress_state.cost_optimization.cache_hit_rate = (
                    progress_state.cost_optimization.cached_calls /
                    progress_state.cost_optimization.total_api_calls * 100.0
                )

            progress_state.cost_optimization.estimated_cost_usd = data.get('estimated_cost', 0.0)
            progress_state.cost_optimization.estimated_savings_usd = data.get('estimated_savings', 0.0)

        elif event_type == 'cost_opt_caching_enabled':
            progress_state.cost_optimization.caching_enabled = True

        elif event_type == 'cost_opt_prompt_optimization':
            progress_state.cost_optimization.prompt_optimization = True

        elif event_type == 'cost_opt_model_routing':
            progress_state.cost_optimization.model_routing = True

        elif event_type == 'cost_opt_ready':
            progress_state.cost_optimization.status = 'ready'

        # Broadcast to WebSocket clients
        await connection_manager.broadcast({
            "type": event_type,
            **data,
            "cost_optimization": progress_state.cost_optimization.to_dict(),
        })

        logger.info(f"[CostOpt] {event_type}: {data.get('message', '')}")

        return web.json_response({
            "status": "ok",
            "event": event_type,
            "cost_optimization": progress_state.cost_optimization.to_dict(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[CostOpt] Update error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


# ═══════════════════════════════════════════════════════════════════════════════
# v6.3: Cross-Repository Intelligence Coordination Endpoints
# ═══════════════════════════════════════════════════════════════════════════════

async def get_cross_repo_status(request: web.Request) -> web.Response:
    """
    v6.3: Get Cross-Repository Intelligence Coordination status.

    Returns cross-repo coordination state including:
    - JARVIS Prime (Tier-0 Local Brain) connection and health
    - Reactor Core training pipeline status
    - Neural Mesh multi-agent coordination
    - State synchronization status
    """
    try:
        return web.json_response({
            "status": "ok",
            "cross_repo": progress_state.cross_repo.to_dict(),
            "timestamp": datetime.now().isoformat(),
        })
    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[CrossRepo] Get status error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


async def update_cross_repo_status(request: web.Request) -> web.Response:
    """
    v6.3: Update Cross-Repository Intelligence status from supervisor.

    Event types:
    - cross_repo_init: Cross-repo coordination initializing
    - cross_repo_prime_connected: JARVIS Prime connected
    - cross_repo_prime_health: JARVIS Prime health update
    - cross_repo_reactor_connected: Reactor Core connected
    - cross_repo_training_active: Training pipeline active
    - cross_repo_neural_mesh_active: Neural Mesh coordination active
    - cross_repo_sync_enabled: State synchronization enabled
    - cross_repo_sync_success: State sync successful
    - cross_repo_sync_failed: State sync failed
    - cross_repo_ready: All cross-repo systems ready
    """
    try:
        data = await request.json()
        event_type = data.get('event', 'unknown')

        # Update state based on event type
        if event_type == 'cross_repo_init':
            progress_state.cross_repo.active = True
            progress_state.cross_repo.status = 'initializing'

        elif event_type == 'cross_repo_prime_connected':
            progress_state.cross_repo.jarvis_prime_connected = True
            progress_state.cross_repo.jarvis_prime_port = data.get('port', 8002)
            progress_state.cross_repo.jarvis_prime_health = 'connected'
            progress_state.cross_repo.jarvis_prime_tier = data.get('tier', 'tier-0')

        elif event_type == 'cross_repo_prime_health':
            progress_state.cross_repo.jarvis_prime_health = data.get('health', 'unknown')

        elif event_type == 'cross_repo_reactor_connected':
            progress_state.cross_repo.reactor_core_connected = True
            progress_state.cross_repo.model_sync_enabled = data.get('model_sync', True)

        elif event_type == 'cross_repo_training_active':
            progress_state.cross_repo.training_pipeline_active = True

        elif event_type == 'cross_repo_training_inactive':
            progress_state.cross_repo.training_pipeline_active = False

        elif event_type == 'cross_repo_neural_mesh_active':
            progress_state.cross_repo.neural_mesh_active = True
            progress_state.cross_repo.neural_mesh_coordinator = data.get('coordinator', 'online')
            progress_state.cross_repo.registered_agents = data.get('agents', 0)
            progress_state.cross_repo.active_conversations = data.get('conversations', 0)

        elif event_type == 'cross_repo_sync_enabled':
            progress_state.cross_repo.state_sync_enabled = True

        elif event_type == 'cross_repo_sync_success':
            progress_state.cross_repo.last_sync_timestamp = datetime.now().isoformat()
            progress_state.cross_repo.sync_failures = 0

        elif event_type == 'cross_repo_sync_failed':
            progress_state.cross_repo.sync_failures += 1

        elif event_type == 'cross_repo_ready':
            progress_state.cross_repo.status = 'ready'

        # Broadcast to WebSocket clients
        await connection_manager.broadcast({
            "type": event_type,
            **data,
            "cross_repo": progress_state.cross_repo.to_dict(),
        })

        logger.info(f"[CrossRepo] {event_type}: {data.get('message', '')}")

        return web.json_response({
            "status": "ok",
            "event": event_type,
            "cross_repo": progress_state.cross_repo.to_dict(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[CrossRepo] Update error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


# ═══════════════════════════════════════════════════════════════════════════════
# End of v6.2/v6.3 Enhanced Intelligence Endpoints
# ═══════════════════════════════════════════════════════════════════════════════


# ═══════════════════════════════════════════════════════════════════════════════
# v87.0: Advanced Predictive Analytics and Health Monitoring Endpoints
# ═══════════════════════════════════════════════════════════════════════════════

async def get_predictive_eta(request: web.Request) -> web.Response:
    """
    v87.0: Get ML-based predictive ETA for startup completion.

    Returns:
        {
            "status": "ok",
            "eta_seconds": float | null,
            "confidence": float | null,
            "current_progress": float,
            "estimated_completion_time": str | null,
            "prediction_method": str,
            "timestamp": str
        }
    """
    try:
        current_progress = progress_state.progress

        # Get ETA prediction from ML calculator
        eta_seconds, confidence = predictive_eta_calculator.predict_eta(current_progress)

        # Calculate estimated completion time
        estimated_completion_time = None
        if eta_seconds is not None:
            completion_dt = datetime.now() + timedelta(seconds=eta_seconds)
            estimated_completion_time = completion_dt.isoformat()

        # Determine prediction method
        method = "unknown"
        if eta_seconds is not None:
            if predictive_eta_calculator._progress_rate_ema is not None:
                method = "ema_historical_fusion"
            else:
                method = "historical_average"

        return web.json_response({
            "status": "ok",
            "eta_seconds": eta_seconds,
            "confidence": confidence,
            "current_progress": current_progress,
            "estimated_completion_time": estimated_completion_time,
            "prediction_method": method,
            "timestamp": datetime.now().isoformat(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[PredictiveETA] Error: {e}")
        return web.json_response({
            "status": "error",
            "message": str(e)
        }, status=500)


async def get_unified_health(request: web.Request) -> web.Response:
    """
    v87.0: Get unified health status across JARVIS, JARVIS-Prime, and Reactor-Core.

    Returns comprehensive health aggregation including:
    - Overall health score (0-100)
    - State: healthy | degraded | critical
    - Individual component health
    - Circuit breaker states
    - Recent health events
    """
    try:
        # Get unified health from cross-repo aggregator
        health_data = await cross_repo_health_aggregator.get_unified_health()

        return web.json_response({
            "status": "ok",
            **health_data,
            "timestamp": datetime.now().isoformat(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[UnifiedHealth] Error: {e}")
        return web.json_response({
            "status": "error",
            "message": str(e),
            "health_available": False
        }, status=500)


async def get_startup_analytics(request: web.Request) -> web.Response:
    """
    v87.0: Get comprehensive startup performance analytics.

    Returns:
        - Historical startup times
        - Average, min, max durations
        - Phase breakdown timing
        - Bottleneck analysis
        - Trend analysis
    """
    try:
        # Get historical data from ETA calculator's DB
        analytics = await asyncio.to_thread(
            predictive_eta_calculator._get_startup_analytics
        )

        return web.json_response({
            "status": "ok",
            **analytics,
            "timestamp": datetime.now().isoformat(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[StartupAnalytics] Error: {e}")
        return web.json_response({
            "status": "error",
            "message": str(e)
        }, status=500)


async def get_supervisor_heartbeat_status(request: web.Request) -> web.Response:
    """
    v87.0: Get supervisor heartbeat monitoring status.

    Returns whether supervisor is alive and last update timestamp.
    """
    try:
        is_alive = supervisor_heartbeat_monitor.is_supervisor_alive()
        last_update = supervisor_heartbeat_monitor._last_supervisor_update

        return web.json_response({
            "status": "ok",
            "supervisor_alive": is_alive,
            "last_update_timestamp": last_update,
            "time_since_update": time.monotonic() - last_update,
            "timeout_threshold": supervisor_heartbeat_monitor.timeout_threshold,
            "timestamp": datetime.now().isoformat(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[SupervisorHeartbeat] Error: {e}")
        return web.json_response({
            "status": "error",
            "message": str(e)
        }, status=500)


async def get_startup_config(request: web.Request) -> web.Response:
    """
    v263.0: Dynamic startup configuration endpoint.

    Returns the backend's actual startup timeout and stall detection parameters
    so the frontend can use progress-aware timeout logic instead of a hardcoded limit.

    The frontend should:
    1. Use startup_timeout_ms as the maximum total startup time
    2. Use stall_timeout_ms to detect when progress has stopped advancing
    3. Only show "timed out" when BOTH conditions are met:
       progress has stalled for stall_timeout_ms AND total time exceeds startup_timeout_ms
    """
    try:
        elapsed_since_progress = time.monotonic() - progress_state.last_progress_change_time

        return web.json_response({
            "status": "ok",
            "startup_timeout_ms": progress_state.startup_timeout_ms,
            "stall_timeout_ms": 120000,  # 2 min of zero progress = stalled
            "current_progress": progress_state.progress,
            "seconds_since_progress_change": round(elapsed_since_progress, 1),
            "timestamp": datetime.now().isoformat(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[StartupConfig] Error: {e}")
        return web.json_response({
            "status": "error",
            "message": str(e)
        }, status=500)


async def get_progress_resume(request: web.Request) -> web.Response:
    """
    v87.0: Resume progress endpoint - Get current progress with sequence tracking.

    This endpoint allows clients that disconnected to resume progress tracking
    without missing updates. Returns:
    - Current progress state
    - Sequence number for update tracking
    - Redirect readiness status
    - Supervisor health
    - Predictive ETA

    Query parameters:
    - last_sequence (optional): Client's last received sequence number
      If provided, server can detect how many updates client missed
    """
    try:
        # Get optional last sequence number from query params
        last_sequence_str = request.query.get('last_sequence')
        last_sequence = int(last_sequence_str) if last_sequence_str else None

        # Calculate missed updates if last_sequence provided
        missed_updates = None
        if last_sequence is not None:
            current_seq = progress_state._sequence_number
            missed_updates = max(0, current_seq - last_sequence - 1)

        # Build comprehensive resume response
        response_data = progress_state.to_dict()

        # Add ETA prediction if not complete
        if progress_state.progress < 100.0:
            try:
                eta_seconds, confidence = predictive_eta_calculator.predict_eta(progress_state.progress)
                if eta_seconds is not None:
                    response_data['predictive_eta'] = {
                        'eta_seconds': eta_seconds,
                        'confidence': confidence,
                        'estimated_completion': (
                            datetime.now() + timedelta(seconds=eta_seconds)
                        ).isoformat(),
                        'prediction_method': 'ema_historical_fusion'
                    }
            except Exception as e:
                logger.debug(f"[Resume] ETA prediction failed: {e}")

        # Add supervisor heartbeat status
        try:
            supervisor_alive = supervisor_heartbeat_monitor.is_supervisor_alive()
            response_data['supervisor_alive'] = supervisor_alive
        except Exception as e:
            logger.debug(f"[Resume] Supervisor check failed: {e}")
            response_data['supervisor_alive'] = None

        # Add unified health if requested
        include_health = request.query.get('include_health', '').lower() == 'true'
        if include_health:
            try:
                health_data = await cross_repo_health_aggregator.get_unified_health()
                response_data['unified_health'] = health_data
            except Exception as e:
                logger.debug(f"[Resume] Health aggregation failed: {e}")

        # Add metadata about resume
        response_data['resume_metadata'] = {
            'client_last_sequence': last_sequence,
            'current_sequence': progress_state._sequence_number,
            'missed_updates': missed_updates,
            'timestamp': datetime.now().isoformat(),
        }

        logger.info(f"[Resume] Client resumed - missed {missed_updates or 0} updates")

        return web.json_response({
            "status": "ok",
            **response_data,
        })

    except ValueError as e:
        return web.json_response({
            "status": "error",
            "message": f"Invalid last_sequence parameter: {str(e)}"
        }, status=400)
    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[Resume] Error: {e}")
        return web.json_response({
            "status": "error",
            "message": str(e)
        }, status=500)


async def get_fd_leak_status(request: web.Request) -> web.Response:
    """
    v87.0: Get file descriptor leak detection status.

    Returns:
    - Current FD count
    - Baseline FD count
    - Peak FD count
    - FD growth
    - Growth rate (FDs per minute)
    - Leak detected (boolean)
    - Leak reason (if detected)
    - Leak duration (if active)
    """
    try:
        stats = file_descriptor_monitor.get_statistics()

        return web.json_response({
            "status": "ok",
            **stats,
            "timestamp": datetime.now().isoformat(),
        })

    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[FDLeak] Error: {e}")
        return web.json_response({
            "status": "error",
            "message": str(e),
            "available": False
        }, status=500)


# ═══════════════════════════════════════════════════════════════════════════════
# End of v87.0 Advanced Analytics Endpoints
# ═══════════════════════════════════════════════════════════════════════════════


async def supervisor_event_handler(request: web.Request) -> web.Response:
    """
    v4.0: Unified supervisor event handler.
    
    Handles all supervisor events including:
    - Zero-Touch updates
    - DMS status
    - Maintenance mode
    - System online/offline
    - Prime directive violations
    
    This is the main integration point between JARVISSupervisor and the loading server.
    """
    try:
        data = await request.json()
        event_type = data.get('type', data.get('event', 'unknown'))

        # Mark supervisor as connected
        progress_state.supervisor_connected = True

        # v87.0: Report supervisor heartbeat activity (clock skew safe)
        supervisor_heartbeat_monitor.report_supervisor_activity()

        # Route to appropriate handler based on event type
        if event_type.startswith('zero_touch_'):
            # Zero-Touch events
            progress_state.zero_touch.active = event_type != 'zero_touch_failed'
            if event_type == 'zero_touch_initiated':
                progress_state.zero_touch.state = 'initiated'
                progress_state.zero_touch.started_at = datetime.now()
            elif event_type == 'zero_touch_staging':
                progress_state.zero_touch.state = 'staging'
            elif event_type == 'zero_touch_validating':
                progress_state.zero_touch.state = 'validating'
                progress_state.zero_touch.validation_progress = data.get('progress', 0)
                progress_state.zero_touch.files_validated = data.get('files_validated', 0)
                progress_state.zero_touch.total_files = data.get('total_files', 0)
            elif event_type == 'zero_touch_applying':
                progress_state.zero_touch.state = 'applying'
            elif event_type == 'zero_touch_complete':
                progress_state.zero_touch.state = 'dms_monitoring'
            elif event_type == 'zero_touch_failed':
                progress_state.zero_touch.state = 'failed'
                progress_state.zero_touch.active = False
            
            progress_state.zero_touch.message = data.get('message', '')
            progress_state.zero_touch.classification = data.get('classification', progress_state.zero_touch.classification)
            
        elif event_type.startswith('dms_'):
            # DMS events
            if event_type == 'dms_probation_start':
                progress_state.dms.active = True
                progress_state.dms.state = 'monitoring'
                progress_state.dms.probation_total = data.get('probation_seconds', 30)
                progress_state.dms.probation_remaining = progress_state.dms.probation_total
                progress_state.dms.started_at = datetime.now()
            elif event_type == 'dms_heartbeat':
                progress_state.dms.health_score = data.get('health_score', 1.0)
                progress_state.dms.probation_remaining = data.get('remaining_seconds', 0)
                progress_state.dms.consecutive_failures = data.get('consecutive_failures', 0)
            elif event_type == 'dms_probation_passed':
                progress_state.dms.active = False
                progress_state.dms.state = 'passed'
                progress_state.zero_touch.state = 'complete'
                progress_state.zero_touch.active = False
            elif event_type == 'dms_rollback_triggered':
                progress_state.dms.state = 'rolling_back'
            elif event_type == 'dms_rollback_complete':
                progress_state.dms.active = False
                progress_state.dms.state = 'idle'
                progress_state.zero_touch.active = False
                
        elif event_type == 'system_updating':
            # Enter maintenance mode for update
            progress_state.phase = 'maintenance'
            progress_state.message = data.get('message', 'Updating JARVIS...')
            
        elif event_type == 'system_online':
            # System back online
            progress_state.phase = 'ready'
            progress_state.is_ready = True
            
        elif event_type == 'agi_os_status':
            # AGI OS status update
            progress_state.agi_os_enabled = data.get('enabled', False)
        
        # Broadcast to all WebSocket clients
        await connection_manager.broadcast({
            "type": event_type,
            **data,
            "zero_touch": progress_state.zero_touch.to_dict(),
            "dms": progress_state.dms.to_dict(),
        })
        
        return web.json_response({
            "status": "ok",
            "event": event_type,
            "processed": True,
        })
        
    except Exception as e:
        metrics.record_error(str(e))
        logger.error(f"[Supervisor Event] Error: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)


# =============================================================================
# Watchdog System
# =============================================================================

async def system_health_watchdog():
    """
    WATCHDOG (not authority): Only kicks in if start_system.py fails to send updates.

    This watchdog does NOT compete with start_system.py. It only acts if:
    1. No progress updates received for 60+ seconds AND
    2. System appears to be ready (both backend + frontend responding)
    """
    logger.info("[Watchdog] Started (fallback only - start_system.py is authority)")

    # Wait before starting
    await asyncio.sleep(config.watchdog_startup_delay)

    last_update_time = metrics.last_progress_update or datetime.now()

    while True:
        try:
            # Check if we've received updates recently
            current_update = metrics.last_progress_update
            if current_update and current_update > last_update_time:
                last_update_time = current_update

            # Calculate silence duration
            silence_seconds = (datetime.now() - last_update_time).total_seconds()

            # Only intervene if extended silence and not complete
            if silence_seconds > config.watchdog_silence_threshold and progress_state.progress < 100:
                logger.warning(f"[Watchdog] No updates for {int(silence_seconds)}s, checking system...")

                # Check if system is ready
                system_ready, reason = await health_checker.check_all_parallel()

                if system_ready:
                    logger.info("[Watchdog] System ready but start_system.py silent - triggering completion")
                    progress_state.update(
                        "complete",
                        "JARVIS is online (watchdog recovery)",
                        100,
                        {
                            "success": True,
                            "redirect_url": f"http://localhost:{config.frontend_port}",
                            "backend_ready": True,
                            "frontend_ready": True,
                            "watchdog_triggered": True
                        }
                    )
                    await connection_manager.broadcast(progress_state.to_dict())
                    break
                else:
                    logger.debug(f"[Watchdog] System not ready: {reason}")

        except Exception as e:
            logger.debug(f"[Watchdog] Check failed: {e}")

        await asyncio.sleep(config.health_check_interval)

    logger.info("[Watchdog] Stopped")


# =============================================================================
# Application Lifecycle
# =============================================================================

async def on_startup(app: web.Application):
    """Initialize services on startup."""
    logger.info("Starting loading server services...")

    # =================================================================
    # v211.0: PARENT DEATH WATCHER - Prevent orphaned processes
    # =================================================================
    # When the supervisor (kernel) crashes, this process should exit too.
    # Without this, the loading server becomes an orphan that persists
    # across restarts, causing "Cleaned N orphaned processes" warnings.
    # =================================================================
    try:
        from backend.utils.parent_death_watcher import start_parent_watcher
        parent_watcher = await start_parent_watcher()
        if parent_watcher:
            logger.info("👁️ [v211.0] Parent death watcher started - will auto-exit if supervisor dies")
            app['parent_watcher'] = parent_watcher
        else:
            logger.debug("👁️ [v211.0] Running standalone - no parent watcher needed")
    except ImportError:
        logger.debug("Parent death watcher not available")
    except Exception as e:
        logger.debug(f"Could not start parent death watcher: {e}")

    # Start health checker
    await health_checker.start()

    # Start WebSocket heartbeat
    await connection_manager.start_heartbeat()

    # Start watchdog
    app['watchdog_task'] = asyncio.create_task(system_health_watchdog())

    # Start graceful shutdown monitoring (v5.0.1)
    # This enables intelligent auto-shutdown when browser disconnects after startup complete
    await shutdown_manager.start_monitoring()

    logger.info("All services started")


async def on_shutdown(app: web.Application):
    """Cleanup on shutdown."""
    logger.info("Shutting down loading server...")

    # =================================================================
    # v211.0: PARENT DEATH WATCHER - Stop monitoring during graceful shutdown
    # =================================================================
    # Stop the parent death watcher first to prevent it from interfering
    # with graceful shutdown. If we don't stop it, it might trigger
    # SIGTERM while we're in the middle of cleanup.
    # =================================================================
    if 'parent_watcher' in app and app['parent_watcher']:
        try:
            from backend.utils.parent_death_watcher import stop_parent_watcher
            await stop_parent_watcher()
            logger.info("👁️ [v211.0] Parent death watcher stopped")
        except Exception as e:
            logger.debug(f"Parent death watcher cleanup: {e}")

    # Stop graceful shutdown monitoring (v5.0.1)
    await shutdown_manager.stop_monitoring()

    # Stop watchdog
    if 'watchdog_task' in app:
        app['watchdog_task'].cancel()
        try:
            await app['watchdog_task']
        except asyncio.CancelledError:
            pass

    # Stop heartbeat
    await connection_manager.stop_heartbeat()

    # Close all WebSocket connections
    await connection_manager.close_all()

    # Stop health checker
    await health_checker.stop()

    logger.info("Shutdown complete")


def create_app() -> web.Application:
    """Create and configure the application."""
    app = web.Application(
        middlewares=[
            cors_middleware,
            metrics_middleware,
            rate_limit_middleware
        ]
    )

    # Routes
    app.router.add_get('/', serve_loading_page)
    app.router.add_get('/loading.html', serve_loading_page)
    app.router.add_get('/preview', serve_preview_page)
    app.router.add_get('/loading-manager.js', serve_loading_manager)

    # Static files (favicon, images, etc.) - catch-all for static assets
    app.router.add_get('/{filename:.*\\.(svg|png|jpg|jpeg|ico|css|woff|woff2|ttf)}', serve_static_file)

    # Health and metrics
    app.router.add_get('/health', health_check)         # Fast liveness check
    app.router.add_get('/health/ping', health_check)    # Alias for liveness
    app.router.add_get('/api/status', detailed_status)  # Detailed system status
    app.router.add_get('/metrics', get_metrics)

    # Progress endpoints
    app.router.add_get('/ws/startup-progress', websocket_handler)
    app.router.add_get('/api/startup-progress', get_progress)
    app.router.add_get('/api/startup-progress/ready', get_ready_status)
    app.router.add_get('/api/progress-history', get_progress_history)
    app.router.add_post('/api/update-progress', update_progress_endpoint)
    
    # v4.0: Zero-Touch and supervisor endpoints
    app.router.add_get('/api/zero-touch/status', get_zero_touch_status)
    app.router.add_post('/api/zero-touch/update', update_zero_touch_status)
    app.router.add_post('/api/dms/update', update_dms_status)
    app.router.add_post('/api/supervisor/event', supervisor_event_handler)

    # v5.0: Two-Tier Agentic Security endpoints
    app.router.add_get('/api/two-tier/status', get_two_tier_status)
    app.router.add_post('/api/two-tier/update', update_two_tier_status)

    # v5.0: Data Flywheel endpoints
    app.router.add_get('/api/flywheel/status', get_flywheel_status)
    app.router.add_post('/api/flywheel/update', update_flywheel_status)

    # v5.0: Learning Goals endpoints
    app.router.add_get('/api/learning-goals/status', get_learning_goals_status)
    app.router.add_post('/api/learning-goals/update', update_learning_goals_status)

    # v5.0: JARVIS-Prime endpoints
    app.router.add_get('/api/jarvis-prime/status', get_jarvis_prime_status)
    app.router.add_post('/api/jarvis-prime/update', update_jarvis_prime_status)

    # v5.0: Reactor-Core endpoints
    app.router.add_get('/api/reactor-core/status', get_reactor_core_status)
    app.router.add_post('/api/reactor-core/update', update_reactor_core_status)

    # v9.2: Training Orchestrator endpoints
    app.router.add_get('/api/training/status', get_training_status)
    app.router.add_post('/api/training/update', update_training_status)
    app.router.add_post('/api/training/trigger', trigger_training)

    # v9.3: Learning Goals Discovery endpoints
    app.router.add_get('/api/learning-goals/status', get_learning_goals_status)
    app.router.add_post('/api/learning-goals/update', update_learning_goals_status)
    app.router.add_post('/api/learning-goals/add', add_learning_goal)
    app.router.add_post('/api/learning-goals/trigger', trigger_discovery)

    # v9.4: Model Manager endpoints
    app.router.add_get('/api/model/status', get_model_status)
    app.router.add_post('/api/model/update', update_model_status)
    app.router.add_post('/api/model/download', trigger_model_download)

    # v9.4: Neural Mesh endpoints
    app.router.add_get('/api/neural-mesh/status', get_neural_mesh_status)
    app.router.add_post('/api/neural-mesh/update', update_neural_mesh_status)

    # v6.2: Voice Biometric Authentication System endpoints
    app.router.add_get('/api/voice-biometrics/status', get_voice_biometrics_status)
    app.router.add_post('/api/voice-biometrics/update', update_voice_biometrics_status)

    # v87.0: Trinity Voice Coordinator endpoints
    app.router.add_get('/api/trinity-voice/status', get_trinity_voice_status)
    app.router.add_post('/api/trinity-voice/test', test_trinity_voice)

    # v6.2: Intelligent Voice Narrator endpoints
    app.router.add_get('/api/narrator/status', get_narrator_status)
    app.router.add_post('/api/narrator/update', update_narrator_status)

    # v6.3: Cost Optimization and Helicone Integration endpoints
    app.router.add_get('/api/cost-optimization/status', get_cost_optimization_status)
    app.router.add_post('/api/cost-optimization/update', update_cost_optimization_status)

    # v6.3: Cross-Repository Intelligence Coordination endpoints
    app.router.add_get('/api/cross-repo/status', get_cross_repo_status)
    app.router.add_post('/api/cross-repo/update', update_cross_repo_status)

    # v87.0: Advanced Predictive Analytics and Health Monitoring endpoints
    app.router.add_get('/api/eta/predict', get_predictive_eta)
    app.router.add_get('/api/health/unified', get_unified_health)
    app.router.add_get('/api/analytics/startup-performance', get_startup_analytics)
    app.router.add_get('/api/supervisor/heartbeat', get_supervisor_heartbeat_status)
    app.router.add_get('/api/startup-config', get_startup_config)
    app.router.add_get('/api/progress/resume', get_progress_resume)
    app.router.add_get('/api/diagnostics/fd-leak', get_fd_leak_status)

    # v5.0.1: Graceful Shutdown endpoints (fixes window termination errors)
    app.router.add_post('/api/shutdown/graceful', graceful_shutdown_endpoint)
    app.router.add_get('/api/shutdown/status', shutdown_status_endpoint)
    app.router.add_post('/api/shutdown/force', force_shutdown_endpoint)

    # CORS preflight
    app.router.add_route('OPTIONS', '/{path:.*}', handle_options)

    # Lifecycle handlers
    app.on_startup.append(on_startup)
    app.on_shutdown.append(on_shutdown)

    # Store references for external access
    app['progress_state'] = progress_state
    app['connection_manager'] = connection_manager
    app['metrics'] = metrics
    app['config'] = config
    app['shutdown_manager'] = shutdown_manager  # v5.0.1: Graceful shutdown

    return app


async def start_server(host: str = '0.0.0.0', port: Optional[int] = None):
    """Start the standalone loading server with v87.0 Trinity Ultra enhancements."""
    global _trinity_heartbeat_task, _component_tracker_task

    port = port or config.loading_port

    # CRITICAL: Initialize asyncio objects within the running event loop
    # This fixes the "attached to a different loop" error that occurs when
    # global objects are created at module import time
    shutdown_manager.initialize_async_objects()
    connection_manager.initialize_async_objects()

    # v87.0: Initialize CrossRepoHealthAggregator with Trinity integration
    logger.info("[v87.0] Initializing cross-repo health aggregator...")
    try:
        await cross_repo_health_aggregator.initialize_trinity_integration()
        logger.info("[v87.0] ✅ Cross-repo health aggregator initialized")
    except Exception as e:
        logger.warning(f"[v87.0] Cross-repo health unavailable: {e}")

    # v87.0: Start Trinity heartbeat monitoring
    logger.info("[v87.0] Starting Trinity heartbeat monitor...")
    _trinity_heartbeat_task = asyncio.create_task(
        trinity_heartbeat_monitor_loop(),
        name="trinity_heartbeat_monitor"
    )

    # v87.0: Start FD leak detection monitor
    logger.info("[v87.0] Starting FD leak detection monitor...")
    _fd_monitor_task = asyncio.create_task(
        fd_monitor_loop(),
        name="fd_leak_monitor"
    )

    # v87.0: Generate fallback static page
    logger.info("[v87.0] Generating fallback static page...")
    try:
        fallback_path = await fallback_static_page_generator.save_to_file()
        if fallback_path:
            logger.info(f"[v87.0] ✅ Fallback page ready: {fallback_path}")
        else:
            logger.warning("[v87.0] ⚠️  Fallback page generation failed")
    except Exception as e:
        logger.warning(f"[v87.0] ⚠️  Fallback page error: {e}")

    # v87.0: Start self-healing watchdog
    logger.info("[v87.0] Starting self-healing watchdog...")
    await self_healing_manager.start_watchdog()

    # v87.0: Log container awareness
    if container_awareness.is_containerized():
        memory_limit = container_awareness.get_memory_limit()
        timeout_mult = container_awareness.get_timeout_multiplier()
        logger.info(
            f"[v87.0] Container detected: "
            f"memory={memory_limit / (1024**3):.1f}GB, "
            f"timeout_multiplier={timeout_mult}x"
        )

    app = create_app()

    runner = web.AppRunner(app)
    await runner.setup()

    # v5.0.1: Register runner with shutdown manager for graceful cleanup
    shutdown_manager.set_app_runner(runner)

    site = web.TCPSite(runner, host, port)
    await site.start()

    # Initialize path resolver and verify paths
    path_resolver.initialize()
    resolved_loading = path_resolver.resolve("loading.html")
    resolved_manager = path_resolver.resolve("loading-manager.js")

    logger.info(f"{'='*70}")
    logger.info(f" JARVIS Loading Server v87.0 - Trinity Ultra Edition")
    logger.info(f"{'='*70}")
    logger.info(f" Server:      http://{host}:{port}")
    logger.info(f" WebSocket:   ws://{host}:{port}/ws/startup-progress")
    logger.info(f" HTTP API:    http://{host}:{port}/api/startup-progress")
    logger.info(f"{'='*70}")
    logger.info(f" v87.0 Trinity Ultra Features:")
    logger.info(f"   ✅ Trinity Heartbeat Reader    - Direct component monitoring")
    logger.info(f"   ✅ Parallel Component Tracker  - J-Prime + Reactor tracking")
    logger.info(f"   ✅ W3C Distributed Tracing     - Cross-repo correlation")
    logger.info(f"   ✅ Lock-Free Progress Updates  - Atomic CAS operations")
    logger.info(f"   ✅ Adaptive Backpressure       - AIMD rate limiting")
    logger.info(f"   ✅ Progress Persistence        - SQLite resume capability")
    logger.info(f"   ✅ Container Awareness         - cgroup detection")
    logger.info(f"   ✅ Event Sourcing Log          - JSONL replay capability")
    logger.info(f"   ✅ Intelligent Messages        - Context-aware generation")
    logger.info(f"   ✅ Self-Healing Restart        - Auto-recovery on crash")
    logger.info(f"   ✅ Predictive ETA Calculator   - ML-based time prediction")
    logger.info(f"   ✅ Cross-Repo Health Monitor   - Unified JARVIS/J-Prime/Reactor")
    logger.info(f"   ✅ Supervisor Heartbeat Watch  - Crash detection (monotonic)")
    logger.info(f"   ✅ Startup Analytics Engine    - Performance trend analysis")
    logger.info(f"   ✅ FD Leak Detection Monitor   - File descriptor leak detection")
    logger.info(f"{'='*70}")
    logger.info(f" Path Resolution:")
    logger.info(f"   loading.html:      {'✓' if resolved_loading else '✗'} {resolved_loading or 'NOT FOUND'}")
    logger.info(f"   loading-manager.js: {'✓' if resolved_manager else '✗'} {resolved_manager or 'NOT FOUND'}")
    logger.info(f"   Search paths:      {len(path_resolver.search_paths)}")
    logger.info(f"{'='*70}")
    logger.info(f" v5.0.1 Graceful Shutdown (fixes window termination):")
    logger.info(f"   POST /api/shutdown/graceful  (request graceful shutdown)")
    logger.info(f"   GET  /api/shutdown/status    (check shutdown state)")
    logger.info(f"   POST /api/shutdown/force     (force immediate shutdown)")
    logger.info(f"{'='*70}")
    logger.info(f" v5.0 Flywheel Endpoints:")
    logger.info(f"   GET  /api/flywheel/status")
    logger.info(f"   POST /api/flywheel/update")
    logger.info(f"   GET  /api/learning-goals/status")
    logger.info(f"   GET  /api/jarvis-prime/status")
    logger.info(f"   GET  /api/reactor-core/status")
    logger.info(f"{'='*70}")
    logger.info(f" v4.0 Zero-Touch Endpoints:")
    logger.info(f"   GET  /api/zero-touch/status")
    logger.info(f"   POST /api/supervisor/event")
    logger.info(f"{'='*70}")
    logger.info(f" v87.0 Advanced Analytics Endpoints:")
    logger.info(f"   GET  /api/eta/predict                    (ML-based ETA prediction)")
    logger.info(f"   GET  /api/health/unified                 (Cross-repo health)")
    logger.info(f"   GET  /api/analytics/startup-performance  (Historical trends)")
    logger.info(f"   GET  /api/supervisor/heartbeat           (Supervisor alive check)")
    logger.info(f"   GET  /api/progress/resume                (Resume with sequence tracking)")
    logger.info(f"   GET  /api/diagnostics/fd-leak            (FD leak detection)")
    logger.info(f"   GET  /api/startup-progress               (Enhanced with ETA)")
    logger.info(f"{'='*70}")
    logger.info(f" CORS:        Enabled for all origins")
    logger.info(f" Rate Limit:  {config.rate_limit_requests} req/{config.rate_limit_window}s")
    logger.info(f" Max WS:      {config.ws_max_connections} connections")
    logger.info(f" Mode:        RELAY (start_system.py + supervisor as authority)")
    logger.info(f" Trace ID:    {current_trace_context.trace_id}")
    logger.info(f"{'='*70}")

    return runner


async def shutdown_server(runner: web.AppRunner):
    """Gracefully shutdown the server."""
    await runner.cleanup()
    logger.info("Server stopped")


async def main():
    """
    Main entry point with intelligent graceful shutdown.

    The server can be shutdown in three ways:
    1. KeyboardInterrupt (Ctrl+C) - traditional signal-based shutdown
    2. Graceful shutdown via HTTP API (/api/shutdown/graceful)
    3. Auto-shutdown when startup completes and browser disconnects

    The graceful shutdown mechanism prevents the "window terminated unexpectedly"
    error by waiting for browsers to naturally disconnect before shutting down.
    """
    runner = await start_server()

    # Create a task that completes on either:
    # 1. Graceful shutdown request (via API or auto-detect)
    # 2. KeyboardInterrupt (Ctrl+C)
    keyboard_event = asyncio.Event()

    def signal_handler():
        keyboard_event.set()

    # Set up signal handlers for graceful handling
    loop = asyncio.get_running_loop()
    try:
        import signal
        for sig in (signal.SIGTERM, signal.SIGINT):
            loop.add_signal_handler(sig, signal_handler)
    except (NotImplementedError, ValueError):
        # Signal handlers not available (Windows or not main thread)
        pass

    try:
        # Wait for either shutdown event or keyboard interrupt
        done, pending = await asyncio.wait(
            [
                asyncio.create_task(shutdown_manager.wait_for_shutdown()),
                asyncio.create_task(keyboard_event.wait()),
            ],
            return_when=asyncio.FIRST_COMPLETED
        )

        # Cancel pending tasks
        for task in pending:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass

        if keyboard_event.is_set():
            logger.info("Received keyboard interrupt signal...")
        else:
            logger.info("Graceful shutdown completed...")

    except KeyboardInterrupt:
        logger.info("Received shutdown signal...")
    finally:
        # Only cleanup runner if shutdown_manager hasn't already done it
        if not shutdown_manager.is_shutting_down:
            await shutdown_server(runner)


# =============================================================================
# StartupProgressReporter - For start_system.py to import and use
# =============================================================================

class StartupProgressReporter:
    """
    Robust progress reporter for use by start_system.py
    
    Features:
    - Async and fire-and-forget modes
    - Automatic retries with backoff
    - Never blocks main startup
    - Connection pooling
    
    Usage in start_system.py:
        from loading_server import StartupProgressReporter, start_loading_server_background
        
        await start_loading_server_background()
        reporter = StartupProgressReporter()
        await reporter.report("init", "Initializing JARVIS...", 5)
        await reporter.complete("JARVIS is online!")
    """
    
    def __init__(self, host: str = None, port: int = None):
        self.host = host or os.getenv('LOADING_SERVER_HOST', 'localhost')
        self.port = port or int(os.getenv('LOADING_SERVER_PORT', '3001'))
        self.timeout = float(os.getenv('PROGRESS_REQUEST_TIMEOUT', '2.0'))
        self.retry_count = int(os.getenv('PROGRESS_RETRY_COUNT', '3'))
        self._session: Optional[aiohttp.ClientSession] = None
        self._last_progress = 0.0
        self._enabled = True
    
    @property
    def update_url(self) -> str:
        return f"http://{self.host}:{self.port}/api/update-progress"
    
    async def _get_session(self) -> Optional[aiohttp.ClientSession]:
        if self._session is None or self._session.closed:
            try:
                self._session = aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=self.timeout)
                )
            except Exception as e:
                logger.debug(f"Failed to create session: {e}")
                self._enabled = False
        return self._session
    
    async def report(
        self,
        stage: str,
        message: str,
        progress: float,
        metadata: Optional[Dict[str, Any]] = None,
        fire_and_forget: bool = True,
        log_entry: Optional[str] = None,
        log_source: Optional[str] = None,
        log_type: str = "info"
    ) -> bool:
        """
        Report progress to loading server with optional operation log.
        
        Args:
            stage: Current stage name (e.g., "init", "backend", "models")
            message: Human-readable status message
            progress: Percentage complete (0-100)
            metadata: Optional additional data
            fire_and_forget: If True, don't wait for response
            log_entry: Optional single log entry to add to operations log
            log_source: Source of the log entry (supervisor, backend, system)
            log_type: Type of log (info, success, error, warning)
        """
        if not self._enabled:
            return False
        
        # Monotonic enforcement
        if progress > self._last_progress:
            self._last_progress = progress
        else:
            progress = self._last_progress
        
        # Build metadata with log entry if provided
        meta = metadata.copy() if metadata else {}
        if log_entry:
            meta["log_entry"] = log_entry
            meta["log_source"] = log_source or "system"
            meta["log_type"] = log_type
        
        payload = {
            "stage": stage,
            "message": message,
            "progress": progress,
            "metadata": meta
        }
        
        if fire_and_forget:
            asyncio.create_task(self._send_with_retry(payload))
            return True
        return await self._send_with_retry(payload)
    
    async def _send_with_retry(self, payload: Dict[str, Any]) -> bool:
        session = await self._get_session()
        if not session:
            return False
        
        for attempt in range(self.retry_count):
            try:
                async with session.post(self.update_url, json=payload) as resp:
                    if resp.status in (200, 201):
                        return True
            except asyncio.TimeoutError:
                pass
            except Exception as e:
                logger.debug(f"Progress update failed: {e}")
            
            if attempt < self.retry_count - 1:
                await asyncio.sleep(0.5 * (attempt + 1))
        return False
    
    async def complete(
        self,
        message: str = "JARVIS is online!",
        redirect_url: str = None,
        success: bool = True
    ) -> bool:
        """Report startup complete"""
        frontend_url = os.getenv('FRONTEND_URL', 'http://localhost:3000')
        metadata = {
            "success": success,
            "redirect_url": redirect_url or frontend_url,
            "backend_ready": True,
            "frontend_ready": True
        }
        return await self.report("complete", message, 100.0, metadata, fire_and_forget=False)
    
    async def fail(self, message: str, error: str = None) -> bool:
        """Report startup failure"""
        return await self.report(
            "failed", message, self._last_progress,
            {"success": False, "error": error or message},
            fire_and_forget=False
        )
    
    async def log(
        self,
        source: str,
        message: str,
        log_type: str = "info"
    ) -> bool:
        """
        Add an entry to the live operations log without changing progress.
        
        Args:
            source: Source of the operation (supervisor, backend, system, etc.)
            message: Log message to display
            log_type: Type of log (info, success, error, warning)
            
        Example:
            await reporter.log("supervisor", "Checking for existing instances...", "info")
            await reporter.log("backend", "FastAPI application started", "success")
        """
        if not self._enabled:
            return False
        
        # Send a progress update at current level with just the log entry
        payload = {
            "stage": "_log",  # Special stage that doesn't change visual progress
            "message": "",
            "progress": self._last_progress,
            "metadata": {
                "log_entry": message,
                "log_source": source,
                "log_type": log_type,
                "is_log_only": True
            }
        }
        
        # Fire and forget for logs
        asyncio.create_task(self._send_with_retry(payload))
        return True
    
    async def log_batch(
        self,
        operations: list
    ) -> bool:
        """
        Add multiple entries to the operations log at once.
        
        Args:
            operations: List of dicts with {source, message, type}
            
        Example:
            await reporter.log_batch([
                {"source": "supervisor", "message": "Started cleanup", "type": "info"},
                {"source": "supervisor", "message": "Killed 2 processes", "type": "success"}
            ])
        """
        if not self._enabled or not operations:
            return False
        
        payload = {
            "stage": "_log_batch",
            "message": "",
            "progress": self._last_progress,
            "metadata": {
                "operations": operations,
                "is_log_only": True
            }
        }
        
        asyncio.create_task(self._send_with_retry(payload))
        return True
    
    async def close(self):
        if self._session and not self._session.closed:
            await self._session.close()
    
    # =========================================================================
    # v4.0: Zero-Touch Update Reporting
    # =========================================================================
    
    async def zero_touch_event(
        self,
        event_type: str,
        message: str = "",
        classification: Optional[str] = None,
        validation_progress: float = 0.0,
        files_validated: int = 0,
        total_files: int = 0,
        commits: int = 0,
        files_changed: int = 0,
        validation_report: Optional[Dict] = None,
        **kwargs
    ) -> bool:
        """
        v4.0: Report Zero-Touch autonomous update event.
        
        Args:
            event_type: One of: zero_touch_initiated, zero_touch_staging, 
                       zero_touch_validating, zero_touch_applying, 
                       zero_touch_complete, zero_touch_failed
            message: Human-readable status message
            classification: Update type (security/critical/minor/major/patch)
            validation_progress: Validation progress (0-100)
            files_validated: Number of files validated
            total_files: Total files to validate
            commits: Number of commits in update
            files_changed: Number of files changed
            validation_report: Validation results dict
        """
        if not self._enabled:
            return False
        
        payload = {
            "event": event_type,
            "message": message,
            "classification": classification,
            "validation_progress": validation_progress,
            "files_validated": files_validated,
            "total_files": total_files,
            "commits": commits,
            "files_changed": files_changed,
            "validation_report": validation_report,
            "active": event_type != 'zero_touch_failed',
            **kwargs
        }
        
        try:
            session = await self._get_session()
            if session:
                url = f"http://{self.host}:{self.port}/api/zero-touch/update"
                async with session.post(url, json=payload) as resp:
                    return resp.status == 200
        except Exception as e:
            logger.debug(f"Zero-Touch event failed: {e}")
        return False
    
    async def dms_event(
        self,
        event_type: str,
        health_score: float = 1.0,
        remaining_seconds: float = 0.0,
        probation_seconds: float = 30.0,
        consecutive_failures: int = 0,
        message: str = "",
        **kwargs
    ) -> bool:
        """
        v4.0: Report Dead Man's Switch event.
        
        Args:
            event_type: One of: dms_probation_start, dms_heartbeat, 
                       dms_probation_passed, dms_rollback_triggered, 
                       dms_rollback_complete
            health_score: Current health score (0.0-1.0)
            remaining_seconds: Remaining probation time
            probation_seconds: Total probation period
            consecutive_failures: Number of consecutive health check failures
            message: Human-readable status message
        """
        if not self._enabled:
            return False
        
        payload = {
            "event": event_type,
            "health_score": health_score,
            "remaining_seconds": remaining_seconds,
            "probation_seconds": probation_seconds,
            "consecutive_failures": consecutive_failures,
            "message": message,
            **kwargs
        }
        
        try:
            session = await self._get_session()
            if session:
                url = f"http://{self.host}:{self.port}/api/dms/update"
                async with session.post(url, json=payload) as resp:
                    return resp.status == 200
        except Exception as e:
            logger.debug(f"DMS event failed: {e}")
        return False
    
    async def supervisor_event(
        self,
        event_type: str,
        **kwargs
    ) -> bool:
        """
        v4.0: Report generic supervisor event.

        Args:
            event_type: Event type identifier
            **kwargs: Additional event data
        """
        if not self._enabled:
            return False

        payload = {
            "type": event_type,
            **kwargs
        }

        try:
            session = await self._get_session()
            if session:
                url = f"http://{self.host}:{self.port}/api/supervisor/event"
                async with session.post(url, json=payload) as resp:
                    return resp.status == 200
        except Exception as e:
            logger.debug(f"Supervisor event failed: {e}")
        return False

    # =========================================================================
    # v5.0: Data Flywheel Reporting
    # =========================================================================

    async def flywheel_event(
        self,
        event_type: str,
        message: str = "",
        experiences: int = 0,
        target: int = 100,
        progress: float = 0.0,
        topic: str = "",
        **kwargs
    ) -> bool:
        """
        v5.0: Report Data Flywheel event.

        Args:
            event_type: One of: flywheel_init, flywheel_collecting, flywheel_scraping,
                       flywheel_training, flywheel_deploying, flywheel_complete
            message: Human-readable status message
            experiences: Number of experiences collected
            target: Target number of experiences
            progress: Progress percentage (0-100)
            topic: Current training topic
        """
        if not self._enabled:
            return False

        payload = {
            "event": event_type,
            "message": message,
            "experiences": experiences,
            "target": target,
            "progress": progress,
            "topic": topic,
            **kwargs
        }

        try:
            session = await self._get_session()
            if session:
                url = f"http://{self.host}:{self.port}/api/flywheel/update"
                async with session.post(url, json=payload) as resp:
                    return resp.status == 200
        except Exception as e:
            logger.debug(f"Flywheel event failed: {e}")
        return False

    async def learning_goal_event(
        self,
        event_type: str,
        topic: str = "",
        progress: float = 0.0,
        total_goals: int = 0,
        **kwargs
    ) -> bool:
        """
        v5.0: Report Learning Goal event.

        Args:
            event_type: One of: learning_goal_discovered, learning_goal_started,
                       learning_goal_progress, learning_goal_completed
            topic: Learning goal topic
            progress: Progress percentage (0-100)
            total_goals: Total number of goals
        """
        if not self._enabled:
            return False

        payload = {
            "event": event_type,
            "topic": topic,
            "progress": progress,
            "total_goals": total_goals,
            **kwargs
        }

        try:
            session = await self._get_session()
            if session:
                url = f"http://{self.host}:{self.port}/api/learning-goals/update"
                async with session.post(url, json=payload) as resp:
                    return resp.status == 200
        except Exception as e:
            logger.debug(f"Learning goal event failed: {e}")
        return False

    async def jarvis_prime_event(
        self,
        event_type: str,
        message: str = "",
        model_name: str = "",
        endpoint: str = "",
        memory_usage_gb: float = 0.0,
        **kwargs
    ) -> bool:
        """
        v5.0: Report JARVIS-Prime event.

        Args:
            event_type: One of: jarvis_prime_init, jarvis_prime_local,
                       jarvis_prime_cloud, jarvis_prime_gemini,
                       jarvis_prime_offline, jarvis_prime_health
            message: Human-readable status message
            model_name: Local model name (for local tier)
            endpoint: Cloud endpoint URL (for cloud tier)
            memory_usage_gb: Current memory usage
        """
        if not self._enabled:
            return False

        payload = {
            "event": event_type,
            "message": message,
            "model_name": model_name,
            "endpoint": endpoint,
            "memory_usage_gb": memory_usage_gb,
            **kwargs
        }

        try:
            session = await self._get_session()
            if session:
                url = f"http://{self.host}:{self.port}/api/jarvis-prime/update"
                async with session.post(url, json=payload) as resp:
                    return resp.status == 200
        except Exception as e:
            logger.debug(f"JARVIS-Prime event failed: {e}")
        return False

    async def reactor_core_event(
        self,
        event_type: str,
        message: str = "",
        progress: float = 0.0,
        epoch: int = 0,
        total_epochs: int = 0,
        model_path: str = "",
        **kwargs
    ) -> bool:
        """
        v5.0: Report Reactor-Core event.

        Args:
            event_type: One of: reactor_core_init, reactor_core_watching,
                       reactor_core_training, reactor_core_exporting,
                       reactor_core_uploading, reactor_core_complete
            message: Human-readable status message
            progress: Progress percentage (0-100)
            epoch: Current training epoch
            total_epochs: Total training epochs
            model_path: Path to completed model
        """
        if not self._enabled:
            return False

        payload = {
            "event": event_type,
            "message": message,
            "progress": progress,
            "epoch": epoch,
            "total_epochs": total_epochs,
            "model_path": model_path,
            **kwargs
        }

        try:
            session = await self._get_session()
            if session:
                url = f"http://{self.host}:{self.port}/api/reactor-core/update"
                async with session.post(url, json=payload) as resp:
                    return resp.status == 200
        except Exception as e:
            logger.debug(f"Reactor-Core event failed: {e}")
        return False


# Global reporter instance
_reporter: Optional[StartupProgressReporter] = None


def get_progress_reporter() -> StartupProgressReporter:
    """Get or create global reporter instance"""
    global _reporter
    if _reporter is None:
        _reporter = StartupProgressReporter()
    return _reporter


async def report_progress(stage: str, message: str, progress: float, metadata: Dict = None) -> bool:
    """Convenience function for quick progress reports"""
    return await get_progress_reporter().report(stage, message, progress, metadata)


async def report_complete(message: str = "JARVIS is online!", redirect_url: str = None) -> bool:
    """Convenience function for completion"""
    return await get_progress_reporter().complete(message, redirect_url)


async def report_failure(message: str, error: str = None) -> bool:
    """Convenience function for failure"""
    return await get_progress_reporter().fail(message, error)


# =============================================================================
# Loading Server Launcher - Start server in background for start_system.py
# =============================================================================

# =============================================================================
# Loading Server Process Management - Robust Background Startup
# =============================================================================
_loading_server_process = None
_loading_server_log_file = None
_loading_server_start_attempts = 0
_MAX_START_ATTEMPTS = 3


async def _check_server_health(host: str, port: int, timeout: float = 1.0) -> bool:
    """Check if loading server is responding to health checks."""
    try:
        connector = aiohttp.TCPConnector(force_close=True)
        async with aiohttp.ClientSession(
            connector=connector,
            timeout=aiohttp.ClientTimeout(total=timeout, connect=timeout / 2)
        ) as session:
            async with session.get(f"http://{host}:{port}/health") as resp:
                return resp.status == 200
    except Exception:
        return False


async def _check_process_alive() -> bool:
    """Check if the loading server subprocess is still running."""
    global _loading_server_process
    if _loading_server_process is None:
        return False
    poll_result = _loading_server_process.poll()
    return poll_result is None  # None means still running


async def _read_process_errors() -> str:
    """Read any error output from the loading server process."""
    global _loading_server_log_file
    if _loading_server_log_file is None:
        return ""
    try:
        log_path = Path(_loading_server_log_file)
        if log_path.exists():
            content = log_path.read_text()
            # Return last 2000 chars if very long
            return content[-2000:] if len(content) > 2000 else content
    except Exception as e:
        return f"Error reading log: {e}"
    return ""


async def _kill_existing_on_port(port: int) -> bool:
    """Kill any existing process on the port."""
    import subprocess
    try:
        # Find process using the port
        result = subprocess.run(
            ["lsof", "-ti", f":{port}"],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0 and result.stdout.strip():
            pids = result.stdout.strip().split('\n')
            for pid in pids:
                try:
                    subprocess.run(["kill", "-9", pid], timeout=2)
                    logger.info(f"Killed existing process on port {port}: PID {pid}")
                except Exception:
                    pass
            await asyncio.sleep(0.5)  # Give time for port to be released
            return True
    except Exception as e:
        logger.debug(f"No existing process to kill on port {port}: {e}")
    return False


async def start_loading_server_background() -> bool:
    """
    Start loading server as a background subprocess with robust error handling.

    Features:
    - Captures stderr/stdout to log file for debugging
    - Checks process health and restarts if crashed
    - Retries with exponential backoff
    - Kills existing processes on the port if needed
    - Returns detailed error info on failure

    Returns True if server is running (either started or already running).
    """
    global _loading_server_process, _loading_server_log_file, _loading_server_start_attempts

    import subprocess
    import tempfile

    host = os.getenv('LOADING_SERVER_HOST', 'localhost')
    port = int(os.getenv('LOADING_SERVER_PORT', '3001'))

    # Check if already running and healthy
    if await _check_server_health(host, port):
        logger.info(f"✓ Loading server already running on port {port}")
        _loading_server_start_attempts = 0  # Reset attempts
        return True

    # If we have a process handle but it's not responding, check if it died
    if _loading_server_process is not None:
        if not await _check_process_alive():
            # Process died - read any error output
            error_output = await _read_process_errors()
            logger.warning(f"Loading server process died. Exit code: {_loading_server_process.poll()}")
            if error_output:
                logger.error(f"Loading server error output:\n{error_output}")
            _loading_server_process = None
        else:
            # Process is running but not responding - might be starting up
            logger.info("Loading server process exists, waiting for it to respond...")
            for i in range(5):
                await asyncio.sleep(0.5)
                if await _check_server_health(host, port):
                    logger.info(f"✓ Loading server now responding on port {port}")
                    return True
            # Still not responding - kill and restart
            logger.warning("Loading server not responding, killing and restarting...")
            await stop_loading_server_background()

    # Check if something else is using the port
    await _kill_existing_on_port(port)

    # Increment attempt counter
    _loading_server_start_attempts += 1
    if _loading_server_start_attempts > _MAX_START_ATTEMPTS:
        logger.error(f"Loading server failed to start after {_MAX_START_ATTEMPTS} attempts")
        return False

    # Start the server with proper output capture
    try:
        logger.info(f"Starting loading server on port {port} (attempt {_loading_server_start_attempts}/{_MAX_START_ATTEMPTS})...")
        script_path = Path(__file__).resolve()

        # Create a log file for capturing output
        jarvis_home = os.environ.get("JARVIS_HOME", str(Path.home() / ".jarvis"))
        log_dir = Path(jarvis_home) / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        _loading_server_log_file = str(log_dir / "loading_server.log")

        # Open log file for writing
        log_file = open(_loading_server_log_file, 'w')

        # Set environment to ensure proper error reporting
        env = os.environ.copy()
        env['PYTHONUNBUFFERED'] = '1'  # Ensure output is not buffered

        _loading_server_process = subprocess.Popen(
            [sys.executable, str(script_path)],
            stdout=log_file,
            stderr=subprocess.STDOUT,  # Combine stderr with stdout
            start_new_session=True,
            env=env
        )

        # Wait for server to be ready with progressive backoff
        max_wait_time = 10.0  # Maximum 10 seconds
        check_interval = 0.2
        elapsed = 0.0

        while elapsed < max_wait_time:
            await asyncio.sleep(check_interval)
            elapsed += check_interval

            # Check if process is still alive
            if not await _check_process_alive():
                exit_code = _loading_server_process.poll()
                error_output = await _read_process_errors()
                logger.error(f"Loading server process crashed! Exit code: {exit_code}")
                if error_output:
                    logger.error(f"Captured output:\n{error_output}")
                _loading_server_process = None

                # Retry if we have attempts left
                if _loading_server_start_attempts < _MAX_START_ATTEMPTS:
                    await asyncio.sleep(1.0)  # Wait before retry
                    return await start_loading_server_background()
                return False

            # Check if responding to health checks
            if await _check_server_health(host, port):
                logger.info(f"✓ Loading server started successfully on port {port} (took {elapsed:.1f}s)")
                _loading_server_start_attempts = 0  # Reset attempts on success
                return True

            # Progressive backoff
            check_interval = min(check_interval * 1.5, 1.0)

        # Server started but not responding to health checks
        logger.warning(f"Loading server process running but not responding to health checks after {max_wait_time}s")
        error_output = await _read_process_errors()
        if error_output:
            logger.warning(f"Server output so far:\n{error_output}")

        # Give it a bit more time - it might be a slow startup
        return True  # Return true since process is running

    except Exception as e:
        logger.error(f"Failed to start loading server: {e}", exc_info=True)
        return False


async def stop_loading_server_background():
    """Stop the loading server subprocess gracefully."""
    global _loading_server_process, _loading_server_log_file, _loading_server_start_attempts

    if _loading_server_process is not None:
        logger.info("Stopping loading server...")

        # Try graceful termination first
        try:
            _loading_server_process.terminate()

            # Wait up to 5 seconds for graceful shutdown
            for _ in range(50):
                if _loading_server_process.poll() is not None:
                    break
                await asyncio.sleep(0.1)

            # Force kill if still running
            if _loading_server_process.poll() is None:
                logger.warning("Loading server didn't stop gracefully, force killing...")
                _loading_server_process.kill()
                await asyncio.sleep(0.2)

        except ProcessLookupError:
            # Process already dead
            pass
        except Exception as e:
            logger.warning(f"Error stopping loading server: {e}")
            try:
                _loading_server_process.kill()
            except Exception:
                pass

        _loading_server_process = None
        logger.info("Loading server stopped")

    # Reset state
    _loading_server_start_attempts = 0


async def get_loading_server_status() -> dict:
    """Get detailed status of the loading server."""
    global _loading_server_process, _loading_server_log_file, _loading_server_start_attempts

    host = os.getenv('LOADING_SERVER_HOST', 'localhost')
    port = int(os.getenv('LOADING_SERVER_PORT', '3001'))

    status = {
        "host": host,
        "port": port,
        "process_running": await _check_process_alive(),
        "health_check_passed": await _check_server_health(host, port),
        "start_attempts": _loading_server_start_attempts,
        "log_file": _loading_server_log_file,
    }

    if _loading_server_process is not None:
        status["pid"] = _loading_server_process.pid
        status["exit_code"] = _loading_server_process.poll()

    return status


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        sys.exit(0)

