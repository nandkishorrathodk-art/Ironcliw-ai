<div align="center">

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

# ‚ö° IRONCLIW-AI ¬∑ JARVIS
### *Just A Rather Very Intelligent System*

**The world's most advanced personal AI agent ‚Äî now fully on Windows.**

[![Python](https://img.shields.io/badge/Python-3.12%2B-blue?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![Windows](https://img.shields.io/badge/Windows-10%2F11-0078D4?style=for-the-badge&logo=windows&logoColor=white)](https://github.com/nandkishorrathodk-art/Ironcliw-ai)
[![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![React](https://img.shields.io/badge/React_18-61DAFB?style=for-the-badge&logo=react&logoColor=black)](https://reactjs.org)
[![Claude AI](https://img.shields.io/badge/Claude_AI-FF6B00?style=for-the-badge&logo=anthropic&logoColor=white)](https://anthropic.com)
[![License: MIT](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![Phase](https://img.shields.io/badge/Port_Phase-12_Complete-success?style=for-the-badge)](WINDOWS_PORT_BLUEPRINT.md)
[![Stars](https://img.shields.io/github/stars/nandkishorrathodk-art/Ironcliw-ai?style=for-the-badge&color=gold)](https://github.com/nandkishorrathodk-art/Ironcliw-ai/stargazers)
[![Code Size](https://img.shields.io/github/languages/code-size/nandkishorrathodk-art/Ironcliw-ai?style=for-the-badge&color=purple)](https://github.com/nandkishorrathodk-art/Ironcliw-ai)

<br/>

> *"Sometimes you gotta run before you can walk."* ‚Äî **Tony Stark**

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

</div>

---

<details>
<summary><h2>üìë Table of Contents</h2></summary>

- [What Is This?](#-what-is-this)
- [System Architecture](#-system-architecture)
- [Data Flow](#-data-flow)
- [Voice Pipeline](#-voice-pipeline)
- [Vision Pipeline](#-vision-pipeline)
- [Ghost Hands Automation](#-ghost-hands--autonomous-automation)
- [Intelligence Core](#-intelligence-core)
- [Platform Support](#%EF%B8%8F-platform-support)
- [Features](#-features)
- [Quick Start](#-quick-start-windows)
- [Configuration Reference](#-configuration-reference)
- [Project Structure](#-project-structure)
- [API Reference](#-api-reference)
- [Voice Commands](#%EF%B8%8F-voice-commands)
- [Startup Flow](#-startup-flow)
- [ML Model Pipeline](#-ml-model-pipeline)
- [Security Architecture](#-security-architecture)
- [Performance](#-performance)
- [Windows Port Status](#-windows-port-status)
- [Dependencies](#-dependencies)
- [Contributing](#-contributing)
- [Credits & Attribution](#-credits--attribution)
- [License](#-license)

</details>

---

## ü§ñ What Is This?

**Ironcliw-AI** is a Windows port of the [drussell23/JARVIS](https://github.com/drussell23/JARVIS) personal AI agent ‚Äî a **self-hosted, voice-activated autonomous assistant** inspired by Iron Man's J.A.R.V.I.S.

It is not just a chatbot. It is a **full autonomous AI operating system** that:

| Capability | Description |
|:-----------|:------------|
| üß† **Thinks** | Multi-LLM reasoning (Claude 3.5 Sonnet + Fireworks Llama 70B) |
| üé§ **Listens** | Wake word "Hey JARVIS" + Whisper STT with 12-model circuit breaker |
| üó£Ô∏è **Speaks** | Microsoft Neural TTS (`en-GB-RyanNeural`) ‚Äî sounds human |
| üëÅÔ∏è **Sees** | Real-time screen capture (30 FPS) + Claude Vision understanding |
| ü§ñ **Acts** | Ghost Hands: autonomous browser, keyboard, mouse control |
| üîê **Verifies** | ECAPA-TDNN voice biometric speaker verification (159ms) |
| üìö **Remembers** | Long-term memory via SQLite + ChromaDB semantic cache |
| ‚òÅÔ∏è **Scales** | Auto-offloads to GCP Spot VMs when local RAM > 80% |
| üõ°Ô∏è **Self-Heals** | Circuit breakers, ML-powered recovery, auto-reload |

### How It Works (30-Second Version)

```
You say "Hey JARVIS, open Chrome and search for AI news"
  ‚Üì
Whisper STT converts speech ‚Üí text
  ‚Üì
ECAPA-TDNN verifies it's YOUR voice (159ms)
  ‚Üì
Claude 3.5 Sonnet understands your intent
  ‚Üì
Ghost Hands launches Chrome via pyautogui
  ‚Üì
Vision system confirms Chrome is open (mss capture)
  ‚Üì
JARVIS types "AI news" and presses Enter
  ‚Üì
Neural TTS says "Done sir, here are the latest AI news results"
```

---

## üèóÔ∏è System Architecture

### High-Level Overview

```mermaid
graph TB
    subgraph User["üë§ User Layer"]
        VOICE["üé§ Voice Input<br/>Hey JARVIS"]
        SCREEN["üñ•Ô∏è Screen<br/>Desktop Activity"]
        BROWSER["üåê Browser<br/>http://localhost:3000"]
    end

    subgraph Frontend["‚öõÔ∏è Frontend ¬∑ React 18 ¬∑ Port 3000"]
        UI["JarvisVoice.js<br/>Voice UI Component"]
        WS_CLIENT["WebSocket Client<br/>socket.io"]
        CHAT["Chat Interface<br/>Message Display"]
    end

    subgraph Backend["üêç FastAPI Backend ¬∑ Port 8010"]
        direction TB
        
        subgraph VoiceSystem["üé§ Voice System"]
            STT["Hybrid STT Router<br/>Whisper + Cloud"]
            TTS["Neural TTS<br/>en-GB-RyanNeural"]
            WAKE["Wake Word Detector<br/>Hey JARVIS"]
            BIOMETRIC["Voice Biometrics<br/>ECAPA-TDNN"]
        end

        subgraph VisionSystem["üëÅÔ∏è Vision System"]
            CAPTURE["Screen Capture<br/>mss ¬∑ 30 FPS"]
            CLAUDE_VIS["Claude Vision API<br/>Image Understanding"]
            CONTEXT["Context Intelligence<br/>App Tracking"]
        end

        subgraph GhostHands["ü§ñ Ghost Hands"]
            ACTUATOR["Background Actuator<br/>pyautogui"]
            BROWSER_CTL["Browser Controller<br/>Selenium/CDP"]
            APP_LAUNCH["App Launcher<br/>Cross-Platform"]
        end

        subgraph IntelCore["üß† Intelligence Core"]
            LLM["Multi-LLM Router<br/>Claude ¬∑ Fireworks"]
            MEMORY["Memory System<br/>SQLite + ChromaDB"]
            SAI["SAI Engine<br/>Situational Awareness"]
            LEARNING["Learning Database<br/>Adaptive Behavior"]
        end

        subgraph Platform["üñ•Ô∏è Platform Adapter"]
            PAL["Platform Abstraction<br/>get_platform()"]
            WIN["WindowsPlatform<br/>pywin32 ¬∑ pyautogui"]
            MAC["MacOSPlatform<br/>osascript ¬∑ Swift"]
        end
    end

    subgraph Cloud["‚òÅÔ∏è GCP Cloud ¬∑ Optional"]
        SPOT_VM["Spot VM<br/>e2-highmem-4<br/>$0.029/hr"]
        CLOUD_SQL["Cloud SQL<br/>Voice Profiles"]
        CLOUD_RUN["Cloud Run<br/>ECAPA Endpoint"]
    end

    VOICE --> STT
    SCREEN --> CAPTURE
    BROWSER --> UI

    UI <--> WS_CLIENT
    WS_CLIENT <-->|WebSocket| Backend

    STT --> LLM
    BIOMETRIC --> LLM
    CAPTURE --> CLAUDE_VIS
    CLAUDE_VIS --> LLM

    LLM --> TTS
    LLM --> GhostHands
    LLM --> SAI

    SAI --> MEMORY
    LEARNING --> MEMORY

    PAL --> WIN
    PAL --> MAC

    Backend -.->|RAM > 80%| SPOT_VM
    BIOMETRIC -.->|Cloud Fallback| CLOUD_RUN
    MEMORY -.->|Sync| CLOUD_SQL

    style User fill:#1a1a2e,stroke:#e94560,color:#fff
    style Frontend fill:#16213e,stroke:#0f3460,color:#fff
    style Backend fill:#0f3460,stroke:#533483,color:#fff
    style Cloud fill:#533483,stroke:#e94560,color:#fff
    style VoiceSystem fill:#1a1a2e,stroke:#e94560,color:#fff
    style VisionSystem fill:#1a1a2e,stroke:#00d2ff,color:#fff
    style GhostHands fill:#1a1a2e,stroke:#7b2ff7,color:#fff
    style IntelCore fill:#1a1a2e,stroke:#ffc107,color:#fff
    style Platform fill:#1a1a2e,stroke:#00e676,color:#fff
```

---

## üîÑ Data Flow

### Request Processing Pipeline

```mermaid
sequenceDiagram
    participant U as üë§ User
    participant F as ‚öõÔ∏è Frontend
    participant API as üêç FastAPI
    participant STT as üé§ Whisper STT
    participant BIO as üîê ECAPA Biometric
    participant LLM as üß† Claude/Fireworks
    participant GH as ü§ñ Ghost Hands
    participant VIS as üëÅÔ∏è Vision
    participant TTS as üó£Ô∏è Neural TTS
    participant MEM as üíæ Memory

    U->>F: Voice: "Hey JARVIS, open Spotify"
    F->>API: WebSocket: audio stream
    API->>STT: Raw audio bytes
    
    Note over STT: Whisper base model<br/>Local inference ~200ms
    STT-->>API: Text: "hey jarvis open spotify"

    API->>BIO: Speaker verification
    Note over BIO: ECAPA-TDNN 192D embedding<br/>Cosine similarity check
    BIO-->>API: ‚úÖ Speaker: Nandkishor (0.87 confidence)

    API->>MEM: Fetch conversation context
    MEM-->>API: Last 10 messages + user preferences

    API->>LLM: Prompt + context + intent
    Note over LLM: Claude 3.5 Sonnet<br/>Goal inference + action planning
    LLM-->>API: Action: launch_app("Spotify")

    API->>GH: Execute: launch Spotify
    Note over GH: pyautogui / os.startfile<br/>Cross-platform launcher
    GH-->>API: ‚úÖ Spotify launched

    API->>VIS: Verify: is Spotify visible?
    Note over VIS: mss screen capture<br/>Claude Vision check
    VIS-->>API: ‚úÖ Spotify window detected

    API->>TTS: "Spotify is now open, sir"
    Note over TTS: edge-tts Neural voice<br/>en-GB-RyanNeural
    TTS-->>F: Audio stream (MP3)
    F->>U: üîä "Spotify is now open, sir"

    API->>MEM: Save interaction
    Note over MEM: SQLite + ChromaDB<br/>Semantic embedding stored
```

### Multi-LLM Routing Decision

```mermaid
graph LR
    INPUT["üì• User Query"] --> ROUTER{"üîÄ LLM Router"}
    
    ROUTER -->|Complex reasoning<br/>Code generation<br/>Vision tasks| CLAUDE["üü† Claude 3.5 Sonnet<br/>anthropic API<br/>~2s latency"]
    
    ROUTER -->|Simple Q&A<br/>Fast response<br/>Cost optimization| FIREWORKS["üîµ Fireworks AI<br/>Llama 70B Instruct<br/>~500ms latency"]
    
    ROUTER -->|Fallback<br/>Rate limited| FALLBACK["üü¢ Fallback Chain<br/>Groq ‚Üí Local"]
    
    CLAUDE --> OUTPUT["üì§ Response"]
    FIREWORKS --> OUTPUT
    FALLBACK --> OUTPUT

    style CLAUDE fill:#ff6b00,stroke:#fff,color:#fff
    style FIREWORKS fill:#0066ff,stroke:#fff,color:#fff
    style FALLBACK fill:#00cc66,stroke:#fff,color:#fff
```

---

## üé§ Voice Pipeline

### Complete Voice Processing Flow

```mermaid
graph TB
    subgraph Input["üé§ Audio Input"]
        MIC["Microphone<br/>sounddevice"]
        WAKE_DET["Wake Word<br/>Detection"]
        VAD["Voice Activity<br/>Detection"]
    end

    subgraph STT_Pipeline["üó£Ô∏è‚Üíüìù Speech-to-Text"]
        WHISPER_LOCAL["Whisper Local<br/>base/small model"]
        CLOUD_STT["Cloud STT<br/>Fallback"]
        CIRCUIT["Circuit Breaker<br/>12 model rotation"]
    end

    subgraph Processing["üß† Processing"]
        SPEAKER_V["Speaker Verification<br/>ECAPA-TDNN"]
        INTENT["Intent Parser<br/>Claude API"]
        CONTEXT_MGR["Context Manager<br/>Conversation State"]
    end

    subgraph TTS_Pipeline["üìù‚Üíüîä Text-to-Speech"]
        EDGE_TTS["edge-tts<br/>en-GB-RyanNeural"]
        PYTTSX3["pyttsx3<br/>Fallback"]
        AUDIO_OUT["Audio Output<br/>Speaker"]
    end

    MIC --> WAKE_DET
    WAKE_DET -->|"Hey JARVIS"| VAD
    VAD -->|Speech segment| WHISPER_LOCAL
    WHISPER_LOCAL -->|Failure| CLOUD_STT
    CLOUD_STT -->|Failure| CIRCUIT
    
    WHISPER_LOCAL --> SPEAKER_V
    SPEAKER_V -->|Verified| INTENT
    INTENT --> CONTEXT_MGR
    
    CONTEXT_MGR --> EDGE_TTS
    EDGE_TTS -->|Failure| PYTTSX3
    EDGE_TTS --> AUDIO_OUT
    PYTTSX3 --> AUDIO_OUT

    style Input fill:#e94560,stroke:#fff,color:#fff
    style STT_Pipeline fill:#0f3460,stroke:#fff,color:#fff
    style Processing fill:#533483,stroke:#fff,color:#fff
    style TTS_Pipeline fill:#16213e,stroke:#fff,color:#fff
```

### Voice Biometric Authentication

```mermaid
graph LR
    AUDIO["üé§ Audio Input"] --> ECAPA["ECAPA-TDNN<br/>Encoder"]
    ECAPA --> EMB["192D Embedding<br/>Vector"]
    EMB --> COS{"Cosine<br/>Similarity"}
    
    DB["üíæ Enrolled<br/>Voiceprints"] --> COS
    
    COS -->|"> 0.75"| PASS["‚úÖ Authenticated<br/>Welcome sir"]
    COS -->|"< 0.75"| FAIL["‚ùå Rejected<br/>Unknown speaker"]
    
    PASS --> UNLOCK["üîì Full Access"]
    FAIL --> LIMITED["üîí Limited Mode"]

    style ECAPA fill:#7b2ff7,stroke:#fff,color:#fff
    style PASS fill:#00cc66,stroke:#fff,color:#fff
    style FAIL fill:#ff4444,stroke:#fff,color:#fff
```

---

## üëÅÔ∏è Vision Pipeline

### Screen Understanding Flow

```mermaid
graph TB
    subgraph Capture["üì∏ Screen Capture"]
        MSS["mss Library<br/>30 FPS capture"]
        MONITORS["Multi-Monitor<br/>Detection"]
        REGION["Region Select<br/>Focus Area"]
    end

    subgraph Analysis["üß† Analysis"]
        CLAUDE_V["Claude Vision API<br/>Image ‚Üí Understanding"]
        OCR["Text Extraction<br/>From Screenshots"]
        APP_DET["App Detection<br/>Active Window"]
    end

    subgraph Intelligence["üí° Intelligence"]
        SEM_CACHE["Semantic Cache<br/>ChromaDB ¬∑ 24h TTL"]
        CONTEXT_INT["Context Intelligence<br/>What user is doing"]
        CHANGE_DET["Change Detection<br/>Delta Analysis"]
    end

    subgraph Actions["‚ö° Actions"]
        NOTIFY["üì¢ Notification<br/>Proactive Alert"]
        ASSIST["ü§ñ Auto-Assist<br/>Help Suggestion"]
        RECORD["üìù Record<br/>Activity Log"]
    end

    MSS --> CLAUDE_V
    MONITORS --> MSS
    REGION --> MSS

    CLAUDE_V --> SEM_CACHE
    CLAUDE_V --> APP_DET
    OCR --> CONTEXT_INT
    APP_DET --> CONTEXT_INT

    SEM_CACHE --> CHANGE_DET
    CONTEXT_INT --> NOTIFY
    CONTEXT_INT --> ASSIST
    CHANGE_DET --> RECORD

    style Capture fill:#00d2ff,stroke:#fff,color:#000
    style Analysis fill:#7b2ff7,stroke:#fff,color:#fff
    style Intelligence fill:#ffc107,stroke:#fff,color:#000
    style Actions fill:#00cc66,stroke:#fff,color:#fff
```

---

## ü§ñ Ghost Hands ‚Äî Autonomous Automation

### Automation Architecture

```mermaid
graph TB
    subgraph Command["üì• Command Input"]
        VOICE_CMD["Voice Command<br/>Open Chrome"]
        TEXT_CMD["Text Command<br/>API Request"]
        AUTO_CMD["Autonomous<br/>Self-initiated"]
    end

    subgraph Planning["üß† Action Planner"]
        INTENT_P["Intent Parser<br/>Claude API"]
        WORKFLOW["Workflow Engine<br/>Multi-step Plans"]
        SAFETY["Safety Check<br/>Confirmation Required?"]
    end

    subgraph Execution["‚ö° Execution Layer"]
        direction TB
        MOUSE["üñ±Ô∏è Mouse Control<br/>pyautogui.click()"]
        KEYBOARD["‚å®Ô∏è Keyboard<br/>pyautogui.write()"]
        APP_CTL["üì± App Control<br/>os.startfile / subprocess"]
        BROWSER_A["üåê Browser<br/>Selenium / CDP"]
        CLIPBOARD["üìã Clipboard<br/>pyperclip"]
        WINDOW["ü™ü Window Mgmt<br/>win32gui"]
    end

    subgraph Verify["‚úÖ Verification"]
        SCREEN_V["Screenshot Check<br/>Did it work?"]
        RETRY["Retry Logic<br/>Max 3 attempts"]
        REPORT["Report Back<br/>TTS Confirmation"]
    end

    VOICE_CMD --> INTENT_P
    TEXT_CMD --> INTENT_P
    AUTO_CMD --> INTENT_P

    INTENT_P --> WORKFLOW
    WORKFLOW --> SAFETY
    SAFETY -->|Safe| Execution
    SAFETY -->|Dangerous| CONFIRM["‚ö†Ô∏è Ask User"]
    CONFIRM --> Execution

    MOUSE --> SCREEN_V
    KEYBOARD --> SCREEN_V
    APP_CTL --> SCREEN_V
    BROWSER_A --> SCREEN_V
    CLIPBOARD --> SCREEN_V
    WINDOW --> SCREEN_V

    SCREEN_V -->|Failed| RETRY
    RETRY --> Execution
    SCREEN_V -->|Success| REPORT

    style Command fill:#e94560,stroke:#fff,color:#fff
    style Planning fill:#533483,stroke:#fff,color:#fff
    style Execution fill:#0f3460,stroke:#fff,color:#fff
    style Verify fill:#00cc66,stroke:#fff,color:#fff
```

### Cross-Platform App Launcher

```mermaid
graph LR
    CMD["Launch App"] --> PLATFORM{"sys.platform?"}
    
    PLATFORM -->|win32| WIN_STRAT["Windows Strategy"]
    PLATFORM -->|darwin| MAC_STRAT["macOS Strategy"]
    
    subgraph WIN_STRAT["ü™ü Windows Launch Chain"]
        URI["1. URI Scheme<br/>ms-settings:"]
        EXE["2. Direct EXE<br/>chrome.exe"]
        START["3. cmd /c start<br/>Start Menu"]
        STARTFILE["4. os.startfile<br/>Fallback"]
    end

    subgraph MAC_STRAT["üçé macOS Launch Chain"]
        OPEN_A["1. open -a<br/>Bundle"]
        OSASCRIPT["2. osascript<br/>AppleScript"]
    end

    URI -->|fail| EXE
    EXE -->|fail| START
    START -->|fail| STARTFILE

    OPEN_A -->|fail| OSASCRIPT

    style WIN_STRAT fill:#0078D4,stroke:#fff,color:#fff
    style MAC_STRAT fill:#333,stroke:#fff,color:#fff
```

---

## üß† Intelligence Core

### Memory & Learning System

```mermaid
graph TB
    subgraph ShortTerm["‚ö° Short-Term Memory"]
        CONV["Conversation Buffer<br/>Last 10 messages"]
        SESSION["Session State<br/>Current context"]
        DEDUP["Dedup Cache<br/>60s window"]
    end

    subgraph LongTerm["üíæ Long-Term Memory"]
        SQLITE["SQLite<br/>Structured data<br/>Conversations, configs"]
        CHROMADB["ChromaDB<br/>Vector embeddings<br/>Semantic search"]
        LEARNING_DB["Learning DB<br/>User preferences<br/>Behavior patterns"]
    end

    subgraph Cloud_Mem["‚òÅÔ∏è Cloud Sync"]
        CLOUD_SQL_M["Cloud SQL<br/>Voice profiles"]
        GCS["Google Cloud Storage<br/>Model checkpoints"]
    end

    CONV --> SQLITE
    SESSION --> CHROMADB
    DEDUP --> SQLITE

    SQLITE <-.-> CLOUD_SQL_M
    CHROMADB --> LEARNING_DB
    LEARNING_DB <-.-> GCS

    style ShortTerm fill:#ffc107,stroke:#fff,color:#000
    style LongTerm fill:#0f3460,stroke:#fff,color:#fff
    style Cloud_Mem fill:#533483,stroke:#fff,color:#fff
```

### Situational Awareness Intelligence (SAI)

```mermaid
graph LR
    INPUTS["üì• Input Signals"] --> SAI_ENGINE{"üîÆ SAI Engine"}
    
    TIME["‚è∞ Time of Day"] --> SAI_ENGINE
    APP["üì± Active App"] --> SAI_ENGINE
    VOLUME["üîä Ambient Audio"] --> SAI_ENGINE
    TYPING["‚å®Ô∏è Typing Speed"] --> SAI_ENGINE
    SCREEN_S["üñ•Ô∏è Screen Content"] --> SAI_ENGINE
    
    SAI_ENGINE --> ROUTINE["üü¢ Routine<br/>Normal behavior"]
    SAI_ENGINE --> FOCUS["üü° Focus<br/>Deep work mode"]
    SAI_ENGINE --> EMERGENCY["üî¥ Emergency<br/>Urgent situation"]
    SAI_ENGINE --> SUSPICIOUS["‚ö†Ô∏è Suspicious<br/>Unusual activity"]
    
    ROUTINE --> LOW_N["Low notifications"]
    FOCUS --> DND["Do Not Disturb"]
    EMERGENCY --> HIGH_P["High priority alert"]
    SUSPICIOUS --> LOG["Security log"]

    style SAI_ENGINE fill:#7b2ff7,stroke:#fff,color:#fff
    style EMERGENCY fill:#ff4444,stroke:#fff,color:#fff
    style SUSPICIOUS fill:#ffc107,stroke:#fff,color:#000
```

---

## üñ•Ô∏è Platform Support

| Platform | Status | Details |
|:---------|:------:|:--------|
| **Windows 10** | ‚úÖ | Full support ‚Äî pywin32, pyautogui, mss, pycaw |
| **Windows 11** | ‚úÖ | Full support ‚Äî toast notifications, Windows Terminal |
| **macOS** | ‚ö†Ô∏è | Upstream ‚Äî see [drussell23/JARVIS](https://github.com/drussell23/JARVIS) |
| **Linux** | üîß | Partial ‚Äî Platform Abstraction Layer compatible |

### Windows Feature Matrix

| Feature | Library | Status |
|:--------|:--------|:------:|
| Window Management | `pywin32` / `win32gui` | ‚úÖ |
| Mouse & Keyboard | `pyautogui` | ‚úÖ |
| Screen Capture | `mss` + `Pillow` | ‚úÖ |
| Notifications | `plyer` ‚Üí `win10toast` | ‚úÖ |
| Volume Control | `pycaw` (COM WASAPI) | ‚úÖ |
| Brightness | WMI + PowerShell | ‚úÖ |
| Screen Lock | `LockWorkStation()` | ‚úÖ |
| Lock Detection | `LogonUI.exe` check | ‚úÖ |
| Sleep Prevention | `SetThreadExecutionState` | ‚úÖ |
| Clipboard | `pyperclip` ‚Üí `clip.exe` | ‚úÖ |
| App Launch | 4-strategy chain | ‚úÖ |
| System Info | `psutil` | ‚úÖ |
| Audio Devices | `sounddevice` | ‚úÖ |
| File Open | `os.startfile()` | ‚úÖ |

---

## ‚ú® Features

### üß† Core Intelligence

<details>
<summary><b>Multi-LLM Routing Engine</b></summary>

JARVIS uses an intelligent router to pick the best LLM for each query:

| Model | Provider | Use Case | Latency |
|:------|:---------|:---------|:--------|
| Claude 3.5 Sonnet | Anthropic | Complex reasoning, code, vision | ~2s |
| Llama 3.1 70B | Fireworks AI | Fast Q&A, conversation | ~500ms |
| Groq Mixtral | Groq | Ultra-fast fallback | ~200ms |

**Cost Optimization**: Routes 70% of queries to Fireworks (cheaper) while keeping Claude for complex tasks.

</details>

<details>
<summary><b>Goal Inference Engine</b></summary>

JARVIS doesn't just respond to commands ‚Äî it **infers your intent**:

```
User: "I need to send an email to John"
JARVIS infers:
  ‚Üí Open email client (Outlook)
  ‚Üí Create new message
  ‚Üí Set recipient: John (from contacts)
  ‚Üí Wait for user to dictate content
  ‚Üí Confirm before sending
```

</details>

<details>
<summary><b>Long-Term Memory System</b></summary>

| Memory Type | Storage | TTL | Purpose |
|:------------|:--------|:----|:--------|
| Conversation | SQLite | Permanent | Chat history |
| Semantic | ChromaDB | 24 hours | Visual context |
| Learning | SQLite | Permanent | User preferences |
| Voice | Cloud SQL | Permanent | Speaker profiles |

</details>

### üé§ Voice System

<details>
<summary><b>Hybrid STT Architecture</b></summary>

```
Audio Input
    ‚Üì
[Whisper base - LOCAL] ‚Üê‚îÄ‚îÄ Primary (200ms)
    ‚Üì (on failure)
[Whisper small - LOCAL] ‚Üê‚îÄ‚îÄ Fallback 1
    ‚Üì (on failure)
[Cloud STT API] ‚Üê‚îÄ‚îÄ Fallback 2
    ‚Üì (on failure)
[Circuit Breaker] ‚Üê‚îÄ‚îÄ 12 model rotation
    ‚Üì
Text Output
```

**Performance**: 
- Cold start: ~2.3s (model loading)
- Warm inference: ~200ms per utterance
- Supported languages: 97+ (via Whisper multilingual)

</details>

<details>
<summary><b>Neural Text-to-Speech</b></summary>

JARVIS speaks with Microsoft's Neural TTS engine:

| Voice | Language | Style |
|:------|:---------|:------|
| `en-GB-RyanNeural` | English (UK) | Professional, warm |
| `en-US-GuyNeural` | English (US) | Fallback |

**Technology Stack:**
- Primary: `edge-tts` (free, no API key needed)
- Fallback: `pyttsx3` (offline SAPI5)
- Output: Real-time MP3 streaming via WebSocket

</details>

<details>
<summary><b>ECAPA-TDNN Voice Biometrics</b></summary>

| Metric | Value |
|:-------|:------|
| Model | ECAPA-TDNN (SpeechBrain) |
| Embedding Dimension | 192D |
| Verification Time | ~159ms |
| Threshold | 0.75 cosine similarity |
| Enrolled Profiles | Per-user |
| Backend Options | Local / Cloud Run / Docker |

**Enrollment**: Say "JARVIS, learn my voice" ‚Äî records 3 samples, extracts embeddings, stores in database.

</details>

### üëÅÔ∏è Vision & Automation

<details>
<summary><b>Real-Time Screen Understanding</b></summary>

| Feature | Spec |
|:--------|:-----|
| Capture Rate | 30 FPS (configurable) |
| Library | `mss` (GPU-accelerated) |
| Analysis | Claude Vision API |
| OCR | Built-in text extraction |
| Cache | ChromaDB semantic (24h TTL) |
| Multi-monitor | Yes (all displays) |

</details>

<details>
<summary><b>Ghost Hands Automation</b></summary>

JARVIS can control your computer autonomously:

| Action | Method | Platform |
|:-------|:-------|:---------|
| Click | `pyautogui.click()` | Windows/macOS |
| Type | `pyautogui.write()` | Windows/macOS |
| Hotkey | `pyautogui.hotkey()` | Windows/macOS |
| App Launch | `os.startfile()` / `open -a` | Windows/macOS |
| Window Focus | `win32gui.SetForegroundWindow()` | Windows |
| Browser | Selenium / CDP | Cross-platform |
| Clipboard | `pyperclip` | Cross-platform |

</details>

### ‚òÅÔ∏è Cloud Integration

<details>
<summary><b>GCP Auto-Scaling Architecture</b></summary>

When local RAM exceeds 80%, JARVIS automatically deploys to GCP:

| Resource | Spec | Cost |
|:---------|:-----|:-----|
| VM Type | `e2-highmem-4` Spot | $0.029/hr |
| RAM | 32 GB | ‚Äî |
| vCPUs | 4 | ‚Äî |
| Auto-idle | Scale to zero after 15min | ‚Äî |
| Region | `us-central1` | ‚Äî |

**Total monthly cost**: ~$15-20 (typical usage)

</details>

---

## üöÄ Quick Start (Windows)

### Prerequisites

| Requirement | Version | Check Command |
|:------------|:--------|:-------------|
| Python | 3.12+ | `python --version` |
| Node.js | 18+ | `node --version` |
| Git | Any | `git --version` |
| .NET SDK | 8.0+ | `dotnet --version` |

### Step-by-Step Installation

#### 1. Clone the Repository
```powershell
git clone https://github.com/nandkishorrathodk-art/Ironcliw-ai.git
cd Ironcliw-ai
```

#### 2. Install Python Dependencies
```powershell
# Core dependencies
python -m pip install -r requirements.txt

# Windows-specific
python -m pip install -r requirements-windows.txt

# Essential extras
python -m pip install edge-tts mss pyautogui pywin32 pyttsx3
```

#### 3. Install Frontend
```powershell
cd frontend
npm install
cd ..
```

#### 4. Configure Environment
```powershell
# Copy Windows template
copy .env.windows .env

# Edit and add your API keys
notepad .env
```

**Minimum required API keys:**
```env
ANTHROPIC_API_KEY=sk-ant-api03-xxxxx
FIREWORKS_API_KEY=fw-xxxxx
```

#### 5. Launch JARVIS
```powershell
python start_system.py
```

#### 6. Open in Browser
```
http://localhost:3000
```

> **üí° First launch** takes ~60-90 seconds (model loading). Subsequent launches are faster (~30s).
> Say **"Hey JARVIS"** to activate voice control.

---

## üîß Configuration Reference

### Environment Variables (`.env`)

<details>
<summary><b>üß† LLM Configuration</b></summary>

| Variable | Default | Description |
|:---------|:--------|:------------|
| `JARVIS_LLM_PROVIDER` | `fireworks` | Primary LLM: `fireworks`, `claude`, `groq` |
| `ANTHROPIC_API_KEY` | ‚Äî | Claude API key |
| `FIREWORKS_API_KEY` | ‚Äî | Fireworks AI API key |
| `JARVIS_GROQ_API_KEY` | ‚Äî | Groq API key (fallback) |
| `JARVIS_LLM_TEMPERATURE` | `0.7` | Response creativity (0-1) |
| `JARVIS_LLM_MAX_TOKENS` | `4096` | Max response length |

</details>

<details>
<summary><b>üé§ Voice Configuration</b></summary>

| Variable | Default | Description |
|:---------|:--------|:------------|
| `WHISPER_MODEL_SIZE` | `base` | STT model: tiny/base/small/medium |
| `JARVIS_TTS_VOICE` | `en-GB-RyanNeural` | Neural TTS voice name |
| `JARVIS_VOICE_BIOMETRIC_ENABLED` | `false` | Enable voice auth |
| `JARVIS_WAKE_WORD` | `hey jarvis` | Wake word phrase |
| `JARVIS_VOICE_GAIN` | `1.0` | Microphone gain multiplier |

</details>

<details>
<summary><b>‚ö° Performance Configuration</b></summary>

| Variable | Default | Description |
|:---------|:--------|:------------|
| `JARVIS_ML_DEVICE` | `cpu` | ML device: `cpu`, `cuda`, `directml` |
| `JARVIS_LAZY_LOAD_MODELS` | `true` | Load models on demand |
| `JARVIS_MEMORY_LIMIT` | `4096` | RAM target (MB) |
| `JARVIS_DYNAMIC_PORTS` | `false` | Auto-assign ports |
| `JARVIS_MAX_WORKERS` | `4` | Thread pool size |

</details>

<details>
<summary><b>ü™ü Windows-Specific Configuration</b></summary>

| Variable | Default | Description |
|:---------|:--------|:------------|
| `JARVIS_AUTO_BYPASS_WINDOWS` | `true` | Skip voice auth on Windows |
| `JARVIS_DISABLE_SWIFT_EXTENSIONS` | `true` | Disable macOS Swift |
| `JARVIS_DISABLE_RUST_EXTENSIONS` | `true` | Disable Rust layer |
| `JARVIS_DISABLE_COREML` | `true` | Disable CoreML |
| `JARVIS_NOTIFICATION_PROVIDER` | `win10toast` | Notification backend |

</details>

<details>
<summary><b>‚òÅÔ∏è Cloud Configuration</b></summary>

| Variable | Default | Description |
|:---------|:--------|:------------|
| `JARVIS_SKIP_GCP` | `true` | Disable GCP integration |
| `JARVIS_SKIP_DOCKER` | `true` | Disable Docker ECAPA |
| `JARVIS_PREFER_CLOUD_RUN` | `false` | Use Cloud Run ECAPA |
| `JARVIS_GCP_PROJECT_ID` | ‚Äî | GCP project ID |
| `JARVIS_SPOT_VM_ZONE` | `us-central1-a` | Spot VM zone |

</details>

---

## üìÅ Project Structure

```
Ironcliw-ai/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ start_system.py              # üöÄ Main launcher (22,000+ lines)
‚îú‚îÄ‚îÄ üìÑ unified_supervisor.py        # Process lifecycle manager
‚îú‚îÄ‚îÄ üìÑ loading_server.py            # Startup loading page server
‚îú‚îÄ‚îÄ üìÑ .env.windows                 # Windows config template
‚îú‚îÄ‚îÄ üìÑ WINDOWS_PORT_BLUEPRINT.md    # 700-line macOS‚ÜíWindows guide
‚îú‚îÄ‚îÄ üìÑ SECURITY.md                  # Security policy
‚îú‚îÄ‚îÄ üìÑ LICENSE                      # MIT License
‚îÇ
‚îú‚îÄ‚îÄ üêç backend/                     # FastAPI Python backend
‚îÇ   ‚îú‚îÄ‚îÄ main.py                     # Entry point (UTF-8 bootstrap)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üì° api/                     # REST API Layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ action_executors.py     # Workflow action handlers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ broadcast_router.py     # WebSocket broadcast
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jarvis_voice_api.py     # Voice REST endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ websocket_router.py     # WebSocket routing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ workflow_engine.py      # Multi-step workflow engine
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ workflow_parser.py      # Action type definitions
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üß† agi_os/                  # AGI Operating System Layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ realtime_voice_communicator.py   # edge-tts Neural TTS
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ notification_bridge.py           # Cross-platform notifications
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ proactive_intelligence.py        # Self-initiated actions
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üé§ voice/                   # Voice Processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid_stt_router.py             # Whisper + Cloud STT
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ speaker_verification_service.py  # ECAPA biometrics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jarvis_agent_voice.py            # Voice agent logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ voice_unlock_integration.py      # Voice unlock flow
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üëÅÔ∏è vision/                  # Vision System
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ continuous_screen_analyzer.py    # 30 FPS capture loop
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ reliable_screenshot_capture.py   # mss capture engine
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ adapters/page.py                 # Web page adapter
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ü§ñ ghost_hands/             # Autonomous Automation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ background_actuator.py           # pyautogui controller
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ yabai_aware_actuator.py          # Window-aware actions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ browser_controller.py            # Browser automation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üí° intelligence/            # AI Intelligence
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ learning_database.py             # Adaptive learning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hybrid_database_sync.py          # Cloud SQL sync
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ situational_awareness.py         # SAI engine
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ‚öôÔ∏è core/                    # Core Infrastructure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py                      # Request pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py                  # Service orchestrator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ process_detector.py              # Advanced PID detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transport_handlers.py            # HTTP/WS handlers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ secret_manager.py                # GCP Secret Manager
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üîß autonomy/                # System Control
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hardware_control.py              # Volume/brightness/sleep
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ action_executor.py               # Autonomous action runner
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üñ•Ô∏è platform_adapter/        # Cross-Platform Abstraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ abstraction.py                   # PlatformInterface + factory
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ windows_platform.py              # Windows implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ macos_platform.py                # macOS implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ linux_platform.py                # Linux implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detector.py                      # OS detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ base.py                          # Shared utilities
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üîê voice_unlock/            # Voice Biometric Auth
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml_engine_registry.py            # ML model management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ startup_integration.py           # Boot-time validation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ intelligent_voice_unlock_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ unified_voice_cache_manager.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cloud_ecapa_client.py            # Cloud Run ECAPA
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üîå system_control/          # OS Integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fast_app_launcher.py             # Cross-platform launcher
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dynamic_app_controller.py        # Intelligent app control
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ location_service.py              # Geo-location
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üß™ tests/                   # Test Suite
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ archive/                         # Legacy tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_windows_platform.py         # Windows platform tests
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ ü™ü windows_native/          # C# Native Extensions
‚îÇ       ‚îú‚îÄ‚îÄ AudioEngine/                     # WASAPI audio capture
‚îÇ       ‚îú‚îÄ‚îÄ ScreenCapture/                   # GDI+ screen capture
‚îÇ       ‚îî‚îÄ‚îÄ SystemControl/                   # Win32 API wrappers
‚îÇ
‚îî‚îÄ‚îÄ ‚öõÔ∏è frontend/                    # React 18 UI
    ‚îú‚îÄ‚îÄ package.json
    ‚îî‚îÄ‚îÄ src/
        ‚îú‚îÄ‚îÄ App.js                           # Main app component
        ‚îú‚îÄ‚îÄ index.js                         # Entry point
        ‚îî‚îÄ‚îÄ components/
            ‚îî‚îÄ‚îÄ JarvisVoice.js               # Voice UI + WebSocket
```

---

## üì° API Reference

### REST Endpoints

| Method | Endpoint | Description |
|:-------|:---------|:------------|
| `POST` | `/api/chat` | Send message to JARVIS |
| `POST` | `/api/voice/transcribe` | Upload audio for STT |
| `GET` | `/api/status` | System health check |
| `GET` | `/api/system/info` | CPU, RAM, disk info |
| `POST` | `/api/vision/capture` | Capture screenshot |
| `POST` | `/api/vision/analyze` | Analyze screen content |
| `POST` | `/api/actions/execute` | Execute automation action |
| `GET` | `/api/voice/profiles` | List voice profiles |
| `POST` | `/api/voice/enroll` | Start voice enrollment |
| `GET` | `/api/memory/search` | Search conversation history |
| `POST` | `/api/system/lock` | Lock workstation |
| `POST` | `/api/system/volume` | Set volume level |

### WebSocket Events

| Event | Direction | Payload |
|:------|:----------|:--------|
| `audio_stream` | Client ‚Üí Server | Raw audio bytes |
| `transcription` | Server ‚Üí Client | `{text, confidence}` |
| `tts_audio` | Server ‚Üí Client | MP3 audio bytes |
| `proactive_notification` | Server ‚Üí Client | `{title, message, urgency}` |
| `system_status` | Server ‚Üí Client | `{cpu, ram, disk}` |
| `vision_update` | Server ‚Üí Client | `{screenshot, analysis}` |
| `ghost_hands_action` | Server ‚Üí Client | `{action, target, status}` |

---

## üó£Ô∏è Voice Commands

### General
| Say This | JARVIS Does |
|:---------|:------------|
| "Hey JARVIS" | Activate listening |
| "What can you do?" | List capabilities |
| "What time is it?" | Speak current time |
| "Tell me a joke" | Generate and speak joke |
| "Goodbye" | End conversation |

### System Control
| Say This | JARVIS Does |
|:---------|:------------|
| "Set volume to 50%" | Adjust system volume (pycaw) |
| "Lock my screen" | `LockWorkStation()` |
| "What's my RAM usage?" | Report system stats |
| "Battery status" | Report battery level |
| "Prevent sleep" | `SetThreadExecutionState` |

### App Control
| Say This | JARVIS Does |
|:---------|:------------|
| "Open Chrome" | Launch Chrome (4-strategy) |
| "Open Spotify" | Launch Spotify |
| "Open VS Code" | Launch code editor |
| "Open Settings" | Launch `ms-settings:` |
| "Open Terminal" | Launch Windows Terminal |
| "Close this window" | `win32gui` close active |

### Vision & Screen
| Say This | JARVIS Does |
|:---------|:------------|
| "Can you see my screen?" | Capture + analyze |
| "What app am I using?" | Active window detection |
| "Read what's on screen" | OCR + text extraction |
| "Start monitoring" | Begin 30 FPS capture |
| "Take a screenshot" | Save PNG to disk |

### Voice Biometrics
| Say This | JARVIS Does |
|:---------|:------------|
| "JARVIS, learn my voice" | Start enrollment (3 samples) |
| "Who am I?" | Speaker identification |
| "Verify me" | Run biometric check |

---

## ‚ö° Startup Flow

```mermaid
graph TB
    START["üöÄ python start_system.py"] --> ENV["Load .env + .env.windows"]
    ENV --> PORTS["Check Ports<br/>8010, 3000, 8001"]
    PORTS --> BACKEND["Start FastAPI Backend<br/>Port 8010"]
    BACKEND --> MODELS["Load ML Models"]
    
    subgraph ModelLoading["üß† Model Loading (Parallel)"]
        WHI["Whisper STT<br/>~2.3s"]
        ECAPA_L["ECAPA-TDNN<br/>Fast-fail on Windows"]
        LLM_INIT["LLM Client Init<br/>Claude + Fireworks"]
    end
    
    MODELS --> ModelLoading
    
    ModelLoading --> VOICE_INIT["üé§ Voice System Init"]
    VOICE_INIT --> VISION_INIT["üëÅÔ∏è Vision System Init"]
    VISION_INIT --> GH_INIT["ü§ñ Ghost Hands Init"]
    GH_INIT --> FRONTEND["‚öõÔ∏è Start Frontend<br/>npm run dev ¬∑ Port 3000"]
    FRONTEND --> BROWSER["üåê Open Browser<br/>http://localhost:3000"]
    BROWSER --> READY["‚úÖ JARVIS READY<br/>~60-90s total"]

    style START fill:#e94560,stroke:#fff,color:#fff
    style READY fill:#00cc66,stroke:#fff,color:#fff
    style ModelLoading fill:#0f3460,stroke:#fff,color:#fff
```

---

## üî¨ ML Model Pipeline

| Model | Purpose | Size | Load Time | Device |
|:------|:--------|:-----|:----------|:-------|
| Whisper `base` | Speech-to-Text | 142 MB | ~2.3s | CPU |
| ECAPA-TDNN | Speaker Verification | ~30 MB | Fast-fail* | CPU/Cloud |
| Claude 3.5 Sonnet | Reasoning & Vision | Cloud | N/A | API |
| Fireworks Llama 70B | Fast Q&A | Cloud | N/A | API |

> ***Fast-fail**: On Windows without `speechbrain`, ECAPA fails immediately (0ms) instead of blocking for 25s. Falls back to Cloud Run endpoint.

---

## üîê Security Architecture

```mermaid
graph TB
    subgraph Security["üîê Security Layers"]
        AUTH["Authentication<br/>Voice Biometric / Bypass"]
        LOGGING["Secure Logging<br/>CWE-117/532 Prevention"]
        SECRETS["Secret Management<br/>.env + GCP Secret Manager"]
        NETWORK["Network Isolation<br/>localhost only"]
        FILE["File Security<br/>Atomic writes ¬∑ 0o600"]
    end

    AUTH --> VERIFY{"Verified?"}
    VERIFY -->|Yes| FULL["üîì Full Access"]
    VERIFY -->|No| LIMITED["üîí Limited Mode"]

    style Security fill:#e94560,stroke:#fff,color:#fff
```

### Security Features

| Feature | Implementation | Status |
|:--------|:---------------|:------:|
| API Key Protection | `.env` only, never in code | ‚úÖ |
| Network Isolation | `localhost` binding only | ‚úÖ |
| Voice Auth | ECAPA-TDNN biometric (optional) | ‚úÖ |
| Log Injection | CWE-117/532 sanitization | ‚úÖ |
| File Permissions | Atomic writes, `0o600` | ‚úÖ |
| Input Sanitization | AppleScript/SQL injection guard | ‚úÖ |
| FIPS Compliance | `hashlib(usedforsecurity=False)` | ‚úÖ |

---

## üìä Performance

### Benchmarks (Acer Swift Neo ‚Äî 16GB RAM, 512GB SSD)

| Metric | Value | Notes |
|:-------|:------|:------|
| Cold Start | ~60-90s | First launch with model loading |
| Warm Start | ~30s | Models cached |
| STT Latency | ~200ms | Whisper base, warm |
| TTS Latency | ~300ms | edge-tts streaming |
| Vision Capture | ~33ms | 30 FPS via mss |
| Voice Auth | ~159ms | ECAPA-TDNN |
| API Response | ~500ms-2s | Depends on LLM |
| RAM Usage | ~3-4 GB | Steady state |
| Peak RAM | ~6-8 GB | During model loading |

### Optimization Tips

| Issue | Solution |
|:------|:---------|
| High RAM (>80%) | Set `JARVIS_MEMORY_LIMIT=3072` |
| Slow startup | Set `JARVIS_LAZY_LOAD_MODELS=true` |
| ECAPA blocking | Already fixed (fast-fail) |
| Port conflicts | Kill stuck ports or change in `.env` |
| No GPU | `JARVIS_ML_DEVICE=cpu` (default) |

---

## üîÑ Windows Port Status

### Phase History

| Phase | Description | Status |
|:------|:------------|:------:|
| 1-5 | Core imports, path fixes | ‚úÖ |
| 6 | `os.uname()` ‚Üí `platform.uname()` | ‚úÖ |
| 7 | `fcntl` ‚Üí `msvcrt` guards | ‚úÖ |
| 8 | Neural TTS (`en-GB-RyanNeural`) | ‚úÖ |
| 9 | Vision: `screencapture` ‚Üí `mss` | ‚úÖ |
| 10 | Ghost Hands: `cliclick` ‚Üí `pyautogui` | ‚úÖ |
| 11 | ECAPA fast-fail, UTF-8, logging fixes | ‚úÖ |
| **12** | **Cross-platform porting (current)** | **‚úÖ** |
| 13 | Full notification system | ‚úÖ |
| 14 | Hardware control (volume/brightness) | ‚úÖ |
| 15-20 | Advanced features | üîß |

### Phase 12 Changes (Latest)

| File | Change |
|:-----|:-------|
| `fast_app_launcher.py` | Full rewrite ‚Äî 4 Windows launch strategies |
| `startup_integration.py` | Keychain‚Üícmdkey, lsof‚Üítaskkill, pkill‚Üítaskkill |
| `ml_engine_registry.py` | ECAPA fast-fail on ERROR state |
| `requirements-windows.txt` | Added plyer, pycaw, comtypes, pyperclip |

---

## üì¶ Dependencies

### Backend (Python)

| Package | Version | Purpose |
|:--------|:--------|:--------|
| `fastapi` | Latest | Web framework |
| `uvicorn` | Latest | ASGI server |
| `websockets` | Latest | WebSocket support |
| `anthropic` | Latest | Claude API client |
| `fireworks-ai` | Latest | Fireworks API client |
| `openai-whisper` | Latest | Local STT |
| `edge-tts` | Latest | Neural TTS (free) |
| `pyautogui` | ‚â•0.9.54 | Mouse & keyboard |
| `pywin32` | ‚â•306 | Windows APIs |
| `mss` | ‚â•9.0 | Screen capture |
| `Pillow` | ‚â•10.0 | Image processing |
| `chromadb` | Latest | Vector database |
| `psutil` | Latest | System monitoring |
| `pycaw` | Latest | Volume control |
| `plyer` | ‚â•2.1 | Notifications |
| `comtypes` | ‚â•1.2 | COM interface |
| `pyperclip` | ‚â•1.8 | Clipboard |
| `sounddevice` | ‚â•0.5 | Audio I/O |
| `pyttsx3` | Latest | Fallback TTS |

### Frontend (Node.js)

| Package | Version | Purpose |
|:--------|:--------|:--------|
| `react` | 18.x | UI framework |
| `react-dom` | 18.x | DOM rendering |
| `socket.io-client` | Latest | WebSocket client |

### Optional (Cloud)

| Package | Purpose |
|:--------|:--------|
| `google-cloud-compute` | GCP Spot VMs |
| `google-cloud-sql` | Cloud SQL profiles |
| `speechbrain` | ECAPA local (GPU) |
| `torchaudio` | Audio ML (GPU) |
| `docker` | Docker ECAPA |

---

## ü§ù Contributing

Contributions are welcome! This is an active Windows port with ongoing improvements.

### How to Contribute

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feat/your-feature`
3. **Commit**: `git commit -m "feat: add your feature"`
4. **Push**: `git push origin feat/your-feature`
5. **Open** a Pull Request

### Priority Areas

| Area | Description | Difficulty |
|:-----|:------------|:-----------|
| üß™ Testing | Add Windows-specific tests | Medium |
| üì¢ Notifications | Enhanced toast with action buttons | Easy |
| üîä Audio | WASAPI native capture | Hard |
| ü§ñ Automation | Browser profile management | Medium |
| üì± System Tray | `pystray` integration | Easy |
| üåê i18n | Multi-language support | Medium |

### Code Style

- Python: Follow PEP 8, use type hints
- Cross-platform: Always use `sys.platform == "win32"` guards
- Async: Use `async/await` for all I/O operations
- Logging: Use `logging` module, never `print()` in production

---

## üìú Credits & Attribution

<div align="center">

| Role | Credit |
|:-----|:-------|
| **Original JARVIS** | [Derek Russell (drussell23)](https://github.com/drussell23) ‚Äî [JARVIS](https://github.com/drussell23/JARVIS) |
| **Windows Port** | [Nandkishor Rathod](https://github.com/nandkishorrathodk-art) |
| **Neural TTS** | Microsoft Azure ‚Äî `en-GB-RyanNeural` via [edge-tts](https://github.com/rany2/edge-tts) |
| **LLM** | [Anthropic Claude](https://anthropic.com) + [Fireworks AI](https://fireworks.ai) |
| **STT** | [OpenAI Whisper](https://github.com/openai/whisper) |
| **Voice Biometrics** | [SpeechBrain ECAPA-TDNN](https://speechbrain.github.io) |
| **Vision** | [mss](https://github.com/BoboTiG/python-mss) + [Pillow](https://pillow.readthedocs.io) |

</div>

---

## üìÑ License

**MIT License** ‚Äî see [LICENSE](LICENSE)

Original JARVIS project by [drussell23](https://github.com/drussell23). 
Windows port and modifications by [Nandkishor Rathod](https://github.com/nandkishorrathodk-art) (2026).

```
MIT License

Copyright (c) 2026 Nandkishor Rathod

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software...
```

---

<div align="center">

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

### ‚ö° Ironcliw-AI ¬∑ JARVIS

**Built with ‚ù§Ô∏è by [Nandkishor Rathod](https://github.com/nandkishorrathodk-art)**

*"I am Iron Man."* ‚Äî Tony Stark

<br/>

[![GitHub](https://img.shields.io/badge/GitHub-nandkishorrathodk--art-181717?style=for-the-badge&logo=github)](https://github.com/nandkishorrathodk-art)
[![Repo](https://img.shields.io/badge/Repo-Ironcliw--ai-0078D4?style=for-the-badge&logo=github)](https://github.com/nandkishorrathodk-art/Ironcliw-ai)

<img src="https://user-images.githubusercontent.com/73097560/115834477-dbab4500-a447-11eb-908a-139a6edaec5c.gif" width="100%">

</div>
