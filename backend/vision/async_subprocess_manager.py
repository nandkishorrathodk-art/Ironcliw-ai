"""
Robust Async Subprocess Manager for Ironcliw Vision System

This module provides a production-ready async subprocess management system with:
- Resource pooling and limits
- Automatic cleanup and lifecycle management
- Comprehensive error handling
- Semaphore leak prevention
- Process tracking and monitoring

The module is designed to handle subprocess execution safely across different
platforms, with special considerations for macOS to prevent segmentation faults
and resource leaks.

Example:
    >>> manager = get_subprocess_manager()
    >>> async with manager.subprocess(['echo', 'hello']) as proc:
    ...     pass
    >>> print(proc.stdout)
    b'hello\\n'
"""

import asyncio
import atexit
import logging
import multiprocessing
import os
import platform
import signal
import time
from collections import deque
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Set, Tuple

import psutil

# Set fork safety for macOS to prevent segmentation faults
if platform.system() == "Darwin":
    # Set environment variable for fork safety
    os.environ["OBJC_DISABLE_INITIALIZE_FORK_SAFETY"] = "YES"
    # Try to set multiprocessing start method if not already set
    try:
        multiprocessing.set_start_method("spawn", force=False)
    except RuntimeError:
        # Already set, that's fine
        pass

logger = logging.getLogger(__name__)


class ProcessState(Enum):
    """Process lifecycle states for tracking subprocess execution.
    
    Attributes:
        PENDING: Process is queued but not yet started
        RUNNING: Process is currently executing
        COMPLETED: Process finished successfully
        FAILED: Process failed with an error
        TERMINATED: Process was terminated by signal
        TIMEOUT: Process was killed due to timeout
    """

    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    TERMINATED = "terminated"
    TIMEOUT = "timeout"


@dataclass
class ProcessInfo:
    """Information about a managed subprocess.
    
    This class tracks the complete lifecycle of a subprocess including
    timing, output, and error information.
    
    Attributes:
        process_id: Unique identifier for the process
        command: Command line arguments that were executed
        state: Current state of the process
        process: The asyncio subprocess object (if running)
        start_time: Unix timestamp when process started
        end_time: Unix timestamp when process ended (if completed)
        stdout: Captured standard output (if capture_output=True)
        stderr: Captured standard error (if capture_output=True)
        return_code: Process exit code (if completed)
        error: Error message (if failed)
        pid: System process ID (if running)
    """

    process_id: str
    command: List[str]
    state: ProcessState
    process: Optional[asyncio.subprocess.Process] = None
    start_time: float = field(default_factory=time.time)
    end_time: Optional[float] = None
    stdout: Optional[bytes] = None
    stderr: Optional[bytes] = None
    return_code: Optional[int] = None
    error: Optional[str] = None
    pid: Optional[int] = None


class AsyncSubprocessManager:
    """
    Advanced async subprocess manager with resource management and cleanup.

    This class provides a robust, production-ready system for managing
    asynchronous subprocesses with automatic resource cleanup, process
    tracking, and comprehensive error handling.

    Features:
    - Subprocess pooling with configurable limits
    - Automatic cleanup on shutdown
    - Process lifecycle tracking
    - Memory and resource monitoring
    - Comprehensive error handling
    - Semaphore leak prevention
    - Platform-specific optimizations (especially macOS)

    Attributes:
        max_concurrent: Maximum number of concurrent subprocesses
        max_queue_size: Maximum number of queued subprocess requests
        process_timeout: Default timeout for subprocess execution
        cleanup_interval: Interval between cleanup cycles
        enable_monitoring: Whether resource monitoring is enabled

    Example:
        >>> manager = AsyncSubprocessManager(max_concurrent=3)
        >>> async with manager.subprocess(['echo', 'test']) as proc:
        ...     pass
        >>> print(proc.return_code)
        0
    """

    _instance = None
    _initialized = False

    def __new__(cls):
        """Singleton pattern to ensure single manager instance.
        
        Returns:
            The singleton AsyncSubprocessManager instance
        """
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(
        self,
        max_concurrent: int = 5,
        max_queue_size: int = 50,
        process_timeout: float = 30.0,
        cleanup_interval: float = 10.0,
        enable_monitoring: bool = True,
    ):
        """
        Initialize the subprocess manager.

        Args:
            max_concurrent: Maximum concurrent subprocesses (reduced on macOS)
            max_queue_size: Maximum queued subprocess requests
            process_timeout: Default timeout for processes in seconds
            cleanup_interval: Interval for cleanup tasks in seconds
            enable_monitoring: Enable resource monitoring and logging

        Note:
            On macOS, max_concurrent is automatically limited to 3 to prevent
            system instability and segmentation faults.
        """
        if self._initialized:
            return

        # Reduce concurrent subprocesses on macOS for safety
        if platform.system() == "Darwin":
            max_concurrent = min(max_concurrent, 3)
            logger.info(f"macOS detected - limiting concurrent subprocesses to {max_concurrent}")

        self.max_concurrent = max_concurrent
        self.max_queue_size = max_queue_size
        self.process_timeout = process_timeout
        self.cleanup_interval = cleanup_interval
        self.enable_monitoring = enable_monitoring

        # Process tracking
        self._processes: Dict[str, ProcessInfo] = {}
        self._active_processes: Set[str] = set()
        self._process_queue: deque = deque(maxlen=max_queue_size)

        # Resource management
        self._semaphore = asyncio.Semaphore(max_concurrent)
        self._shutdown_event = asyncio.Event()
        self._cleanup_task: Optional[asyncio.Task] = None

        # Statistics
        self._stats = {
            "total_started": 0,
            "total_completed": 0,
            "total_failed": 0,
            "total_timeout": 0,
            "total_terminated": 0,
            "peak_concurrent": 0,
        }

        # Register cleanup handlers
        atexit.register(self._sync_cleanup)
        self._setup_signal_handlers()

        # Start background tasks
        self._start_background_tasks()

        self._initialized = True
        logger.info(f"AsyncSubprocessManager initialized: max_concurrent={max_concurrent}")

    def _setup_signal_handlers(self):
        """Setup signal handlers for graceful shutdown.
        
        Registers handlers for SIGTERM and SIGINT to ensure proper cleanup
        when the application is terminated.
        """

        def signal_handler(signum, frame):
            """Handle shutdown signals by initiating async shutdown."""
            logger.info(f"Received signal {signum}, initiating shutdown...")
            asyncio.create_task(self.shutdown())

        for sig in [signal.SIGTERM, signal.SIGINT]:
            signal.signal(sig, signal_handler)

    def _start_background_tasks(self):
        """Start background cleanup and monitoring tasks.
        
        Creates the cleanup loop task if it doesn't exist or has completed.
        """
        if self._cleanup_task is None or self._cleanup_task.done():
            self._cleanup_task = asyncio.create_task(self._cleanup_loop())

    async def _cleanup_loop(self):
        """Background task for periodic cleanup and monitoring.
        
        Runs continuously until shutdown, performing:
        - Dead process cleanup
        - Resource usage monitoring
        - Statistics logging
        
        Raises:
            asyncio.CancelledError: When the task is cancelled during shutdown
        """
        while not self._shutdown_event.is_set():
            try:
                await asyncio.sleep(self.cleanup_interval)
                await self._cleanup_dead_processes()

                if self.enable_monitoring:
                    self._log_resource_usage()

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in cleanup loop: {e}")

    async def _cleanup_dead_processes(self):
        """Clean up terminated processes and free resources.
        
        Removes processes that have been completed for more than 60 seconds
        to prevent memory leaks while maintaining recent process history.
        """
        to_remove = []

        for process_id, info in self._processes.items():
            if info.state in [
                ProcessState.COMPLETED,
                ProcessState.FAILED,
                ProcessState.TERMINATED,
                ProcessState.TIMEOUT,
            ]:
                # Process is done, check if it's been long enough to clean up
                if info.end_time and (time.time() - info.end_time) > 60:
                    to_remove.append(process_id)
            elif info.process and info.process.returncode is not None:
                # Process terminated but not yet marked
                info.state = ProcessState.COMPLETED
                info.end_time = time.time()
                info.return_code = info.process.returncode
                self._active_processes.discard(process_id)

        # Remove old processes
        for process_id in to_remove:
            del self._processes[process_id]
            logger.debug(f"Cleaned up old process: {process_id}")

    def _log_resource_usage(self):
        """Log current resource usage for monitoring.
        
        Logs memory usage, file descriptor count, and process statistics
        for debugging and monitoring purposes.
        """
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            num_fds = process.num_fds() if hasattr(process, "num_fds") else 0

            logger.debug(
                f"Subprocess Manager Stats: "
                f"active={len(self._active_processes)}/{self.max_concurrent}, "
                f"tracked={len(self._processes)}, "
                f"memory={memory_mb:.1f}MB, "
                f"fds={num_fds}"
            )
        except Exception as e:
            logger.debug(f"Could not get resource usage: {e}")

    @asynccontextmanager
    async def subprocess(
        self,
        command: List[str],
        timeout: Optional[float] = None,
        cwd: Optional[str] = None,
        env: Optional[Dict[str, str]] = None,
        capture_output: bool = True,
        **kwargs,
    ):
        """
        Context manager for running a subprocess with automatic cleanup.

        This is the primary interface for executing subprocesses. It handles
        resource allocation, process lifecycle management, and automatic cleanup.

        Args:
            command: Command and arguments to execute
            timeout: Process timeout in seconds (uses default if None)
            cwd: Working directory for the process
            env: Environment variables (inherits current env if None)
            capture_output: Whether to capture stdout/stderr
            **kwargs: Additional arguments passed to create_subprocess_exec

        Yields:
            ProcessInfo: Object containing process details and results

        Raises:
            Exception: Any error that occurs during process execution

        Example:
            >>> async with manager.subprocess(['ls', '-la']) as proc:
            ...     pass
            >>> print(proc.stdout.decode())
            total 0
            drwxr-xr-x  2 user user  60 Jan  1 12:00 .
        """
        process_id = f"proc_{time.time()}_{id(command)}"
        timeout = timeout or self.process_timeout
        info = ProcessInfo(process_id=process_id, command=command, state=ProcessState.PENDING)

        async with self._semaphore:
            try:
                # Track process
                self._processes[process_id] = info
                self._active_processes.add(process_id)
                self._stats["total_started"] += 1

                # Update peak concurrent
                current_concurrent = len(self._active_processes)
                if current_concurrent > self._stats["peak_concurrent"]:
                    self._stats["peak_concurrent"] = current_concurrent

                # Prepare subprocess arguments
                kwargs.update(
                    {
                        "stdout": asyncio.subprocess.PIPE if capture_output else None,
                        "stderr": asyncio.subprocess.PIPE if capture_output else None,
                        "cwd": cwd,
                        "env": env or os.environ.copy(),
                    }
                )

                # Only use start_new_session on non-macOS systems to avoid segfaults
                if platform.system() != "Darwin":
                    kwargs["start_new_session"] = True

                # Create subprocess
                info.process = await asyncio.create_subprocess_exec(*command, **kwargs)
                info.pid = info.process.pid
                info.state = ProcessState.RUNNING

                logger.debug(
                    f"Started subprocess {process_id}: {' '.join(command)} (PID: {info.pid})"
                )

                # Yield control to caller
                yield info

                # Wait for completion with timeout
                try:
                    stdout, stderr = await asyncio.wait_for(
                        info.process.communicate(), timeout=timeout
                    )
                    info.stdout = stdout
                    info.stderr = stderr
                    info.return_code = info.process.returncode
                    info.state = ProcessState.COMPLETED
                    info.end_time = time.time()
                    self._stats["total_completed"] += 1

                    # Wait for process to fully terminate
                    if info.process.returncode is None:
                        await info.process.wait()

                    logger.debug(f"Subprocess {process_id} completed with code {info.return_code}")

                except asyncio.TimeoutError:
                    logger.warning(f"Subprocess {process_id} timed out after {timeout}s")
                    info.state = ProcessState.TIMEOUT
                    info.error = f"Process timed out after {timeout} seconds"
                    self._stats["total_timeout"] += 1
                    await self._terminate_process(info)

            except Exception as e:
                logger.error(f"Error running subprocess {process_id}: {e}")
                info.state = ProcessState.FAILED
                info.error = str(e)
                info.end_time = time.time()
                self._stats["total_failed"] += 1

                if info.process:
                    await self._terminate_process(info)

                raise

            finally:
                # Clean up
                self._active_processes.discard(process_id)

                # Ensure process is terminated and pipes are closed
                if info.process:
                    if info.process.returncode is None:
                        await self._terminate_process(info)

                    # Always close pipes to prevent semaphore leaks
                    try:
                        if info.process.stdout and not info.process.stdout.is_closing():
                            info.process.stdout.close()
                        if info.process.stderr and not info.process.stderr.is_closing():
                            info.process.stderr.close()
                        if info.process.stdin and not info.process.stdin.is_closing():
                            info.process.stdin.close()
                    except Exception as e:
                        logger.debug(f"Error closing pipes for {process_id}: {e}")

    async def _terminate_process(self, info: ProcessInfo):
        """Terminate a process gracefully, then forcefully if needed.
        
        Attempts graceful termination first (SIGTERM), then force kills
        (SIGKILL) if the process doesn't respond within 5 seconds.
        
        Args:
            info: ProcessInfo object containing the process to terminate
        """
        if not info.process:
            return

        try:
            # Try graceful termination first
            info.process.terminate()

            try:
                await asyncio.wait_for(info.process.wait(), timeout=5.0)
                logger.debug(f"Process {info.process_id} terminated gracefully")
            except asyncio.TimeoutError:
                # Force kill if graceful termination fails
                try:
                    info.process.kill()
                    await info.process.wait()
                    logger.warning(f"Process {info.process_id} force killed")
                except ProcessLookupError:
                    pass  # Process already dead

            # Close all pipes after termination
            try:
                if info.process.stdout and not info.process.stdout.is_closing():
                    info.process.stdout.close()
                if info.process.stderr and not info.process.stderr.is_closing():
                    info.process.stderr.close()
                if info.process.stdin and not info.process.stdin.is_closing():
                    info.process.stdin.close()
            except Exception as e:
                logger.debug(f"Error closing pipes during termination: {e}")

            info.state = ProcessState.TERMINATED
            info.end_time = time.time()
            self._stats["total_terminated"] += 1

        except Exception as e:
            logger.error(f"Error terminating process {info.process_id}: {e}")

    async def run_command(
        self, command: List[str], timeout: Optional[float] = None, check: bool = False, **kwargs
    ) -> Tuple[int, Optional[bytes], Optional[bytes]]:
        """
        Run a command and return results.

        Convenience method for executing a subprocess and getting its results
        without needing to use the context manager directly.

        Args:
            command: Command and arguments to execute
            timeout: Process timeout in seconds
            check: If True, raise CalledProcessError for non-zero exit codes
            **kwargs: Additional arguments passed to subprocess()

        Returns:
            Tuple containing (return_code, stdout, stderr)

        Raises:
            subprocess.CalledProcessError: If check=True and process fails

        Example:
            >>> code, stdout, stderr = await manager.run_command(['echo', 'hello'])
            >>> print(code, stdout)
            0 b'hello\\n'
        """
        async with self.subprocess(command, timeout=timeout, **kwargs) as info:
            pass  # Process runs in context manager

        if check and info.return_code != 0:
            from subprocess import CalledProcessError

            raise CalledProcessError(
                info.return_code or -1, command, output=info.stdout, stderr=info.stderr
            )

        return info.return_code or 0, info.stdout, info.stderr

    async def shutdown(self, timeout: float = 10.0):
        """
        Shutdown the subprocess manager and clean up all resources.

        Performs graceful shutdown by:
        1. Signaling shutdown to background tasks
        2. Cancelling cleanup tasks
        3. Terminating all active processes
        4. Logging final statistics
        5. Clearing all tracking data

        Args:
            timeout: Maximum time to wait for processes to terminate

        Example:
            >>> await manager.shutdown()
            INFO:Shutting down AsyncSubprocessManager...
            INFO:AsyncSubprocessManager shutdown complete
        """
        logger.info("Shutting down AsyncSubprocessManager...")

        # Signal shutdown
        self._shutdown_event.set()

        # Cancel background tasks
        if self._cleanup_task and not self._cleanup_task.done():
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass

        # Terminate all active processes
        tasks = []
        for process_id in list(self._active_processes):
            if process_id in self._processes:
                info = self._processes[process_id]
                tasks.append(self._terminate_process(info))

        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)

        # Log final statistics
        logger.info(
            f"Subprocess Manager Statistics: "
            f"started={self._stats['total_started']}, "
            f"completed={self._stats['total_completed']}, "
            f"failed={self._stats['total_failed']}, "
            f"timeout={self._stats['total_timeout']}, "
            f"terminated={self._stats['total_terminated']}, "
            f"peak_concurrent={self._stats['peak_concurrent']}"
        )

        # Clear all tracking
        self._processes.clear()
        self._active_processes.clear()

        logger.info("AsyncSubprocessManager shutdown complete")

    def _sync_cleanup(self):
        """Synchronous cleanup for atexit handler.
        
        This method is called automatically when the Python interpreter
        exits to ensure all subprocesses are properly terminated.
        """
        try:
            # Try to get the event loop
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                # No running loop, try to get or create one
                try:
                    loop = asyncio.get_event_loop()
                except RuntimeError:
                    # Create a new event loop if none exists
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)

            if not loop.is_closed():
                # Run async shutdown
                loop.run_until_complete(self.shutdown())
        except Exception as e:
            logger.error(f"Error in sync cleanup: {e}")
            # Force kill any remaining processes
            for info in self._processes.values():
                if info.process and info.pid:
                    try:
                        os.kill(info.pid, signal.SIGKILL)
                    except (ProcessLookupError, PermissionError):
                        pass

    def get_statistics(self) -> Dict[str, Any]:
        """Get current statistics about subprocess execution.
        
        Returns:
            Dictionary containing execution statistics including:
            - total_started: Total processes started
            - total_completed: Total processes completed successfully
            - total_failed: Total processes that failed
            - total_timeout: Total processes that timed out
            - total_terminated: Total processes terminated by signal
            - peak_concurrent: Maximum concurrent processes reached
            - active_processes: Currently active processes
            - tracked_processes: Total tracked processes
            - queue_size: Current queue size

        Example:
            >>> stats = manager.get_statistics()
            >>> print(f"Success rate: {stats['total_completed'] / stats['total_started']:.2%}")
        """
        return {
            **self._stats,
            "active_processes": len(self._active_processes),
            "tracked_processes": len(self._processes),
            "queue_size": len(self._process_queue),
        }

    def get_active_processes(self) -> List[ProcessInfo]:
        """Get list of currently active processes.
        
        Returns:
            List of ProcessInfo objects for all currently running processes

        Example:
            >>> active = manager.get_active_processes()
            >>> for proc in active:
            ...     print(f"PID {proc.pid}: {' '.join(proc.command)}")
        """
        return [
            info
            for process_id, info in self._processes.items()
            if process_id in self._active_processes
        ]


# Global singleton instance
_subprocess_manager: Optional[AsyncSubprocessManager] = None


def get_subprocess_manager() -> AsyncSubprocessManager:
    """Get or create the global subprocess manager instance.
    
    Returns the singleton AsyncSubprocessManager instance, creating it
    if it doesn't exist. This ensures a single manager is used throughout
    the application.
    
    Returns:
        The global AsyncSubprocessManager instance

    Example:
        >>> manager = get_subprocess_manager()
        >>> async with manager.subprocess(['echo', 'test']) as proc:
        ...     pass
    """
    global _subprocess_manager
    if _subprocess_manager is None:
        _subprocess_manager = AsyncSubprocessManager()
    return _subprocess_manager


# Convenience function for simple command execution
async def run_command(
    command: List[str], timeout: Optional[float] = None, **kwargs
) -> Tuple[int, Optional[bytes], Optional[bytes]]:
    """
    Convenience function to run a command using the global manager.

    This is a simple wrapper around the global subprocess manager's
    run_command method for easy subprocess execution.

    Args:
        command: Command and arguments to execute
        timeout: Process timeout in seconds
        **kwargs: Additional arguments passed to the subprocess manager

    Returns:
        Tuple containing (return_code, stdout, stderr)

    Example:
        >>> code, stdout, stderr = await run_command(['echo', 'hello world'])
        >>> print(stdout.decode().strip())
        hello world
    """
    manager = get_subprocess_manager()
    return await manager.run_command(command, timeout=timeout, **kwargs)