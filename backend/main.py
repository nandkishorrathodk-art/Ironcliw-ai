#!/usr/bin/env python3
"""
JARVIS AI Backend - Optimized Main Entry Point with Advanced Intelligence
v17.8.0 - PRD v2.0 Voice Biometric Intelligence Edition

This backend loads 10 critical components that power the JARVIS AI system:

1. CHATBOTS (Claude Vision Chatbot)
   - Powers conversational AI with Claude 3.5 Sonnet
   - Enables screen analysis and vision capabilities
   - Handles all natural language understanding and generation

2. VISION (Screen Capture & Multi-Space Analysis + YOLOv8)
   - Real-time screen monitoring with Swift-based capture
   - Video streaming at 30 FPS with purple recording indicator
   - Multi-Space Desktop Vision: Monitors all macOS desktop spaces
   - YOLOv8 Integration: Real-time UI element detection (10-20x faster than Claude)
     * 5 model sizes from nano (3MB) to xlarge (68MB)
     * Detects 20+ UI types: buttons, icons, Control Center, TV UI, monitors
     * Free after download, real-time capable (10-20 FPS)
   - Hybrid YOLO-Claude Vision: Intelligent task routing
     * YOLO-first for UI detection (fast, accurate, free)
     * Claude for OCR and complex semantic analysis
     * Hybrid mode for comprehensive understanding
   - Enhanced Window Detection with UI element tracking
   - Multi-Monitor Layout Detection with vision-based awareness
   - Computer vision analysis for understanding screen content
   - Integration Architecture: 9-stage processing pipeline with dynamic memory management
   - Bloom Filter Network: Hierarchical duplicate detection system
   - Predictive Engine: State-based prediction with Markov chains
   - Semantic Cache LSH: Intelligent caching with locality-sensitive hashing
   - Quadtree Spatial Intelligence: Optimized region-based processing
   - VSMS Core: Visual State Management System with scene understanding

3. MEMORY (M1 Mac Optimized Memory Manager)
   - Prevents memory leaks and manages resources efficiently
   - Critical for long-running sessions and video processing
   - Provides memory pressure alerts and automatic cleanup
   - Integration with Orchestrator for dynamic component allocation

4. VOICE (JARVIS Voice Interface with CoreML Acceleration)
   - Voice activation with "Hey JARVIS" wake word
   - CoreML Voice Engine: Hardware-accelerated VAD on Apple Neural Engine
     * 232KB model (4-bit quantized Silero VAD)
     * <10ms inference latency
     * ~5-10MB runtime memory
     * Zero CPU usage (runs on Neural Engine)
   - Text-to-speech with multiple voice options
   - Real-time voice command processing
   - Adaptive threshold learning for improved accuracy

5. ML_MODELS (Machine Learning Models)
   - Sentiment analysis and NLP capabilities
   - Lazy-loaded to optimize startup time
   - Powers intelligent text understanding

6. MONITORING (System Health & Metrics)
   - Tracks API performance and resource usage
   - Provides health checks and status endpoints
   - Essential for production stability
   - Integration metrics tracking for all vision components

7. VOICE UNLOCK (PRD v2.0 BEAST MODE Multi-Modal Biometric Authentication)
   - Advanced probabilistic voice biometric authentication for macOS
   - PRD v2.0 Voice Biometric Intelligence (NEW!):
     * ML Fine-Tuning: AAM-Softmax + Center Loss + Triplet Loss
     * Score Calibration: Platt Scaling (30+ samples), Isotonic (100+ samples)
     * Adaptive Thresholds: Auto-adjusts toward 90%/95%/98% targets
     * Comprehensive Anti-Spoofing: Replay, Synthesis, Voice Conversion, Environmental
   - Multi-Modal Verification: 5 independent biometric signals
     * Deep learning embeddings (ECAPA-TDNN 192D)
     * Mahalanobis distance (statistical with adaptive covariance)
     * Acoustic features (pitch, formants, spectral analysis)
     * Physics-based validation (vocal tract constraints, harmonics)
     * Anti-spoofing detection (replay, synthesis, voice conversion)
   - Cloud SQL storage: 50+ acoustic features per speaker profile
   - PostgreSQL database via GCP Cloud SQL proxy (127.0.0.1:5432)
   - Bayesian confidence with uncertainty quantification
   - Adaptive threshold learning (zero hardcoded values)
   - Personalized responses using verified speaker identity
   - Screensaver and system integration

8. WAKE WORD (Hands-free Activation)
   - "Hey JARVIS" wake word detection
   - Always-listening mode with zero button clicks
   - Multi-engine detection (Porcupine, Vosk, WebRTC)
   - Adaptive sensitivity and anti-spoofing
   - Customizable wake words and responses

9. DISPLAY MONITOR (External Display Management)
   - Automatic AirPlay/external display detection
   - Multi-method detection (AppleScript, CoreGraphics, Yabai)
   - Voice announcements for display availability
   - Smart caching for 3-5x performance improvement
   - Auto-connect or prompt modes
   - Living Room TV monitoring (configurable)
   - Zero hardcoding - fully configuration-driven
   - Event-driven callbacks for custom integrations

10. INTELLIGENCE STACK (UAE + SAI + Learning Database) - ADVANCED! üß†
   - UAE (Unified Awareness Engine): Context intelligence + decision fusion
   - SAI (Situational Awareness): Real-time UI monitoring (10s interval)
   - Learning Database: Persistent memory with async SQLite + ChromaDB
   - Predictive Intelligence: Learns patterns and predicts actions
   - Cross-Session Memory: Remembers across restarts
   - Temporal Pattern Recognition: Time-based behavior learning
   - Self-Healing: Adapts automatically to environment changes
   - Confidence-Weighted Decisions: Fuses context + real-time perception
   - Zero Hardcoding: Fully dynamic, learns everything
   - Capabilities:
     * Learns user patterns across macOS workspace
     * Predicts actions before you ask (proactive)
     * Adapts to UI changes automatically (reactive)
     * Remembers preferences forever (persistent)
     * Gets smarter over time (continuous learning)

11. NEURAL MESH (v9.4 Production Multi-Agent System) - NEW! üï∏Ô∏è
   - Distributed Intelligence Coordination: 60+ specialized agents working in parallel
   - Production-Grade Architecture: 4-tier hierarchy (Foundation ‚Üí Core ‚Üí Advanced ‚Üí Specialized)
   - Knowledge Graph: Shared semantic memory across all agents
   - Communication Bus: Real-time event-driven messaging (10,000 msg/s capacity)
   - Multi-Agent Orchestration: Complex task decomposition and agent collaboration
   - JARVIS Bridge: Connects all JARVIS systems (Main, Prime, Reactor Core)
   - Health Monitoring: Continuous health checks and auto-recovery
   - Google Workspace Agent (v2.0 - Chief of Staff):
     * Three-Tier Waterfall: Google API ‚Üí macOS Local ‚Üí Computer Use
     * Gmail Integration: Read, search, compose, send emails via natural language
     * Calendar Management: Schedule meetings, check availability, manage events
     * Google Drive: Access and manage documents with full context awareness
     * Natural Language Interface: "Check my emails", "Schedule meeting tomorrow at 2pm"
     * Smart Delegation: Automatically routes to best available method
     * Voice Announcements: Real-time status updates during startup
   - Agent Types: GoogleWorkspace, SOP Enforcer, Repository Intelligence, Infrastructure
   - Voice Integration: Intelligent narrator announces agent registration and status
   - Async/Parallel: All operations non-blocking for maximum performance

All 11 components must load successfully for full JARVIS functionality.
The system uses parallel imports to reduce startup time from ~20s to ~7-9s.

Enhanced Vision Features (v13.3.1):
- Integration Orchestrator with 1.2GB memory budget
- 4 operating modes: Normal, Pressure, Critical, Emergency
- Cross-language optimization: Python, Rust, Swift
- Intelligent component coordination based on system resources
- Proactive Vision Intelligence System with real-time monitoring
- Multi-Space Desktop Vision: Sees across all macOS desktop spaces
- Smart Space Queries: "Where is Cursor IDE?", "What's on Desktop 2?"
- Debugging Assistant: Auto-detects code errors and syntax issues
- Research Helper: Monitors multi-tab research workflows
- Workflow Optimizer: Identifies repetitive patterns and suggests optimizations
- Privacy Protection: Auto-pauses during sensitive content (passwords, banking)
- Natural Voice Communication: Speaks suggestions and warnings naturally

Browser Automation Features (v13.4.0):
- Natural Language Browser Control: "Open Safari and go to Google"
- Chained Commands: "Open a new tab and search for weather"
- Dynamic Browser Discovery: Controls any browser without hardcoding
- Smart Context: Remembers which browser you're using between commands
- Type & Search: "Type python tutorials and press enter"
- Tab Management: "Open another tab", "Open a new tab in Chrome"
- Cross-Browser Support: Safari, Chrome, Firefox, and others
- AppleScript Integration: Native macOS browser control

Startup Narrator Voice Announcements (v6.2):
- Intelligent Voice Feedback: Real-time spoken status updates during initialization
- Security Milestones: Announces two-tier security, VBIA, visual threat detection
- Neural Mesh Status: Coordinator online, agent registration, Google Workspace ready
- Cross-Repo Integration: Announces when JARVIS, Prime, and Reactor Core connect
- Adaptive Pacing: 2-3 second intervals, non-blocking, doesn't slow startup
- Environment-Aware: Dynamic announcements based on visual security settings
- Example Announcements:
  * "Initializing two-tier security architecture."
  * "Neural Mesh coordinator online."
  * "Google Workspace Agent registered. Gmail, Calendar, and Drive ready."
  * "Voice biometric authentication ready. Visual threat detection enabled."
  * "Cross-repository integration complete. Intelligence shared across all platforms."
  * "Neural Mesh fully operational. 60 agents coordinated."
- Configuration: Enable/disable via STARTUP_NARRATOR_VOICE environment variable
"""

# =============================================================================
# CRITICAL: PYTHON 3.9 COMPATIBILITY PATCH - MUST BE FIRST!
# =============================================================================
# This MUST happen BEFORE any module that imports google-api-core or other
# packages that use importlib.metadata.packages_distributions() which was
# added in Python 3.10. Without this patch, Python 3.9 users see:
#   "module 'importlib.metadata' has no attribute 'packages_distributions'"
#
# The patch adds a fallback implementation for packages_distributions().
# =============================================================================
import sys as _sys
if _sys.version_info < (3, 10):
    try:
        from importlib import metadata as _metadata
        if not hasattr(_metadata, 'packages_distributions'):
            # Create minimal fallback that returns empty mapping
            # Full implementation is in utils/python39_compat.py if needed
            def _packages_distributions_fallback():
                """Minimal fallback for packages_distributions on Python 3.9."""
                try:
                    # Try importlib_metadata backport first
                    import importlib_metadata as _backport
                    if hasattr(_backport, 'packages_distributions'):
                        return _backport.packages_distributions()
                except ImportError:
                    pass
                # Return empty mapping as last resort
                return {}
            _metadata.packages_distributions = _packages_distributions_fallback
    except Exception:
        pass  # Silently fail if we can't patch

# =============================================================================
# CRITICAL: Fork-Safety Configuration - MUST BE FIRST
# =============================================================================
# On macOS, fork() in a multi-threaded process causes crashes:
#   "multi-threaded process forked"
#   "BUG IN CLIENT OF LIBDISPATCH: trying to lock recursively"
#
# Root cause: fork() only copies the calling thread, but locks from other
# threads are copied in a locked state, causing deadlock in the child process.
#
# Solution:
# 1. Set multiprocessing to use 'spawn' instead of 'fork' (uses posix_spawn)
# 2. Set environment variable to disable Objective-C fork safety checks
# 3. Use asyncio.create_subprocess_exec() instead of subprocess.Popen
#    (asyncio uses posix_spawn which is fork-safe)
#
# This MUST happen before ANY other imports that might spawn threads or processes.
# =============================================================================
import multiprocessing
import os
import sys

# Set spawn mode IMMEDIATELY - before anything else can start threads/processes
if sys.platform == "darwin":
    # Must be called before any other multiprocessing usage
    try:
        multiprocessing.set_start_method("spawn", force=True)
    except RuntimeError:
        pass  # Already set

    # Additional fork-safety environment variables for macOS
    os.environ["OBJC_DISABLE_INITIALIZE_FORK_SAFETY"] = "YES"

    # Disable various library threading that can cause fork issues
    os.environ["OMP_NUM_THREADS"] = "1"  # OpenMP
    os.environ["MKL_NUM_THREADS"] = "1"  # Intel MKL
    os.environ["OPENBLAS_NUM_THREADS"] = "1"  # OpenBLAS
    os.environ["VECLIB_MAXIMUM_THREADS"] = "1"  # macOS Accelerate

import subprocess

# =============================================================================
# CRITICAL: NUMBA JIT DISABLE - MUST BE FIRST BEFORE ANY OTHER IMPORTS
# =============================================================================
# This MUST happen BEFORE any module that might import numba (whisper, librosa,
# scipy with JIT, etc.). Setting it here prevents the circular import error:
#   "cannot import name 'get_hashable_key' from partially initialized module"
#
# The error occurs because:
# 1. Multiple threads try to import numba simultaneously during parallel imports
# 2. Thread A starts importing numba.core.utils
# 3. Thread B also tries to import before Thread A finishes
# 4. Thread B sees a partially initialized module and crashes
#
# By disabling JIT at the very start, we prevent numba from doing the complex
# compilation that causes race conditions. Whisper/librosa work fine without JIT.
# =============================================================================
os.environ["NUMBA_DISABLE_JIT"] = "1"
os.environ["NUMBA_NUM_THREADS"] = "1"
os.environ["NUMBA_THREADING_LAYER"] = "workqueue"

# Set additional platform-specific environment variables
if sys.platform == "darwin":  # macOS specific
    # Note: OBJC_DISABLE_INITIALIZE_FORK_SAFETY already set in fork-safety section above
    os.environ["PYTHONUNBUFFERED"] = "1"

# Enable HuggingFace/Transformers offline mode to prevent network timeouts
# Models are already cached locally - no need for network requests
# This prevents "ReadTimeoutError" when checking for model updates
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"
os.environ["HF_DATASETS_OFFLINE"] = "1"
os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"  # Also disable telemetry

# =============================================================================
# CRITICAL: Python 3.9 Compatibility Patch - MUST be before ANY package imports
# =============================================================================
# google-api-core and other packages use importlib.metadata.packages_distributions()
# which was added in Python 3.10. This patches it for Python 3.9 compatibility.
# This MUST happen before any imports that might trigger google-api-core or similar.
# =============================================================================
try:
    import importlib.metadata as _metadata
    if not hasattr(_metadata, 'packages_distributions'):
        # Create a fallback implementation for Python 3.9
        def _packages_distributions_fallback():
            """Fallback for packages_distributions() on Python 3.9."""
            from collections import defaultdict
            result = defaultdict(list)
            try:
                for dist in _metadata.distributions():
                    if dist.name:
                        # Map top-level packages to distribution names
                        if dist.files:
                            for file in dist.files:
                                parts = str(file).split('/')
                                if parts and parts[0] and not parts[0].startswith('_'):
                                    top_level = parts[0].replace('.py', '')
                                    if top_level and '.' not in top_level:
                                        result[top_level].append(dist.name)
                        # Also add the distribution name itself
                        result[dist.name.replace('-', '_').lower()].append(dist.name)
            except Exception:
                pass  # Return empty dict on any error
            return dict(result)

        _metadata.packages_distributions = _packages_distributions_fallback
        print("[STARTUP] ‚úÖ Patched importlib.metadata.packages_distributions for Python 3.9")
except Exception as e:
    print(f"[STARTUP] ‚ö†Ô∏è Could not patch importlib.metadata: {e}")

# =============================================================================
# v7.0 FIX: CENTRALIZED numba pre-import - MUST be first before ANY other imports
# =============================================================================
# numba has threading issues when imported from multiple threads simultaneously
# causing: "cannot import name 'get_hashable_key' from partially initialized module"
#
# v7.0 Solution: Use centralized numba_preload module with:
# 1. RLock for recursive import safety
# 2. Blocking wait mechanism (threading.Event) 
# 3. Complete submodule initialization (ALL problematic modules)
# 4. Per-thread tracking to detect races
# 5. Global marker for other modules to check
#
# This MUST happen before parallel_import_components() or any ThreadPoolExecutor
# Other modules (whisper_audio_fix) use wait_for_numba() which BLOCKS until done
#
# v7.0 also adds module-level initialization in whisper_audio_fix.py so that
# by the time any ThreadPoolExecutor worker tries to import whisper, numba is
# already fully initialized in sys.modules with all submodules loaded.
# =============================================================================
_numba_init_success = False
try:
    from core.numba_preload import (
        ensure_numba_initialized, 
        get_numba_status,
        set_numba_bypass_marker
    )
    
    # This blocks until numba is fully initialized in THIS thread (main thread)
    # Other threads that call wait_for_numba() will BLOCK until this completes
    # v7.0: Increased timeout to 60s for slower systems
    _numba_init_success = ensure_numba_initialized(timeout=60.0)
    _numba_status = get_numba_status()
    
    # Set global marker so other modules know initialization was attempted
    set_numba_bypass_marker()
    
    if _numba_init_success:
        submodules = _numba_status.get('submodules_loaded', 0)
        print(f"[STARTUP] ‚úÖ numba {_numba_status['version']} pre-initialized via centralized loader (main thread, {submodules} submodules)")
    elif _numba_status.get('status') == 'not_installed':
        print("[STARTUP] numba not installed (optional)")
    else:
        print(f"[STARTUP] ‚ö†Ô∏è numba pre-initialization issue: {_numba_status.get('error', 'unknown')}")
        
except ImportError:
    # If core.numba_preload doesn't exist yet, fall back to direct import
    print("[STARTUP] numba_preload not available, using direct import")
    try:
        os.environ['NUMBA_DISABLE_JIT'] = '1'
        os.environ['NUMBA_NUM_THREADS'] = '1'
        import numba
        from numba.core import utils as _numba_utils
        if hasattr(_numba_utils, 'get_hashable_key'):
            _ = _numba_utils.get_hashable_key
        print(f"[STARTUP] ‚úÖ numba {numba.__version__} pre-initialized (fallback)")
        os.environ.pop('NUMBA_DISABLE_JIT', None)
        os.environ.pop('NUMBA_NUM_THREADS', None)
        _numba_init_success = True
        os.environ['_JARVIS_NUMBA_INIT_ATTEMPTED'] = '1'
    except ImportError:
        print("[STARTUP] numba not installed (optional)")
        os.environ['_JARVIS_NUMBA_INIT_ATTEMPTED'] = '1'
    except Exception as e:
        print(f"[STARTUP] ‚ö†Ô∏è numba pre-import fallback warning: {e}")
        os.environ['_JARVIS_NUMBA_INIT_ATTEMPTED'] = '1'
except Exception as e:
    print(f"[STARTUP] ‚ö†Ô∏è numba pre-import warning: {e}")
    os.environ['_JARVIS_NUMBA_INIT_ATTEMPTED'] = '1'

# Clean up leaked semaphores from previous runs FIRST
if sys.platform == "darwin":  # macOS specific
    try:
        # Get current user
        user = os.getenv("USER", "")
        if user:
            # Clean up semaphores (macOS xargs doesn't have -r flag)
            result = subprocess.run(
                f"ipcs -s 2>/dev/null | grep {user} | awk '{{print $2}}' | while read id; do ipcrm -s $id 2>/dev/null; done",
                shell=True,
                capture_output=True,
                timeout=5,
            )
            if result.returncode == 0:
                print(f"[STARTUP] Cleaned up leaked semaphores")
    except Exception as e:
        print(f"[STARTUP] Semaphore cleanup warning: {e}")

    # Verify spawn mode is set (already configured at module level above)
    try:
        current_method = multiprocessing.get_start_method()
        if current_method == "spawn":
            print("[STARTUP] ‚úÖ Fork-safe multiprocessing mode: spawn")
        else:
            print(f"[STARTUP] ‚ö†Ô∏è Unexpected multiprocessing mode: {current_method}")
            multiprocessing.set_start_method("spawn", force=True)
    except Exception as e:
        print(f"[STARTUP] Multiprocessing check: {e}")

# Now continue with other imports
import asyncio
import logging
import time
from contextlib import asynccontextmanager
from datetime import datetime
from typing import Optional

# DEBUG: Print location info
print(f"[STARTUP-DEBUG] Running from: {os.path.abspath(__file__)}")
print(f"[STARTUP-DEBUG] Working directory: {os.getcwd()}")
print(f"[STARTUP-DEBUG] Python path: {sys.path[:3]}")  # First 3 entries

# DEBUG: Coordinate diagnostic and PyAutoGUI intercept are now DEFERRED to lifespan
# v4.0: These were blocking module load and preventing fast startup
# They will be run in background after server is serving
_DEFERRED_DEBUG_TASKS = []

def _run_deferred_coordinate_diagnostic():
    """Run coordinate diagnostic in background - deferred from module load"""
    try:
        from pathlib import Path as _DebugPath
        _project_root = _DebugPath(__file__).resolve().parent.parent
        _diag_script = _project_root / "diagnose_coordinate_doubling.py"

        if _diag_script.exists():
            print("[STARTUP-DEBUG] Running coordinate diagnostic (deferred)...")
            exec(_diag_script.read_text())
    except Exception as e:
        print(f"[STARTUP-DEBUG] Coordinate diagnostic failed: {e}")

def _run_deferred_pyautogui_intercept():
    """Install PyAutoGUI intercept in background - deferred from module load"""
    try:
        from pathlib import Path as _IntPath
        _project_root = _IntPath(__file__).resolve().parent.parent

        if str(_project_root) not in sys.path:
            sys.path.insert(0, str(_project_root))

        import pyautogui_intercept
        pyautogui_intercept.install_intercept()
        print("[STARTUP-DEBUG] ‚úÖ PyAutoGUI intercept installed (deferred)")
    except Exception as e:
        print(f"[STARTUP-DEBUG] PyAutoGUI intercept failed: {e}")

# Queue for background execution
_DEFERRED_DEBUG_TASKS.append(_run_deferred_coordinate_diagnostic)
_DEFERRED_DEBUG_TASKS.append(_run_deferred_pyautogui_intercept)
print("[STARTUP] Debug tasks deferred to background for fast startup")

# Enable enhanced ML model logging
try:
    from enable_ml_logging import configure_ml_logging

    ml_logger_instance, memory_visualizer = configure_ml_logging()
    logger = logging.getLogger(__name__)
    logger.info("üöÄ Enhanced ML model logging enabled")
    ML_LOGGING_ENABLED = True
except ImportError:
    # Configure logging early with more detail
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )
    logger = logging.getLogger(__name__)
    # v109.3: Enhanced ML logging is optional - use INFO not WARNING
    logger.info("Enhanced ML logging not available - using standard logging")
    ML_LOGGING_ENABLED = False

# Enable debug logging for specific modules
logging.getLogger("api.jarvis_voice_api").setLevel(logging.DEBUG)
logging.getLogger("api.jarvis_factory").setLevel(logging.DEBUG)
logging.getLogger("chatbots.claude_vision_chatbot").setLevel(logging.DEBUG)
# Add weather-specific debug logging
logging.getLogger("voice.jarvis_agent_voice").setLevel(logging.DEBUG)
logging.getLogger("workflows.weather_app_vision_unified").setLevel(logging.DEBUG)
logging.getLogger("system_control.unified_vision_weather").setLevel(logging.DEBUG)
logging.getLogger("api.voice_websocket_handler").setLevel(logging.DEBUG)

# Silence noisy third-party library DEBUG logs
# Use NullHandler + setLevel to completely silence DEBUG output
for _lib in [
    "speechbrain", "speechbrain.utils", "speechbrain.utils.checkpoints",
    "speechbrain.utils.fetching", "speechbrain.utils.parameter_transfer",
    "speechbrain.dataio", "speechbrain.lobes", "speechbrain.inference",
    "huggingface_hub", "transformers", "torch", "torchaudio",
    "urllib3", "httpx", "httpcore", "filelock", "datasets"
]:
    _logger = logging.getLogger(_lib)
    _logger.setLevel(logging.WARNING)
    _logger.propagate = False  # Don't propagate to root logger

# Check if we're in optimized mode - default to True for faster startup
OPTIMIZE_STARTUP = os.getenv("OPTIMIZE_STARTUP", "true").lower() == "true"
PARALLEL_IMPORTS = os.getenv("BACKEND_PARALLEL_IMPORTS", "true").lower() == "true"
LAZY_LOAD_MODELS = os.getenv("BACKEND_LAZY_LOAD_MODELS", "true").lower() == "true"

if OPTIMIZE_STARTUP:
    logger.info("üöÄ Running in OPTIMIZED startup mode")
    logger.info(f"  Parallel imports: {PARALLEL_IMPORTS}")
    logger.info(f"  Lazy load models: {LAZY_LOAD_MODELS}")

# Fix TensorFlow import issues before any other imports
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["USE_TORCH"] = "1"
os.environ["USE_TF"] = "0"

# FastAPI and core imports (always needed)
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles

# Import centralized secret manager
try:
    from core.secret_manager import get_anthropic_key
    SECRET_MANAGER_AVAILABLE = True
except ImportError:
    SECRET_MANAGER_AVAILABLE = False

# Import TaskLifecycleManager for proper async task management (prevents SIGKILL)
try:
    from core.task_lifecycle_manager import (
        get_task_manager,
        TaskPriority,
        spawn_task,
        spawn_monitor_task,
        is_shutting_down,
        shutdown_all_tasks,
    )
    TASK_LIFECYCLE_AVAILABLE = True
except ImportError:
    TASK_LIFECYCLE_AVAILABLE = False
    logger.warning("TaskLifecycleManager not available - using basic task tracking")

# Import Advanced Thread Manager for bulletproof thread management
try:
    from core.thread_manager import (
        get_thread_manager,
        AdvancedThreadManager,
        ThreadPolicy,
        create_managed_thread,
        shutdown_all_threads_async,
        shutdown_all_threads,
        # v2.0: Comprehensive shutdown coordinator
        comprehensive_shutdown,
        get_shutdown_coordinator,
        register_http_client,
        get_http_client_registry,
        close_all_http_clients,
    )
    THREAD_MANAGER_AVAILABLE = True
    logger.info("‚úÖ Advanced Thread Manager available (v2.0 with HTTP client registry)")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è  Advanced Thread Manager not available: {e}")
    THREAD_MANAGER_AVAILABLE = False
    comprehensive_shutdown = None
    register_http_client = None

# Load environment variables (force override of system env vars)
try:
    from dotenv import load_dotenv

    load_dotenv(override=True)  # Force .env to override existing environment variables
except ImportError:
    pass

# Global component storage
components = {}
import_times = {}

# =============================================================================
# EARLY SHUTDOWN HOOK REGISTRATION
# =============================================================================
# Register shutdown hook as early as possible to ensure GCP VMs are cleaned up
# even if JARVIS crashes during startup. This provides the "Local Cleanup" layer
# of the Triple-Lock safety system for preventing orphaned VMs.
# =============================================================================
try:
    from scripts.shutdown_hook import register_handlers as _register_shutdown_handlers
    _register_shutdown_handlers()
    logger.debug("‚úÖ GCP shutdown hook registered (atexit + signals)")
except ImportError:
    logger.debug("‚ö†Ô∏è Shutdown hook not available - VM cleanup may be affected")

# Dynamic Component Manager
dynamic_component_manager = None
DYNAMIC_LOADING_ENABLED = False

# GCP VM Manager
gcp_vm_manager = None
# Cost-safe default: Spot/VM auto-creation must be explicitly enabled.
# Accept legacy env var GCP_VM_ENABLED, but default to JARVIS_SPOT_VM_ENABLED=false.
_gcp_vm_flag = os.getenv("GCP_VM_ENABLED")
if _gcp_vm_flag is None:
    _gcp_vm_flag = os.getenv("JARVIS_SPOT_VM_ENABLED", "false")
GCP_VM_ENABLED = str(_gcp_vm_flag).lower() == "true"

try:
    from core.dynamic_component_manager import get_component_manager

    logger.info("‚úÖ Dynamic Component Manager available")
    DYNAMIC_LOADING_ENABLED = os.getenv("DYNAMIC_COMPONENT_LOADING", "true").lower() == "true"
    if DYNAMIC_LOADING_ENABLED:
        logger.info("üß© Dynamic Component Loading: ENABLED")
    else:
        logger.info(
            "‚ö†Ô∏è Dynamic Component Loading: DISABLED (set DYNAMIC_COMPONENT_LOADING=true to enable)"
        )
except ImportError:
    logger.warning("‚ö†Ô∏è Dynamic Component Manager not available - using legacy loading")
    DYNAMIC_LOADING_ENABLED = False

# v4.0: Thread Manager initialization is now DEFERRED to lifespan
# This was blocking module load and preventing fast startup
thread_manager = None
_thread_manager_initialized = False

def _init_thread_manager_lazy():
    """Initialize thread manager lazily - called from lifespan or on first use"""
    global thread_manager, _thread_manager_initialized, THREAD_MANAGER_AVAILABLE
    
    if _thread_manager_initialized:
        return thread_manager
    
    _thread_manager_initialized = True
    
    if not THREAD_MANAGER_AVAILABLE:
        return None
    
    try:
        # Create custom policy optimized for JARVIS
        thread_policy = ThreadPolicy(
            graceful_shutdown_timeout=8.0,
            forceful_shutdown_timeout=5.0,
            terminate_shutdown_timeout=4.0,
            emergency_shutdown_timeout=3.0,
            max_threads=500,
            max_thread_lifetime=7200.0,
            warn_thread_age=1800.0,
            enable_health_check=True,
            health_check_interval=60.0,
            enable_deadlock_detection=True,
            deadlock_check_interval=120.0,
            auto_cleanup_orphans=True,
            orphan_check_interval=90.0,
            force_daemon_on_shutdown=True,
            log_thread_creation=False,
            log_thread_completion=False,
            log_stack_traces=True,
            capture_full_stack=False,
            use_thread_pool=True,
            thread_pool_size=None,
            recycle_threads=True
        )

        thread_manager = get_thread_manager(policy=thread_policy)
        logger.info("üßµ Advanced Thread Manager initialized (lazy)")
        return thread_manager
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize Thread Manager: {e}")
        THREAD_MANAGER_AVAILABLE = False
        return None

# Queue for background init
_DEFERRED_DEBUG_TASKS.append(_init_thread_manager_lazy)
print("[STARTUP] Thread manager deferred to background for fast startup")


async def parallel_import_components():
    """Import all components in parallel for faster startup"""
    start_time = time.time()
    logger.info("‚ö° Starting parallel component imports...")

    import concurrent.futures

    # Use managed executor for clean shutdown if available
    if THREAD_MANAGER_AVAILABLE:
        from core.thread_manager import ManagedThreadPoolExecutor
        _ImportExecutor = ManagedThreadPoolExecutor
    else:
        _ImportExecutor = concurrent.futures.ThreadPoolExecutor

    # Define import tasks
    import_tasks = {
        "chatbots": import_chatbots,
        "vision": import_vision_system,
        "memory": import_memory_system,
        "voice": import_voice_system,
        "ml_models": import_ml_models,
        "monitoring": import_monitoring,
        "voice_unlock": import_voice_unlock,
        "wake_word": import_wake_word,
        "context": import_context_api,
        "display_monitor": import_display_monitor,
        "goal_inference": import_goal_inference,
    }

    # Use thread pool for imports (ManagedThreadPoolExecutor for clean shutdown)
    # Note: ManagedThreadPoolExecutor supports 'name' param, fallback ThreadPoolExecutor uses 'thread_name_prefix'
    if THREAD_MANAGER_AVAILABLE:
        executor_instance = ManagedThreadPoolExecutor(max_workers=4, name='parallel-imports')
    else:
        executor_instance = concurrent.futures.ThreadPoolExecutor(max_workers=4, thread_name_prefix='parallel-imports')
    with executor_instance as executor:
        # Submit all import tasks
        futures = {name: executor.submit(func) for name, func in import_tasks.items()}

        # Wait for completion
        for name, future in futures.items():
            try:
                result = future.result(timeout=10)
                components[name] = result
                logger.info(f"  ‚úÖ {name} loaded")
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è {name} failed: {e}")
                components[name] = None

    elapsed = time.time() - start_time
    logger.info(f"‚ö° Parallel imports completed in {elapsed:.1f}s")


def import_chatbots():
    """Import chatbot components"""
    chatbots = {}

    try:
        from chatbots.claude_vision_chatbot import ClaudeVisionChatbot

        chatbots["vision"] = ClaudeVisionChatbot
        chatbots["vision_available"] = True
    except ImportError:
        try:
            from chatbots.claude_chatbot import ClaudeChatbot

            chatbots["claude"] = ClaudeChatbot
            chatbots["vision_available"] = False
        except ImportError:
            pass

    return chatbots


def import_vision_system():
    """Import vision components"""
    vision = {}

    try:
        from vision.claude_vision_analyzer_main import ClaudeVisionAnalyzer
        from vision.video_stream_capture import MACOS_CAPTURE_AVAILABLE, VideoStreamCapture

        vision["analyzer"] = ClaudeVisionAnalyzer
        vision["video_capture"] = VideoStreamCapture
        vision["macos_available"] = MACOS_CAPTURE_AVAILABLE
        vision["available"] = True
    except ImportError:
        vision["available"] = False

    # Check purple indicator separately
    try:
        pass

        vision["purple_indicator"] = True
    except ImportError:
        vision["purple_indicator"] = False

    return vision


def import_memory_system():
    """Import memory management"""
    memory = {}

    try:
        from memory.memory_api import MemoryAPI
        from memory.memory_manager import ComponentPriority, M1MemoryManager

        memory["manager_class"] = M1MemoryManager  # type: ignore
        memory["priority"] = ComponentPriority
        memory["api"] = MemoryAPI
        memory["available"] = True
    except ImportError:
        memory["available"] = False

        # Create stubs (renamed to avoid shadowing imported class for type checker)
        class _M1MemoryManagerStub:
            async def start_monitoring(self):
                pass

            async def stop_monitoring(self):
                pass

            async def get_memory_snapshot(self):
                from types import SimpleNamespace

                return SimpleNamespace(
                    state=SimpleNamespace(value="normal"),
                    percent=0.5,
                    available=8 * 1024 * 1024 * 1024,
                    total=16 * 1024 * 1024 * 1024,
                )

            def register_component(self, *args, **kwargs):
                pass

        memory["manager_class"] = _M1MemoryManagerStub

    return memory


def import_voice_system():
    """Import voice components"""
    voice = {}

    try:
        from api.voice_api import VoiceAPI

        voice["api"] = VoiceAPI
        voice["available"] = True
    except ImportError:
        voice["available"] = False

    try:
        from api.enhanced_voice_routes import router as enhanced_voice_router

        voice["enhanced_router"] = enhanced_voice_router
        voice["enhanced_available"] = True
    except ImportError:
        voice["enhanced_available"] = False

    try:
        from api.jarvis_voice_api import jarvis_api
        from api.jarvis_voice_api import router as jarvis_voice_router

        voice["jarvis_router"] = jarvis_voice_router
        voice["jarvis_api"] = jarvis_api
        voice["jarvis_available"] = True
    except ImportError as e:
        logger.exception(f"Failed to import JARVIS Voice API: {e}")
        voice["jarvis_available"] = False
    except Exception as e:
        logger.exception(f"Unexpected error importing JARVIS Voice API: {e}")
        voice["jarvis_available"] = False

    return voice


def import_ml_models():
    """Import ML models (lazy load if enabled)"""
    ml = {}

    if LAZY_LOAD_MODELS:
        logger.info("  üì¶ ML models will be loaded on demand")
        ml["lazy_loaded"] = True
        return ml

    try:
        from api.model_status_api import router as model_status_router
        from ml_model_loader import get_loader_status, initialize_models

        ml["initialize_models"] = initialize_models
        ml["get_status"] = get_loader_status
        ml["status_router"] = model_status_router
        ml["available"] = True
    except ImportError:
        ml["available"] = False

    return ml


def import_monitoring():
    """Import monitoring components"""
    monitoring = {}

    try:
        from api.monitoring_api import router as monitoring_router

        monitoring["router"] = monitoring_router
        monitoring["available"] = True
    except ImportError:
        monitoring["available"] = False

    return monitoring


def import_voice_unlock():
    """Import voice unlock components including enhanced VBI (v4.0)"""
    import logging

    logger = logging.getLogger(__name__)

    voice_unlock = {}

    try:
        from api.voice_unlock_api import initialize_voice_unlock
        from api.voice_unlock_api import router as voice_unlock_router

        voice_unlock["router"] = voice_unlock_router
        voice_unlock["initialize"] = initialize_voice_unlock
        voice_unlock["available"] = True

        # Try to initialize immediately
        if initialize_voice_unlock():
            voice_unlock["initialized"] = True
            logger.info("  ‚úÖ Voice Unlock API initialized")
        else:
            voice_unlock["initialized"] = False
            logger.warning("  ‚ö†Ô∏è  Voice Unlock API initialization failed")

        # Also import the startup integration for WebSocket server
        try:
            from voice_unlock.startup_integration import (
                initialize_voice_unlock_system,
                shutdown_voice_unlock_system,
                voice_unlock_startup,
            )

            voice_unlock["startup_integration"] = True
            voice_unlock["initialize_system"] = initialize_voice_unlock_system
            voice_unlock["shutdown_system"] = shutdown_voice_unlock_system
            voice_unlock["startup_manager"] = voice_unlock_startup
        except ImportError:
            logger.warning("  ‚ö†Ô∏è  Voice Unlock startup integration not available")
            voice_unlock["startup_integration"] = False

        # Import Enhanced VBI (v4.0) - LangGraph, ChromaDB, Langfuse, Cost Tracking
        try:
            from voice_unlock.voice_biometric_intelligence import (
                get_voice_biometric_intelligence,
            )
            voice_unlock["vbi_available"] = True
            voice_unlock["get_vbi"] = get_voice_biometric_intelligence
            logger.info("  ‚úÖ Voice Biometric Intelligence (VBI v4.0) available")

            # Import enhanced modules status
            from voice_unlock import (
                get_voice_auth_reasoning_graph,
                get_voice_pattern_memory,
                get_voice_auth_orchestrator,
                get_voice_auth_tracer,
                get_voice_auth_cost_tracker,
            )
            voice_unlock["enhanced_modules"] = {
                "reasoning_graph": get_voice_auth_reasoning_graph,
                "pattern_memory": get_voice_pattern_memory,
                "orchestrator": get_voice_auth_orchestrator,
                "langfuse_tracer": get_voice_auth_tracer,
                "cost_tracker": get_voice_auth_cost_tracker,
            }
            logger.info("  ‚úÖ VBI Enhanced Modules (LangGraph, ChromaDB, Langfuse, Cost) imported")
        except ImportError as e:
            logger.warning(f"  ‚ö†Ô∏è  VBI v4.0 not available: {e}")
            voice_unlock["vbi_available"] = False

    except ImportError as e:
        logger.warning(f"  ‚ö†Ô∏è  Voice Unlock not available: {e}")
        voice_unlock["available"] = False
        voice_unlock["initialized"] = False

    return voice_unlock


def import_wake_word():
    """Import wake word detection components"""
    wake_word = {}

    try:
        from api.wake_word_api import initialize_wake_word
        from api.wake_word_api import router as wake_word_router
        from api.wake_word_api import wake_service

        wake_word["router"] = wake_word_router
        wake_word["initialize"] = initialize_wake_word
        wake_word["service"] = wake_service
        wake_word["available"] = True

        # Try to initialize immediately
        if initialize_wake_word():
            wake_word["initialized"] = True
            logger.info("  ‚úÖ Wake Word detection initialized")
        else:
            wake_word["initialized"] = False
            logger.warning("  ‚ö†Ô∏è  Wake Word initialization failed")

    except ImportError as e:
        logger.warning(f"  ‚ö†Ô∏è  Wake Word not available: {e}")
        wake_word["available"] = False
        wake_word["initialized"] = False

    return wake_word


def import_context_api():
    """Import context intelligence API components"""
    context = {}

    try:
        from api.context_api import router as context_router

        context["router"] = context_router
        context["available"] = True
        logger.info("  ‚úÖ Context Intelligence API loaded")
    except ImportError as e:
        logger.warning(f"  ‚ö†Ô∏è  Context API not available: {e}")
        context["available"] = False

    return context


def import_display_monitor():
    """Import display monitor components"""
    display_monitor = {}

    try:
        from display.advanced_display_monitor import AdvancedDisplayMonitor, get_display_monitor
        from display.display_config_manager import get_config_manager
        from display.display_voice_handler import create_voice_handler

        display_monitor["get_monitor"] = get_display_monitor
        display_monitor["monitor_class"] = AdvancedDisplayMonitor
        display_monitor["voice_handler_factory"] = create_voice_handler
        display_monitor["config_manager_factory"] = get_config_manager
        display_monitor["available"] = True

        logger.info("  ‚úÖ Display Monitor components loaded")

    except ImportError as e:
        logger.warning(f"  ‚ö†Ô∏è  Display Monitor not available: {e}")
        display_monitor["available"] = False

    return display_monitor


def import_goal_inference():
    """Import Goal Inference and Learning Database components with auto-configuration"""
    goal_inference = {}

    try:
        # Import Goal Inference + Autonomous Engine Integration
        import json
        import os
        from pathlib import Path

        from backend.intelligence.goal_autonomous_uae_integration import get_integration
        from backend.intelligence.learning_database import get_learning_database

        # Load or create configuration
        config_path = Path("backend/config/integration_config.json")

        # Check for environment variable overrides
        preset_override = os.getenv("JARVIS_GOAL_PRESET", None)
        automation_override = os.getenv("JARVIS_GOAL_AUTOMATION", None)

        if config_path.exists():
            with open(config_path, "r") as f:
                config = json.load(f)
        else:
            # Create default configuration automatically
            logger.info("  üìù Creating default Goal Inference configuration...")
            config = _create_default_goal_config()
            config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(config_path, "w") as f:
                json.dump(config, f, indent=2)
            logger.info(f"  ‚úÖ Configuration created at {config_path}")

        # Apply preset if environment variable is set
        if preset_override:
            logger.info(f"  üéØ Applying preset from environment: {preset_override}")
            config = _apply_preset_to_config(config, preset_override)
            with open(config_path, "w") as f:
                json.dump(config, f, indent=2)

        # Apply automation override if environment variable is set
        if automation_override:
            automation_enabled = automation_override.lower() == "true"
            config["integration"]["enable_automation"] = automation_enabled
            logger.info(
                f"  ü§ñ Automation override: {'ENABLED' if automation_enabled else 'DISABLED'}"
            )
            with open(config_path, "w") as f:
                json.dump(config, f, indent=2)

        # Initialize integration with config
        integration = get_integration()
        goal_inference["integration"] = integration
        goal_inference["available"] = True
        goal_inference["config"] = config

        # Initialize learning database with config
        db_config = {
            "cache_size": config.get("performance", {}).get("max_prediction_cache_size", 1000),
            "cache_ttl_seconds": config.get("performance", {}).get("cache_ttl_seconds", 3600),
            "enable_ml_features": config.get("learning", {}).get("enabled", True),
            "auto_optimize": True,
            "batch_insert_size": 100,
        }
        learning_db = get_learning_database
        goal_inference["learning_db"] = learning_db
        goal_inference["db_config"] = db_config

        # Log configuration
        logger.info("  ‚úÖ Goal Inference + Learning Database loaded")
        logger.info(f"     ‚Ä¢ Goal Confidence: {config['goal_inference']['min_goal_confidence']}")
        logger.info(
            f"     ‚Ä¢ Proactive Suggestions: {config['integration']['enable_proactive_suggestions']}"
        )
        logger.info(f"     ‚Ä¢ Automation: {config['integration']['enable_automation']}")
        logger.info(f"     ‚Ä¢ Learning: {config['learning']['enabled']}")
        logger.info(f"     ‚Ä¢ Database Cache: {db_config['cache_size']} entries")

        # Get current metrics
        try:
            metrics = integration.get_metrics()
            if metrics.get("goals_inferred", 0) > 0:
                logger.info(
                    f"     ‚Ä¢ Previous session: {metrics['goals_inferred']} goals, {metrics.get('actions_executed', 0)} actions"
                )
                logger.info(f"     ‚Ä¢ Success rate: {metrics.get('success_rate', 0):.1%}")
        except Exception as e:
            logger.debug(f"Could not load metrics: {e}")

        # Apply configuration to integration
        _apply_config_to_integration(integration, config)

    except ImportError as e:
        logger.warning(f"  ‚ö†Ô∏è  Goal Inference not available: {e}")
        goal_inference["available"] = False

    except Exception as e:
        logger.warning(f"  ‚ö†Ô∏è  Goal Inference initialization failed: {e}")
        goal_inference["available"] = False

    return goal_inference


def _create_default_goal_config():
    """Create default Goal Inference configuration"""
    return {
        "goal_inference": {
            "min_goal_confidence": 0.75,
            "goal_confidence_threshold": 0.75,
            "enable_learning": True,
            "max_active_goals": 10,
            "goal_timeout_minutes": 30,
            "pattern_learning_enabled": True,
        },
        "autonomous_decisions": {
            "min_decision_confidence": 0.70,
            "enable_predictive_display": True,
            "auto_connect_threshold": 0.85,
            "max_concurrent_actions": 5,
            "learning_rate": 0.01,
            "exploration_rate": 0.1,
        },
        "integration": {
            "enable_proactive_suggestions": True,
            "proactive_suggestion_threshold": 0.85,
            "enable_automation": False,
            "automation_threshold": 0.95,
            "feedback_window_minutes": 30,
            "cache_duration_minutes": 5,
        },
        "display_optimization": {
            "enable_predictive_connection": True,
            "preload_resources": True,
            "predictive_confidence_threshold": 0.85,
            "default_display": "Living Room TV",
            "connection_patterns": {
                "meeting_preparation": "Living Room TV",
                "project_completion": "External Monitor",
                "presentation": "Living Room TV",
                "casual_viewing": "Living Room TV",
            },
        },
        "learning": {
            "enabled": True,
            "min_samples_for_pattern": 3,
            "pattern_confidence_boost": 0.05,
            "success_rate_threshold": 0.7,
            "feedback_weight": 0.1,
            "save_state_interval_minutes": 60,
        },
        "user_preferences": {
            "verbose_suggestions": False,
            "explain_reasoning": True,
            "show_confidence_scores": False,
            "auto_accept_high_confidence": False,
            "notification_style": "subtle",
        },
        "performance": {
            "max_prediction_cache_size": 100,
            "cache_ttl_seconds": 300,
            "parallel_processing": True,
            "max_workers": 4,
            "timeout_seconds": 5,
        },
        "safety": {
            "require_confirmation_for_automation": True,
            "max_automation_actions_per_day": 50,
            "blacklist_actions": [],
            "whitelist_actions": ["connect_display", "open_application", "organize_workspace"],
            "risk_tolerance": 0.5,
        },
        "logging": {
            "log_predictions": True,
            "log_decisions": True,
            "log_learning_events": True,
            "metrics_tracking": True,
            "debug_mode": False,
        },
    }


def _apply_preset_to_config(config, preset):
    """Apply a configuration preset"""
    presets = {
        "aggressive": {
            "goal_inference.min_goal_confidence": 0.65,
            "autonomous_decisions.min_decision_confidence": 0.60,
            "integration.proactive_suggestion_threshold": 0.75,
            "integration.enable_automation": True,
            "learning.pattern_confidence_boost": 0.10,
        },
        "balanced": {
            "goal_inference.min_goal_confidence": 0.75,
            "autonomous_decisions.min_decision_confidence": 0.70,
            "integration.proactive_suggestion_threshold": 0.85,
            "integration.enable_automation": False,
            "learning.pattern_confidence_boost": 0.05,
        },
        "conservative": {
            "goal_inference.min_goal_confidence": 0.85,
            "autonomous_decisions.min_decision_confidence": 0.80,
            "integration.proactive_suggestion_threshold": 0.90,
            "integration.enable_automation": False,
            "learning.pattern_confidence_boost": 0.02,
        },
        "learning": {
            "learning.enabled": True,
            "learning.min_samples_for_pattern": 2,
            "learning.pattern_confidence_boost": 0.10,
            "learning.feedback_weight": 0.15,
            "autonomous_decisions.exploration_rate": 0.2,
        },
        "performance": {
            "performance.max_prediction_cache_size": 200,
            "performance.cache_ttl_seconds": 600,
            "performance.parallel_processing": True,
            "display_optimization.preload_resources": True,
        },
    }

    if preset in presets:
        for path, value in presets[preset].items():
            keys = path.split(".")
            current = config
            for key in keys[:-1]:
                if key not in current:
                    current[key] = {}
                current = current[key]
            current[keys[-1]] = value

    return config


def _apply_config_to_integration(integration, config):
    """Apply configuration settings to integration"""
    try:
        # Apply goal inference settings
        if hasattr(integration, "goal_inference"):
            goal_config = config.get("goal_inference", {})
            integration.goal_inference.min_confidence = goal_config.get("min_goal_confidence", 0.75)
            integration.goal_inference.max_active_goals = goal_config.get("max_active_goals", 10)

        # Apply autonomous decision settings
        if hasattr(integration, "autonomous_engine"):
            auto_config = config.get("autonomous_decisions", {})
            integration.autonomous_engine.min_confidence = auto_config.get(
                "min_decision_confidence", 0.70
            )
            integration.autonomous_engine.learning_rate = auto_config.get("learning_rate", 0.01)

        # Apply integration settings
        integration_config = config.get("integration", {})
        integration.enable_proactive = integration_config.get("enable_proactive_suggestions", True)
        integration.enable_automation = integration_config.get("enable_automation", False)

    except Exception as e:
        logger.debug(f"Could not apply all config settings: {e}")


async def memory_pressure_callback(pressure_level: str):
    """
    Callback for memory pressure changes - triggers GCP VM creation if needed

    Args:
        pressure_level: 'low', 'medium', 'high', 'critical'
    """
    global gcp_vm_manager  #

    logger.info(f"üìä Memory pressure changed: {pressure_level}")

    # Only create VM on high or critical pressure
    if pressure_level not in ["high", "critical"]:
        return

    if not GCP_VM_ENABLED:
        logger.info("‚ö†Ô∏è  GCP VM creation disabled (GCP_VM_ENABLED=false)")
        return

    try:
        # Check if GCP VM Manager is initialized
        if gcp_vm_manager is None:  # Check if gcp_vm_manager is initialized
            # Initialize GCP VM Manager
            from core.gcp_vm_manager import get_gcp_vm_manager

            # Get GCP VM Manager instance
            gcp_vm_manager = await get_gcp_vm_manager()

        # Get current memory snapshot
        from core.platform_memory_monitor import get_memory_monitor

        memory_monitor = get_memory_monitor()
        snapshot = await memory_monitor.get_memory_pressure()

        # Determine if VM should be created based on memory pressure level
        should_create, reason, confidence = await gcp_vm_manager.should_create_vm(
            snapshot,  # Memory snapshot
            trigger_reason=f"Memory pressure: {pressure_level}",  # Trigger reason
        )

        if should_create:
            logger.info(f"üöÄ Creating GCP Spot VM: {reason} (confidence: {confidence:.2%})")

            # Determine which components to offload
            components_to_offload = []
            if pressure_level == "critical":
                # Offload heavy components
                components_to_offload = ["VISION", "CHATBOTS", "ML_MODELS", "LOCAL_LLM"]
            else:
                # Just offload the heaviest
                components_to_offload = ["VISION", "CHATBOTS"]

            # Create GCP VM instance
            vm_instance = await gcp_vm_manager.create_vm(
                components=components_to_offload,  # Components to offload
                trigger_reason=f"Memory pressure: {pressure_level} - {reason}",  # Trigger reason
                # Metadata for tracking
                metadata={
                    "pressure_level": pressure_level,  # Pressure level
                    "confidence": confidence,  # Confidence
                    "local_ram_gb": snapshot.total_gb if snapshot else 0,  # Local RAM GB
                    "used_ram_gb": snapshot.used_gb if snapshot else 0,  # Used RAM GB
                },
            )

            if vm_instance:
                logger.info(f"‚úÖ GCP VM created: {vm_instance.name}")
                logger.info(f"   IP: {vm_instance.ip_address}")
                logger.info(f"   Components: {', '.join(vm_instance.components)}")
            else:
                logger.error("‚ùå Failed to create GCP VM")
        else:
            logger.info(f"‚ÑπÔ∏è  VM creation not needed: {reason}")

    except Exception as e:
        logger.error(f"Error in memory pressure callback: {e}", exc_info=True)


# =============================================================================
# PARALLEL STARTUP MODE (v1.0.0) - Server starts IMMEDIATELY
# =============================================================================
# Enable via: JARVIS_PARALLEL_STARTUP=true
# This mode starts the server within 1-2 seconds and runs heavy initialization
# in background tasks. The /health/startup endpoint tracks progress.
# =============================================================================
PARALLEL_STARTUP_ENABLED = os.getenv("JARVIS_PARALLEL_STARTUP", "true").lower() == "true"


@asynccontextmanager
async def parallel_lifespan(app: FastAPI):
    """
    Parallel lifespan handler - server starts IMMEDIATELY.

    v4.0: Ultra-fast startup - all heavy initialization runs in background.
    Server should respond to /health/ping within 1-2 seconds.

    Key optimizations:
    - Module-level debug tasks deferred to background
    - Thread manager initialized lazily
    - All ML models loaded in background
    """
    # Robust import that works in both contexts:
    # 1. When running via `python -m backend.main` (PYTHONPATH includes project root)
    # 2. When app object is passed directly (working dir may be backend/)
    try:
        from backend.core.parallel_initializer import ParallelInitializer
    except ImportError:
        from core.parallel_initializer import ParallelInitializer

    # v4.0: Ultra-minimal pre-yield setup
    logger.info("=" * 60)
    logger.info("PARALLEL STARTUP MODE v4.0.0 (Ultra-Fast)")
    logger.info("=" * 60)
    logger.info("Server accepting requests in <2s, heavy init in background")
    logger.info("=" * 60)

    # Create parallel initializer
    initializer = ParallelInitializer(app)

    # Minimal setup - server ready in <2s
    await initializer.minimal_setup()

    # =========================================================================
    # YIELD - Server starts serving requests NOW
    # =========================================================================
    logger.info("")
    logger.info("=" * 60)
    logger.info("SERVER IS NOW ACCEPTING REQUESTS")
    logger.info("  - /health/ping   -> Liveness probe (instant)")
    logger.info("  - /health/startup -> Initialization progress")
    logger.info("  - /health        -> Full status (after init)")
    logger.info("=" * 60)
    logger.info("")

    try:
        # =================================================================
        # PERFORMANCE OPTIMIZER: Attach to app state for lifecycle management
        # =================================================================
        if PERFORMANCE_OPTIMIZER_AVAILABLE:
            try:
                from core.performance_optimizer import get_optimizer
                optimizer = get_optimizer()
                app.state.performance_optimizer = optimizer
                logger.info("üìä Performance Optimizer initialized and attached to app.state")
            except Exception as e:
                logger.debug(f"Could not attach Performance Optimizer to app.state: {e}")
                app.state.performance_optimizer = None
        else:
            app.state.performance_optimizer = None

        # =================================================================
        # HYPER-SPEED AI LOADER: Ghost Proxies for instant model access
        # =================================================================
        # Initialize AI model manager for zero-copy loading with Ghost Proxies
        # Models load in background while server responds immediately
        try:
            from core.ai_loader import get_ai_manager, get_config as get_ai_config
            ai_manager = get_ai_manager()
            app.state.ai_manager = ai_manager

            ai_config = get_ai_config()
            logger.info("üß† Hyper-Speed AI Loader initialized")
            logger.info(f"   Workers: {ai_config.max_workers}, Quantize: {ai_config.quantize_default}")
            logger.info("   Ghost Proxies ready for instant model access")
        except ImportError as e:
            logger.debug(f"AI Loader not available: {e}")
            app.state.ai_manager = None
        except Exception as e:
            logger.warning(f"AI Loader initialization failed: {e}")
            app.state.ai_manager = None

        # =================================================================
        # v78.0: Advanced Startup Orchestrator (Background Init)
        # =================================================================
        # Initialize orchestrator in background for dynamic discovery
        app.state.orchestrator_hooks = None
        app.state.discovered_config = None

        async def _init_orchestrator_background():
            """Initialize Advanced Orchestrator in background."""
            try:
                from core.supervisor_orchestrator_bridge import (
                    get_orchestrator_hooks,
                    OrchestratorBridgeConfig,
                )

                config = OrchestratorBridgeConfig.from_env()
                if config.enabled:
                    hooks = await get_orchestrator_hooks(config=config)
                    app.state.orchestrator_hooks = hooks
                    app.state.discovered_config = hooks.discovered_config
                    logger.info("‚úÖ v78.0 Orchestrator: Active (background init)")
            except Exception as e:
                logger.debug(f"Orchestrator background init: {e}")

        # Launch orchestrator in background
        asyncio.create_task(_init_orchestrator_background(), name="orchestrator_init")

        # =================================================================
        # v77.4 UNIFIED CODING COUNCIL: Background initialization
        # =================================================================
        # In parallel mode, start Coding Council initialization as background task
        # to avoid blocking server startup while keeping self-evolution ready
        async def _init_coding_council_background():
            """Initialize Coding Council in background for parallel startup."""
            try:
                from core.coding_council.startup import (
                    initialize_coding_council_startup,
                    is_initialized as is_coding_council_initialized,
                )

                logger.info("üß¨ Coding Council: Starting background initialization...")
                success = await initialize_coding_council_startup()

                if success:
                    app.state.coding_council_initialized = True
                    logger.info("‚úÖ Coding Council online (background init)")
                else:
                    app.state.coding_council_initialized = False
                    logger.warning("‚ö†Ô∏è Coding Council: Background init incomplete")
            except ImportError as e:
                logger.debug(f"Coding Council not available: {e}")
                app.state.coding_council_initialized = False
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Coding Council background init failed: {e}")
                app.state.coding_council_initialized = False

        # Launch Coding Council initialization in background
        asyncio.create_task(_init_coding_council_background(), name="coding_council_init")

        # =================================================================
        # TRINITY ECOSYSTEM: Initialize Prime Router & Graceful Degradation
        # =================================================================
        # v85.0: Enhanced Trinity initialization with intelligent repo discovery
        # and cross-repo verification. Uses IntelligentRepoDiscovery for
        # zero-hardcoding path resolution.
        app.state.trinity_initialized = False
        app.state.prime_router = None
        app.state.trinity_discovery = None
        app.state.trinity_verification = None

        async def _init_trinity_background():
            """
            v85.0: Initialize Trinity ecosystem in background.

            Enhancements:
            - IntelligentRepoDiscovery for dynamic repo path resolution
            - Cross-repo verification to confirm all components are running
            - Enhanced health status with discovery confidence scores
            """
            try:
                # =============================================================
                # v85.0: Initialize IntelligentRepoDiscovery
                # =============================================================
                try:
                    from core.trinity_integrator import get_repo_discovery, get_resource_aware_launcher
                except ImportError:
                    from backend.core.trinity_integrator import get_repo_discovery, get_resource_aware_launcher

                logger.info("üî± Trinity v85.0: Starting intelligent repo discovery...")

                try:
                    discovery = await get_repo_discovery()
                    app.state.trinity_discovery = discovery

                    # Discover all repos concurrently
                    all_repos = await discovery.discover_all()

                    for repo_id, result in all_repos.items():
                        if result.path:
                            logger.info(
                                f"   üìÇ {repo_id}: {result.path} "
                                f"(strategy: {result.strategy_used.name}, confidence: {result.confidence:.0%})"
                            )
                        else:
                            logger.debug(f"   ‚ö†Ô∏è {repo_id}: Not found ({result.error})")

                except Exception as e:
                    logger.debug(f"   Repo discovery not available: {e}")

                # =============================================================
                # Initialize Prime Router (connects to JARVIS-Prime)
                # =============================================================
                try:
                    from core.prime_router import get_prime_router
                except ImportError:
                    from backend.core.prime_router import get_prime_router

                logger.info("üî± Trinity: Starting Prime Router initialization...")
                router = await get_prime_router()
                app.state.prime_router = router

                # =============================================================
                # Initialize Graceful Degradation
                # =============================================================
                try:
                    from core.graceful_degradation import get_degradation
                except ImportError:
                    from backend.core.graceful_degradation import get_degradation

                degradation = get_degradation()
                app.state.graceful_degradation = degradation

                # =============================================================
                # Check if JARVIS-Prime is available
                # =============================================================
                router_status = router.get_status()
                prime_available = router_status.get("prime_client", {}).get("available", False)

                if prime_available:
                    logger.info("‚úÖ Trinity: JARVIS-Prime (Mind) connected")
                else:
                    logger.info("‚ö†Ô∏è Trinity: JARVIS-Prime not available, using cloud fallback")

                # =============================================================
                # v85.0: Cross-repo verification
                # =============================================================
                try:
                    import json
                    from pathlib import Path

                    trinity_dir = Path.home() / ".jarvis" / "trinity" / "components"
                    verification = {
                        "jarvis_body": False,
                        "jarvis_prime": False,
                        "reactor_core": False,
                    }

                    for component, file_name in [
                        ("jarvis_body", "jarvis_body.json"),
                        ("jarvis_prime", "jarvis_prime.json"),
                        ("reactor_core", "reactor_core.json"),
                    ]:
                        path = trinity_dir / file_name
                        if path.exists():
                            try:
                                with open(path) as f:
                                    data = json.load(f)
                                import time
                                age = time.time() - data.get("timestamp", 0)
                                if age < 60:  # Fresh heartbeat
                                    verification[component] = True
                            except Exception:
                                pass

                    app.state.trinity_verification = verification
                    verified_count = sum(1 for v in verification.values() if v)
                    logger.info(f"üî± Trinity v85.0: {verified_count}/3 components verified online")

                except Exception as e:
                    logger.debug(f"   Verification check skipped: {e}")

                app.state.trinity_initialized = True
                logger.info("‚úÖ Trinity v85.0 ecosystem initialized (background init)")

            except ImportError as e:
                logger.debug(f"Trinity modules not available: {e}")
                app.state.trinity_initialized = False
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Trinity initialization failed: {e}")
                app.state.trinity_initialized = False

        # Launch Trinity initialization in background
        asyncio.create_task(_init_trinity_background(), name="trinity_init_v85")

        yield

        # =====================================================================
        # v4.0: Run deferred debug tasks in background AFTER server is serving
        # =====================================================================
        async def run_deferred_tasks():
            """Run deferred tasks in background thread"""
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor(max_workers=2, thread_name_prefix='deferred') as executor:
                for task in _DEFERRED_DEBUG_TASKS:
                    try:
                        executor.submit(task)
                    except Exception as e:
                        logger.debug(f"Deferred task failed: {e}")
        
        # Start deferred tasks without awaiting
        asyncio.create_task(run_deferred_tasks())
        
        # =================================================================
        # SUPERVISOR COORDINATION: Attach progress bridge for /health/startup
        # =================================================================
        try:
            from core.supervisor.supervisor_integration import get_progress_bridge
            bridge = get_progress_bridge()
            bridge.attach_app(app)
            logger.info("üìä Progress bridge attached for supervisor coordination")
        except ImportError:
            logger.debug("Supervisor integration not available")
        except Exception as e:
            logger.debug(f"Could not attach progress bridge: {e}")
        
    finally:
        # Shutdown
        logger.info("Shutting down parallel startup...")

        # =================================================================
        # v77.4 UNIFIED CODING COUNCIL: Graceful shutdown
        # =================================================================
        if hasattr(app.state, 'coding_council_initialized') and app.state.coding_council_initialized:
            try:
                from core.coding_council.startup import shutdown_coding_council_startup

                logger.info("üß¨ Shutting down Coding Council...")
                await shutdown_coding_council_startup()
                logger.info("‚úÖ Coding Council shutdown complete")
            except ImportError:
                pass  # Coding Council not available
            except Exception as e:
                logger.debug(f"Coding Council shutdown error (non-critical): {e}")

        # =================================================================
        # TRINITY ECOSYSTEM: Graceful shutdown
        # =================================================================
        if hasattr(app.state, 'trinity_initialized') and app.state.trinity_initialized:
            try:
                logger.info("üî± Shutting down Trinity ecosystem...")
                if hasattr(app.state, 'prime_router') and app.state.prime_router:
                    await app.state.prime_router.close()
                logger.info("‚úÖ Trinity ecosystem shutdown complete")
            except Exception as e:
                logger.debug(f"Trinity shutdown error (non-critical): {e}")

        # =================================================================
        # HYPER-SPEED AI LOADER: Graceful shutdown
        # =================================================================
        if hasattr(app.state, 'ai_manager') and app.state.ai_manager:
            try:
                logger.info("üß† Shutting down AI Loader...")
                await app.state.ai_manager.shutdown()
                logger.info("üß† AI Loader shutdown complete")
            except Exception as e:
                logger.debug(f"AI Loader shutdown error (non-critical): {e}")

        # =================================================================
        # PERFORMANCE OPTIMIZER: Graceful shutdown
        # =================================================================
        if hasattr(app.state, 'performance_optimizer') and app.state.performance_optimizer:
            try:
                logger.info("üìä Shutting down Performance Optimizer...")
                await app.state.performance_optimizer.shutdown()
                logger.info("üìä Performance Optimizer shutdown complete")
            except Exception as e:
                logger.debug(f"Performance Optimizer shutdown error (non-critical): {e}")

        await initializer.shutdown()

        # Clean up any remaining state
        logger.info("Parallel startup shutdown complete")


@asynccontextmanager  # type: ignore[arg-type]
async def lifespan(app: FastAPI):  # type: ignore[misc]
    """Optimized lifespan handler with parallel initialization"""
    logger.info("üöÄ Starting optimized JARVIS backend...")
    start_time = time.time()

    # =================================================================
    # SUPERVISOR COORDINATION: Attach progress bridge for /health/startup
    # =================================================================
    try:
        from core.supervisor.supervisor_integration import get_progress_bridge
        bridge = get_progress_bridge()
        bridge.attach_app(app)
        logger.info("üìä Progress bridge attached for supervisor coordination")
    except ImportError:
        logger.debug("Supervisor integration not available")
    except Exception as e:
        logger.debug(f"Could not attach progress bridge: {e}")

    # =================================================================
    # v95.3: READINESS STATE MANAGER - Proper liveness/readiness/startup probes
    # =================================================================
    # This fixes the ROOT CAUSE of /health/ready returning 503:
    # The ReadinessStateManager tracks initialization phases and component
    # readiness. Without this, health checks fail because the manager is
    # never transitioned to READY phase.
    # =================================================================
    readiness_manager = None
    try:
        from core.readiness_state_manager import (
            get_readiness_manager,
            ComponentCategory,
            InitializationPhase,
        )
        
        readiness_manager = get_readiness_manager("jarvis-body")
        app.state.readiness_manager = readiness_manager
        
        # Start phase transition
        await readiness_manager.start()
        logger.info("üìä [v95.3] ReadinessStateManager: STARTING phase")
        
        # Register critical components for tracking
        await readiness_manager.register_component("websocket", ComponentCategory.CRITICAL)
        await readiness_manager.register_component("service_registry", ComponentCategory.CRITICAL)
        await readiness_manager.register_component("ghost_proxies", ComponentCategory.IMPORTANT)
        await readiness_manager.register_component("ml_engine", ComponentCategory.IMPORTANT)
        await readiness_manager.register_component("voice_unlock", ComponentCategory.IMPORTANT)
        await readiness_manager.register_component("neural_mesh", ComponentCategory.OPTIONAL)
        await readiness_manager.register_component("trinity", ComponentCategory.OPTIONAL)
        
        # Transition to INITIALIZING
        await readiness_manager.mark_initializing()
        logger.info("üìä [v95.3] ReadinessStateManager: INITIALIZING phase")

        # v2.0: EARLY INLINE READINESS - Mark websocket ready if already mounted
        # Routes are mounted during app construction (before lifespan), so check now
        if getattr(app.state, 'websocket_mounted', False):
            try:
                await readiness_manager.mark_component_ready("websocket", healthy=True)
                logger.info("üìä [v2.0] websocket marked READY (early in lifespan)")
            except Exception as rm_err:
                logger.debug(f"Could not mark websocket ready: {rm_err}")

    except ImportError:
        logger.debug("ReadinessStateManager not available - using legacy health checks")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [v95.3] ReadinessStateManager init failed: {e}")

    # =================================================================
    # v95.2: CRITICAL - Early jarvis-body registration for cross-repo discovery
    # =================================================================
    # This MUST happen immediately so that jarvis-prime and reactor-core
    # can discover jarvis-body during their startup. Without this, external
    # services wait 120s and then give up.
    try:
        from core.service_registry import get_service_registry

        registry = get_service_registry()

        # v95.2: Use the new public method for immediate registration
        success = await registry.ensure_owner_registered_immediately()
        if success:
            logger.info("üìù [v95.2] jarvis-body registered early for cross-repo discovery")
        else:
            logger.warning("‚ö†Ô∏è [v95.2] Early registration may have failed - continuing")

        # Also start self-heartbeat loop
        await registry.start_cleanup_task()
        logger.info("üíì [v95.2] jarvis-body heartbeat loop started")

        # v2.0: INLINE READINESS - Mark service_registry as ready NOW
        # This is a CRITICAL component, marking it ready helps auto-transition
        if readiness_manager:
            try:
                await readiness_manager.mark_component_ready("service_registry", healthy=True)
                logger.info("üìä [v2.0] service_registry marked READY (inline)")
            except Exception as rm_err:
                logger.debug(f"Could not mark service_registry ready: {rm_err}")

    except ImportError:
        logger.debug("Service registry not available - skipping early registration")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è [v95.2] Early registration warning (continuing): {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CRITICAL: Pre-startup cleanup of stuck ML processes
    # This prevents the "startup timeout - please check logs" issue caused by
    # stuck processes from previous sessions blocking port 8010 or holding
    # ML model locks.
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        from core.process_isolated_ml_loader import cleanup_before_startup

        logger.info("üßπ Running pre-startup cleanup (stuck process detection)...")
        cleanup_results = await cleanup_before_startup(port=8010)

        if cleanup_results.get('ml_processes_cleaned', 0) > 0:
            logger.info(f"   Cleaned {cleanup_results['ml_processes_cleaned']} stuck ML processes")
        if cleanup_results.get('port_freed'):
            logger.info(f"   Freed port 8010 from stuck process")
        if cleanup_results.get('zombies_cleaned', 0) > 0:
            logger.info(f"   Cleaned {cleanup_results['zombies_cleaned']} zombie processes")

        logger.info("‚úÖ Pre-startup cleanup complete")

    except ImportError:
        logger.debug("Process isolated ML loader not available - skipping pre-startup cleanup")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Pre-startup cleanup failed (continuing anyway): {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CRITICAL: Memory-Aware Startup v17.8.7 - No LOCAL_MINIMAL Gap!
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Architecture Decision:
    # - RAM >= 6GB ‚Üí LOCAL_FULL (preload ML models locally, instant voice unlock)
    # - RAM < 6GB  ‚Üí CLOUD_FIRST (use GCP Spot VM, prevents "Processing..." hang)
    # - RAM < 2GB  ‚Üí CLOUD_ONLY (emergency mode)
    #
    # We eliminated LOCAL_MINIMAL because it caused "Processing..." hangs when
    # ML models were lazily loaded during voice unlock on RAM-constrained systems.
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    startup_decision = None
    cloud_ml_router = None
    try:
        from core.memory_aware_startup import (
            determine_startup_mode,
            activate_cloud_ml_if_needed,
            StartupMode,
        )

        startup_decision = await determine_startup_mode()
        app.state.startup_decision = startup_decision

        # Log the decision with clear explanation
        logger.info(f"üß† Memory-Aware Startup Decision:")
        logger.info(f"   Mode: {startup_decision.mode.value}")
        logger.info(f"   Reason: {startup_decision.reason}")
        logger.info(f"   Use Cloud ML: {startup_decision.use_cloud_ml}")

        # If cloud ML is needed, activate GCP Spot VM
        if startup_decision.gcp_vm_required:
            logger.info("‚òÅÔ∏è  CLOUD_FIRST mode - activating GCP ML backend...")
            cloud_result = await activate_cloud_ml_if_needed(startup_decision)
            app.state.cloud_ml_result = cloud_result

            if cloud_result.get("success"):
                logger.info(f"‚úÖ GCP ML backend active: {cloud_result.get('vm_id', 'N/A')}")
                logger.info(f"   Cost: ~${cloud_result.get('cost_per_hour', 0.029)}/hour")
            else:
                # v17.8.7: Don't fall back to LOCAL_MINIMAL (it's eliminated!)
                # CloudMLRouter has built-in failover to local if cloud fails
                logger.warning(f"‚ö†Ô∏è  GCP VM creation failed: {cloud_result.get('error')}")
                logger.warning("   CloudMLRouter will failover to local on first request")

        # Initialize CloudMLRouter with the startup decision
        # This ensures voice unlock uses the correct backend based on RAM
        try:
            from core.cloud_ml_router import get_cloud_ml_router
            cloud_ml_router = await get_cloud_ml_router()
            app.state.cloud_ml_router = cloud_ml_router
            logger.info(f"‚úÖ CloudMLRouter initialized (backend: {cloud_ml_router._current_backend.value})")
        except Exception as router_e:
            logger.warning(f"‚ö†Ô∏è  CloudMLRouter initialization failed: {router_e}")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # CLOUD ECAPA CLIENT v18.2.0 - Hybrid Cloud ML Backend Integration
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # This integrates the advanced CloudECAPAClient with:
        # - Multi-backend support: Cloud Run ‚Üí Spot VM ‚Üí Local fallback
        # - Intelligent routing based on memory pressure and cost
        # - Per-backend cost tracking with daily budget enforcement
        # - Auto-scaling Spot VMs with idle timeout
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        cloud_ecapa_client = None
        try:
            # Check if cloud ECAPA was already initialized by start_system.py
            cloud_ecapa_pre_initialized = os.getenv("CLOUD_ECAPA_INITIALIZED", "false").lower() == "true"

            if cloud_ecapa_pre_initialized:
                logger.info("‚òÅÔ∏è  CloudECAPAClient v18.2.0 - Pre-initialized by start_system.py")
                logger.info(f"   ‚Üí Backend: {os.getenv('CLOUD_ECAPA_BACKEND', 'unknown')}")
            else:
                # Initialize CloudECAPAClient if not already done
                logger.info("‚òÅÔ∏è  Initializing CloudECAPAClient v18.2.0...")

            from voice_unlock.cloud_ecapa_client import get_cloud_ecapa_client, CloudECAPAClient

            cloud_ecapa_client = await get_cloud_ecapa_client()

            if cloud_ecapa_client:
                # Check if already initialized
                if not cloud_ecapa_pre_initialized:
                    init_result = await cloud_ecapa_client.initialize()
                    if init_result.get("success"):
                        logger.info(f"‚úÖ CloudECAPAClient ready (backend: {init_result.get('backend', 'unknown')})")
                    else:
                        logger.warning(f"‚ö†Ô∏è  CloudECAPAClient init issue: {init_result.get('error')}")

                # Store in app state for runtime access
                app.state.cloud_ecapa_client = cloud_ecapa_client

                # Get current status
                status = cloud_ecapa_client.get_status()
                logger.info(f"   ‚Üí Cloud Run healthy: {status.get('cloud_run_healthy', False)}")
                logger.info(f"   ‚Üí Spot VM enabled: {status.get('spot_vm_enabled', False)}")
                logger.info(f"   ‚Üí Cache enabled: {status.get('cache_enabled', True)}")

                # If CloudMLRouter exists, connect it to CloudECAPAClient
                if cloud_ml_router:
                    cloud_ml_router._cloud_ecapa_client = cloud_ecapa_client
                    logger.info("   ‚Üí Linked to CloudMLRouter for unified routing")
            else:
                logger.warning("‚ö†Ô∏è  CloudECAPAClient not available - using local fallback")

        except ImportError as e:
            logger.debug(f"CloudECAPAClient import not available: {e}")
            logger.info("   ‚Üí Voice unlock will use local ECAPA encoder")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  CloudECAPAClient initialization error: {e}")
            logger.info("   ‚Üí Voice unlock will fallback to local processing")

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # VBI DEBUG TRACER & ECAPA PRE-WARM - Ensures no cold starts during voice unlock
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        try:
            from core.vbi_debug_tracer import prewarm_vbi_at_startup, get_orchestrator

            logger.info("üî• Pre-warming VBI (Voice Biometric Intelligence) Pipeline...")
            vbi_prewarm_result = await prewarm_vbi_at_startup()

            if vbi_prewarm_result:
                logger.info("‚úÖ VBI Pipeline pre-warmed - ready for instant voice unlock")
                # Store orchestrator in app state for access elsewhere
                app.state.vbi_orchestrator = get_orchestrator()
            else:
                logger.warning("‚ö†Ô∏è  VBI pre-warm incomplete - first unlock may be slower")

        except ImportError as e:
            logger.debug(f"VBI Debug Tracer not available: {e}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  VBI pre-warm failed: {e}")
            logger.info("   ‚Üí Voice unlock will work but first request may be slower")

    except ImportError:
        logger.debug("Memory-aware startup not available - using defaults")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Memory-aware startup check failed: {e}")
        logger.warning("   Continuing with default startup mode")

    # Start event loop watchdog to detect blocking ML operations
    try:
        from core.ml_operation_watchdog import start_event_loop_watchdog, stop_event_loop_watchdog
        loop = asyncio.get_running_loop()
        start_event_loop_watchdog(loop)
        logger.info("üêï ML operation watchdog started - monitoring event loop health")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not start event loop watchdog: {e}")

    # Initialize dynamic component manager if enabled
    global dynamic_component_manager, DYNAMIC_LOADING_ENABLED, gcp_vm_manager
    if DYNAMIC_LOADING_ENABLED and get_component_manager:
        logger.info("üß© Initializing Dynamic Component Management System...")
        dynamic_component_manager = get_component_manager()
        app.state.component_manager = dynamic_component_manager

        # Register memory pressure callback for GCP VM creation
        if GCP_VM_ENABLED:
            logger.info("‚òÅÔ∏è  GCP VM auto-creation enabled")
            dynamic_component_manager.memory_monitor.register_callback(memory_pressure_callback)
            logger.info("‚úÖ Memory pressure callback registered")

        # Start memory pressure monitoring (tracked for cleanup)
        if TASK_LIFECYCLE_AVAILABLE:
            task_mgr = get_task_manager()
            await task_mgr.spawn_monitor(
                "dynamic_component_monitor",
                dynamic_component_manager.start_monitoring()
            )
        else:
            asyncio.create_task(
                dynamic_component_manager.start_monitoring(),
                name="dynamic_component_monitor"
            )
        logger.info(f"   Memory limit: {dynamic_component_manager.memory_limit_gb}GB")
        logger.info(f"   ARM64 optimized: {dynamic_component_manager.arm64_optimizer.is_arm64}")
        logger.info("‚úÖ Dynamic component loading enabled")

    # Note: Legacy process cleanup removed - replaced by process_isolated_ml_loader cleanup above

    # Run parallel imports if enabled
    if DYNAMIC_LOADING_ENABLED and dynamic_component_manager:
        # Dynamic loading mode - load only CORE components at startup
        logger.info("üß© Loading CORE components dynamically...")
        try:
            from core.dynamic_component_manager import ComponentPriority

            # Load only CORE priority components at startup
            core_components = [
                name
                for name, comp in dynamic_component_manager.components.items()
                if comp.priority == ComponentPriority.CORE
            ]

            # IMPORTANT: Always include vision as CORE to prevent multi-space query issues
            if "vision" not in core_components:
                core_components.append("vision")
                logger.info(
                    "   ‚ö†Ô∏è Vision not in CORE list, adding it to ensure multi-space queries work"
                )

            # IMPORTANT: Voice unlock API is CRITICAL for biometric authentication
            # Must be loaded at startup to mount the /api/voice-unlock router
            components["voice_unlock"] = import_voice_unlock()
            if components["voice_unlock"] and components["voice_unlock"].get("router"):
                logger.info("   ‚úÖ voice_unlock loaded (API router available)")
            else:
                logger.warning("   ‚ö†Ô∏è voice_unlock loaded but router not available")

            # IMPORTANT: Voice system must be loaded for JARVIS voice interface
            # This provides jarvis_available, jarvis_router, etc. for /voice/jarvis/* endpoints
            components["voice"] = import_voice_system()
            if components["voice"] and components["voice"].get("jarvis_available"):
                logger.info("   ‚úÖ voice system loaded (JARVIS available)")
            else:
                logger.warning("   ‚ö†Ô∏è voice system loaded but JARVIS not available")

            logger.info(f"   Loading {len(core_components)} CORE components: {core_components}")

            for comp_name in core_components:
                success = await dynamic_component_manager.load_component(comp_name)
                if success:
                    comp = dynamic_component_manager.components[comp_name]
                    components[comp_name] = comp.instance
                    logger.info(f"   ‚úÖ {comp_name} loaded ({comp.memory_estimate_mb}MB)")
                else:
                    logger.warning(f"   ‚ö†Ô∏è {comp_name} failed to load")

            logger.info(
                f"‚úÖ Dynamic component loading active - {len(core_components)} CORE components loaded"
            )
            logger.info(f"   Other components will load on-demand based on user commands")

        except Exception as e:
            logger.error(f"Dynamic loading failed, falling back to legacy mode: {e}")
            DYNAMIC_LOADING_ENABLED = False

    if not DYNAMIC_LOADING_ENABLED:
        # Legacy mode - load all components at startup
        if OPTIMIZE_STARTUP and PARALLEL_IMPORTS:
            await parallel_import_components()
        else:
            # Sequential imports (legacy mode)
            logger.info("Running sequential imports (legacy mode)")
            components["chatbots"] = import_chatbots()
            components["vision"] = import_vision_system()
            components["memory"] = import_memory_system()
            components["voice"] = import_voice_system()
            components["ml_models"] = import_ml_models()
            components["monitoring"] = import_monitoring()
            components["voice_unlock"] = import_voice_unlock()
            components["wake_word"] = import_wake_word()
            components["display_monitor"] = import_display_monitor()
            components["goal_inference"] = import_goal_inference()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ADVANCED COMPONENT WARMUP (Pre-initialize for instant response)
    # Skipped when startup_decision indicates low RAM / cloud-first mode
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    should_skip_warmup = (
        startup_decision is not None and startup_decision.skip_component_warmup
    )

    if should_skip_warmup:
        logger.info("‚è≠Ô∏è  Skipping component warmup (cloud-first mode)")
        logger.info(f"   Reason: {startup_decision.reason if startup_decision else 'Low RAM'}")
        logger.info("   Components will lazy-load on first use")
    else:
        try:
            logger.info("üöÄ Starting advanced component warmup...")
            from api.unified_command_processor import get_unified_processor

            processor = get_unified_processor(app=app)
            warmup_report = await processor.warmup_components()

            if warmup_report:
                logger.info(
                    f"‚úÖ Component warmup complete! "
                    f"{warmup_report['ready_count']}/{warmup_report['total_count']} ready "
                    f"in {warmup_report['total_load_time']:.2f}s"
                )
                app.state.warmup_report = warmup_report
            else:
                logger.warning("‚ö†Ô∏è Component warmup failed, using lazy initialization")
        except Exception as e:
            logger.error(f"‚ùå Component warmup error: {e}", exc_info=True)
            logger.warning("‚ö†Ô∏è Falling back to lazy initialization")

    # Initialize memory manager (NON-BLOCKING - spawns monitoring as background task)
    memory_class = components.get("memory", {}).get("manager_class")
    if memory_class:
        app.state.memory_manager = memory_class()
        # CRITICAL FIX: Don't await start_monitoring() - it's an infinite loop!
        # Spawn it as a tracked background task instead
        if TASK_LIFECYCLE_AVAILABLE:
            try:
                task_mgr = get_task_manager()
                await task_mgr.spawn_monitor(
                    "memory_manager_monitor",
                    app.state.memory_manager.start_monitoring()
                )
                logger.info("‚úÖ Memory manager initialized (monitoring task spawned)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Memory manager task spawn failed: {e}")
                # Fallback: spawn untracked task
                asyncio.create_task(
                    app.state.memory_manager.start_monitoring(),
                    name="memory_manager_monitor"
                )
                logger.info("‚úÖ Memory manager initialized (fallback)")
        else:
            # Fallback without TaskLifecycleManager
            asyncio.create_task(
                app.state.memory_manager.start_monitoring(),
                name="memory_manager_monitor"
            )
            logger.info("‚úÖ Memory manager initialized (basic)")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # VOICE UNLOCK ML MODEL PREWARMING (Critical for instant "unlock my screen")
    # Skipped when startup_decision indicates low RAM / cloud-first mode
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    should_skip_ml_prewarm = (
        startup_decision is not None and
        (startup_decision.skip_local_whisper or startup_decision.skip_local_ecapa)
    )

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ML ENGINE REGISTRY - Handles both local and cloud routing
    # CRITICAL FIX v3.0: Registry MUST be created even if prewarm fails
    # This prevents "Voice verification failed (confidence: 0.0%)" errors
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    ml_prewarm_task = None  # Store task for potential cleanup
    registry = None  # CRITICAL: Initialize before try block

    async def _create_registry_robust():
        """
        Robust registry creation with multiple fallback strategies.
        Returns registry or None if all strategies fail.
        """
        nonlocal registry

        # Strategy 1: Standard async creation
        try:
            try:
                from backend.voice_unlock.ml_engine_registry import get_ml_registry
            except ImportError:
                from voice_unlock.ml_engine_registry import get_ml_registry
            registry = await get_ml_registry()
            if registry:
                logger.info("   ‚úÖ Registry created via get_ml_registry()")
                return registry
        except Exception as e:
            logger.debug(f"   Strategy 1 failed: {e}")

        # Strategy 2: Sync creation with auto_create
        try:
            try:
                from backend.voice_unlock.ml_engine_registry import get_ml_registry_sync
            except ImportError:
                from voice_unlock.ml_engine_registry import get_ml_registry_sync
            registry = get_ml_registry_sync(auto_create=True)
            if registry:
                logger.info("   ‚úÖ Registry created via get_ml_registry_sync(auto_create=True)")
                return registry
        except Exception as e:
            logger.debug(f"   Strategy 2 failed: {e}")

        # Strategy 3: Direct instantiation
        try:
            try:
                from backend.voice_unlock.ml_engine_registry import MLEngineRegistry
            except ImportError:
                from voice_unlock.ml_engine_registry import MLEngineRegistry
            registry = MLEngineRegistry()
            logger.info("   ‚úÖ Registry created via direct MLEngineRegistry()")
            return registry
        except Exception as e:
            logger.debug(f"   Strategy 3 failed: {e}")

        return None

    try:
        logger.info("üî• Initializing ML Engine Registry (Hybrid Local/Cloud)...")

        # Use robust creation with multiple fallbacks
        registry = await _create_registry_robust()

        if registry:
            logger.info(f"   ‚Üí ML Engine Registry initialized with engines: {list(registry._engines.keys())}")

            if should_skip_ml_prewarm:
                logger.info("‚òÅÔ∏è  Cloud-first mode detected - registry will route to GCP")
                logger.info(f"   Reason: {startup_decision.reason if startup_decision else 'Low RAM'}")

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # NON-BLOCKING PREWARM - FastAPI can respond to health checks!
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            def on_prewarm_complete(status):
                """Callback when prewarm finishes."""
                if status.is_ready:
                    if registry.is_using_cloud:
                        logger.info("‚úÖ BACKGROUND ML PREWARM COMPLETE (CLOUD MODE)")
                        logger.info(f"   ‚Üí Cloud endpoint: {registry.cloud_endpoint}")
                    else:
                        duration_ms = status.prewarm_duration_ms or 0
                        logger.info(f"‚úÖ BACKGROUND ML PREWARM COMPLETE in {duration_ms:.0f}ms (LOCAL MODE)")
                        logger.info(f"   ‚Üí {status.ready_count}/{status.total_count} engines ready")
                else:
                    logger.warning(f"‚ö†Ô∏è BACKGROUND ML PREWARM PARTIAL: {status.ready_count}/{status.total_count} ready")

            # Launch background prewarm - THIS RETURNS IMMEDIATELY!
            try:
                ml_prewarm_task = registry.prewarm_background(
                    parallel=True,
                    startup_decision=startup_decision,
                    on_complete=on_prewarm_complete,
                )
                logger.info("üîÑ ML prewarm launched as BACKGROUND TASK")
                logger.info("   ‚Üí Voice unlock will use on-demand loading if prewarm incomplete")
            except Exception as prewarm_err:
                logger.warning(f"   ‚ö†Ô∏è Background prewarm failed: {prewarm_err}")
                logger.info("   ‚Üí Voice unlock will use on-demand loading instead")
        else:
            logger.error("‚ùå All registry creation strategies failed!")

    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è ML Engine Registry import issue: {e}")
        # Try emergency creation
        if registry is None:
            registry = await _create_registry_robust()

        # Fallback to legacy prewarmer if registry still not available
        if not should_skip_ml_prewarm and registry is None:
            try:
                try:
                    from backend.voice_unlock.ml_model_prewarmer import prewarm_voice_unlock_models_background
                except ImportError:
                    from voice_unlock.ml_model_prewarmer import prewarm_voice_unlock_models_background
                await prewarm_voice_unlock_models_background()
                logger.info("   ‚Üí Using legacy background prewarmer")
            except Exception:
                pass

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è ML Engine Registry initialization error: {e}")
        # Try emergency creation
        if registry is None:
            registry = await _create_registry_robust()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CRITICAL VERIFICATION: Ensure registry exists
    # This MUST succeed or voice unlock will return 0% confidence
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if registry is None:
        logger.error("=" * 60)
        logger.error("‚ùå CRITICAL: ML Engine Registry was NOT created!")
        logger.error("   Voice unlock WILL fail with 0% confidence!")
        logger.error("   Attempting final emergency creation...")
        logger.error("=" * 60)
        try:
            from voice_unlock.ml_engine_registry import MLEngineRegistry
            registry = MLEngineRegistry()
            logger.info("   ‚úÖ Emergency registry created successfully")
        except Exception as final_err:
            logger.error(f"   ‚ùå Final emergency creation failed: {final_err}")
            logger.error("   Voice unlock is DISABLED until restart")

    # Store registry in app state for runtime access
    if registry:
        app.state.ml_registry = registry
        logger.info("‚úÖ ML Engine Registry stored in app.state.ml_registry")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # NEURAL MESH INITIALIZATION (Multi-Agent Collaboration System)
    # Skipped when startup_decision indicates low RAM / cloud-first mode
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    should_skip_neural_mesh = (
        startup_decision is not None and startup_decision.skip_neural_mesh
    )

    if should_skip_neural_mesh:
        logger.info("‚è≠Ô∏è  Skipping Neural Mesh initialization (cloud-first mode)")
        logger.info(f"   Reason: {startup_decision.reason if startup_decision else 'Low RAM'}")
        neural_mesh_result = None
    else:
        try:
            from neural_mesh.integration import (
                initialize_neural_mesh,
                NeuralMeshConfig,
                is_neural_mesh_initialized,
            )

            # Initialize Neural Mesh with Crew system in background
            logger.info("üß† Initializing Neural Mesh Multi-Agent System...")
            neural_mesh_config = NeuralMeshConfig(
                enable_crew=True,
                enable_monitoring=True,
                enable_knowledge_graph=True,
                enable_communication_bus=True,
                lazy_load=True,
            )

            # Run initialization asynchronously
            neural_mesh_result = await initialize_neural_mesh(neural_mesh_config)

            if neural_mesh_result.get("status") == "initialized":
                app.state.neural_mesh = neural_mesh_result
                components_loaded = neural_mesh_result.get("components", [])
                elapsed = neural_mesh_result.get("elapsed_seconds", 0)
                logger.info(
                    f"‚úÖ Neural Mesh initialized in {elapsed:.2f}s "
                    f"(components: {', '.join(components_loaded)})"
                )
            elif neural_mesh_result.get("status") == "already_initialized":
                app.state.neural_mesh = neural_mesh_result
                logger.info("‚úÖ Neural Mesh already initialized")
            else:
                logger.warning(
                    f"‚ö†Ô∏è Neural Mesh initialization: {neural_mesh_result.get('status')} "
                    f"- {neural_mesh_result.get('error', 'unknown error')}"
                )
        except ImportError as e:
            logger.debug(f"Neural Mesh not available: {e}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Neural Mesh initialization skipped: {e}")

    # Initialize Goal Inference and start background tasks
    goal_inference = components.get("goal_inference", {})
    if goal_inference and goal_inference.get("integration"):
        try:
            integration = goal_inference["integration"]
            app.state.goal_inference_integration = integration

            # Start background tasks for learning and pattern optimization
            # These are graceful loops that check for shutdown and handle CancelledError

            async def periodic_database_cleanup():
                """Clean up old patterns and optimize database (graceful loop)"""
                task_mgr = get_task_manager() if TASK_LIFECYCLE_AVAILABLE else None
                try:
                    while True:
                        # Check for shutdown before sleeping
                        if task_mgr and task_mgr.is_shutting_down():
                            logger.info("Database cleanup: shutdown requested, exiting")
                            break
                        try:
                            # Interruptible sleep (check every 30s for shutdown)
                            for _ in range(120):  # 120 * 30s = 3600s = 1 hour
                                if task_mgr and task_mgr.is_shutting_down():
                                    break
                                await asyncio.sleep(30)
                            if task_mgr and task_mgr.is_shutting_down():
                                break
                            if hasattr(integration, "learning_db"):
                                # Clean up old patterns
                                integration.learning_db.cleanup_old_patterns(days=30)
                                # Optimize database
                                integration.learning_db.optimize()
                                logger.debug("‚úÖ Goal Inference database cleanup completed")
                        except asyncio.CancelledError:
                            raise  # Re-raise for proper cancellation
                        except Exception as e:
                            logger.error(f"Database cleanup error: {e}")
                except asyncio.CancelledError:
                    logger.info("Database cleanup task cancelled")
                    raise

            async def periodic_pattern_analysis():
                """Analyze patterns and update confidence scores (graceful loop)"""
                task_mgr = get_task_manager() if TASK_LIFECYCLE_AVAILABLE else None
                try:
                    while True:
                        # Check for shutdown before sleeping
                        if task_mgr and task_mgr.is_shutting_down():
                            logger.info("Pattern analysis: shutdown requested, exiting")
                            break
                        try:
                            # Interruptible sleep (check every 30s for shutdown)
                            for _ in range(60):  # 60 * 30s = 1800s = 30 minutes
                                if task_mgr and task_mgr.is_shutting_down():
                                    break
                                await asyncio.sleep(30)
                            if task_mgr and task_mgr.is_shutting_down():
                                break
                            if hasattr(integration, "learning_db"):
                                # Analyze patterns
                                patterns = integration.learning_db.analyze_patterns()
                                # Update confidence scores based on success rates
                                for pattern in patterns:
                                    if pattern.get("success_rate", 0) > 0.9:
                                        integration.learning_db.boost_pattern_confidence(
                                            pattern["id"], boost=0.05
                                        )
                                logger.debug("‚úÖ Pattern analysis completed")
                        except asyncio.CancelledError:
                            raise  # Re-raise for proper cancellation
                        except Exception as e:
                            logger.error(f"Pattern analysis error: {e}")
                except asyncio.CancelledError:
                    logger.info("Pattern analysis task cancelled")
                    raise

            # Start background tasks (tracked for cleanup)
            if TASK_LIFECYCLE_AVAILABLE:
                task_mgr = get_task_manager()
                await task_mgr.spawn_monitor("goal_inference_db_cleanup", periodic_database_cleanup())
                await task_mgr.spawn_monitor("goal_inference_pattern_analysis", periodic_pattern_analysis())
            else:
                asyncio.create_task(periodic_database_cleanup(), name="goal_inference_db_cleanup")
                asyncio.create_task(periodic_pattern_analysis(), name="goal_inference_pattern_analysis")

            logger.info("‚úÖ Goal Inference background tasks started")
            logger.info("   ‚Ä¢ Database cleanup: every 1 hour")
            logger.info("   ‚Ä¢ Pattern analysis: every 30 minutes")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not start Goal Inference tasks: {e}")

    # Initialize vision analyzer BEFORE UAE (so UAE can use it)
    logger.info("üëÅÔ∏è  Initializing Claude Vision Analyzer...")
    vision = components.get("vision", {})
    vision_analyzer = None
    if vision.get("available"):
        analyzer_class = vision.get("analyzer")
        # Get API key with fallback chain: Secret Manager -> environment
        if SECRET_MANAGER_AVAILABLE:
            api_key = get_anthropic_key()
        else:
            api_key = os.getenv("ANTHROPIC_API_KEY")
        if analyzer_class and api_key:
            vision_analyzer = analyzer_class(api_key)
            app.state.vision_analyzer = vision_analyzer
            logger.info("‚úÖ Claude Vision Analyzer initialized and stored in app.state")
        else:
            logger.warning("‚ö†Ô∏è  Vision analyzer available but no ANTHROPIC_API_KEY set")
    else:
        logger.warning("‚ö†Ô∏è  Vision system not available")

    # Initialize UAE (Unified Awareness Engine) with LAZY LOADING for memory optimization
    # This prevents 10GB+ memory usage at startup by loading on first use
    try:
        # Check if lazy loading is enabled (default: True for memory efficiency)
        lazy_load_intelligence = os.getenv("JARVIS_LAZY_INTELLIGENCE", "true").lower() == "true"

        if lazy_load_intelligence:
            logger.info("üß† UAE/SAI/Learning DB: LAZY LOADING enabled (loads on first use)")
            logger.info("   üíæ Memory saved: ~8-10GB at startup")
            logger.info("   ‚ö° Intelligence components will initialize when needed")

            # Store initialization parameters for lazy loading
            app.state.uae_lazy_config = {
                "vision_analyzer": vision_analyzer,
                "sai_monitoring_interval": 5.0,
                "enable_auto_start": True,
                "enable_learning_db": True,
                "enable_yabai": True,
                "enable_proactive_intelligence": True,
            }
            app.state.uae_engine = None  # Will be initialized on first use
            app.state.learning_db = None
            app.state.uae_initializing = False

            # Initialize Hybrid Orchestrator (always initialized)
            logger.info("üåê Initializing Hybrid Orchestrator (Local + GCP)...")
            try:
                from backend.core.hybrid_orchestrator import get_orchestrator

                hybrid_orchestrator = get_orchestrator()
                await hybrid_orchestrator.start()
                app.state.hybrid_orchestrator = hybrid_orchestrator
                logger.info("‚úÖ Hybrid Orchestrator initialized (intelligent routing active)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Hybrid Orchestrator not available: {e}")
                app.state.hybrid_orchestrator = None

        else:
            logger.info(
                "üß† Initializing UAE (Unified Awareness Engine) with Learning Database + Yabai..."
            )
            from intelligence.uae_integration import get_learning_db, get_yabai, initialize_uae

            # Use the vision analyzer we just created
            if vision_analyzer:
                logger.info("‚úÖ Connecting vision analyzer to UAE + SAI + Learning Database")

            # Create voice callback for Phase 4 Proactive Intelligence
            async def voice_callback(text: str):
                """Voice callback for proactive suggestions"""
                try:
                    voice = components.get("voice", {})
                    jarvis_api = voice.get("jarvis_api")
                    if jarvis_api:
                        await jarvis_api.speak({"text": text})
                        logger.debug(f"[PROACTIVE-VOICE] Spoke: {text}")
                    else:
                        logger.warning("[PROACTIVE-VOICE] JARVIS API not available")
                except Exception as e:
                    logger.error(f"[PROACTIVE-VOICE] Error: {e}")

            # Create notification callback for Phase 4 Proactive Intelligence
            async def notification_callback(title: str, message: str, priority: str = "low"):
                """Notification callback for proactive suggestions"""
                try:
                    # Log notification (can be extended to use macOS notifications)
                    logger.info(f"[PROACTIVE-NOTIFY] [{priority.upper()}] {title}: {message}")
                    # Future: Can integrate with macOS notification center
                    # osascript -e 'display notification "message" with title "title"'
                except Exception as e:
                    logger.error(f"[PROACTIVE-NOTIFY] Error: {e}")

            # Initialize UAE with SAI + Learning Database + Yabai + Proactive Intelligence
            logger.info("üîß Initializing FULL intelligence stack (24/7 mode)...")
            logger.info("   Step 1/8: Learning Database initialization...")
            logger.info("   Step 2/8: Behavioral Pattern Learning...")
            logger.info("   Step 3/8: Yabai Spatial Intelligence (workspace monitoring)...")
            logger.info("   Step 4/8: Situational Awareness Engine (SAI)...")
            logger.info("   Step 5/8: Context Intelligence Layer...")
            logger.info("   Step 6/8: Decision Fusion Engine + 24/7 monitoring...")
            logger.info("   Step 7/8: Goal-Oriented Workflow Prediction...")
            logger.info("   Step 8/8: Proactive Communication Engine (Magic)...")

            uae = await initialize_uae(
                vision_analyzer=vision_analyzer,
                sai_monitoring_interval=5.0,  # Enhanced 24/7 mode: 5 seconds
                enable_auto_start=True,  # Start monitoring immediately
                enable_learning_db=True,  # Enable persistent memory
                enable_yabai=True,  # Enable Yabai spatial intelligence
                enable_proactive_intelligence=True,  # Enable Phase 4: Proactive Communication
                voice_callback=voice_callback,  # Natural voice suggestions
                notification_callback=notification_callback,  # Visual notifications
            )

            if uae and uae.is_active:
                app.state.uae_engine = uae

                # Get Learning DB instance
                learning_db = get_learning_db()
                if learning_db:
                    app.state.learning_db = learning_db

                    # Get Learning DB metrics
                    try:
                        metrics = await learning_db.get_learning_metrics()

                        # Get Yabai instance and metrics
                        yabai = get_yabai()
                        yabai_active = yabai is not None and yabai.yabai_available

                        logger.info(
                            "‚úÖ UAE + SAI + Learning Database + Yabai + Proactive Intelligence initialized successfully"
                        )
                        logger.info("   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
                        logger.info(
                            "   üß† PHASE 4 INTELLIGENCE STACK: FULLY OPERATIONAL (24/7 MODE)"
                        )
                        logger.info("   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
                        logger.info("   üìç PHASE 1: Environmental Awareness")
                        logger.info(
                            "   ‚Ä¢ SAI (Situational Awareness): ‚úÖ Active (5s monitoring - 24/7)"
                        )
                        logger.info(
                            f"   ‚Ä¢ Yabai Spatial Intelligence: {'‚úÖ Active (workspace monitoring)' if yabai_active else '‚ö†Ô∏è  Not available'}"
                        )
                        logger.info("   ‚Ä¢ Context Intelligence: ‚úÖ Active (with persistent memory)")
                        logger.info("")
                        logger.info("   üìç PHASE 2: Decision Intelligence")
                        logger.info("   ‚Ä¢ Decision Fusion Engine: ‚úÖ Active (confidence-weighted)")
                        logger.info("   ‚Ä¢ Cross-Session Memory: ‚úÖ Enabled (survives restarts)")
                        logger.info("")
                        logger.info("   üìç PHASE 3: Behavioral Learning (Smart)")
                        logger.info("   ‚Ä¢ Learning Database: ‚úÖ Active (async + ChromaDB)")
                        logger.info("   ‚Ä¢ Predictive Intelligence: ‚úÖ Enabled (temporal patterns)")
                        logger.info("   ‚Ä¢ 24/7 Behavioral Learning: ‚úÖ Enabled (always watching)")
                        logger.info("   ‚Ä¢ Workflow Pattern Recognition: ‚úÖ Active")
                        logger.info("")
                        logger.info("   üìç PHASE 4: Proactive Communication (Magic)")
                        logger.info("   ‚Ä¢ Natural Language Suggestions: ‚úÖ Active")
                        logger.info("   ‚Ä¢ Voice Output: ‚úÖ Enabled (JARVIS API)")
                        logger.info("   ‚Ä¢ Predictive App Launching: ‚úÖ Active")
                        logger.info("   ‚Ä¢ Workflow Optimization Tips: ‚úÖ Active")
                        logger.info("   ‚Ä¢ Smart Space Switching: ‚úÖ Active")
                        logger.info("   ‚Ä¢ Context-Aware Timing: ‚úÖ Enabled (focus-level detection)")
                        logger.info("   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
                        logger.info("   üìä LEARNING DATABASE METRICS:")
                        logger.info(f"   ‚Ä¢ Total Patterns: {metrics['patterns']['total_patterns']}")
                        logger.info(
                            f"   ‚Ä¢ Display Patterns: {metrics['display_patterns']['total_display_patterns']}"
                        )
                        logger.info(
                            f"   ‚Ä¢ Pattern Cache Hit Rate: {metrics['cache_performance']['pattern_cache_hit_rate']:.1%}"
                        )
                        logger.info("   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
                        logger.info("   üéØ CAPABILITIES:")
                        logger.info("   ‚Ä¢ Learns user patterns across all macOS workspace")
                        logger.info("   ‚Ä¢ Predicts actions before you ask")
                        logger.info("   ‚Ä¢ Proactively suggests apps and workflows naturally")
                        logger.info("   ‚Ä¢ Speaks suggestions with human-like communication")
                        logger.info("   ‚Ä¢ Adapts to UI changes automatically")
                        logger.info("   ‚Ä¢ Remembers preferences across restarts")
                        logger.info("   ‚Ä¢ Self-healing when environment changes")
                        logger.info(
                            "   ‚Ä¢ Respects your focus level (no interruptions during deep work)"
                        )
                        logger.info("   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
                        logger.info("   üí¨ PROACTIVE EXAMPLES:")
                        logger.info(
                            "   ‚Ä¢ 'Hey, you usually open Slack around this time. Want me to launch it?'"
                        )
                        logger.info(
                            "   ‚Ä¢ 'I noticed your email workflow is slower than usual. Try filtering first.'"
                        )
                        logger.info(
                            "   ‚Ä¢ 'You typically switch to Space 2 when coding. Should I move you there?'"
                        )
                        logger.info("   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
                    except Exception as e:
                        logger.warning(f"Could not get Learning DB metrics: {e}")
                else:
                    logger.info("‚úÖ UAE + SAI initialized successfully")
                    logger.info("   ‚Ä¢ SAI monitoring: Active (10s interval)")
                    logger.info("   ‚Ä¢ Context intelligence: Active")
                    logger.info("   ‚Ä¢ Display clicker: Will use UAE+SAI enhanced mode")
                    logger.info("   ‚Ä¢ Proactive adaptation: Enabled")
                    logger.warning("   ‚ö†Ô∏è  Learning Database: Not active (no persistent memory)")
            else:
                logger.warning("‚ö†Ô∏è UAE initialized but not active")

            # Initialize Hybrid Orchestrator (always initialized)
            logger.info("üåê Initializing Hybrid Orchestrator (Local + GCP)...")
            try:
                from backend.core.hybrid_orchestrator import get_orchestrator

                hybrid_orchestrator = get_orchestrator()
                await hybrid_orchestrator.start()
                app.state.hybrid_orchestrator = hybrid_orchestrator
                logger.info("‚úÖ Hybrid Orchestrator initialized (intelligent routing active)")
                logger.info("   ‚Ä¢ Local Mac (16GB) - Vision, Voice, macOS features")
                logger.info("   ‚Ä¢ GCP Cloud (32GB) - ML, NLP, heavy processing")
                logger.info("   ‚Ä¢ UAE/SAI/CAI integrated for intelligent routing")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Hybrid Orchestrator not available: {e}")
                app.state.hybrid_orchestrator = None

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not initialize UAE + Learning Database: {e}")
        logger.info("   Falling back to SAI-only mode for display connections")

    # Discover running services (if dynamic CORS is available)
    try:
        from api.dynamic_cors_handler import AutoPortDiscovery

        services = await AutoPortDiscovery.discover_services()
        if services:
            logger.info(f"üîç Discovered services: {services}")
            config = AutoPortDiscovery.get_recommended_config(services)
            logger.info(f"üìù Recommended config: {config}")
    except Exception as e:
        logger.debug(f"Service discovery skipped: {e}")

    # Initialize Rust acceleration for vision system with self-healing
    try:
        from vision.dynamic_component_loader import get_component_loader
        from vision.rust_startup_integration import initialize_rust_acceleration

        # Start self-healing and dynamic component loader
        logger.info("üîß Initializing self-healing system...")
        loader = get_component_loader()
        await loader.start()  # This also starts the self-healer

        # Initialize Rust acceleration
        rust_config = await initialize_rust_acceleration()

        if rust_config.get("available"):
            app.state.rust_acceleration = rust_config
            logger.info("ü¶Ä Rust acceleration initialized:")

            # Log performance boosts
            boosts = rust_config.get("performance_boost", {})
            if boosts:
                for component, boost in boosts.items():
                    if boost > 1.0:
                        logger.info(f"   ‚Ä¢ {component}: {boost:.1f}x faster")

            # Log memory savings
            mem_savings = rust_config.get("memory_savings", {})
            if mem_savings.get("enabled"):
                logger.info(f"   ‚Ä¢ Memory pool: {mem_savings['rust_pool_mb']}MB")
                logger.info(f"   ‚Ä¢ Estimated savings: {mem_savings['estimated_savings_percent']}%")
        else:
            logger.info("ü¶Ä Rust acceleration not available (Python fallback active)")
            logger.debug(f"   Reason: {rust_config.get('fallback_reason', 'Unknown')}")

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not initialize Rust acceleration: {e}")
        app.state.rust_acceleration = {"available": False}

    # Connect vision analyzer to other components (analyzer already initialized earlier)
    if hasattr(app.state, "vision_analyzer") and app.state.vision_analyzer:
        logger.info("üîó Connecting vision analyzer to other JARVIS components...")

        # Connect Vision Navigator to vision analyzer (for display connection)
        try:
            from display.vision_ui_navigator import get_vision_navigator

            navigator = get_vision_navigator()
            navigator.set_vision_analyzer(app.state.vision_analyzer)
            logger.info("‚úÖ Vision Navigator connected to Claude Vision analyzer")
            logger.info("   üëÅÔ∏è JARVIS can now navigate Control Center using vision!")
        except Exception as e:
            logger.debug(f"Vision Navigator connection skipped: {e}")

        # Set vision analyzer in vision websocket manager
        try:
            from api.vision_websocket import set_vision_analyzer

            set_vision_analyzer(app.state.vision_analyzer)
            logger.info("‚úÖ Vision analyzer set in vision websocket manager")
        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è Could not set vision analyzer in websocket: {e}")

        # Set app state in JARVIS factory for dependency injection
        try:
            from api.jarvis_factory import set_app_state

            set_app_state(app.state)
            logger.info("‚úÖ App state set in JARVIS factory")
        except ImportError:
            logger.warning("‚ö†Ô∏è JARVIS factory not available for dependency injection")
    else:
        logger.warning("‚ö†Ô∏è Vision analyzer not available - vision features disabled")

    # Initialize proactive monitoring components
    try:
        # Set JARVIS API in vision command handler for voice integration
        from api.vision_command_handler import vision_command_handler

        voice = components.get("voice", {})
        if voice.get("jarvis_api"):
            vision_command_handler.jarvis_api = voice["jarvis_api"]
            logger.info("‚úÖ JARVIS voice API connected to pure vision command handler")

        # Initialize pure intelligence with API key
        if api_key:
            await vision_command_handler.initialize_intelligence(api_key)
            logger.info("‚úÖ Pure vision intelligence initialized")

        # ========================================================================
        # Initialize Context Integration Bridge (Priority 1-3 Features)
        # Multi-Space Context Tracking + Implicit Reference + Cross-Space Intelligence
        # ========================================================================
        try:
            from backend.core.context.context_integration_bridge import (
                initialize_integration_bridge,
            )

            logger.info("üß† Initializing Context Intelligence System...")
            logger.info("   Priority 1: Multi-Space Context Tracking")
            logger.info("   Priority 2: 'What Does It Say?' Understanding")
            logger.info("   Priority 3: Cross-Space Intelligence")

            # Initialize bridge with auto-start
            bridge = await initialize_integration_bridge(auto_start=True)
            app.state.context_bridge = bridge

            # Integrate with PureVisionIntelligence for vision updates
            if hasattr(vision_command_handler, "vision_intelligence"):
                logger.info("   üîó Connecting Vision Intelligence to Context Bridge...")
                # Store bridge reference in vision intelligence so it can feed updates
                vision_command_handler.vision_intelligence.context_bridge = bridge
                logger.info("   ‚úÖ Vision Intelligence connected to Context Bridge")

            # Integrate with AsyncPipeline for command processing
            jarvis_api = voice.get("jarvis_api")
            if jarvis_api and hasattr(jarvis_api, "async_pipeline"):
                jarvis_api.async_pipeline.context_bridge = bridge
                logger.info("   ‚úÖ AsyncPipeline connected to Context Bridge")

            # Get intelligence summary
            summary = bridge.get_workspace_intelligence_summary()
            logger.info("‚úÖ Context Intelligence System initialized:")
            logger.info(
                f"   ‚Ä¢ Multi-Space Context Tracking: Active ({summary.get('total_spaces', 0)} spaces)"
            )
            logger.info(f"   ‚Ä¢ Implicit Reference Resolution: Enabled")
            logger.info(f"   ‚Ä¢ Cross-Space Intelligence: Enabled")
            logger.info(
                f"   ‚Ä¢ Natural Language Queries: 'what does it say?', 'what am I working on?'"
            )
            logger.info(f"   ‚Ä¢ Workspace Synthesis: Combining context from all spaces")

        except ImportError as e:
            logger.warning(f"   ‚ö†Ô∏è Context Intelligence System not available: {e}")
            app.state.context_bridge = None
        except Exception as e:
            logger.error(
                f"   ‚ùå Context Intelligence initialization failed: {e}",
                exc_info=True,
            )
            app.state.context_bridge = None

        # ========================================================================
        # Initialize ALL 6 Upgraded v2.0 Systems with HybridMonitoring Integration
        # ========================================================================
        logger.info("\n" + "=" * 60)
        logger.info("üöÄ INITIALIZING v2.0 INTELLIGENT SYSTEMS")
        logger.info("=" * 60)

        try:
            # Get HybridProactiveMonitoringManager (if available)
            hybrid_monitoring = None
            try:
                from context_intelligence.managers.hybrid_proactive_monitoring_manager import (
                    get_hybrid_proactive_monitoring_manager,
                )

                hybrid_monitoring = get_hybrid_proactive_monitoring_manager()
                logger.info("‚úÖ HybridProactiveMonitoringManager: Available")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è HybridMonitoring not available: {e}")

            # Get ImplicitReferenceResolver (from context bridge)
            implicit_resolver = None
            if hasattr(app.state, "context_bridge") and app.state.context_bridge:
                try:
                    implicit_resolver = app.state.context_bridge.implicit_resolver
                    logger.info("‚úÖ ImplicitReferenceResolver: Available")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è ImplicitResolver not available: {e}")

            # 1. TemporalQueryHandler v3.0
            try:
                from context_intelligence.handlers.temporal_query_handler import (
                    initialize_temporal_query_handler,
                )
                from context_intelligence.managers import get_change_detection_manager
                from core.conversation_tracker import get_conversation_tracker  # type: ignore[import-not-found]

                temporal_handler = initialize_temporal_query_handler(
                    proactive_monitoring_manager=hybrid_monitoring,
                    change_detection_manager=get_change_detection_manager(),
                    implicit_resolver=implicit_resolver,
                    conversation_tracker=get_conversation_tracker(),
                )
                app.state.temporal_handler = temporal_handler
                logger.info("‚úÖ TemporalQueryHandler v3.0 initialized")
                logger.info("   ‚Ä¢ Pattern analysis, predictive analysis, anomaly detection")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è TemporalQueryHandler v3.0 init failed: {e}")

            # 2. ErrorRecoveryManager v2.0
            try:
                from autonomy.error_recovery import ErrorRecoveryManager
                from context_intelligence.managers import get_change_detection_manager

                error_recovery = ErrorRecoveryManager(
                    hybrid_monitoring_manager=hybrid_monitoring,
                    implicit_resolver=implicit_resolver,
                    change_detection_manager=get_change_detection_manager(),
                )
                app.state.error_recovery = error_recovery
                logger.info("‚úÖ ErrorRecoveryManager v2.0 initialized")
                logger.info("   ‚Ä¢ Proactive error detection, frequency tracking, auto-healing")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è ErrorRecoveryManager v2.0 init failed: {e}")

            # 3. StateIntelligence v2.0
            try:
                from context_intelligence.managers import get_change_detection_manager
                from vision.intelligence.state_intelligence import initialize_state_intelligence

                async def handle_stuck_alert(alert):
                    """Handle stuck state alerts"""
                    logger.warning(f"[STUCK-STATE] {alert['message']}")

                state_intelligence = initialize_state_intelligence(
                    user_id="default",
                    hybrid_monitoring_manager=hybrid_monitoring,
                    implicit_resolver=implicit_resolver,
                    change_detection_manager=get_change_detection_manager(),
                    stuck_alert_callback=handle_stuck_alert,
                )
                app.state.state_intelligence = state_intelligence

                # Start stuck state monitoring
                asyncio.create_task(state_intelligence.start_stuck_state_monitoring())

                logger.info("‚úÖ StateIntelligence v2.0 initialized")
                logger.info("   ‚Ä¢ Auto-recording, stuck state detection, productivity tracking")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è StateIntelligence v2.0 init failed: {e}")

            # 4. StateDetectionPipeline v2.0
            try:
                from context_intelligence.managers import get_change_detection_manager
                from vision.intelligence.state_detection_pipeline import StateDetectionPipeline

                async def handle_state_transition(transition):
                    """Handle state transition alerts"""
                    logger.info(
                        f"[STATE-TRANSITION] Space {transition['space_id']}: "
                        f"{transition['from_state']} ‚Üí {transition['to_state']}"
                    )

                async def handle_new_state(new_state):
                    """Handle unknown state detection"""
                    logger.info(f"[NEW-STATE] Unknown state in Space {new_state['space_id']}")

                state_detection = StateDetectionPipeline(
                    hybrid_monitoring_manager=hybrid_monitoring,
                    implicit_resolver=implicit_resolver,
                    change_detection_manager=get_change_detection_manager(),
                    state_transition_callback=handle_state_transition,
                    new_state_callback=handle_new_state,
                )
                app.state.state_detection = state_detection
                logger.info("‚úÖ StateDetectionPipeline v2.0 initialized")
                logger.info("   ‚Ä¢ Auto-triggered detection, visual signature learning")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è StateDetectionPipeline v2.0 init failed: {e}")

            # 5. ComplexComplexityHandler v2.0
            try:
                from context_intelligence.handlers.complex_complexity_handler import (
                    initialize_complex_complexity_handler,
                )
                from context_intelligence.managers import (
                    get_capture_strategy_manager,
                    get_ocr_strategy_manager,
                )

                complex_handler = initialize_complex_complexity_handler(
                    temporal_handler=(
                        app.state.temporal_handler
                        if hasattr(app.state, "temporal_handler")
                        else None
                    ),
                    capture_manager=get_capture_strategy_manager(),
                    ocr_manager=get_ocr_strategy_manager(),
                    implicit_resolver=implicit_resolver,
                    hybrid_monitoring_manager=hybrid_monitoring,
                    prefer_monitoring_cache=True,
                )
                app.state.complex_handler = complex_handler
                logger.info("‚úÖ ComplexComplexityHandler v2.0 initialized")
                logger.info("   ‚Ä¢ Ultra-fast queries (87% faster), monitoring cache enabled")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è ComplexComplexityHandler v2.0 init failed: {e}")

            # 6. PredictiveQueryHandler v2.0
            try:
                from context_intelligence.handlers.predictive_query_handler import (
                    initialize_predictive_handler,
                )

                predictive_handler = initialize_predictive_handler(
                    context_graph=None,  # TODO: Add context graph if available
                    hybrid_monitoring_manager=hybrid_monitoring,
                    implicit_resolver=implicit_resolver,
                    enable_vision=True,
                    claude_api_key=api_key,
                )
                app.state.predictive_handler = predictive_handler
                logger.info("‚úÖ PredictiveQueryHandler v2.0 initialized")
                logger.info("   ‚Ä¢ Progress tracking, bug prediction, workflow suggestions")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è PredictiveQueryHandler v2.0 init failed: {e}")

            logger.info("\n" + "=" * 60)
            logger.info("‚ú® ALL 6 v2.0 SYSTEMS INITIALIZED")
            logger.info("=" * 60)
            logger.info("üéØ Enhanced Capabilities:")
            logger.info("   1. TemporalQueryHandler    - ML-powered temporal analysis")
            logger.info("   2. ErrorRecoveryManager    - Proactive error detection & healing")
            logger.info("   3. StateIntelligence       - Auto-learning state patterns")
            logger.info("   4. StateDetectionPipeline  - Visual signature learning")
            logger.info("   5. ComplexComplexityHandler - 87% faster complex queries")
            logger.info("   6. PredictiveQueryHandler  - Intelligent predictions")
            logger.info("\nüöÄ All systems integrated with HybridMonitoring & ImplicitResolver!")
            logger.info("=" * 60 + "\n")

        except Exception as e:
            logger.error(f"‚ùå v2.0 Systems initialization failed: {e}", exc_info=True)

        # ========================================================================
        # Initialize AGI OS - Autonomous General Intelligence Operating System
        # ========================================================================
        logger.info("\n" + "=" * 60)
        logger.info("üß† INITIALIZING AGI OS - Autonomous Intelligence Layer")
        logger.info("=" * 60)

        try:
            from agi_os import (
                start_agi_os,
                get_voice_communicator,
                get_owner_identity,
                get_unified_vision,
                integrate_all,
                VoiceMode,
            )

            # Start the AGI OS coordinator
            app.state.agi_os = await start_agi_os()
            logger.info(f"‚úÖ AGI OS State: {app.state.agi_os.state.value}")

            # Get owner identity for personalized interactions
            owner_service = await get_owner_identity()
            owner = await owner_service.get_current_owner()
            owner_name = owner.name.split()[0] if owner.name else "Sir"
            app.state.owner_service = owner_service
            logger.info(f"‚úÖ Owner Identity: {owner.name} (source: {owner.identity_source.value})")

            # Initialize voice communicator with Daniel TTS
            voice = await get_voice_communicator()
            app.state.agi_voice = voice
            logger.info("‚úÖ Voice Communicator: Daniel TTS ready")

            # Initialize unified vision interface
            vision = await get_unified_vision()
            app.state.agi_vision = vision
            patterns = vision.get_detection_patterns()
            logger.info(f"‚úÖ Unified Vision: {len(patterns)} proactive detection patterns")

            # Integrate with existing JARVIS systems
            integration_result = await integrate_all(
                screen_analyzer=getattr(app.state, 'vision_analyzer', None),
                decision_engine=getattr(app.state, 'decision_engine', None),
            )
            logger.info(f"‚úÖ System Integration: {integration_result.get('integrated_count', 0)} bridges connected")

            # Report component status
            working = sum(1 for s in app.state.agi_os.component_status.values() if s.available)
            total = len(app.state.agi_os.component_status)

            logger.info("\n" + "=" * 60)
            logger.info("ü§ñ AGI OS INITIALIZED")
            logger.info("=" * 60)
            logger.info("üéØ Autonomous Capabilities:")
            logger.info("   ‚Ä¢ Real-time voice communication (Daniel TTS)")
            logger.info("   ‚Ä¢ Voice-based approval workflows")
            logger.info("   ‚Ä¢ Proactive event streaming")
            logger.info("   ‚Ä¢ Intelligent action orchestration")
            logger.info("   ‚Ä¢ Dynamic owner identification")
            logger.info(f"\n‚ú® {working}/{total} AGI OS components operational")
            logger.info(f"üë§ Owner: {owner_name}")
            logger.info("=" * 60 + "\n")

            # Initialize and use the intelligent startup announcer (primary)
            # Falls back to legacy greeter if intelligent system unavailable
            try:
                from agi_os.intelligent_startup_announcer import (
                    get_intelligent_announcer,
                    StartupType,
                )

                announcer = await get_intelligent_announcer()
                app.state.intelligent_announcer = announcer

                # Generate and deliver intelligent startup message
                if getattr(app.state, 'voice_enabled', True):
                    asyncio.create_task(announcer.announce_startup(StartupType.COLD_BOOT))

                logger.info("‚úÖ IntelligentStartupAnnouncer: Dynamic context-aware greetings enabled")

            except Exception as e:
                logger.debug(f"Intelligent announcer not available: {e}, trying legacy greeter")

                # Fallback to legacy startup greeter
                try:
                    from agi_os import get_startup_greeter

                    greeter = await get_startup_greeter()
                    app.state.startup_greeter = greeter

                    if getattr(app.state, 'voice_enabled', True):
                        asyncio.create_task(greeter.greet_on_startup())

                    logger.info("‚úÖ StartupGreeter (legacy): Initialized with wake detection")

                except Exception as e2:
                    logger.warning(f"‚ö†Ô∏è All startup greeters failed: {e2}")
                    # Final fallback to simple greeting
                    if voice and getattr(app.state, 'voice_enabled', True):
                        asyncio.create_task(
                            voice.speak(
                                f"JARVIS online. Ready for your command, {owner_name}.",
                                mode=VoiceMode.NORMAL
                            )
                        )

        except ImportError as e:
            logger.warning(f"‚ö†Ô∏è AGI OS not available: {e}")
            app.state.agi_os = None
        except Exception as e:
            logger.error(f"‚ùå AGI OS initialization failed: {e}", exc_info=True)
            app.state.agi_os = None

        # Log proactive monitoring configuration
        proactive_config = app.state.vision_analyzer.get_proactive_config()
        if proactive_config["proactive_enabled"]:
            logger.info("‚úÖ Proactive Vision Intelligence System initialized with:")
            logger.info(f"   - Confidence threshold: {proactive_config['confidence_threshold']}")
            logger.info(
                f"   - Voice announcements: {'enabled' if proactive_config['voice_enabled'] else 'disabled'}"
            )
            logger.info("   - Debugging Assistant: Auto-detects code errors")
            logger.info("   - Research Helper: Monitors multi-tab workflows")
            logger.info("   - Workflow Optimizer: Identifies repetitive patterns")
            logger.info("   - Privacy Protection: Auto-pauses for sensitive content")
            logger.info("   - Say 'Start monitoring my screen' to activate intelligent assistance")
        else:
            logger.info("‚ö†Ô∏è Proactive monitoring disabled in configuration")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not initialize proactive monitoring components: {e}")

    # Initialize weather system with vision
    try:
        from system_control.macos_controller import MacOSController
        from system_control.weather_system_config import initialize_weather_system

        controller = MacOSController()
        weather_bridge = initialize_weather_system(app.state.vision_analyzer, controller)
        app.state.weather_system = weather_bridge
        logger.info("‚úÖ Weather system initialized with vision")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not initialize weather system: {e}")

    # Initialize vision status manager
    try:
        from vision.vision_status_integration import (
            initialize_vision_status,
            setup_vision_status_callbacks,
        )

        # Initialize after WebSocket is mounted
        async def setup_vision_status():
            await asyncio.sleep(0.5)  # Give WebSocket time to initialize
            success = await initialize_vision_status(app)
            if success:
                setup_vision_status_callbacks(app)
                logger.info("‚úÖ Vision status manager initialized and connected")

        asyncio.create_task(setup_vision_status())
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not initialize vision status manager: {e}")

    # NOTE: This elif was orphaned/unreachable - analyzer_class check already done at line 988
    # elif analyzer_class:
    #     logger.warning("‚ö†Ô∏è Vision analyzer available but no ANTHROPIC_API_KEY set")

    # Initialize ML models if not lazy loading
    ml = components.get("ml_models", {})
    if ml.get("available") and not LAZY_LOAD_MODELS:
        init_func = ml.get("initialize_models")
        if init_func:
            if ML_LOGGING_ENABLED:
                logger.info("\n" + "=" * 60)
                logger.info("ü§ñ INITIALIZING ML MODELS WITH SMART LAZY LOADING")
                logger.info("=" * 60)
                logger.info("Target: <35% memory usage (5.6GB of 16GB)")
                logger.info("Strategy: Load models one-at-a-time, only when needed")
                logger.info("Watch the console for real-time loading details...\n")
            asyncio.create_task(init_func())
            logger.info("‚úÖ ML models initialization started")

    elapsed = time.time() - start_time
    logger.info(f"‚ú® Optimized startup completed in {elapsed:.1f}s")

    # Mount routers during startup
    mount_routers()

    # Log final status with component details
    logger.info("\n" + "=" * 60)
    logger.info("ü§ñ JARVIS Backend (Optimized) Ready!")

    # Count and display loaded components
    loaded_count = sum(1 for c in components.values() if c)
    logger.info(f"üìä Components loaded: {loaded_count}/{len(components)}")

    # Show status of each component
    component_status = [
        (
            "‚úÖ" if components.get("chatbots") else "‚ùå",
            "CHATBOTS    - AI conversation & vision analysis",
        ),
        (
            "‚úÖ" if components.get("vision") else "‚ùå",
            "VISION      - Screen capture & real-time monitoring",
        ),
        (
            "‚úÖ" if components.get("memory") else "‚ùå",
            "MEMORY      - Resource management & optimization",
        ),
        (
            "‚úÖ" if components.get("voice") else "‚ùå",
            "VOICE       - Voice activation & speech synthesis",
        ),
        (
            "‚úÖ" if components.get("ml_models") else "‚ùå",
            "ML_MODELS   - NLP & sentiment analysis",
        ),
        (
            "‚úÖ" if components.get("monitoring") else "‚ùå",
            "MONITORING  - System health & metrics",
        ),
        (
            "‚úÖ" if components.get("voice_unlock") else "‚ùå",
            "VOICE_UNLOCK - Biometric Mac authentication",
        ),
        (
            "‚úÖ" if components.get("wake_word") else "‚ùå",
            "WAKE_WORD   - Hands-free 'Hey JARVIS' activation",
        ),
    ]

    for status, desc in component_status:
        logger.info(f"   {status} {desc}")

    logger.info(f"üöÄ Mode: {'Optimized' if OPTIMIZE_STARTUP else 'Legacy'}")

    if loaded_count == 8:
        logger.info("‚ú® All systems operational - JARVIS is fully functional!")
    else:
        logger.warning(f"‚ö†Ô∏è  Only {loaded_count}/8 components loaded - some features may be limited")

    logger.info("=" * 60 + "\n")

    # Initialize Voice Unlock system components (WebSocket server)
    voice_unlock = components.get("voice_unlock") or {}
    if (
        voice_unlock
        and voice_unlock.get("startup_integration")
        and voice_unlock.get("initialize_system")
    ):
        try:
            logger.info("üîê Starting Voice Unlock system components...")
            init_system = voice_unlock["initialize_system"]
            success = await init_system()
            if success:
                app.state.voice_unlock_system = voice_unlock["startup_manager"]
                # Also set app.state.voice_unlock so health check reports correct status
                app.state.voice_unlock = voice_unlock
                logger.info("‚úÖ Voice Unlock system started")
                logger.info("   Say 'Hey JARVIS, unlock my mac' when screen is locked")
            else:
                logger.warning("‚ö†Ô∏è Voice Unlock system failed to start")
        except Exception as e:
            logger.error(f"Failed to start Voice Unlock system: {e}")

    # Initialize wake word service after all components are loaded
    wake_word = components.get("wake_word", {})
    if wake_word.get("service") and wake_word.get("initialized"):
        # Define activation callback that sends to WebSocket clients
        async def wake_word_activation_callback(data):
            """Handle wake word activation"""
            logger.info(f"Wake word activated: {data}")
            # This will be sent through WebSocket to connected clients
            # The frontend will handle the actual response

        try:
            # Start the wake word service with callback
            wake_service = wake_word["service"]
            if wake_service:
                success = await wake_service.start(wake_word_activation_callback)
                if success:
                    app.state.wake_service = wake_service
                    logger.info("üé§ Wake word detection service started - Say 'Hey JARVIS'!")
                else:
                    logger.warning("‚ö†Ô∏è Wake word service failed to start")
        except Exception as e:
            logger.error(f"Failed to start wake word service: {e}")

    # Register with autonomous systems
    try:
        from core.autonomous_orchestrator import get_orchestrator
        from core.zero_config_mesh import get_mesh

        orchestrator = get_orchestrator()
        mesh = get_mesh()

        # Start autonomous systems
        await orchestrator.start()
        await mesh.start()

        # Register backend service
        backend_port = int(os.getenv("BACKEND_PORT", "8010"))
        await orchestrator.register_service("jarvis_backend", backend_port, "http")
        await mesh.join(
            {
                "name": "jarvis_backend",
                "port": backend_port,
                "protocol": "http",
                "type": "backend",
                "endpoints": {
                    "health": "/health",
                    "vision": "/vision",
                    "voice": "/voice",
                    "chat": "/chat",
                },
            }
        )

        app.state.orchestrator = orchestrator
        app.state.mesh = mesh

        logger.info("‚úÖ Registered with autonomous orchestrator and mesh network")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not register with autonomous systems: {e}")

    # Initialize Cost Tracking System (Priority 2: Cost Monitoring & Alerts)
    try:
        from core.cost_tracker import initialize_cost_tracking

        await initialize_cost_tracking()
        logger.info("‚úÖ Cost Tracking System initialized")
        logger.info("   ‚Ä¢ Auto-cleanup enabled for orphaned VMs")
        logger.info("   ‚Ä¢ Real-time cost monitoring active")
        logger.info("   ‚Ä¢ Alert system configured")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Cost tracking initialization failed: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CLOUD SQL PROXY + VOICE VERIFICATION HEALTH CHECKS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        logger.info("üîê Initializing Cloud SQL Proxy + Voice Verification...")
        from intelligence.cloud_database_adapter import get_database_adapter
        from voice.speaker_verification_service import SpeakerVerificationService

        # Initialize database (will auto-start Cloud SQL proxy if needed)
        db_adapter = await get_database_adapter()
        app.state.db_adapter = db_adapter

        if db_adapter.is_cloud:
            logger.info("‚úÖ Cloud SQL proxy started and database connected")
            logger.info(f"   ‚Ä¢ Connection: {db_adapter.config.connection_name}")
            logger.info(f"   ‚Ä¢ Database: {db_adapter.config.db_name}")
            logger.info(f"   ‚Ä¢ Host: {db_adapter.config.db_host}:{db_adapter.config.db_port}")
        else:
            logger.info("‚úÖ Using local SQLite database")
            logger.info(f"   ‚Ä¢ Path: {db_adapter.config.sqlite_path}")

        # Initialize voice verification service
        voice_verification = SpeakerVerificationService()
        await voice_verification.initialize()
        app.state.voice_verification = voice_verification

        # Validate profiles and dimensions
        profile_count = len(voice_verification.speaker_profiles)
        model_dim = voice_verification.current_model_dimension

        if profile_count > 0:
            logger.info(f"‚úÖ Voice Verification initialized successfully")
            logger.info(f"   ‚Ä¢ Loaded profiles: {profile_count}")
            logger.info(f"   ‚Ä¢ Model dimension: {model_dim}D")

            # Validate each profile
            for name, profile in voice_verification.speaker_profiles.items():
                import numpy as np
                emb_shape = np.array(profile['embedding']).shape
                emb_dim = emb_shape[0] if len(emb_shape) == 1 else emb_shape[1]

                if emb_dim == model_dim:
                    logger.info(f"   ‚Ä¢ {name}: {emb_dim}D ‚úÖ (matches model)")
                else:
                    logger.warning(f"   ‚Ä¢ {name}: {emb_dim}D ‚ö†Ô∏è (expected {model_dim}D)")
        else:
            logger.warning("‚ö†Ô∏è No voice profiles loaded - voice unlock disabled")
            logger.warning("   ‚Üí Enroll a voice profile to enable biometric authentication")

    except Exception as e:
        logger.error(f"‚ùå Cloud SQL/Voice Verification initialization failed: {e}", exc_info=True)
        logger.warning("   ‚Üí Voice unlock features will be disabled")
        app.state.db_adapter = None
        app.state.voice_verification = None

    # =================================================================
    # PERFORMANCE OPTIMIZER: Attach to app state for lifecycle management
    # =================================================================
    if PERFORMANCE_OPTIMIZER_AVAILABLE:
        try:
            from core.performance_optimizer import get_optimizer
            optimizer = get_optimizer()
            app.state.performance_optimizer = optimizer
            logger.info("üìä Performance Optimizer initialized and attached to app.state")
        except Exception as e:
            logger.debug(f"Could not attach Performance Optimizer to app.state: {e}")
            app.state.performance_optimizer = None
    else:
        app.state.performance_optimizer = None

    # =================================================================
    # HYPER-SPEED AI LOADER: Ghost Proxies for instant model access
    # =================================================================
    # Initialize AI model manager for zero-copy loading with Ghost Proxies
    # Models load in background while server responds immediately
    try:
        from core.ai_loader import get_ai_manager, get_config as get_ai_config
        ai_manager = get_ai_manager()
        app.state.ai_manager = ai_manager

        ai_config = get_ai_config()
        logger.info("üß† Hyper-Speed AI Loader initialized")
        logger.info(f"   Workers: {ai_config.max_workers}, Quantize: {ai_config.quantize_default}")
        logger.info("   Ghost Proxies ready for instant model access")
    except ImportError as e:
        logger.debug(f"AI Loader not available: {e}")
        app.state.ai_manager = None
    except Exception as e:
        logger.warning(f"AI Loader initialization failed: {e}")
        app.state.ai_manager = None

    # =================================================================
    # v78.0: Advanced Startup Orchestrator Integration
    # =================================================================
    # Provides enterprise-grade startup patterns:
    # - Dynamic configuration discovery (zero hardcoding)
    # - Circuit breakers with exponential backoff
    # - Connection verification loops
    # - Cross-repo Trinity health monitoring
    # =================================================================
    app.state.orchestrator_hooks = None
    app.state.discovered_config = None
    try:
        from core.supervisor_orchestrator_bridge import (
            get_orchestrator_hooks,
            OrchestratorBridgeConfig,
        )

        orchestrator_config = OrchestratorBridgeConfig.from_env()
        if orchestrator_config.enabled:
            orchestrator_hooks = await get_orchestrator_hooks(config=orchestrator_config)
            app.state.orchestrator_hooks = orchestrator_hooks
            app.state.discovered_config = orchestrator_hooks.discovered_config

            if orchestrator_hooks.discovered_config:
                config = orchestrator_hooks.discovered_config
                logger.info("=" * 60)
                logger.info("v78.0 ADVANCED ORCHESTRATOR: Initialized")
                logger.info("=" * 60)
                logger.info(f"   ‚Ä¢ Repos discovered: {len(config.repo_paths)}")
                logger.info(f"   ‚Ä¢ Trinity dir: {config.trinity_dir}")
                logger.info(f"   ‚Ä¢ Dynamic ports: {config.ports}")
                logger.info("   ‚Ä¢ Circuit breakers: Enabled")
                logger.info("   ‚Ä¢ Connection verification: Ready")
            else:
                logger.info("‚úÖ v78.0 Orchestrator: Active (minimal mode)")

    except ImportError as e:
        logger.debug(f"Advanced Orchestrator not available: {e}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Advanced Orchestrator initialization failed: {e}")

    # =================================================================
    # v77.4 UNIFIED CODING COUNCIL: Self-Evolution + IDE Integration
    # =================================================================
    # Coding Council enables AI-powered code evolution across Trinity repos:
    # - Anthropic Claude engine (Aider/MetaGPT-style operations)
    # - Cross-repo Trinity synchronization
    # - IDE bridge with LSP server and WebSocket handler
    # - ARM64 NEON SIMD acceleration (40-50x faster)
    # - Adaptive framework selection with Thompson sampling
    # =================================================================
    coding_council_initialized = False
    try:
        from core.coding_council.startup import (
            initialize_coding_council_startup,
            shutdown_coding_council_startup,
            is_initialized as is_coding_council_initialized,
        )

        logger.info("=" * 60)
        logger.info("v77.4 UNIFIED CODING COUNCIL: Initializing")
        logger.info("=" * 60)

        coding_council_initialized = await initialize_coding_council_startup()

        if coding_council_initialized:
            app.state.coding_council_initialized = True
            logger.info("‚úÖ Coding Council online - Self-evolution capabilities active")
            logger.info("   ‚Ä¢ Anthropic Engine: Claude API for code changes")
            logger.info("   ‚Ä¢ IDE Bridge: LSP + WebSocket integration")
            logger.info("   ‚Ä¢ Trinity Sync: Cross-repo file synchronization")
            logger.info("   ‚Ä¢ ARM64 Acceleration: NEON SIMD enabled")
        else:
            logger.warning("‚ö†Ô∏è Coding Council: Running in limited mode")
            app.state.coding_council_initialized = False

    except ImportError as e:
        logger.debug(f"Coding Council not available: {e}")
        app.state.coding_council_initialized = False
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Coding Council initialization failed: {e}")
        logger.warning("   ‚Üí Code evolution features will be disabled")
        app.state.coding_council_initialized = False

    # =================================================================
    # PROJECT TRINITY: Unified Cognitive Architecture Integration
    # =================================================================
    # Trinity connects JARVIS (Body) ‚Üî J-Prime (Mind) ‚Üî Reactor Core (Nerves)
    # This enables distributed AI reasoning, surveillance, and action execution
    # across all three repositories with file-based message passing.
    # =================================================================
    trinity_initialized = False
    try:
        from system.trinity_initializer import (
            initialize_trinity,
            is_trinity_initialized,
            get_trinity_status,
            JARVIS_INSTANCE_ID,
        )

        logger.info("=" * 60)
        logger.info("PROJECT TRINITY: Initializing JARVIS Body Connection")
        logger.info("=" * 60)

        trinity_initialized = await initialize_trinity(app)

        if trinity_initialized:
            app.state.trinity_initialized = True
            app.state.trinity_instance_id = JARVIS_INSTANCE_ID

            status = get_trinity_status()
            logger.info("‚úÖ PROJECT TRINITY: JARVIS Body Online")
            logger.info(f"   ‚Ä¢ Instance ID: {JARVIS_INSTANCE_ID[:16]}...")
            logger.info(f"   ‚Ä¢ Connected: {status.get('connected', False)}")
            logger.info(f"   ‚Ä¢ Heartbeat: {status.get('heartbeat_interval', 5.0)}s")
            logger.info("   ‚Ä¢ Mind ‚Üî Body ‚Üî Nerves: Distributed architecture active")
        else:
            logger.warning("‚ö†Ô∏è PROJECT TRINITY: Running in standalone mode")
            app.state.trinity_initialized = False

    except ImportError as e:
        logger.debug(f"Trinity initializer not available: {e}")
        app.state.trinity_initialized = False
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è PROJECT TRINITY initialization failed: {e}")
        logger.warning("   ‚Üí JARVIS will operate in standalone mode")
        app.state.trinity_initialized = False

    # =================================================================
    # v2.0: READINESS STATE MANAGER - Final Readiness Check
    # =================================================================
    # v2.0 ENHANCEMENT: Critical components marked ready INLINE during init.
    # System auto-transitions to READY when all CRITICAL components ready.
    # This section now handles OPTIONAL/IMPORTANT components and fallback.
    # =================================================================
    if readiness_manager:
        try:
            # v2.0: Check if auto-transition already happened
            already_ready = readiness_manager.state.is_ready

            if already_ready:
                logger.info("üìä [v2.0] System already READY via auto-transition")
            else:
                # Fallback: Explicitly mark critical components if not already ready
                # (This handles edge cases where inline marking failed)
                logger.info("üìä [v2.0] Auto-transition didn't happen - marking components explicitly")

                if "websocket" in readiness_manager.state.components:
                    comp = readiness_manager.state.components["websocket"]
                    if not comp.is_ready:
                        await readiness_manager.mark_component_ready("websocket", healthy=True)

                if "service_registry" in readiness_manager.state.components:
                    comp = readiness_manager.state.components["service_registry"]
                    if not comp.is_ready:
                        await readiness_manager.mark_component_ready("service_registry", healthy=True)

            # Mark OPTIONAL/IMPORTANT components based on initialization status
            # (These don't block readiness but help with HEALTHY state)
            if trinity_initialized:
                await readiness_manager.mark_component_ready("trinity", healthy=True)

            if hasattr(app.state, 'neural_mesh_initialized') and app.state.neural_mesh_initialized:
                await readiness_manager.mark_component_ready("neural_mesh", healthy=True)

            if hasattr(app.state, 'voice_unlock') and app.state.voice_unlock.get("initialized", False):
                await readiness_manager.mark_component_ready("voice_unlock", healthy=True)

            # Ghost proxies and ML engine are marked as "in progress" - they may still be warming
            await readiness_manager.update_component_progress("ghost_proxies", 50.0)
            await readiness_manager.update_component_progress("ml_engine", 50.0)

            # v2.0: Explicit transition only if auto-transition didn't happen
            if not readiness_manager.state.is_ready:
                await readiness_manager.mark_ready()
                logger.info("üìä [v2.0] ReadinessStateManager: READY phase (explicit fallback)")

            logger.info("üìä [v2.0] ReadinessStateManager: System accepting traffic")
            
            elapsed = time.time() - start_time
            logger.info(f"üéâ JARVIS fully initialized in {elapsed:.1f}s - /health/ready returns 200 OK")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [v95.3] ReadinessStateManager READY transition error: {e}")
            # Even if manager fails, don't block startup
    else:
        # No readiness manager - log startup complete without it
        elapsed = time.time() - start_time
        logger.info(f"üéâ JARVIS initialized in {elapsed:.1f}s (legacy health check mode)")

    yield

    # Cleanup
    logger.info("üõë Shutting down JARVIS backend...")

    # =================================================================
    # v95.3: READINESS STATE MANAGER - Shutdown transition
    # =================================================================
    if hasattr(app.state, 'readiness_manager') and app.state.readiness_manager:
        try:
            await app.state.readiness_manager.start_shutdown()
            logger.info("üìä [v95.3] ReadinessStateManager: SHUTTING_DOWN phase")
        except Exception as e:
            logger.debug(f"ReadinessStateManager shutdown transition error: {e}")

    # =================================================================
    # v77.4 UNIFIED CODING COUNCIL: Graceful shutdown
    # =================================================================
    if hasattr(app.state, 'coding_council_initialized') and app.state.coding_council_initialized:
        try:
            from core.coding_council.startup import shutdown_coding_council_startup

            logger.info("üß¨ Shutting down Coding Council...")
            await shutdown_coding_council_startup()
            logger.info("‚úÖ Coding Council shutdown complete")
        except ImportError:
            pass  # Coding Council not available
        except Exception as e:
            logger.debug(f"Coding Council shutdown error (non-critical): {e}")

    # =================================================================
    # PROJECT TRINITY: Graceful shutdown
    # =================================================================
    if hasattr(app.state, 'trinity_initialized') and app.state.trinity_initialized:
        try:
            from system.trinity_initializer import shutdown_trinity

            logger.info("üîó Shutting down PROJECT TRINITY...")
            await shutdown_trinity()
            logger.info("‚úÖ PROJECT TRINITY shutdown complete")
        except ImportError:
            pass  # Trinity not available
        except Exception as e:
            logger.debug(f"Trinity shutdown error (non-critical): {e}")

    # =================================================================
    # HYPER-SPEED AI LOADER: Graceful shutdown
    # =================================================================
    if hasattr(app.state, 'ai_manager') and app.state.ai_manager:
        try:
            logger.info("üß† Shutting down AI Loader...")
            await app.state.ai_manager.shutdown()
            logger.info("üß† AI Loader shutdown complete")
        except Exception as e:
            logger.debug(f"AI Loader shutdown error (non-critical): {e}")

    # =================================================================
    # PERFORMANCE OPTIMIZER: Graceful shutdown
    # =================================================================
    if hasattr(app.state, 'performance_optimizer') and app.state.performance_optimizer:
        try:
            logger.info("üìä Shutting down Performance Optimizer...")
            await app.state.performance_optimizer.shutdown()
            logger.info("üìä Performance Optimizer shutdown complete")
        except Exception as e:
            logger.debug(f"Performance Optimizer shutdown error (non-critical): {e}")

    # Stop event loop watchdog
    try:
        from core.ml_operation_watchdog import stop_event_loop_watchdog, get_watchdog_stats
        stats = get_watchdog_stats()
        stop_event_loop_watchdog()
        logger.info(f"üêï Watchdog stopped - Stats: {stats.total_operations} ops, "
                   f"{stats.timeout_operations} timeouts, {stats.error_operations} errors")
    except Exception as e:
        logger.debug(f"Watchdog cleanup: {e}")

    # Notify all WebSocket clients about shutdown
    try:
        from api.jarvis_voice_api import broadcast_shutdown_notification

        await broadcast_shutdown_notification()
        logger.info("‚úÖ Shutdown notifications sent to WebSocket clients")
        # Give clients a brief moment to receive the notification
        await asyncio.sleep(0.5)
    except Exception as e:
        logger.warning(f"Failed to broadcast shutdown notification: {e}")

    # Shutdown Neural Mesh Multi-Agent System
    try:
        from neural_mesh.integration import shutdown_neural_mesh, is_neural_mesh_initialized

        if is_neural_mesh_initialized():
            logger.info("üß† Shutting down Neural Mesh...")
            await shutdown_neural_mesh()
            logger.info("‚úÖ Neural Mesh shutdown complete")
    except ImportError:
        pass  # Neural Mesh not available
    except Exception as e:
        logger.warning(f"Neural Mesh shutdown error: {e}")

    # Shutdown Voice Auth Intelligence Services (Langfuse, etc.)
    try:
        from api.voice_auth_intelligence_api import shutdown_voice_auth_services

        logger.info("üîê Shutting down Voice Auth Intelligence services...")
        shutdown_voice_auth_services()
        logger.info("‚úÖ Voice Auth Intelligence services shutdown complete")
    except ImportError:
        logger.debug("Voice Auth Intelligence API not available")
    except Exception as e:
        logger.warning(f"Voice Auth Intelligence shutdown error: {e}")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # COMPREHENSIVE SHUTDOWN v2.0 - Single entry point for all cleanup
    # Handles: HTTP clients (aiohttp/httpx), Thread pools, PyTorch, Executors
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if THREAD_MANAGER_AVAILABLE and comprehensive_shutdown:
        try:
            logger.info("üßµ Starting Comprehensive Shutdown v2.0...")
            logger.info("   4-phase cleanup: HTTP Clients ‚Üí Executors ‚Üí Threads ‚Üí Third-party")

            # Execute comprehensive shutdown with 20-second timeout
            shutdown_stats = await comprehensive_shutdown(timeout=20.0)

            # Log results
            if shutdown_stats.get("success"):
                logger.info(f"‚úÖ Comprehensive shutdown complete in {shutdown_stats.get('total_duration', 0):.2f}s")
            else:
                logger.warning(f"‚ö†Ô∏è Comprehensive shutdown completed with issues:")
                for error in shutdown_stats.get("errors", []):
                    logger.warning(f"   ‚Ä¢ {error}")

            # Log phase details
            phases = shutdown_stats.get("phases", {})
            if "http_clients" in phases and not phases["http_clients"].get("skipped"):
                http_stats = phases["http_clients"]
                logger.info(f"   ‚Ä¢ HTTP Clients: {http_stats.get('closed', 0)} closed, "
                           f"{http_stats.get('gc_collected', 0)} GC'd")
            if "executors" in phases and not phases["executors"].get("skipped"):
                exec_stats = phases["executors"]
                logger.info(f"   ‚Ä¢ Executors: {exec_stats.get('successful', 0)}/{exec_stats.get('total_executors', 0)} shutdown")
            if "third_party" in phases:
                tp_stats = phases["third_party"]
                logger.info(f"   ‚Ä¢ Third-party: {tp_stats.get('remaining_non_daemon', 0)} threads remaining")

            # Print detailed thread report if available
            if thread_manager and hasattr(thread_manager, 'print_report'):
                logger.info("   üìä Final Thread Report:")
                thread_manager.print_report()

        except Exception as e:
            logger.error(f"‚ùå Comprehensive shutdown failed: {e}", exc_info=True)
            logger.warning("   Falling back to legacy shutdown...")
            # Fallback to legacy shutdown
            try:
                await shutdown_all_threads_async(timeout=10.0)
            except Exception as fallback_e:
                logger.error(f"   Fallback shutdown also failed: {fallback_e}")
    elif not THREAD_MANAGER_AVAILABLE:
        logger.warning("‚ö†Ô∏è  Thread Manager not available - manual thread cleanup required")

    # Shutdown ML Continuous Learning Engine
    try:
        from voice_unlock.continuous_learning_engine import shutdown_learning_engine

        logger.info("üß† Shutting down ML Continuous Learning Engine...")
        await shutdown_learning_engine()
        logger.info("‚úÖ ML Continuous Learning Engine shutdown complete")
    except ImportError:
        logger.debug("ML Continuous Learning Engine not available")
    except Exception as e:
        logger.error(f"Failed to shutdown ML Learning Engine: {e}")

    # Cleanup Cloud SQL database connections
    try:
        if hasattr(app.state, "db_adapter") and app.state.db_adapter:
            logger.info("üîê Closing Cloud SQL database connections...")
            from intelligence.cloud_database_adapter import close_database_adapter

            await close_database_adapter()
            logger.info("‚úÖ Database connections closed")
    except Exception as e:
        logger.error(f"Failed to close database connections: {e}")

    # Cleanup GCP VM Manager (before cost tracker to finalize costs)
    # Uses the Triple-Lock safety system shutdown hook for robust cleanup
    # v2.0: Enhanced with comprehensive logging and cost integration
    try:
        from scripts.shutdown_hook import cleanup_remote_resources
        
        logger.info("üßπ Cleaning up GCP VMs via Triple-Lock shutdown hook...")
        cleanup_result = await cleanup_remote_resources(
            timeout=30.0,
            reason="Backend lifespan shutdown"
        )
        
        # Extract and use all result fields for comprehensive logging
        cleanup_success = cleanup_result.get("success", False)
        vms_cleaned = cleanup_result.get("vms_cleaned", 0)
        cleanup_method = cleanup_result.get("method", "unknown")
        cleanup_errors = cleanup_result.get("errors", [])
        
        if cleanup_success:
            if vms_cleaned > 0:
                logger.info(f"‚úÖ GCP VM cleanup complete: {vms_cleaned} VM(s) via {cleanup_method}")
                logger.info(f"   Method used: {cleanup_method}")
            else:
                logger.info(f"‚úÖ GCP VM cleanup complete (no VMs to clean, method: {cleanup_method})")
        else:
            logger.warning(f"‚ö†Ô∏è GCP VM cleanup had issues:")
            logger.warning(f"   Success: {cleanup_success}")
            logger.warning(f"   VMs cleaned: {vms_cleaned}")
            logger.warning(f"   Method: {cleanup_method}")
            for error in cleanup_errors:
                logger.warning(f"   Error: {error}")
        
        # Notify cost tracker of cleanup results for accurate tracking
        try:
            from core.cost_tracker import get_cost_tracker
            cost_tracker = get_cost_tracker()
            if hasattr(cost_tracker, 'record_shutdown_cleanup'):
                await cost_tracker.record_shutdown_cleanup(cleanup_result, "Backend lifespan shutdown")
        except Exception as cost_err:
            logger.debug(f"Cost tracker notification skipped: {cost_err}")
            
    except ImportError:
        # Fallback to direct cleanup if shutdown hook not available
        logger.info("üßπ Shutdown hook not available, using direct GCP VM Manager cleanup...")
        if gcp_vm_manager is not None:
            try:
                await gcp_vm_manager.cleanup_all_vms(reason="Backend lifespan shutdown (direct)")
                await gcp_vm_manager.cleanup()
                logger.info("‚úÖ GCP VM Manager direct cleanup complete")
            except Exception as vm_err:
                logger.error(f"Direct VM Manager cleanup failed: {vm_err}")
        else:
            logger.info("‚ÑπÔ∏è No GCP VM Manager instance to clean up")
    except Exception as e:
        logger.error(f"Failed to cleanup GCP VM Manager: {e}")

    # Shutdown Cost Tracking System & Infrastructure Orchestrator (v10.0)
    try:
        from core.cost_tracker import get_cost_tracker

        tracker = get_cost_tracker()
        if tracker:
            # Generate final cost report before shutdown
            try:
                stats = tracker.get_stats()
                if stats.get("total_cost_usd", 0) > 0:
                    logger.info(f"üí∞ Session Cost Summary:")
                    logger.info(f"   Total: ${stats.get('total_cost_usd', 0):.4f}")
                    logger.info(f"   VMs: {stats.get('vm_sessions', 0)} sessions")
                    logger.info(f"   Cloud Run: {stats.get('cloud_run_requests', 0)} requests")
            except Exception:
                pass

            await tracker.shutdown()
            logger.info("‚úÖ Cost Tracking System shutdown complete")
    except Exception as e:
        logger.error(f"Failed to shutdown cost tracker: {e}")

    # Shutdown Infrastructure Orchestrator & OrphanDetectionLoop (v10.0)
    try:
        from core.infrastructure_orchestrator import (
            cleanup_infrastructure_on_shutdown,
            get_reconciler,
        )

        reconciler = get_reconciler()

        # Stop Cloud SQL if configured (saves ~$10/month when JARVIS not running)
        stop_sql_on_shutdown = os.getenv("JARVIS_STOP_SQL_ON_SHUTDOWN", "false").lower() == "true"
        if stop_sql_on_shutdown and reconciler:
            logger.info("üóÑÔ∏è Stopping Cloud SQL (cost optimization)...")
            try:
                await reconciler.stop_cloud_sql()
                logger.info("‚úÖ Cloud SQL stopped (saves ~$10/month)")
            except Exception as sql_e:
                logger.debug(f"Cloud SQL stop failed: {sql_e}")

        # Run full infrastructure cleanup (stops orphan loop, releases locks, destroys resources we created)
        await cleanup_infrastructure_on_shutdown()
        logger.info("‚úÖ Infrastructure Orchestrator shutdown complete")

    except ImportError:
        logger.debug("Infrastructure Orchestrator not available for shutdown")
    except Exception as e:
        logger.error(f"Failed to shutdown Infrastructure Orchestrator: {e}")

    # Stop autonomous systems
    if hasattr(app.state, "orchestrator"):
        try:
            await app.state.orchestrator.stop()
            logger.info("‚úÖ Autonomous orchestrator stopped")
        except Exception as e:
            logger.error(f"Failed to stop orchestrator: {e}")

    if hasattr(app.state, "mesh"):
        try:
            await app.state.mesh.stop()
            logger.info("‚úÖ Mesh network stopped")
        except Exception as e:
            logger.error(f"Failed to stop mesh: {e}")

    # Save current code state for next startup
    try:
        from process_cleanup_manager import ProcessCleanupManager

        manager = ProcessCleanupManager()
        manager._save_code_state()
        logger.info("‚úÖ Code state saved for next startup")
    except Exception as e:
        logger.error(f"Failed to save code state: {e}")

    # Shutdown Unified Context Bridge
    if hasattr(app.state, "context_bridge"):
        try:
            from backend.core.unified_context_bridge import shutdown_context_bridge

            await shutdown_context_bridge()
            logger.info("‚úÖ Unified Context Bridge stopped")
        except Exception as e:
            logger.error(f"Failed to stop Context Bridge: {e}")

    # Shutdown Goal Inference Integration
    if hasattr(app.state, "goal_inference_integration"):
        try:
            integration = app.state.goal_inference_integration
            if hasattr(integration, "learning_db"):
                # Save final state and close connections
                integration.learning_db.close()
            logger.info("‚úÖ Goal Inference Integration stopped")
        except Exception as e:
            logger.error(f"Failed to stop Goal Inference: {e}")

    # Shutdown UAE (Unified Awareness Engine) + Learning Database + Yabai
    if hasattr(app.state, "uae_engine"):
        try:
            from intelligence.uae_integration import get_learning_db, get_yabai, shutdown_uae

            logger.info("üß† Shutting down Intelligence Stack...")

            # Get Learning DB metrics before shutdown
            learning_db = get_learning_db()
            if learning_db:
                try:
                    metrics = await learning_db.get_learning_metrics()
                    logger.info("   üìä Final Learning Database Stats:")
                    logger.info(
                        f"   ‚Ä¢ Total Patterns Learned: {metrics['patterns']['total_patterns']}"
                    )
                    logger.info(
                        f"   ‚Ä¢ Display Patterns: {metrics['display_patterns']['total_display_patterns']}"
                    )
                    logger.info(f"   ‚Ä¢ Total Actions Logged: {metrics['actions']['total_actions']}")
                    logger.info(f"   ‚Ä¢ Success Rate: {metrics['actions']['success_rate']:.1f}%")
                    logger.info(
                        f"   ‚Ä¢ Cache Hit Rate: {metrics['cache_performance']['pattern_cache_hit_rate']:.1%}"
                    )
                except Exception as e:
                    logger.debug(f"Could not get final metrics: {e}")

            # Get Phase 2 metrics before shutdown
            logger.info("   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            logger.info("   üìä PHASE 2 INTELLIGENCE STACK - FINAL STATS")
            logger.info("   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")

            # Yabai metrics
            yabai = get_yabai()
            if yabai and yabai.is_monitoring:
                try:
                    yabai_metrics = yabai.get_metrics()
                    logger.info("   üó∫Ô∏è  Yabai Spatial Intelligence:")
                    logger.info(f"      ‚Ä¢ Spaces Monitored: {yabai_metrics['spaces_monitored']}")
                    logger.info(f"      ‚Ä¢ Windows Tracked: {yabai_metrics['windows_tracked']}")
                    logger.info(f"      ‚Ä¢ Space Changes: {yabai_metrics['total_space_changes']}")
                    logger.info(f"      ‚Ä¢ Monitoring Cycles: {yabai_metrics['monitoring_cycles']}")
                    logger.info(f"      ‚Ä¢ Events Emitted: {yabai_metrics.get('events_emitted', 0)}")
                    logger.info(
                        f"      ‚Ä¢ Session Duration: {yabai_metrics['session_duration_minutes']:.1f} minutes"
                    )
                except Exception as e:
                    logger.debug(f"Could not get Yabai metrics: {e}")

            # Pattern Learner metrics
            from intelligence.uae_integration import get_pattern_learner_sync

            pattern_learner = get_pattern_learner_sync()
            if pattern_learner:
                try:
                    pl_stats = pattern_learner.get_statistics()
                    logger.info("   üß† Workspace Pattern Learner (ML):")
                    logger.info(f"      ‚Ä¢ Total Patterns Learned: {pl_stats['total_patterns']}")
                    logger.info(f"      ‚Ä¢ Workflows Detected: {pl_stats['workflows_detected']}")
                    logger.info(f"      ‚Ä¢ Temporal Patterns: {pl_stats['temporal_patterns']}")
                    logger.info(f"      ‚Ä¢ Spatial Preferences: {pl_stats['spatial_preferences']}")
                    logger.info(
                        f"      ‚Ä¢ Predictions Generated: {pl_stats['predictions_generated']}"
                    )
                    logger.info(f"      ‚Ä¢ ML Clustering Runs: {pl_stats['clustering_runs']}")
                except Exception as e:
                    logger.debug(f"Could not get Pattern Learner stats: {e}")

            # Bridge metrics
            from intelligence.uae_integration import get_integration_bridge

            bridge = get_integration_bridge()
            if bridge and bridge.is_active:
                try:
                    bridge_metrics = bridge.get_metrics()
                    logger.info("   üîó Yabai ‚Üî SAI Integration Bridge:")
                    logger.info(f"      ‚Ä¢ Events Bridged: {bridge_metrics['events_bridged']}")
                    logger.info(f"      ‚Ä¢ Yabai ‚Üí SAI: {bridge_metrics['yabai_to_sai']}")
                    logger.info(f"      ‚Ä¢ SAI ‚Üí Yabai: {bridge_metrics['sai_to_yabai']}")
                    logger.info(f"      ‚Ä¢ Contexts Enriched: {bridge_metrics['contexts_enriched']}")
                    logger.info(
                        f"      ‚Ä¢ Actions Coordinated: {bridge_metrics['actions_coordinated']}"
                    )
                except Exception as e:
                    logger.debug(f"Could not get Bridge metrics: {e}")

            logger.info("   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")

            # Shutdown UAE + Learning DB + Yabai + Phase 2
            await shutdown_uae()

            logger.info("‚úÖ UAE (Unified Awareness Engine) stopped")
            logger.info("‚úÖ SAI (Situational Awareness) stopped")
            logger.info("‚úÖ Yabai (Spatial Intelligence) stopped")
            logger.info("‚úÖ Pattern Learner (ML) finalized")
            logger.info("‚úÖ Integration Bridge (Yabai‚ÜîSAI) stopped")
            logger.info("‚úÖ Learning Database closed (all data persisted)")
        except Exception as e:
            logger.error(f"Failed to stop UAE + Learning Database + Yabai: {e}")

    # Stop Voice Unlock system
    voice_unlock = components.get("voice_unlock") or {}
    if hasattr(app.state, "voice_unlock_system") and voice_unlock.get("shutdown_system"):
        try:
            shutdown_system = voice_unlock["shutdown_system"]
            await shutdown_system()
            logger.info("‚úÖ Voice Unlock system stopped")
        except Exception as e:
            logger.error(f"Failed to stop Voice Unlock system: {e}")

    # Stop display monitoring (Component #9)
    if hasattr(app.state, "display_monitor"):
        try:
            await app.state.display_monitor.stop()
            logger.info("‚úÖ Display monitoring stopped")
        except Exception as e:
            logger.error(f"Error stopping display monitoring: {e}")

    # Stop dynamic component loader and self-healer
    try:
        from vision.dynamic_component_loader import get_component_loader

        loader = get_component_loader()
        await loader.stop()
        logger.info("‚úÖ Self-healing system stopped")
    except Exception as e:
        logger.error(f"Error stopping self-healing: {e}")

    if hasattr(app.state, "memory_manager"):
        await app.state.memory_manager.stop_monitoring()

    # Clean up any remaining asyncio event loops in threads
    logger.info("üßπ Cleaning up asyncio threads...")
    try:
        import threading

        # Give asyncio tasks a moment to finish
        await asyncio.sleep(0.5)

        # Count remaining threads for logging
        remaining_threads = [t for t in threading.enumerate() if t.name.startswith('asyncio_')]
        if remaining_threads:
            logger.info(f"   ‚Ä¢ {len(remaining_threads)} asyncio threads cleaning up...")

            # Wait a bit longer for them to finish
            await asyncio.sleep(1.0)

        logger.info("‚úÖ Thread cleanup complete")
    except Exception as e:
        logger.error(f"Error during thread cleanup: {e}")


# Apply vision monitoring fix
try:
    pass

    logger.info("Vision monitoring fix applied")
except Exception as e:
    logger.warning(f"Could not apply vision fix: {e}")

# Force reload vision handler to get latest fixes
try:
    pass

    logger.info("Vision handler reloaded with multi-space fixes")
except Exception as e:
    logger.warning(f"Could not reload vision handler: {e}")

# Create FastAPI app with appropriate lifespan
# PARALLEL_STARTUP_ENABLED is True by default for fast startup
selected_lifespan = parallel_lifespan if PARALLEL_STARTUP_ENABLED else lifespan
logger.info(f"Creating FastAPI app (parallel_startup={PARALLEL_STARTUP_ENABLED})...")
app = FastAPI(
    title="JARVIS Backend (Optimized)",
    version="13.5.0-parallel-startup",
    lifespan=selected_lifespan,
)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üîí ULTRA-FAST LOCK ENDPOINT (MODULE-LEVEL - Always Available)
# Registered immediately at import time, bypasses ALL infrastructure
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
@app.post("/lock-now")
@app.get("/lock-now")
async def ultra_fast_lock():
    """
    Ultra-minimal lock endpoint that bypasses all infrastructure.
    Registered at module level for immediate availability.
    Tries multiple lock methods in order of reliability.
    """
    import asyncio
    import shutil

    async def run_cmd(cmd, timeout=3.0):
        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await asyncio.wait_for(proc.communicate(), timeout=timeout)
            return proc.returncode == 0
        except Exception:
            return False

    # Method 1: AppleScript Cmd+Ctrl+Q (works on all macOS versions)
    if shutil.which("osascript"):
        script = 'tell application "System Events" to keystroke "q" using {command down, control down}'
        if await run_cmd(["osascript", "-e", script]):
            return {"success": True, "method": "applescript"}

    # Method 2: CGSession (older macOS location)
    cgsession_old = "/System/Library/CoreServices/Menu Extras/User.menu/Contents/Resources/CGSession"
    if os.path.exists(cgsession_old):
        if await run_cmd([cgsession_old, "-suspend"]):
            return {"success": True, "method": "cgsession"}

    # Method 3: LockScreen binary (newer macOS)
    lockscreen = "/System/Library/CoreServices/RemoteManagement/AppleVNCServer.bundle/Contents/Support/LockScreen.app/Contents/MacOS/LockScreen"
    if os.path.exists(lockscreen):
        if await run_cmd([lockscreen]):
            return {"success": True, "method": "lockscreen"}

    # Method 4: pmset display sleep
    if shutil.which("pmset"):
        if await run_cmd(["pmset", "displaysleepnow"]):
            return {"success": True, "method": "pmset"}

    # Method 5: ScreenSaver
    if os.path.exists("/System/Library/CoreServices/ScreenSaverEngine.app"):
        if await run_cmd(["open", "-a", "ScreenSaverEngine"]):
            return {"success": True, "method": "screensaver"}

    return {"success": False, "error": "all_methods_failed"}

logger.info("‚úÖ Ultra-fast /lock-now endpoint registered (module-level)")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üß† CONTEXT-INTELLIGENT LOCK ENDPOINT (MODULE-LEVEL)
# Uses lightweight context intelligence without full VBI
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
@app.post("/lock-with-context")
async def lock_with_context(request: Request):
    """
    Context-intelligent lock endpoint.

    Uses lightweight verification (NOT full VBI):
    - RecentSpeakerCache for fast speaker recognition (~5ms)
    - Behavioral patterns (time of day, recent activity)
    - Temporal context (last unlock time, session duration)

    This provides context awareness without blocking like full VBI.

    Request body (optional):
    {
        "audio_b64": "base64_encoded_audio",  # Optional: for speaker cache check
        "context": {
            "timestamp": "ISO datetime",
            "session_duration_ms": 12345,
            "last_activity_ms": 1000
        }
    }
    """
    import asyncio
    import shutil
    from datetime import datetime
    import base64

    start_time = datetime.now()
    context_result = {
        "speaker_recognized": False,
        "behavioral_match": True,  # Default true - lock is owner-only action
        "temporal_valid": True,
        "confidence": 0.85,  # Default confidence for lock (no verification needed)
        "verification_ms": 0,
    }

    try:
        # Parse request body if provided
        body = {}
        try:
            body = await request.json()
        except Exception:
            pass  # No body is fine - context is optional for lock

        # =====================================================================
        # LIGHTWEIGHT CONTEXT INTELLIGENCE (Non-blocking)
        # =====================================================================

        # 1. Check RecentSpeakerCache if audio provided (fast path: ~5ms)
        audio_b64 = body.get("audio_b64")
        if audio_b64:
            try:
                from voice_unlock.cloud_ecapa_client import get_recent_speaker_cache
                cache = get_recent_speaker_cache()

                if cache and cache.CACHE_ENABLED:
                    audio_data = base64.b64decode(audio_b64)
                    # Quick fingerprint check (no cloud call)
                    cache_result = await asyncio.wait_for(
                        cache.check_fast_path(audio_data),
                        timeout=0.1  # 100ms max - don't block lock
                    )
                    if cache_result:
                        embedding, speaker_name, confidence = cache_result
                        context_result["speaker_recognized"] = True
                        context_result["speaker_name"] = speaker_name
                        context_result["confidence"] = confidence
                        context_result["cache_hit"] = True
            except asyncio.TimeoutError:
                pass  # Don't block lock for slow cache
            except Exception as e:
                logger.debug(f"Context cache check skipped: {e}")

        # 2. Behavioral context check (from request body)
        ctx = body.get("context", {})
        if ctx:
            session_duration = ctx.get("session_duration_ms", 0)
            last_activity = ctx.get("last_activity_ms", 0)

            # Reasonable session = behavioral match
            if session_duration > 0 and session_duration < 86400000:  # < 24 hours
                context_result["behavioral_match"] = True
                context_result["session_duration_ms"] = session_duration

            # Recent activity = likely same user
            if last_activity > 0 and last_activity < 300000:  # < 5 minutes
                context_result["recent_activity"] = True

        context_result["verification_ms"] = (datetime.now() - start_time).total_seconds() * 1000

    except Exception as e:
        logger.warning(f"Context check failed (proceeding with lock): {e}")

    # =====================================================================
    # EXECUTE LOCK (Always proceeds - lock is a safe operation)
    # =====================================================================
    async def run_cmd(cmd, timeout=3.0):
        try:
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await asyncio.wait_for(proc.communicate(), timeout=timeout)
            return proc.returncode == 0
        except Exception:
            return False

    lock_success = False
    lock_method = "none"

    # Method 1: AppleScript Cmd+Ctrl+Q (works on all macOS versions)
    if shutil.which("osascript"):
        script = 'tell application "System Events" to keystroke "q" using {command down, control down}'
        if await run_cmd(["osascript", "-e", script]):
            lock_success = True
            lock_method = "applescript"

    # Method 2: CGSession (older macOS location)
    if not lock_success:
        cgsession_old = "/System/Library/CoreServices/Menu Extras/User.menu/Contents/Resources/CGSession"
        if os.path.exists(cgsession_old):
            if await run_cmd([cgsession_old, "-suspend"]):
                lock_success = True
                lock_method = "cgsession"

    # Method 3: LockScreen binary (newer macOS)
    if not lock_success:
        lockscreen = "/System/Library/CoreServices/RemoteManagement/AppleVNCServer.bundle/Contents/Support/LockScreen.app/Contents/MacOS/LockScreen"
        if os.path.exists(lockscreen):
            if await run_cmd([lockscreen]):
                lock_success = True
                lock_method = "lockscreen"

    # Method 4: pmset display sleep
    if not lock_success and shutil.which("pmset"):
        if await run_cmd(["pmset", "displaysleepnow"]):
            lock_success = True
            lock_method = "pmset"

    # Method 5: ScreenSaver
    if not lock_success and os.path.exists("/System/Library/CoreServices/ScreenSaverEngine.app"):
        if await run_cmd(["open", "-a", "ScreenSaverEngine"]):
            lock_success = True
            lock_method = "screensaver"

    # Invalidate speaker cache on lock (security)
    if lock_success:
        try:
            from voice_unlock.cloud_ecapa_client import get_recent_speaker_cache
            cache = get_recent_speaker_cache()
            if cache:
                asyncio.create_task(cache.invalidate())  # Fire-and-forget
        except Exception:
            pass

    total_ms = (datetime.now() - start_time).total_seconds() * 1000

    return {
        "success": lock_success,
        "method": lock_method,
        "context_intelligence": context_result,
        "total_ms": round(total_ms, 2),
        "message": "Screen locked successfully" if lock_success else "Lock failed - all methods exhausted"
    }

logger.info("‚úÖ Context-intelligent /lock-with-context endpoint registered (module-level)")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ROBUST DYNAMIC CORS CONFIGURATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
logger.info("üîí Configuring CORS security...")

# Detect environment
IS_PRODUCTION = os.getenv("ENVIRONMENT", "development").lower() == "production"
IS_DOCKER = os.path.exists("/.dockerenv")
BACKEND_PORT = os.getenv("BACKEND_PORT", "8010")

def build_cors_origins():
    """
    Build comprehensive CORS origins list dynamically

    Supports:
    - Multiple ports (3000, 3001, 8000, 8010, 8080)
    - Both localhost and 127.0.0.1
    - WebSocket origins (ws://)
    - IPv6 localhost ([::1])
    - Docker networking
    - Custom origins from environment
    - Production domains
    """
    origins = set()

    # Development origins - localhost variations
    dev_ports = [3000, 3001, 8000, 8010, 8080]
    for port in dev_ports:
        origins.add(f"http://localhost:{port}")
        origins.add(f"http://127.0.0.1:{port}")
        origins.add(f"ws://localhost:{port}")
        origins.add(f"ws://127.0.0.1:{port}")

    # Add current backend port explicitly
    origins.add(f"http://localhost:{BACKEND_PORT}")
    origins.add(f"http://127.0.0.1:{BACKEND_PORT}")
    origins.add(f"ws://localhost:{BACKEND_PORT}")
    origins.add(f"ws://127.0.0.1:{BACKEND_PORT}")

    # IPv6 localhost support
    for port in dev_ports:
        origins.add(f"http://[::1]:{port}")
        origins.add(f"ws://[::1]:{port}")

    # Docker networking support
    if IS_DOCKER:
        origins.add("http://host.docker.internal:3000")
        origins.add("http://host.docker.internal:8010")
        origins.add("ws://host.docker.internal:8010")

    # Custom origins from environment (comma-separated)
    custom_origins = os.getenv("CORS_ORIGINS", "")
    if custom_origins:
        for origin in custom_origins.split(","):
            origin = origin.strip()
            if origin:
                origins.add(origin)
                # Also add WebSocket version if HTTP
                if origin.startswith("http://"):
                    origins.add(origin.replace("http://", "ws://"))
                elif origin.startswith("https://"):
                    origins.add(origin.replace("https://", "wss://"))

    # Production domains (if specified)
    prod_domain = os.getenv("PRODUCTION_DOMAIN", "")
    if prod_domain and IS_PRODUCTION:
        origins.add(f"https://{prod_domain}")
        origins.add(f"wss://{prod_domain}")
        origins.add(f"https://www.{prod_domain}")
        origins.add(f"wss://www.{prod_domain}")

    return sorted(list(origins))

# Build origins list
allowed_origins = build_cors_origins()

# Allowed headers (comprehensive list)
allowed_headers = [
    "accept",
    "accept-encoding",
    "authorization",
    "content-type",
    "dnt",
    "origin",
    "user-agent",
    "x-csrftoken",
    "x-requested-with",
    "x-api-key",
    "x-client-id",
    "x-session-id",
    "cache-control",
]

# Exposed headers (for client access)
exposed_headers = [
    "content-length",
    "content-type",
    "x-request-id",
    "x-response-time",
    "x-rate-limit-limit",
    "x-rate-limit-remaining",
    "x-rate-limit-reset",
]

# Allowed methods
allowed_methods = ["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH", "HEAD"]

# Configure CORS middleware
try:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=allowed_origins,
        allow_credentials=True,
        allow_methods=allowed_methods,
        allow_headers=allowed_headers,
        expose_headers=exposed_headers,
        max_age=3600,  # Cache preflight requests for 1 hour
    )

    # Log CORS configuration
    logger.info("‚úÖ CORS configured successfully")
    logger.info(f"   ‚Ä¢ Environment: {'Production' if IS_PRODUCTION else 'Development'}")
    logger.info(f"   ‚Ä¢ Backend Port: {BACKEND_PORT}")
    logger.info(f"   ‚Ä¢ Allowed Origins: {len(allowed_origins)} configured")

    # Show first few origins in development
    if not IS_PRODUCTION:
        logger.info(f"   ‚Ä¢ Sample Origins: {', '.join(allowed_origins[:5])}")
        if len(allowed_origins) > 5:
            logger.info(f"   ‚Ä¢ ... and {len(allowed_origins) - 5} more")

    # Security warnings
    if IS_PRODUCTION and "*" in allowed_origins:
        logger.error("‚ö†Ô∏è  SECURITY WARNING: Wildcard CORS origin in production!")

    if not IS_PRODUCTION:
        logger.info("   ‚Ä¢ Dev Mode: Permissive CORS for all localhost ports")
    else:
        logger.info("   ‚Ä¢ Production Mode: Restricted CORS origins")

except Exception as e:
    logger.error(f"‚ùå Failed to configure CORS: {e}")
    # Minimal fallback CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["http://localhost:3000", f"http://localhost:{BACKEND_PORT}"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    logger.warning("‚ö†Ô∏è  Using minimal fallback CORS configuration")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# WEBSOCKET ORIGIN VALIDATION MIDDLEWARE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# This middleware ensures WebSocket connections are properly allowed
# in development mode, fixing 403 errors during handshake

from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response

class WebSocketOriginMiddleware(BaseHTTPMiddleware):
    """
    Custom middleware to handle WebSocket origin validation.

    FastAPI's CORSMiddleware doesn't properly handle WebSocket upgrade
    requests in all cases. This middleware ensures development origins
    are allowed and logs connection attempts for debugging.
    """

    def __init__(self, app, allowed_origins: set = None, allow_all_in_dev: bool = True):
        super().__init__(app)
        self.allowed_origins = allowed_origins or set()
        self.allow_all_in_dev = allow_all_in_dev
        self.is_production = IS_PRODUCTION

        # Build set of allowed origins for fast lookup
        self._build_origin_set()

    def _build_origin_set(self):
        """Build comprehensive origin set dynamically."""
        self.origin_set = set(self.allowed_origins)

        if not self.is_production:
            # In development, allow common localhost variations
            dev_ports = [3000, 3001, 5173, 8000, 8010, 8080]
            for port in dev_ports:
                self.origin_set.add(f"http://localhost:{port}")
                self.origin_set.add(f"http://127.0.0.1:{port}")
                self.origin_set.add(f"https://localhost:{port}")
                self.origin_set.add(f"https://127.0.0.1:{port}")

            # Add special origins
            self.origin_set.add("null")  # For file:// origins
            self.origin_set.add("")      # For missing origin (some clients)

            # Add IPv6 localhost
            for port in dev_ports:
                self.origin_set.add(f"http://[::1]:{port}")
                self.origin_set.add(f"https://[::1]:{port}")

    def _is_origin_allowed(self, origin: str) -> bool:
        """Check if origin is allowed."""
        if not origin:
            # Allow missing origin in development
            return not self.is_production

        # Check exact match
        if origin in self.origin_set:
            return True

        # In development, allow any localhost origin
        if not self.is_production:
            origin_lower = origin.lower()
            if any(local in origin_lower for local in ['localhost', '127.0.0.1', '[::1]']):
                return True

        return False

    async def dispatch(self, request: Request, call_next):
        # Check if this is a WebSocket upgrade request
        is_websocket = (
            request.headers.get("upgrade", "").lower() == "websocket" or
            request.scope.get("type") == "websocket"
        )

        if is_websocket:
            origin = request.headers.get("origin", "")

            # Log WebSocket connection attempt (useful for debugging)
            logger.debug(f"[WS-ORIGIN] WebSocket upgrade from origin: '{origin}' path: {request.url.path}")

            # Validate origin
            if not self._is_origin_allowed(origin):
                logger.warning(f"[WS-ORIGIN] Rejected WebSocket from unauthorized origin: '{origin}'")
                # Return 403 for unauthorized origins
                return Response(
                    content="Forbidden: Origin not allowed",
                    status_code=403,
                    media_type="text/plain"
                )

            # Log successful validation
            if origin:
                logger.debug(f"[WS-ORIGIN] Allowed WebSocket from origin: '{origin}'")

        return await call_next(request)

# Add WebSocket origin middleware (runs before CORS)
try:
    app.add_middleware(
        WebSocketOriginMiddleware,
        allowed_origins=set(allowed_origins),
        allow_all_in_dev=not IS_PRODUCTION
    )
    logger.info("‚úÖ WebSocket origin validation middleware configured")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è  Could not add WebSocket origin middleware: {e}")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PERFORMANCE PROFILING MIDDLEWARE - Request latency tracking
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Light integration: Only profiling/metrics, no replacement of existing
# Connection Orchestrator or FAISS caching systems
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
PERFORMANCE_OPTIMIZER_AVAILABLE = False
try:
    from core.performance_optimizer import (
        get_optimizer,
        get_config,
        get_profiler,
        ProfileSample,
    )
    from starlette.middleware.base import BaseHTTPMiddleware
    from starlette.requests import Request
    from starlette.responses import Response
    import time as _perf_time

    class PerformanceProfilingMiddleware(BaseHTTPMiddleware):
        """
        FastAPI middleware for request latency tracking and profiling.

        Features:
        - Non-blocking request timing
        - Automatic slow request detection
        - Error rate tracking
        - Zero impact on existing DB/Cache systems

        Logs format: [PERF] GET /health took 12ms
        """

        async def dispatch(self, request: Request, call_next) -> Response:
            config = get_config()
            profiler = get_profiler()

            start_time = _perf_time.time()
            path = request.url.path
            method = request.method

            success = True
            error_msg = None
            status_code = 200

            try:
                response = await call_next(request)
                status_code = response.status_code
                success = status_code < 400
                if not success:
                    error_msg = f"HTTP {status_code}"
                return response

            except Exception as e:
                success = False
                error_msg = str(e)
                raise

            finally:
                end_time = _perf_time.time()
                duration_ms = (end_time - start_time) * 1000

                # Record profile sample (non-blocking)
                sample = ProfileSample(
                    name=f"HTTP:{method}:{path}",
                    start_time=start_time,
                    end_time=end_time,
                    duration_ms=duration_ms,
                    success=success,
                    error=error_msg,
                )

                # Fire-and-forget recording to avoid blocking the response
                try:
                    asyncio.create_task(profiler.record(sample))
                except Exception:
                    pass  # Never block on profiling

                # Log performance (always log, threshold checked in profiler)
                if config.profile_enabled:
                    if duration_ms > config.profile_slow_threshold_ms:
                        logger.warning(f"[PERF] {method} {path} took {duration_ms:.1f}ms (SLOW)")
                    else:
                        logger.debug(f"[PERF] {method} {path} took {duration_ms:.1f}ms")

    # Add middleware to app
    app.add_middleware(PerformanceProfilingMiddleware)
    PERFORMANCE_OPTIMIZER_AVAILABLE = True

    # Log config status
    _perf_config = get_config()
    _profile_status = "ENABLED" if _perf_config.profile_enabled else "disabled (set JARVIS_PROFILE_ENABLED=true to enable)"
    _cache_status = "ENABLED" if _perf_config.cache_enabled else "disabled"
    logger.info(f"‚úÖ Performance Optimizer attached (Profiling Mode)")
    logger.info(f"   Profiling: {_profile_status}")
    logger.info(f"   Caching: {_cache_status} (L2 Redis: {'enabled' if _perf_config.cache_l2_enabled else 'disabled'})")
    logger.info(f"   Slow threshold: {_perf_config.profile_slow_threshold_ms}ms")

except ImportError as e:
    logger.debug(f"Performance Optimizer not available: {e}")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è  Performance Optimizer initialization failed: {e}")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# VOICE UNLOCK API - Mount at module level for proper route registration
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Routes must be registered BEFORE the server starts, NOT during lifespan
try:
    from api.voice_unlock_api import router as voice_unlock_router
    _routes_before = len(app.routes)
    app.include_router(voice_unlock_router, tags=["voice_unlock"])
    _routes_after = len(app.routes)
    _voice_routes = [getattr(r, 'path', '') for r in app.routes if hasattr(r, 'path') and 'voice-unlock' in getattr(r, 'path', '')]
    logger.info(f"‚úÖ Voice Unlock API mounted at /api/voice-unlock (module level)")
    logger.info(f"   Routes before: {_routes_before}, after: {_routes_after}, voice-unlock: {len(_voice_routes)}")
    if _voice_routes:
        for vr in _voice_routes[:3]:
            logger.info(f"   - {vr}")
        if len(_voice_routes) > 3:
            logger.info(f"   ... and {len(_voice_routes) - 3} more")

except ImportError as e:
    logger.warning(f"‚ö†Ô∏è  Voice Unlock API not available: {e}")
except Exception as e:
    logger.error(f"‚ùå Failed to mount Voice Unlock API: {e}")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TRINITY HEALTH API - Monitor JARVIS-Prime and Reactor-Core
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
try:
    from api.trinity_health_api import router as trinity_health_router
    app.include_router(trinity_health_router, tags=["trinity"])
    logger.info("‚úÖ Trinity Health API mounted at /health/trinity, /health/prime, /health/routing")
except ImportError as e:
    logger.debug(f"Trinity Health API not available: {e}")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è  Failed to mount Trinity Health API: {e}")


# =============================================================================
# Lightweight Health Check Endpoints (Non-blocking)
# =============================================================================

@app.api_route("/health/startup", methods=["GET", "HEAD"])
async def health_startup():
    """
    Startup progress endpoint - shows detailed initialization status.

    This is the SINGLE SOURCE OF TRUTH for startup progress when running
    under the supervisor. The supervisor polls this endpoint and broadcasts
    to the loading page.

    Returns immediately even during initialization.
    """
    # Priority 1: Check for supervisor progress bridge (unified coordination)
    try:
        from core.supervisor.supervisor_integration import get_progress_bridge
        bridge = get_progress_bridge()
        if bridge._initialized:
            return bridge.get_status()
    except ImportError:
        pass

    # Priority 2: Check for parallel initializer
    if hasattr(app.state, "parallel_initializer"):
        return app.state.parallel_initializer.get_status()

    # Priority 3: Fallback to app.state (may be set by progress bridge)
    phase = getattr(app.state, "startup_phase", "UNKNOWN")
    progress = getattr(app.state, "startup_progress", 0.0)
    message = getattr(app.state, "startup_message", "Initializing...")
    ready = getattr(app.state, "components_ready", set())
    failed = getattr(app.state, "components_failed", set())
    is_complete = getattr(app.state, "startup_complete", False)

    return {
        "phase": phase,
        "progress": progress,
        "message": message,
        "components": {
            "ready": list(ready) if isinstance(ready, set) else ready,
            "failed": list(failed) if isinstance(failed, set) else failed,
        },
        "ready_for_requests": True,
        "full_mode": phase == "FULL_MODE" or is_complete,
        "is_complete": is_complete,
    }


@app.get("/health/ping")
async def health_ping():
    """
    Ultra-lightweight liveness probe - returns immediately.

    Use this endpoint for health checks that need sub-millisecond response.
    Does NOT check any services, just confirms the event loop is responsive.
    """
    return {"status": "ok", "message": "pong"}


@app.get("/health/ready")
async def health_ready():
    """
    Quick readiness probe - confirms key services are OPERATIONALLY READY.

    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    CRITICAL: This endpoint determines when JARVIS is ready for USER INTERACTION
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    v95.3: Uses ReadinessStateManager as the PRIMARY source of truth.
    Falls back to legacy component checks if manager is unavailable.

    Returns ready=True ONLY when:
    1. ReadinessStateManager is in READY phase (primary), OR
    2. Legacy checks pass: ML models loaded, Voice system initialized, Core APIs functional

    This prevents false positives where the loading page redirects before
    JARVIS can actually respond to user commands.
    """
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # v95.3: PRIMARY CHECK - ReadinessStateManager (fast path)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # If ReadinessStateManager is available and reports READY, return immediately.
    # This is the fix for the 503 error - the manager is now properly transitioned
    # to READY during lifespan initialization.
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    try:
        from core.readiness_state_manager import (
            get_readiness_manager,
            ProbeType,
            InitializationPhase,
        )
        
        manager = get_readiness_manager("jarvis-body")
        
        if manager and manager.state.phase == InitializationPhase.READY:
            # Fast path: Manager says we're ready
            probe = manager.handle_probe(ProbeType.READINESS)
            
            # Get component status for details
            component_status = {}
            for name, comp in manager.state.components.items():
                component_status[name] = {
                    "healthy": comp.healthy,
                    "progress": comp.progress,
                    "category": comp.category.value if comp.category else "unknown",
                }
            
            return {
                "status": "ready",
                "ready": True,
                "operational": True,
                "ghosts_ready": True,  # Assumed ready when manager is READY
                "source": "ReadinessStateManager",
                "phase": manager.state.phase.value,
                "uptime": time.time() - (manager.state.started_at or time.time()),
                "details": {
                    "manager_phase": manager.state.phase.value,
                    "components": component_status,
                    "event_loop": True,
                    "manager_healthy": manager.is_healthy(),
                },
                "services": {
                    "ready": list(manager.state.components.keys()),
                    "failed": [],
                },
            }
        elif manager:
            # Manager exists but not READY - include phase info
            phase_info = {
                "phase": manager.state.phase.value,
                "is_initializing": manager.state.phase == InitializationPhase.INITIALIZING,
                "is_shutting_down": manager.state.phase == InitializationPhase.SHUTTING_DOWN,
            }
            # Fall through to legacy checks but include manager info
        else:
            phase_info = None
            
    except ImportError:
        phase_info = None
    except Exception as e:
        phase_info = {"error": str(e)[:50]}
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # LEGACY CHECKS: Component-by-component verification
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # These checks provide detailed diagnostics when manager isn't in READY state.
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    details = {}
    if phase_info:
        details["readiness_manager"] = phase_info
    critical_services_ready = []
    critical_services_failed = []

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CHECK 1: ML Engine Registry (CRITICAL for voice unlock)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    ml_warmup_info = {}
    ml_ready = False
    try:
        from voice_unlock.ml_engine_registry import get_ml_warmup_status, is_voice_unlock_ready
        ml_warmup_info = get_ml_warmup_status()
        details["ml_warmup"] = ml_warmup_info

        if ml_warmup_info.get("is_warming_up"):
            details["ml_models_ready"] = False
            details["ml_models_status"] = "warming_up"
            details["ml_progress"] = ml_warmup_info.get("progress", 0.0)
            details["ml_current_engine"] = ml_warmup_info.get("current_engine")
            critical_services_failed.append("ml_models")
        elif ml_warmup_info.get("is_ready"):
            details["ml_models_ready"] = True
            details["ml_models_status"] = "ready"
            ml_ready = True
            critical_services_ready.append("ml_models")
        else:
            details["ml_models_ready"] = False
            details["ml_models_status"] = "not_started"
            critical_services_failed.append("ml_models")
    except ImportError:
        details["ml_models_ready"] = False
        details["ml_models_status"] = "not_available"
        # ML not available is degraded but not a blocker
    except Exception as e:
        details["ml_models_ready"] = False
        details["ml_models_status"] = f"error: {str(e)[:50]}"
        critical_services_failed.append("ml_models")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CHECK 2: Voice System (CRITICAL for user interaction)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    voice_ready = False
    if hasattr(app.state, "voice_unlock"):
        voice_initialized = app.state.voice_unlock.get("initialized", False)
        details["voice_unlock_ready"] = voice_initialized
        if voice_initialized:
            voice_ready = True
            critical_services_ready.append("voice_unlock")
        else:
            critical_services_failed.append("voice_unlock")
    else:
        details["voice_unlock_ready"] = False
        # Voice unlock not available is acceptable (degraded mode)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CHECK 3: Speaker Verification Service (IMPORTANT for voice biometrics)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    speaker_service_ready = False
    try:
        from voice.speaker_verification_service import get_speaker_service
        speaker_svc = get_speaker_service()
        if speaker_svc and speaker_svc._initialized:
            details["speaker_service_ready"] = True
            speaker_service_ready = True
            critical_services_ready.append("speaker_service")
        else:
            details["speaker_service_ready"] = False
            critical_services_failed.append("speaker_service")
    except ImportError:
        details["speaker_service_ready"] = False
    except Exception:
        details["speaker_service_ready"] = False

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CHECK 4: WebSocket System (CRITICAL for frontend communication)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # v3.0: Robust, fail-safe WebSocket check with intelligent state detection
    # 
    # Strategy:
    # 1. Try to import the WebSocket module - import failure = definitely not ready
    # 2. Check if manager singleton exists (lazy init means it's created on first use)
    # 3. Verify route registration as secondary confirmation
    # 4. On ANY error, mark as FAILED (fail-safe, never assume ready on error)
    #
    # The manager is created lazily on first WebSocket connection, so we also
    # check route registration which happens at startup.
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    websocket_ready = False
    websocket_check_method = "unknown"
    
    try:
        # Step 1: Verify module can be imported (catches broken dependencies)
        import api.unified_websocket as ws_module
        
        # Step 2: Check if the manager singleton already exists
        # Access the module's private variable directly (don't call get_ws_manager which creates it)
        existing_ws_manager = getattr(ws_module, '_ws_manager', None)
        
        if existing_ws_manager is not None:
            # Manager exists - WebSocket is definitely operational
            details["websocket_ready"] = True
            websocket_ready = True
            websocket_check_method = "manager_exists"
            critical_services_ready.append("websocket")
            
            # Store in app.state for other components
            app.state.unified_websocket_manager = existing_ws_manager
            
            # Collect connection stats for observability
            if hasattr(existing_ws_manager, 'connections'):
                details["websocket_connections"] = len(existing_ws_manager.connections)
            if hasattr(existing_ws_manager, 'metrics'):
                metrics = existing_ws_manager.metrics
                details["websocket_total_connections"] = metrics.get("total_connections", 0)
                details["websocket_uptime"] = time.time() - metrics.get("uptime_start", time.time())
        else:
            # Manager not yet created (lazy init) - check if route is registered
            # Route registration happens at app startup, so this is reliable
            ws_routes = []
            for route in app.routes:
                route_path = getattr(route, 'path', '')
                if '/ws' in route_path:
                    ws_routes.append(route_path)
            
            if ws_routes:
                # Routes exist - WebSocket endpoint is registered and can accept connections
                # The manager will be created on first connection
                details["websocket_ready"] = True
                details["websocket_routes"] = ws_routes[:3]  # Show up to 3 routes
                details["websocket_note"] = "Routes registered, manager lazy-init"
                websocket_ready = True
                websocket_check_method = "route_registered"
                critical_services_ready.append("websocket")
            else:
                # No routes registered - WebSocket is NOT available
                details["websocket_ready"] = False
                details["websocket_note"] = "No WebSocket routes registered"
                websocket_check_method = "no_routes"
                critical_services_failed.append("websocket")
                
    except ImportError as e:
        # Module import failed - WebSocket is definitely NOT ready
        details["websocket_ready"] = False
        details["websocket_error"] = f"Import failed: {str(e)[:100]}"
        websocket_check_method = "import_failed"
        critical_services_failed.append("websocket")
        
    except Exception as e:
        # Any other error - mark as FAILED (fail-safe, never assume ready on error)
        details["websocket_ready"] = False
        details["websocket_error"] = f"Check failed: {str(e)[:100]}"
        websocket_check_method = "check_error"
        critical_services_failed.append("websocket")
    
    # Record the check method for debugging
    details["websocket_check_method"] = websocket_check_method

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CHECK 5: Database Connection (IMPORTANT for persistence)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    database_ready = False
    try:
        from intelligence.learning_database import get_learning_db
        db = get_learning_db()
        if db and db._initialized:
            details["database_connected"] = True
            database_ready = True
            critical_services_ready.append("database")
        else:
            details["database_connected"] = False
    except ImportError:
        details["database_connected"] = False
    except Exception:
        details["database_connected"] = False

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CHECK 6: Ghost Proxy Models (CRITICAL - Frontend must wait for these!)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Ghost Proxies are "empty shells" that materialize in the background.
    # The frontend MUST wait until critical proxies are ready before redirecting.
    # This prevents the "instant ready" false positive.
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    ghosts_ready = True
    ghost_stats = {
        "total": 0,
        "ready": 0,
        "loading": 0,
        "failed": 0,
        "by_category": {},
    }
    ghost_details = {}

    try:
        from core.proxy_helpers import get_all_proxy_stats, is_proxy_ready

        # Get all proxy stats dynamically (no hardcoding!)
        all_proxies = get_all_proxy_stats(app.state)

        # Define which categories are CRITICAL for user interaction
        # These must be ready before we report "ready" to frontend
        critical_categories = {"voice", "vision"}

        for name, proxy_info in all_proxies.items():
            category = proxy_info.get("category", "other")
            is_ready = proxy_info.get("ready", False)
            is_loading = proxy_info.get("loading", False)
            state = proxy_info.get("state", "unknown")

            ghost_stats["total"] += 1

            # Track by category
            if category not in ghost_stats["by_category"]:
                ghost_stats["by_category"][category] = {
                    "total": 0, "ready": 0, "loading": 0
                }
            ghost_stats["by_category"][category]["total"] += 1

            if is_ready:
                ghost_stats["ready"] += 1
                ghost_stats["by_category"][category]["ready"] += 1
                ghost_details[name] = "ready"
            elif is_loading:
                ghost_stats["loading"] += 1
                ghost_stats["by_category"][category]["loading"] += 1
                ghost_details[name] = "loading"

                # If a CRITICAL category model is still loading, we're NOT ready
                if category in critical_categories:
                    ghosts_ready = False
            elif state == "failed":
                ghost_stats["failed"] += 1
                ghost_details[name] = "failed"
            else:
                ghost_details[name] = state

                # Unknown/ghost state in critical category = not ready
                if category in critical_categories:
                    ghosts_ready = False

        # Additional check: Require at least one model per critical category
        for category in critical_categories:
            cat_stats = ghost_stats["by_category"].get(category, {})
            if cat_stats.get("ready", 0) == 0 and cat_stats.get("total", 0) > 0:
                # Category has models but none ready
                ghosts_ready = False
                ghost_details[f"{category}_category"] = "no_models_ready"
            elif cat_stats.get("total", 0) == 0:
                # Category has no models registered yet
                ghosts_ready = False
                ghost_details[f"{category}_category"] = "not_registered"

        details["ghost_proxies"] = {
            "ready": ghosts_ready,
            "stats": ghost_stats,
            "models": ghost_details,
        }

        if ghosts_ready:
            critical_services_ready.append("ghost_proxies")
        else:
            critical_services_failed.append("ghost_proxies")

    except ImportError:
        # proxy_helpers not available - can't check ghosts
        details["ghost_proxies"] = {"available": False, "ready": True}
        # Don't block on missing module
    except Exception as e:
        details["ghost_proxies"] = {"error": str(e)[:100], "ready": False}
        ghosts_ready = False
        critical_services_failed.append("ghost_proxies")

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # DETERMINE OVERALL READINESS (Progressive Model v3.1 - Ghost-Aware + Graceful Degradation)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Progressive readiness levels:
    # 1. ghosts_ready ‚Üí Ghost Proxies (Voice + Vision) are materialized
    # 2. interactive_ready ‚Üí ParallelInitializer says user can interact
    # 3. websocket_ready ‚Üí WebSocket routes registered
    # 4. voice_operational ‚Üí voice features work
    # 5. ml_ready ‚Üí full ML capabilities
    #
    # v3.1: GRACEFUL DEGRADATION - Don't block forever on failed Ghost Proxies!
    # After STARTUP_GRACE_PERIOD seconds, allow degraded mode startup.
    # This prevents infinite "warming_up" loops when AI models fail to load.
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # v3.1: Calculate startup elapsed time for graceful degradation
    STARTUP_GRACE_PERIOD = float(os.getenv("JARVIS_STARTUP_GRACE_PERIOD", "45.0"))
    startup_time = getattr(app.state, '_startup_time', None)
    if startup_time is None:
        # Track startup time on first health check
        app.state._startup_time = time.time()
        startup_time = app.state._startup_time

    startup_elapsed = time.time() - startup_time
    grace_period_exceeded = startup_elapsed > STARTUP_GRACE_PERIOD

    # v3.1: Check if we should force degraded mode due to grace period
    # This prevents blocking forever on failed Ghost Proxies
    if grace_period_exceeded and not ghosts_ready:
        # Only LOADING ghosts should block - FAILED ghosts are accepted in degraded mode
        still_loading = ghost_stats.get("loading", 0)
        if still_loading == 0:
            # No ghosts loading, just failed ones - accept degraded mode
            ghosts_ready = True  # Override for degraded mode
            details["grace_period_degraded"] = True
            details["grace_period_reason"] = "No Ghost Proxies actively loading, accepting failed models"
        elif startup_elapsed > STARTUP_GRACE_PERIOD * 2:
            # Double grace period exceeded - force accept even with loading
            ghosts_ready = True
            details["grace_period_forced"] = True
            details["grace_period_reason"] = f"Force accept after {startup_elapsed:.0f}s (>2x grace period)"

    details["startup_elapsed"] = round(startup_elapsed, 1)
    details["grace_period"] = STARTUP_GRACE_PERIOD
    details["grace_period_exceeded"] = grace_period_exceeded

    voice_operational = ml_ready or speaker_service_ready or voice_ready

    # v2.0: Check ParallelInitializer's interactive_ready state
    interactive_ready = getattr(app.state, 'interactive_ready', False)
    if hasattr(app.state, 'parallel_initializer'):
        pi = app.state.parallel_initializer
        if hasattr(pi, 'is_interactive_ready') and callable(pi.is_interactive_ready):
            interactive_ready = pi.is_interactive_ready()
        details["parallel_initializer_interactive"] = interactive_ready

    core_ready = websocket_ready or interactive_ready  # Either is sufficient
    full_ready = core_ready and voice_operational

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # v3.0 GHOST-AWARE READINESS DECISION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # The server HTTP endpoint is up, but AI models may still be loading.
    # We MUST wait for Ghost Proxies to materialize before signaling "ready".
    #
    # Status progression:
    # 1. "initializing" ‚Üí HTTP not ready
    # 2. "warming_up"   ‚Üí HTTP ready, Ghost Proxies still loading (KEEP WAITING)
    # 3. "ready"        ‚Üí Everything ready, frontend can proceed
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    if not ghosts_ready and core_ready:
        # Server is up but AI models are still loading
        # This is the KEY FIX: Frontend must wait!
        status = "warming_up"
        ready = False  # <-- CRITICAL: Keep frontend waiting

        # Add helpful info about what's loading
        loading_models = [k for k, v in ghost_details.items() if v == "loading"]
        details["warming_up_reason"] = {
            "message": "AI models are initializing in background",
            "models_loading": loading_models[:5],  # Show up to 5
            "total_loading": ghost_stats.get("loading", 0),
            "total_ready": ghost_stats.get("ready", 0),
        }

    elif full_ready and ghosts_ready and len(critical_services_failed) == 0:
        status = "ready"
        ready = True

    elif full_ready and ghosts_ready:
        status = "degraded"  # Some non-critical services failed but usable
        ready = True

    elif core_ready and ghosts_ready and ml_warmup_info.get("is_warming_up"):
        # Ghosts ready but ML engine still optimizing - good enough
        status = "operational"
        ready = True

    elif interactive_ready and ghosts_ready:
        # ParallelInitializer says we're interactive AND ghosts ready
        status = "interactive"
        ready = True

    elif websocket_ready and ghosts_ready:
        # WebSocket ready + ghosts ready - user can interact
        status = "websocket_ready"
        ready = True

    elif core_ready and not ghosts_ready:
        # Redundant case covered above, but explicit for clarity
        status = "warming_up"
        ready = False

    else:
        status = "initializing"
        ready = False

    # Event loop is always responsive if we got here
    details["event_loop"] = True

    # Add ghost status to top-level for easy frontend access
    details["ghosts_materialized"] = ghosts_ready

    return {
        "status": status,
        "ready": ready,
        "operational": core_ready,
        "ghosts_ready": ghosts_ready,  # Explicit field for frontend
        "details": details,
        "services": {
            "ready": critical_services_ready,
            "failed": critical_services_failed,
        }
    }


# Full Health check endpoint (comprehensive but slower)
@app.api_route("/health", methods=["GET", "HEAD"])
async def health_check():
    """Quick health check endpoint"""
    vision_details = {}
    ml_audio_details = {}
    vision_status = {}

    # Check vision status manager
    if hasattr(app.state, "vision_status_manager"):
        vision_status = app.state.vision_status_manager.get_status()

    # Check vision component status
    if hasattr(app.state, "vision_analyzer"):
        try:
            # Check orchestrator status
            orchestrator = await app.state.vision_analyzer.get_orchestrator()
            if orchestrator:
                status = await orchestrator.get_system_status()
                vision_details["orchestrator"] = {
                    "enabled": True,
                    "mode": status["system_mode"],
                    "memory_usage_mb": status["memory_usage_mb"],
                    "active_components": sum(
                        1 for v in status["components"].values() if v == "active"
                    ),
                }
            else:
                vision_details["orchestrator"] = {"enabled": False}
        except Exception:
            vision_details["orchestrator"] = {"enabled": False}

    # Check ML audio system status
    if hasattr(app.state, "ml_audio_state"):
        ml_state = app.state.ml_audio_state
        ml_audio_details = {
            "enabled": True,
            "active_streams": len(ml_state.active_streams),
            "total_processed": ml_state.total_processed,
            "uptime_hours": round(ml_state.get_uptime(), 2),
            "capabilities": ml_state.system_capabilities,
            "performance": ml_state.get_performance_metrics(),
            "quality_insights": ml_state.get_quality_insights(),
        }

    # Check Rust acceleration status
    rust_details = {}
    if hasattr(app.state, "rust_acceleration"):
        rust_config = app.state.rust_acceleration
        if rust_config.get("available"):
            rust_details = {
                "enabled": True,
                "components": rust_config.get("components", {}),
                "performance_boost": rust_config.get("performance_boost", {}),
                "memory_savings": rust_config.get("memory_savings", {}),
            }
        else:
            rust_details = {"enabled": False}

    # Check self-healing status
    self_healing_details = {}
    try:
        from vision.rust_self_healer import get_self_healer

        healer = get_self_healer()
        health_report = healer.get_health_report()
        self_healing_details = {
            "enabled": health_report.get("running", False),
            "fix_attempts": health_report.get("total_fix_attempts", 0),
            "success_rate": health_report.get("success_rate", 0.0),
            "last_successful_build": health_report.get("last_successful_build"),
        }
    except Exception:
        self_healing_details = {"enabled": False}

    # Check voice unlock status
    voice_unlock_details = {}
    if hasattr(app.state, "voice_unlock") and app.state.voice_unlock.get("initialized"):
        try:
            pass

            # Get basic status without initializing service
            voice_unlock_details = {
                "enabled": True,
                "initialized": True,
                "api_available": True,
            }
        except Exception:
            voice_unlock_details = {"enabled": False, "initialized": False}
    else:
        voice_unlock_details = {"enabled": False, "initialized": False}

    # Check dynamic component manager status
    component_manager_details = {}
    if hasattr(app.state, "component_manager"):
        mgr = app.state.component_manager
        component_manager_details = {
            "enabled": True,
            "total_components": len(mgr.components),
            "memory_pressure": mgr.memory_monitor.current_pressure().value,
            "arm64_optimized": mgr.arm64_optimizer.is_arm64,
            "m1_detected": mgr.arm64_optimizer.is_m1,
            "config_loaded": (os.path.exists(mgr.config_path) if mgr.config_path else False),
            "advanced_preloader": {
                "predictor_active": mgr.advanced_predictor is not None,
                "dependency_resolver_active": mgr.dependency_resolver is not None,
                "smart_cache_active": mgr.smart_cache is not None,
            },
        }
    else:
        component_manager_details = {"enabled": False}

    # v77.2: Check Coding Council status for self-evolution capabilities
    coding_council_details = {}
    try:
        from core.coding_council.integration import get_coding_council_health
        coding_council_details = await get_coding_council_health()
    except ImportError:
        coding_council_details = {"enabled": False, "reason": "module_not_installed"}
    except Exception as e:
        coding_council_details = {"enabled": False, "error": str(e)}

    return {
        "status": "healthy",
        "mode": "optimized" if OPTIMIZE_STARTUP else "legacy",
        "parallel_imports": PARALLEL_IMPORTS,
        "lazy_models": LAZY_LOAD_MODELS,
        "components": {name: bool(comp) for name, comp in components.items() if comp is not None},
        "vision_status": vision_status,
        "vision_enhanced": vision_details,
        "ml_audio_system": ml_audio_details,
        "rust_acceleration": rust_details,
        "self_healing": self_healing_details,
        "voice_unlock": voice_unlock_details,
        "component_manager": component_manager_details,
        "coding_council": coding_council_details,
    }


# ============================================================================
# Zero-Touch Update Support: JARVIS Busy State Endpoint
# ============================================================================

@app.api_route("/health/busy", methods=["GET", "HEAD"])
async def health_busy():
    """
    Check if JARVIS is currently busy with active tasks.
    
    Used by the Zero-Touch autonomous update system to determine
    if it's safe to apply updates without interrupting active work.
    
    Returns:
        busy: True if JARVIS is processing tasks
        active_tasks: Count of currently running tasks
        active_operations: List of active operation types
        safe_to_update: True if update can proceed safely
    """
    active_tasks = 0
    active_operations = []
    
    # Check for active voice processing
    if hasattr(app.state, "voice_engine"):
        try:
            voice_state = app.state.voice_engine
            if hasattr(voice_state, "is_processing") and voice_state.is_processing:
                active_tasks += 1
                active_operations.append("voice_processing")
        except Exception:
            pass
    
    # Check for active ML audio streams
    if hasattr(app.state, "ml_audio_state"):
        try:
            ml_state = app.state.ml_audio_state
            if hasattr(ml_state, "active_streams"):
                stream_count = len(ml_state.active_streams)
                if stream_count > 0:
                    active_tasks += stream_count
                    active_operations.append(f"audio_streams:{stream_count}")
        except Exception:
            pass
    
    # Check for active vision tasks
    if hasattr(app.state, "vision_analyzer"):
        try:
            analyzer = app.state.vision_analyzer
            if hasattr(analyzer, "is_processing") and analyzer.is_processing:
                active_tasks += 1
                active_operations.append("vision_processing")
        except Exception:
            pass
    
    # Check for active AI conversations (chat completions in progress)
    if hasattr(app.state, "active_conversations"):
        try:
            conv_count = len(app.state.active_conversations)
            if conv_count > 0:
                active_tasks += conv_count
                active_operations.append(f"conversations:{conv_count}")
        except Exception:
            pass
    
    # Check for active tool executions
    if hasattr(app.state, "tool_orchestrator"):
        try:
            orchestrator = app.state.tool_orchestrator
            if hasattr(orchestrator, "active_executions"):
                exec_count = len(orchestrator.active_executions)
                if exec_count > 0:
                    active_tasks += exec_count
                    active_operations.append(f"tool_executions:{exec_count}")
        except Exception:
            pass
    
    # Check for active file operations
    if hasattr(app.state, "file_operations_in_progress"):
        try:
            file_ops = app.state.file_operations_in_progress
            if file_ops and len(file_ops) > 0:
                active_tasks += len(file_ops)
                active_operations.append(f"file_operations:{len(file_ops)}")
        except Exception:
            pass
    
    is_busy = active_tasks > 0
    
    return {
        "busy": is_busy,
        "active_tasks": active_tasks,
        "active_operations": active_operations,
        "safe_to_update": not is_busy,
        "timestamp": datetime.now().isoformat(),
    }


# ============================================================================
# Performance Optimizer Stats Endpoint
# ============================================================================

@app.api_route("/health/performance", methods=["GET", "HEAD"])
async def health_performance():
    """
    Get performance profiling statistics.

    Returns latency tracking, cache stats, and connection pool metrics.
    Enable detailed profiling with: JARVIS_PROFILE_ENABLED=true

    Returns:
        cache: Cache hit rates and eviction stats
        pools: Connection pool usage stats
        profiler: Request latency percentiles (if enabled)
        config: Current performance configuration
    """
    if not PERFORMANCE_OPTIMIZER_AVAILABLE:
        return {
            "status": "unavailable",
            "message": "Performance Optimizer not loaded",
            "timestamp": datetime.now().isoformat(),
        }

    try:
        from core.performance_optimizer import get_optimizer, get_config

        optimizer = get_optimizer()
        config = get_config()
        stats = optimizer.get_stats()

        return {
            "status": "ok",
            "cache": stats.get("cache", {}),
            "pools": stats.get("pools", {}),
            "profiler": stats.get("profiler", {}),
            "config": {
                "profile_enabled": config.profile_enabled,
                "cache_enabled": config.cache_enabled,
                "cache_l2_enabled": config.cache_l2_enabled,
                "slow_threshold_ms": config.profile_slow_threshold_ms,
            },
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "timestamp": datetime.now().isoformat(),
        }


# ============================================================================
# Hyper-Speed AI Loader Stats Endpoint
# ============================================================================

@app.api_route("/health/ai-loader", methods=["GET", "HEAD"])
async def health_ai_loader():
    """
    Get AI model loading statistics and Ghost Proxy status.

    Returns:
        models: Status and metrics for each registered model
        summary: Counts of ready/loading/failed models
        config: AI loader configuration
    """
    if not hasattr(app.state, 'ai_manager') or app.state.ai_manager is None:
        return {
            "status": "unavailable",
            "message": "AI Loader not initialized",
            "timestamp": datetime.now().isoformat(),
        }

    try:
        stats = app.state.ai_manager.get_stats()

        return {
            "status": "ok",
            "models": stats.get("models", {}),
            "summary": stats.get("summary", {}),
            "config": stats.get("config", {}),
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "timestamp": datetime.now().isoformat(),
        }


@app.get("/hybrid/status")
async def hybrid_status():
    """
    Get hybrid cloud routing status and SAI learning metrics.

    Returns comprehensive status of:
    - RAM usage and trends
    - GCP deployment state
    - SAI learning statistics
    - Component locations (local vs GCP)
    - Migration metrics
    - Crash prevention stats
    """
    # Check if hybrid coordinator is available (from start_system.py)
    # Note: This endpoint works even if start_system.py isn't running
    # It will show the last known state or indicate hybrid is inactive

    try:
        # Try to import and check if coordinator is running
        # This is a read-only status check
        from datetime import datetime

        import psutil

        # Get current RAM state
        mem = psutil.virtual_memory()
        ram_state = {
            "total_gb": mem.total / (1024**3),
            "used_gb": mem.used / (1024**3),
            "available_gb": mem.available / (1024**3),
            "percent": mem.percent,
            "status": (
                "EMERGENCY"
                if mem.percent >= 95
                else (
                    "CRITICAL"
                    if mem.percent >= 85
                    else (
                        "WARNING"
                        if mem.percent >= 75
                        else "ELEVATED" if mem.percent >= 60 else "OPTIMAL"
                    )
                )
            ),
        }

        # Check if running on GCP (via environment detection)
        import os

        is_gcp = os.path.exists("/.dockerenv") or os.getenv("GCP_PROJECT_ID") is not None

        # Try to load SAI learned parameters from database
        learned_params = {}
        try:
            # Check if learning database has hybrid parameters
            import json
            import sys
            from pathlib import Path

            sys.path.insert(0, str(Path(__file__).parent))
            from intelligence.learning_database import get_learning_database

            db = await get_learning_database()

            # Query for latest hybrid learning stats
            if db and db.db:
                async with db.db.cursor() as cursor:
                    await cursor.execute(
                        """
                        SELECT metadata
                        FROM patterns
                        WHERE pattern_type = 'hybrid_threshold'
                        ORDER BY last_seen DESC
                        LIMIT 1
                    """
                    )
                    result = await cursor.fetchone()

                    if result and len(result) > 0:
                        row_data = result[0] if isinstance(result, (list, tuple)) else result  # type: ignore[index]
                        if row_data:
                            metadata = json.loads(str(row_data))
                    learned_params = {
                        "thresholds": metadata.get("thresholds", {}),
                        "confidence": metadata.get("confidence", {}),
                        "component_weights": metadata.get("component_weights", {}),
                        "stats": metadata.get("stats", {}),
                        "last_updated": metadata.get("last_updated"),
                    }

            await db.close()

        except Exception as e:
            learned_params = {"error": f"Could not load learning data: {str(e)}"}

        # Build response
        response = {
            "timestamp": datetime.now().isoformat(),
            "hybrid_enabled": os.getenv("JARVIS_HYBRID_MODE", "auto") in ["auto", "true", "1"],
            "current_location": "gcp" if is_gcp else "local",
            "ram": ram_state,
            "gcp_available": is_gcp or bool(os.getenv("GCP_PROJECT_ID")),
            "sai_learning": (
                learned_params
                if learned_params
                else {
                    "status": "No learned parameters yet",
                    "note": "Run start_system.py to enable learning",
                }
            ),
            "features": {
                "crash_prevention": True,
                "auto_scaling": True,
                "predictive_routing": True,
                "cost_optimization": True,
                "persistent_learning": True,
            },
            "thresholds": learned_params.get(
                "thresholds",
                {
                    "warning": 0.75,
                    "critical": 0.85,
                    "optimal": 0.60,
                    "emergency": 0.95,
                    "note": "Default values (not yet learned)",
                },
            ),
        }

        return response

    except Exception as e:
        # Return error but still provide basic info
        return {
            "timestamp": datetime.now().isoformat(),
            "hybrid_enabled": False,
            "error": str(e),
            "message": "Hybrid routing status unavailable. Run start_system.py to enable.",
            "features": {
                "crash_prevention": True,
                "auto_scaling": True,
                "predictive_routing": True,
                "cost_optimization": True,
                "persistent_learning": True,
            },
        }


@app.get("/autonomous/status")
async def autonomous_status():
    """Get autonomous orchestrator and mesh network status"""
    orchestrator_status = None
    mesh_status = None

    if hasattr(app.state, "orchestrator"):
        try:
            orchestrator_status = app.state.orchestrator.get_status()
        except Exception as e:
            orchestrator_status = {"error": str(e)}

    if hasattr(app.state, "mesh"):
        try:
            mesh_status = app.state.mesh.get_status()
        except Exception as e:
            mesh_status = {"error": str(e)}

    return {
        "autonomous_enabled": orchestrator_status is not None or mesh_status is not None,
        "orchestrator": orchestrator_status,
        "mesh": mesh_status,
    }


# =============================================================================
# PROJECT TRINITY API - Cross-Repo Status (v80.0)
# =============================================================================

@app.get("/trinity/status")
async def trinity_status():
    """
    v80.0: Get PROJECT TRINITY cross-repo status.

    Returns comprehensive status of:
    - JARVIS Body (local backend)
    - J-Prime (cognitive layer)
    - Reactor-Core (neural/training layer)
    - Cross-repo health monitoring
    - Circuit breaker states
    - Startup coordination status

    This endpoint provides real-time visibility into the Trinity
    distributed architecture and cross-repo communication health.
    """
    import time
    import json
    from pathlib import Path

    trinity_dir = Path.home() / ".jarvis" / "trinity"

    # Collect component status from heartbeat files
    components = {}
    component_files = {
        "jarvis_body": trinity_dir / "components" / "jarvis_body.json",
        "j_prime": trinity_dir / "components" / "j_prime.json",
        "reactor_core": trinity_dir / "components" / "reactor_core.json",
    }

    for name, path in component_files.items():
        if path.exists():
            try:
                with open(path) as f:
                    data = json.load(f)
                age = time.time() - data.get("timestamp", 0)
                components[name] = {
                    "status": "online" if age < 30 else "stale",
                    "instance_id": data.get("instance_id"),
                    "last_heartbeat_seconds_ago": round(age, 1),
                    "metrics": data.get("metrics", {}),
                }
            except Exception as e:
                components[name] = {"status": "error", "error": str(e)}
        else:
            components[name] = {"status": "offline"}

    # Get v80.0 health monitor status if available
    health_monitor_status = None
    try:
        from core.advanced_startup_orchestrator import get_health_monitor
        monitor = await get_health_monitor()
        health_monitor_status = await monitor.get_aggregate_health()
    except Exception as e:
        health_monitor_status = {"error": str(e)}

    # Calculate aggregate status
    online_count = sum(1 for c in components.values() if c.get("status") == "online")
    total_count = len(components)

    if online_count == total_count:
        overall_status = "full_distributed"
    elif online_count >= 1:
        overall_status = "partial"
    else:
        overall_status = "offline"

    return {
        "trinity_enabled": True,
        "v80_cross_repo_enabled": True,
        "overall_status": overall_status,
        "components_online": f"{online_count}/{total_count}",
        "components": components,
        "health_monitor": health_monitor_status,
        "architecture": {
            "jarvis_body": "Execution layer (Computer Use, Vision, Actions)",
            "j_prime": "Cognitive layer (Reasoning, Planning, Decisions)",
            "reactor_core": "Neural layer (Training, Learning, Optimization)",
        },
    }


@app.get("/trinity/health")
async def trinity_health():
    """
    v80.0: Get real-time Trinity health check with circuit breaker status.

    Performs live health checks against all Trinity components and returns:
    - Individual component health
    - Circuit breaker states
    - Latency measurements
    - Trend analysis
    """
    try:
        from core.advanced_startup_orchestrator import get_health_monitor

        monitor = await get_health_monitor()

        # Get aggregate health with all metrics
        aggregate = await monitor.get_aggregate_health()

        # Get individual component status
        component_health = {}
        for component in ["jarvis", "j_prime", "reactor_core"]:
            if component in monitor._health_cache:
                cached = monitor._health_cache[component]
                breaker = monitor._circuit_breakers.get(component, {})
                component_health[component] = {
                    "status": cached.get("status", "unknown"),
                    "latency_ms": cached.get("latency_ms", 0),
                    "circuit_breaker": {
                        "state": breaker.get("state", "closed"),
                        "failures": breaker.get("failures", 0),
                        "last_failure": breaker.get("last_failure"),
                    },
                }
            else:
                component_health[component] = {"status": "not_checked"}

        return {
            "healthy": aggregate.get("overall_status") == "healthy",
            "overall_status": aggregate.get("overall_status", "unknown"),
            "components": component_health,
            "aggregate": aggregate,
            "v80_enabled": True,
        }
    except ImportError:
        return {
            "healthy": False,
            "error": "v80.0 health monitor not available",
            "v80_enabled": False,
        }
    except Exception as e:
        return {
            "healthy": False,
            "error": str(e),
            "v80_enabled": True,
        }


# =============================================================================
# DATA FLYWHEEL API - Self-Improving Learning Loop (v8.0)
# =============================================================================

@app.get("/flywheel/status")
async def flywheel_status():
    """Get Data Flywheel status for self-improving learning"""
    try:
        from autonomy.unified_data_flywheel import get_data_flywheel, get_flywheel_status
        status = await get_flywheel_status()
        return {
            "enabled": True,
            **status,
        }
    except ImportError:
        return {"enabled": False, "message": "Data Flywheel not available"}
    except Exception as e:
        return {"enabled": False, "error": str(e)}


@app.post("/flywheel/trigger")
async def flywheel_trigger(
    include_web_scraping: bool = True,
    include_training: bool = True,
    force: bool = False,
):
    """Manually trigger a Data Flywheel cycle"""
    try:
        from autonomy.unified_data_flywheel import get_data_flywheel
        flywheel = get_data_flywheel()

        if flywheel.is_running:
            return {"success": False, "error": "Flywheel already running"}

        # Run in background task
        import asyncio
        asyncio.create_task(
            flywheel.run_full_cycle(
                include_web_scraping=include_web_scraping,
                include_training=include_training,
                force=force,
            )
        )

        return {
            "success": True,
            "message": "Flywheel cycle started",
            "options": {
                "include_web_scraping": include_web_scraping,
                "include_training": include_training,
                "force": force,
            }
        }
    except ImportError:
        return {"success": False, "error": "Data Flywheel not available"}
    except Exception as e:
        return {"success": False, "error": str(e)}


@app.get("/flywheel/learning-goals")
async def flywheel_learning_goals():
    """Get current learning goals for JARVIS"""
    try:
        from pathlib import Path
        import json

        goals_file = Path(__file__).parent.parent / "data" / "learning_goals.json"
        if goals_file.exists():
            data = json.loads(goals_file.read_text())
            return {
                "enabled": True,
                "goals": data.get("topics", []),
                "count": len(data.get("topics", [])),
            }
        return {"enabled": True, "goals": [], "count": 0}
    except Exception as e:
        return {"enabled": False, "error": str(e)}


@app.post("/flywheel/learning-goals/add")
async def flywheel_add_learning_goal(topic: str, priority: int = 5, urls: list = None):
    """Add a new learning goal for JARVIS to study"""
    try:
        from pathlib import Path
        import json

        goals_file = Path(__file__).parent.parent / "data" / "learning_goals.json"
        goals_file.parent.mkdir(parents=True, exist_ok=True)

        # Load existing goals
        if goals_file.exists():
            data = json.loads(goals_file.read_text())
        else:
            data = {"topics": []}

        # Check for duplicates
        for g in data["topics"]:
            if g["topic"].lower() == topic.lower():
                return {"success": False, "error": "Topic already exists"}

        # Add new goal
        data["topics"].append({
            "topic": topic,
            "priority": priority,
            "source": "user",
            "urls": urls or [],
        })

        # Save
        goals_file.write_text(json.dumps(data, indent=2))

        return {"success": True, "message": f"Added learning goal: {topic}"}
    except Exception as e:
        return {"success": False, "error": str(e)}


@app.get("/components/status")
async def component_status():
    """Get dynamic component manager status with performance metrics"""
    if not hasattr(app.state, "component_manager"):
        return {"enabled": False, "message": "Dynamic component loading not enabled"}

    mgr = app.state.component_manager
    status = mgr.get_status()

    return {
        "enabled": True,
        "config_path": mgr.config_path,
        "memory_limit_gb": mgr.memory_limit_gb,
        **status,  # Unpack all status fields
    }


@app.get("/components/metrics")
async def component_metrics():
    """Get detailed performance metrics"""
    if not hasattr(app.state, "component_manager"):
        return {"enabled": False}

    mgr = app.state.component_manager
    status = mgr.get_status()

    # Calculate efficiency score
    total_loads = status["performance"]["total_loads"]
    cache_hit_rate = status["performance"]["cache_hit_rate"]
    memory_saved = status["memory"]["saved_mb"]

    efficiency_score = 0
    if total_loads > 0:
        # Score based on cache hits, memory savings, and load count
        efficiency_score = min(100, (cache_hit_rate * 0.4) + (min(memory_saved / 100, 50) * 0.6))

    return {
        "enabled": True,
        "timestamp": datetime.now().isoformat(),
        "efficiency_score": round(efficiency_score, 1),
        "metrics": {
            "component_utilization": {
                "total": status["total_components"],
                "loaded": status["loaded_components"],
                "utilization_percent": (
                    round(
                        (status["loaded_components"] / status["total_components"]) * 100,
                        1,
                    )
                    if status["total_components"] > 0
                    else 0
                ),
            },
            "memory_metrics": status["memory"],
            "performance_metrics": status["performance"],
            "platform_info": status["platform"],
        },
    }


@app.get("/autonomous/services")
async def autonomous_services():
    """Get list of all discovered services"""
    if not hasattr(app.state, "orchestrator"):
        return {"error": "Orchestrator not available"}

    try:
        return app.state.orchestrator.get_frontend_config()
    except Exception as e:
        return {"error": str(e)}


# Mount routers based on available components
def mount_routers():
    """Mount API routers based on loaded components"""
    import os  # Ensure os is available in this scope

    # Memory API
    memory = components.get("memory", {})
    if memory.get("available") and hasattr(app.state, "memory_manager"):
        memory_api_class = memory.get("api")
        if memory_api_class:
            memory_api = memory_api_class(app.state.memory_manager)
            app.include_router(memory_api.router, prefix="/memory", tags=["memory"])
            logger.info("‚úÖ Memory API mounted")

    # Voice API
    voice = components.get("voice", {})
    if voice and voice.get("jarvis_available"):
        app.include_router(voice["jarvis_router"], prefix="/voice/jarvis", tags=["jarvis"])
        logger.info("‚úÖ JARVIS Voice API mounted")

        # Set JARVIS instance in unified WebSocket pipeline
        try:
            from api.unified_websocket import set_jarvis_instance

            jarvis_api = voice.get("jarvis_api")
            if jarvis_api:
                set_jarvis_instance(jarvis_api)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not set JARVIS in WebSocket pipeline: {e}")

    if voice and voice.get("enhanced_available"):
        app.include_router(voice["enhanced_router"], prefix="/voice/enhanced", tags=["voice"])
        logger.info("‚úÖ Enhanced Voice API mounted")

    # ML Model Status API
    ml = components.get("ml_models", {})
    if ml and ml.get("status_router"):
        app.include_router(ml["status_router"], prefix="/models", tags=["models"])
        logger.info("‚úÖ Model Status API mounted")

    # Monitoring API (router already has /monitoring prefix)
    monitoring = components.get("monitoring", {})
    if monitoring and monitoring.get("router"):
        app.include_router(monitoring["router"], tags=["monitoring"])
        logger.info("‚úÖ Monitoring API mounted at /monitoring")

    # Supervisor Broadcast API (for update notifications + maintenance mode)
    try:
        from api.broadcast_router import broadcast_router, alt_router
        app.include_router(broadcast_router, tags=["broadcast"])
        app.include_router(alt_router, tags=["broadcast"])
        logger.info("‚úÖ Broadcast API mounted at /api/broadcast")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Broadcast API not available: {e}")

    # Context Intelligence API (router already has /context prefix)
    context = components.get("context", {})
    if context and context.get("router"):
        app.include_router(context["router"], tags=["context"])
        logger.info("‚úÖ Context Intelligence API mounted at /context")

    # Voice Unlock API (router already has /api/voice-unlock prefix)
    voice_unlock = components.get("voice_unlock", {})
    logger.info(f"üîç Voice Unlock check: components.get('voice_unlock') = {bool(voice_unlock)}")
    if voice_unlock:
        logger.info(f"üîç Voice Unlock router present: {bool(voice_unlock.get('router'))}")
        logger.info(f"üîç Voice Unlock keys: {list(voice_unlock.keys())}")
    if voice_unlock and voice_unlock.get("router"):
        app.include_router(voice_unlock["router"], tags=["voice_unlock"])
        logger.info("‚úÖ Voice Unlock API mounted at /api/voice-unlock")
    else:
        logger.warning("‚ö†Ô∏è Voice Unlock router NOT mounted - router missing from components")

    # Voice Authentication Intelligence API (LangGraph + Langfuse + ChromaDB + Cache)
    try:
        from api.voice_auth_intelligence_api import router as voice_auth_intelligence_router
        app.include_router(voice_auth_intelligence_router, tags=["voice_auth_intelligence"])
        logger.info("‚úÖ Voice Auth Intelligence API mounted at /api/voice-auth-intelligence")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Voice Auth Intelligence API not available: {e}")

    # Startup Progress API (for loading page)
    try:
        from api.startup_progress_api import router as startup_progress_router
        app.include_router(startup_progress_router, tags=["startup"])
        logger.info("‚úÖ Startup Progress API mounted at /ws/startup-progress")
    except ImportError as e:
        logger.warning(f"Could not import startup progress router: {e}")
        if voice_unlock.get("initialized"):
            app.state.voice_unlock = voice_unlock
            logger.info("‚úÖ Voice Unlock service ready")

    # Startup Voice API (for loading page voice announcement)
    try:
        from api.startup_voice_api import router as startup_voice_router
        app.include_router(startup_voice_router, tags=["startup"])
        logger.info("‚úÖ Startup Voice API mounted at /api/startup-voice")
    except ImportError as e:
        logger.warning(f"Could not import startup voice router: {e}")

    # Screen Control API - HTTP REST endpoints for unlock/lock
    try:
        from api.screen_control_api import router as screen_control_router

        app.include_router(screen_control_router)
        logger.info("‚úÖ Screen Control REST API mounted at /api/screen")
    except Exception as e:
        logger.warning(f"Screen Control API not available: {e}")

    # Note: /lock-now endpoint is registered at module level (immediately after app creation)
    # for guaranteed availability regardless of component initialization

    # Wake Word API - Always mount (has stub functionality)
    try:
        from api.wake_word_api import router as wake_word_router

        # Router already has prefix="/api/wake-word", don't add it again
        app.include_router(wake_word_router)
        logger.info("‚úÖ Wake Word API mounted at /api/wake-word")

        # Check if the full service is available
        wake_word = components.get("wake_word", {})
        if wake_word and wake_word.get("initialized"):
            app.state.wake_word = wake_word
            logger.info("‚úÖ Wake Word detection service available")
        else:
            logger.info("‚ÑπÔ∏è  Wake Word API available (stub mode - service not initialized)")
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è Wake Word API not available: {e}")

    # Rust API (if Rust components are available)
    if hasattr(app.state, "rust_acceleration") and app.state.rust_acceleration.get("available"):
        try:
            from api.rust_api import router as rust_router

            app.include_router(rust_router, prefix="/rust", tags=["rust"])
            logger.info("‚úÖ Rust acceleration API mounted")
        except ImportError:
            logger.debug("Rust API not available")

    # Self-healing API
    try:
        from api.self_healing_api import router as self_healing_router

        app.include_router(self_healing_router, prefix="/self-healing", tags=["self-healing"])
        logger.info("‚úÖ Self-healing API mounted")
    except ImportError:
        logger.debug("Self-healing API not available")

    # Agentic API - Computer Use + Autonomous Task Execution
    try:
        from api.agentic_api import router as agentic_router

        app.include_router(agentic_router, tags=["agentic"])
        logger.info("‚úÖ Agentic API mounted at /api/agentic")
    except ImportError as e:
        logger.debug(f"Agentic API not available: {e}")

    # Reactor-Core Feedback API - Real-time Training Status Receiver
    try:
        from api.reactor_core_api import router as reactor_core_router

        app.include_router(reactor_core_router, tags=["reactor-core"])
        logger.info("‚úÖ Reactor-Core API mounted at /reactor-core")
    except ImportError as e:
        logger.debug(f"Reactor-Core API not available: {e}")

    # v77.2: Coding Council Evolution API - Self-Evolution Capabilities
    try:
        from core.coding_council.integration import create_coding_council_router

        coding_council_router = create_coding_council_router()
        app.include_router(coding_council_router, tags=["coding-council"])
        logger.info("‚úÖ Coding Council API mounted at /api/evolution")
        logger.info("   Endpoints: /api/evolution/trigger, /api/evolution/status, /api/evolution/rollback")
    except ImportError as e:
        logger.debug(f"Coding Council API not available: {e}")

    # v77.3: Coding Council IDE Integration Routes
    try:
        from core.coding_council.startup import register_coding_council_routes
        register_coding_council_routes(app)
        logger.info("‚úÖ Coding Council IDE routes mounted at /coding-council/*")
        logger.info("   IDE endpoints: /coding-council/ide/status, /coding-council/ide/suggest")
        logger.info("   Trinity endpoints: /coding-council/ide/trinity/repos, /coding-council/ide/trinity/publish")
    except ImportError as e:
        logger.debug(f"Coding Council IDE routes not available: {e}")

    # Context Intelligence API (Priority 1-3 features)
    if hasattr(app.state, "context_bridge") and app.state.context_bridge:
        from pydantic import BaseModel

        class ContextQueryRequest(BaseModel):
            query: str
            current_space_id: Optional[int] = None

        @app.post("/context/query", tags=["context"])
        async def query_context(request: ContextQueryRequest):
            """
            Natural language query interface for workspace context.

            Examples:
            - "what does it say?" ‚Üí Find and explain most recent error
            - "what's the error?" ‚Üí Find most recent error
            - "what am I working on?" ‚Üí Synthesize workspace activity
            - "what's related?" ‚Üí Show cross-space relationships
            """
            try:
                response = await app.state.context_bridge.handle_user_query(
                    request.query, request.current_space_id
                )
                return {"success": True, "response": response}
            except Exception as e:
                logger.error(f"Context query failed: {e}")
                return {"success": False, "error": str(e)}

        @app.get("/context/summary", tags=["context"])
        async def get_context_summary():
            """Get comprehensive workspace intelligence summary"""
            try:
                summary = app.state.context_bridge.get_workspace_intelligence_summary()
                return {"success": True, "summary": summary}
            except Exception as e:
                logger.error(f"Context summary failed: {e}")
                return {"success": False, "error": str(e)}

        @app.post("/context/ocr_update", tags=["context"])
        async def process_ocr_update(
            space_id: int,
            app_name: str,
            ocr_text: str,
            screenshot_path: Optional[str] = None,
        ):
            """Process OCR update from vision system"""
            try:
                await app.state.context_bridge.process_ocr_update(
                    space_id=space_id,
                    app_name=app_name,
                    ocr_text=ocr_text,
                    screenshot_path=screenshot_path,
                )
                return {"success": True}
            except Exception as e:
                logger.error(f"OCR update failed: {e}")
                return {"success": False, "error": str(e)}

        logger.info("‚úÖ Context Intelligence API mounted at /context")
        logger.info("   ‚Ä¢ POST /context/query - Natural language queries")
        logger.info("   ‚Ä¢ GET  /context/summary - Workspace intelligence summary")
        logger.info("   ‚Ä¢ POST /context/ocr_update - Vision system integration")

    # Unified WebSocket API - replaces individual WebSocket endpoints
    try:
        from api.unified_websocket import router as unified_ws_router

        app.include_router(unified_ws_router, tags=["websocket"])
        logger.info("‚úÖ Unified WebSocket API mounted at /ws")

        # v2.0: Flag that WebSocket is mounted - readiness will be marked in lifespan
        app.state.websocket_mounted = True

    except ImportError as e:
        logger.warning(f"Could not import unified WebSocket router: {e}")

        # Fallback to individual WebSocket APIs if unified not available
        try:
            from api.vision_websocket import router as vision_ws_router  # type: ignore[attr-defined]

            app.include_router(vision_ws_router, prefix="/vision", tags=["vision"])
            logger.info("‚úÖ Vision WebSocket API mounted (fallback)")
        except ImportError as e:
            logger.warning(f"Could not import vision WebSocket router: {e}")

    # Vision WebSocket endpoint at /vision/ws/vision
    try:
        from api.vision_ws_endpoint import router as vision_ws_endpoint_router
        from api.vision_ws_endpoint import set_vision_analyzer

        app.include_router(vision_ws_endpoint_router, tags=["vision"])
        
        # Also mount notification vision API router (fixes 404 /vision/ws issue)
        try:
            from api.notification_vision_api import router as notification_vision_router
            from api.notification_vision_api import notification_websocket
            
            # Mount at root level to support /notifications/ws
            app.include_router(notification_vision_router)
            
            # Explicitly alias /vision/ws to the same handler (Robust fix)
            app.add_api_websocket_route("/vision/ws", notification_websocket)
            logger.info("‚úÖ Notification WebSocket mounted at /notifications/ws and aliased to /vision/ws")
        except ImportError:
            logger.warning("Notification Vision API router could not be imported")
        except Exception as e:
            logger.error(f"Error mounting notification router: {e}")
            
        # Set vision analyzer if available
        vision = components.get("vision", {})
        if vision and vision.get("analyzer"):
            set_vision_analyzer(vision["analyzer"])

        logger.info("‚úÖ Vision WebSocket endpoint mounted at /vision/ws/vision")
    except ImportError as e:
        logger.warning(f"Could not import vision WebSocket endpoint: {e}")

    # Multi-Monitor Display Routes
    try:
        from api.display_routes import router as display_router

        app.include_router(display_router, tags=["displays"])
        logger.info("‚úÖ Multi-Monitor display routes configured")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Multi-Monitor display routes not available: {e}")

    # Proximity-Aware Display Routes (Phase 1.2)
    try:
        from api.proximity_display_api import router as proximity_display_router

        app.include_router(proximity_display_router, tags=["proximity-display"])
        logger.info("‚úÖ Proximity-Aware Display API configured")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Proximity-Aware Display API not available: {e}")

    # Advanced Display Monitor (Component #9) - Multi-method detection with voice integration
    try:
        display_monitor_comp = components.get("display_monitor", {})
        if display_monitor_comp.get("available"):
            logger.info("üñ•Ô∏è  Initializing Advanced Display Monitor (Component #9)...")

            # Create voice handler for display monitor
            voice_handler_factory = display_monitor_comp.get("voice_handler_factory")
            get_monitor = display_monitor_comp.get("get_monitor")

            if voice_handler_factory and get_monitor:
                # Create voice handler
                voice_handler = voice_handler_factory()

                # Get monitor instance with voice integration
                monitor = get_monitor(voice_handler=voice_handler)

                # Register as the app's monitor instance (singleton pattern)
                from display.advanced_display_monitor import set_app_display_monitor

                set_app_display_monitor(monitor)
                logger.info("   ‚úÖ Display monitor registered as singleton")

                # Connect Vision Navigator with Claude Vision analyzer
                try:
                    from display.vision_ui_navigator import get_vision_navigator  # type: ignore[attr-defined]

                    navigator = get_vision_navigator()

                    # Connect vision analyzer if available
                    if hasattr(app.state, "vision_analyzer"):
                        navigator.set_vision_analyzer(app.state.vision_analyzer)
                        # Also connect to monitor
                        monitor.vision_analyzer = app.state.vision_analyzer
                        logger.info("   ‚úÖ Vision Navigator connected to Claude Vision")
                        logger.info("   üëÅÔ∏è JARVIS can now SEE and CLICK UI elements!")
                    else:
                        logger.warning(
                            "   ‚ö†Ô∏è Vision analyzer not available yet (will connect later)"
                        )

                except Exception as nav_err:
                    logger.warning(f"   ‚ö†Ô∏è Could not initialize Vision Navigator: {nav_err}")

                # Set WebSocket manager for UI notifications
                try:
                    from api.unified_websocket import ws_manager

                    monitor.set_websocket_manager(ws_manager)
                    if ws_manager is not None:
                        ws_manager.display_monitor = (  # type: ignore[attr-defined]
                            monitor  # Allow ws_manager to send current status to new clients
                        )
                    logger.info("   ‚úÖ Display monitor connected to WebSocket")
                except Exception as ws_err:
                    logger.warning(f"   ‚ö†Ô∏è Could not connect display monitor to WebSocket: {ws_err}")

                # Store monitor in app state for access by other components
                app.state.display_monitor = monitor

                # Start monitoring automatically
                async def start_display_monitoring():
                    await asyncio.sleep(2)  # Wait for system to fully initialize
                    await monitor.start()
                    logger.info("   ‚úÖ Display monitoring started")
                    logger.info("   üì∫ Monitoring for configured displays (Living Room TV)")
                    logger.info("   üé§ Voice announcements enabled")
                    logger.info("   ‚ö° Smart caching enabled (3-5x performance)")
                    logger.info("   üîç Detection methods: AppleScript, CoreGraphics, Yabai")

                asyncio.create_task(start_display_monitoring())
                logger.info("‚úÖ Advanced Display Monitor configured")
            else:
                logger.warning("   ‚ö†Ô∏è Display monitor factories not available")
        else:
            logger.warning("‚ö†Ô∏è  Display Monitor not available (component not loaded)")

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Display Monitor initialization failed: {e}")
        import traceback

        logger.debug(traceback.format_exc())

    # ML Audio API (with built-in fallback) - Always mount regardless of WebSocket status
    try:
        from api.ml_audio_api import router as ml_audio_router

        app.include_router(ml_audio_router, tags=["ML Audio"])
        logger.info("‚úÖ ML Audio API mounted")
    except ImportError as e:
        logger.error(f"Could not import ML Audio router: {e}")

    # Network Recovery API (kept separate as it's not WebSocket)
    try:
        from api.network_recovery_api import router as network_recovery_router

        app.include_router(network_recovery_router, tags=["Network Recovery"])
        logger.info("‚úÖ Network Recovery API mounted")
    except ImportError as e:
        logger.warning(f"Could not import Network Recovery router: {e}")

    # ML Audio functionality is now included in the unified ml_audio_api.py

    # Auto Configuration API (for dynamic client configuration)
    try:
        from api.auto_config_endpoint import router as auto_config_router

        app.include_router(auto_config_router, tags=["Auto Configuration"])
        logger.info("‚úÖ Auto Configuration API mounted - clients can auto-discover settings")
    except ImportError as e:
        logger.warning(f"Could not import Auto Config router: {e}")

    # Autonomous Service API (for zero-configuration mode)
    try:
        # Check if we should use memory-optimized version
        use_memory_optimized = os.getenv("MEMORY_OPTIMIZED_MODE", "true").lower() == "true"

        if use_memory_optimized:
            # Import memory-optimized orchestrator
            from core.memory_optimized_orchestrator import get_memory_optimized_orchestrator  # type: ignore[import-not-found]

            orchestrator = get_memory_optimized_orchestrator(
                memory_limit_mb=400
            )  # 400MB limit for orchestrator

            # Start it in background
            async def start_orchestrator():
                await orchestrator.start()
                logger.info("‚úÖ Memory-optimized orchestrator started (400MB limit)")

            asyncio.create_task(start_orchestrator())
            logger.info("üöÄ Using memory-optimized autonomous orchestrator")

        # Always mount the API router
        from api.autonomous_service_api import router as autonomous_router

        app.include_router(autonomous_router)
        logger.info("‚úÖ Autonomous Service API mounted")
        logger.info("ü§ñ Zero-configuration mode enabled - services will auto-discover")

    except ImportError as e:
        logger.warning(f"Autonomous Service API not available: {e}")

    # Mount Hybrid Cloud Cost Monitoring API
    try:
        from core.cost_tracker import initialize_cost_tracking
        from routers.hybrid import router as hybrid_router

        app.include_router(hybrid_router)
        logger.info("‚úÖ Hybrid Cloud Cost Monitoring API mounted at /hybrid")

        # Initialize cost tracking database
        asyncio.create_task(initialize_cost_tracking())

    except ImportError as e:
        logger.warning(f"Hybrid Cloud API not available: {e}")

    # Model Lifecycle Management API (Phase 3.1+)
    try:
        from api.model_routes import router as model_router

        app.include_router(model_router)
        logger.info("‚úÖ Model Lifecycle Management API mounted at /models")
        logger.info("   Endpoints: /models/registry, /models/select, /models/execute")
        logger.info("   Monitoring: /models/metrics, /models/health")
        logger.info("   Lifecycle: /models/lifecycle/status, /models/lifecycle/ram")

    except ImportError as e:
        logger.warning(f"Model Lifecycle Management API not available: {e}")

    # Mount static files for auto-config script
    try:
        import os

        static_dir = os.path.join(os.path.dirname(__file__), "static")
        if not os.path.exists(static_dir):
            os.makedirs(static_dir)
        app.mount("/static", StaticFiles(directory=static_dir), name="static")
        logger.info(
            "‚úÖ Static files mounted - auto-config script available at /static/jarvis-auto-config.js"
        )
    except Exception as e:
        logger.warning(f"Could not mount static files: {e}")

    # Mount landing-page directory for loading screen
    try:
        import os
        from pathlib import Path

        landing_page_dir = Path(__file__).parent.parent / "landing-page"
        if landing_page_dir.exists():
            app.mount("/loading", StaticFiles(directory=str(landing_page_dir), html=True), name="loading")
            logger.info(
                f"‚úÖ Landing page mounted - loading screen available at /loading/loading.html"
            )
        else:
            logger.warning(f"Landing page directory not found: {landing_page_dir}")
    except Exception as e:
        logger.warning(f"Could not mount landing page: {e}")


# Note: Startup tasks are now handled in the lifespan handler above


# Simple command endpoint for testing
@app.post("/api/command")
async def process_command(request: dict):
    """Simple command endpoint for testing"""
    command = request.get("command", "")

    # =========================================================================
    # ULTRA FAST PATH v2.0: Screen lock commands bypass ALL context processing
    # Robust wake phrase removal + flexible lock detection
    # =========================================================================
    import re

    # Strip wake phrases (handles typos from voice recognition)
    wake_patterns = [
        r'\bhey\s+jarvis\b', r'\bhey\s+jarvus\b', r'\bhey\s+drivers\b',
        r'\bhey\s+jarvas\b', r'\bokay\s+jarvis\b', r'\byo\s+jarvis\b',
        r'\bjarvis\b', r'\bjarvus\b', r'\bdrivers\b'
    ]

    cleaned_command = command.lower().strip()
    wake_phrase_detected = None

    for pattern in wake_patterns:
        match = re.search(pattern, cleaned_command, re.IGNORECASE)
        if match:
            wake_phrase_detected = match.group(0)
            cleaned_command = re.sub(pattern, '', cleaned_command, count=1, flags=re.IGNORECASE)
            cleaned_command = re.sub(r'\s+', ' ', cleaned_command).strip()
            break

    # Detect lock command (flexible pattern matching)
    has_lock = re.search(r'\block\b', cleaned_command)
    has_unlock = re.search(r'\bunlock\b', cleaned_command)
    has_target = re.search(r'\b(screen|mac|computer|it|this)\b', cleaned_command)

    is_lock_command = has_lock and not has_unlock and (has_target or len(cleaned_command.split()) <= 3)

    if is_lock_command:
        logger.info(f"[MAIN] üîí LOCK command detected - DIRECT EXECUTION")
        logger.info(f"[MAIN]    Original: '{command}'")
        logger.info(f"[MAIN]    Cleaned:  '{cleaned_command}'")
        if wake_phrase_detected:
            logger.info(f"[MAIN]    Wake phrase removed: '{wake_phrase_detected}'")

        try:
            from api.unified_command_processor import UnifiedCommandProcessor
            processor = UnifiedCommandProcessor()

            # Use cleaned command (wake phrase removed) for processing
            result = await asyncio.wait_for(
                processor.process_command(cleaned_command),
                timeout=10.0
            )
            logger.info(f"[MAIN] ‚úÖ Lock command completed successfully")
            return result
        except asyncio.TimeoutError:
            logger.error("[MAIN] ‚ùå Lock command timed out")
            return {
                "success": False,
                "response": "Lock command timed out. Please try again.",
                "error": "timeout"
            }
        except Exception as e:
            logger.error(f"[MAIN] ‚ùå Lock command failed: {e}")
            return {
                "success": False,
                "response": f"Failed to lock screen: {str(e)}",
                "error": str(e)
            }

    # Trigger intelligent preloading (Phase 2) if available
    if hasattr(app.state, "component_manager") and app.state.component_manager:
        mgr = app.state.component_manager
        if mgr.advanced_predictor:
            try:
                # Predict and preload next 1-3 components in background
                asyncio.create_task(mgr.predict_and_preload(command, steps_ahead=3))
                logger.debug(f"üîÆ Advanced preloading triggered for: '{command[:50]}'")
            except Exception as e:
                logger.debug(f"Advanced preloading failed: {e}")

    # Use unified command processor if available
    try:
        from api.unified_command_processor import UnifiedCommandProcessor

        # Use enhanced Context Intelligence for screen lock/unlock
        USE_ENHANCED_CONTEXT = True

        if USE_ENHANCED_CONTEXT:
            try:
                from api.simple_context_handler_enhanced import wrap_with_enhanced_context

                processor = UnifiedCommandProcessor()
                context_handler = wrap_with_enhanced_context(processor)
                result = await context_handler.process_with_context(command)
            except ImportError as e:
                logger.error(f"Enhanced context handler not available: {e}")
                # Fallback to simple context
                try:
                    from api.simple_context_handler import wrap_with_simple_context

                    processor = UnifiedCommandProcessor()
                    context_handler = wrap_with_simple_context(processor)
                    result = await context_handler.process_with_context(command)
                except ImportError:
                    processor = UnifiedCommandProcessor()
                    result = await processor.process_command(command)
        else:
            # Use standard processor
            processor = UnifiedCommandProcessor()
            result = await processor.process_command(command)
        return result
    except Exception as e:
        logger.error(f"Command processing error: {e}")
        return {"error": str(e), "command": command}


# Basic test endpoint
@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "JARVIS Backend (Optimized) is running",
        "version": "13.4.0-browser-automation",
        "proactive_vision_enabled": hasattr(app.state, "vision_analyzer"),
        "components": {name: bool(comp) for name, comp in components.items() if comp is not None},
    }


# Note: Main WebSocket endpoint is now handled by unified_websocket router at /ws
# This provides a single endpoint for all WebSocket communication


# ML Audio WebSocket compatibility endpoint
@app.websocket("/audio/ml/stream")
async def ml_audio_websocket_compat(websocket: WebSocket):
    """ML Audio WebSocket endpoint for backward compatibility with enhanced features"""
    await websocket.accept()
    logger.info(
        "ML Audio WebSocket connection (legacy endpoint) - providing enhanced compatibility"
    )

    # Initialize variables that may be imported in try block
    ws_manager = None
    connection_capabilities: dict = {}
    client_id: str = ""

    try:
        # Import unified handler and datetime
        from datetime import datetime

        from api.unified_websocket import connection_capabilities, ws_manager

        # Get client info
        client_host = websocket.client.host if websocket.client else "unknown"
        client_id = f"ml_audio_{client_host}_{datetime.now().timestamp()}"

        # Send enhanced welcome message with system capabilities
        ml_state = getattr(app.state, "ml_audio_state", None)
        welcome_msg = {
            "type": "connection_established",
            "client_id": client_id,
            "server_time": datetime.now().isoformat(),
            "capabilities": ml_state.system_capabilities if ml_state else {},
            "recommended_config": (
                ml_state.get_client_recommendations(client_id, "")
                if ml_state
                else {"chunk_size": 512, "sample_rate": 16000, "format": "base64"}
            ),
            "migration_notice": {
                "message": "This endpoint provides full compatibility. For best performance, consider using /ws",
                "new_endpoint": "/ws",
                "benefits": [
                    "unified_interface",
                    "better_performance",
                    "more_features",
                ],
            },
        }
        await websocket.send_json(welcome_msg)

        # Add to unified connections with ML audio context
        if ws_manager is not None and hasattr(ws_manager, 'connections'):
            ws_manager.connections[client_id] = websocket
        connection_capabilities[client_id] = {"ml_audio_stream", "legacy_client"}

        # Track stream if ML state available
        if ml_state:
            ml_state.active_streams[client_id] = {
                "started_at": datetime.now(),
                "processed_chunks": 0,
                "total_bytes": 0,
                "quality_scores": [],
                "websocket": True,
            }

        while True:
            # Receive message
            data = await websocket.receive_json()

            # Convert to unified format
            unified_msg = {
                "type": "ml_audio_stream",
                "audio_data": data.get("audio_data", data.get("data", "")),
                "sample_rate": data.get("sample_rate", 16000),
                "format": data.get("format", "base64"),
            }

            # Handle through unified manager
            if ws_manager is not None and hasattr(ws_manager, 'handle_message'):
                response = await ws_manager.handle_message(client_id, unified_msg)  # type: ignore[misc]
            else:
                response = {"status": "ok"}

            # Send response
            await websocket.send_json(response)

    except WebSocketDisconnect:
        logger.info("ML Audio WebSocket disconnected (legacy)")
        if ws_manager is not None and hasattr(ws_manager, 'connections') and client_id in ws_manager.connections:
            if hasattr(ws_manager, 'disconnect'):
                ws_manager.disconnect(client_id)
    except Exception as e:
        logger.error(f"ML Audio WebSocket error: {e}")
        try:
            await websocket.close()
        except Exception:
            pass


# =====================================================================
# STARTUP PROGRESS ENDPOINTS - Real-time loading progress for frontend
# =====================================================================

@app.websocket("/ws/startup-progress")
async def startup_progress_websocket(websocket: WebSocket):
    """WebSocket endpoint for real-time startup progress updates"""
    await websocket.accept()
    logger.info("[StartupProgress] WebSocket client connected")

    from core.startup_progress_broadcaster import get_startup_broadcaster

    broadcaster = get_startup_broadcaster()
    broadcaster.register_websocket(websocket)

    try:
        # Send current state immediately
        current_state = broadcaster.get_current_state()
        await websocket.send_json({
            "event_type": "state_sync",
            "progress": current_state["progress"],
            "phase": current_state["phase"],
            "message": current_state["message"],
            "current_component": current_state["current_component"],
            "components": current_state["components"],
            "memory": current_state["memory"],
            "startup_mode": current_state["startup_mode"],
            "elapsed_seconds": current_state["elapsed_seconds"]
        })

        # Send recent events for context
        recent_events = broadcaster.get_recent_events(count=20)
        for event in recent_events:
            await websocket.send_json(event)

        # Keep connection alive and handle pings
        while True:
            try:
                data = await asyncio.wait_for(websocket.receive_text(), timeout=30.0)
                if data == "ping":
                    await websocket.send_json({"type": "pong"})
            except asyncio.TimeoutError:
                # Send keepalive
                await websocket.send_json({"type": "keepalive"})

    except WebSocketDisconnect:
        logger.info("[StartupProgress] WebSocket client disconnected")
    except Exception as e:
        logger.error(f"[StartupProgress] WebSocket error: {e}")
    finally:
        broadcaster.unregister_websocket(websocket)


@app.get("/api/startup-progress")
async def get_startup_progress():
    """HTTP polling endpoint for startup progress (fallback)"""
    from core.startup_progress_broadcaster import get_startup_broadcaster

    broadcaster = get_startup_broadcaster()
    state = broadcaster.get_current_state()

    return {
        "progress": state["progress"],
        "phase": state["phase"],
        "message": state["message"],
        "stage": state["current_component"] or state["phase"],
        "metadata": {
            "memory_available_gb": state["memory"]["available_gb"] if state["memory"] else None,
            "memory_pressure": state["memory"]["pressure_percent"] if state["memory"] else None,
            "startup_mode": state["startup_mode"],
            "components_ready": len([c for c in state["components"].values() if c.get("status") == "complete"]),
            "components_failed": len([c for c in state["components"].values() if c.get("status") == "failed"]),
            "total_components": len(state["components"]) if state["components"] else 0,
            "elapsed_seconds": state["elapsed_seconds"]
        }
    }


# =============================================================================
# ML AUDIO ERROR HANDLING ENDPOINTS
# =============================================================================
# These endpoints support the frontend MLAudioHandler for intelligent error
# recovery and telemetry.
# =============================================================================

@app.post("/audio/ml/error")
async def audio_ml_error(request: dict):
    """
    Receive and process audio errors from frontend for ML-based recovery.
    
    This endpoint receives error telemetry from the frontend MLAudioHandler
    and can optionally return ML-recommended recovery strategies.
    """
    from datetime import datetime
    
    error_code = request.get("error_code", "unknown")
    browser = request.get("browser", "unknown")
    session_duration = request.get("session_duration", 0)
    permission_state = request.get("permission_state", "unknown")
    retry_count = request.get("retry_count", 0)
    
    logger.info(f"üé§ Audio error reported: {error_code} (browser={browser}, permission={permission_state})")
    
    # Generate recovery strategy based on error type
    strategy = None
    
    if error_code in ["not-allowed", "permission-denied"]:
        # Permission-based errors - don't recommend retry, show instructions
        strategy = {
            "action": {
                "type": "show_instructions",
                "params": {
                    "permission_type": "microphone",
                    "browser": browser,
                    "instructions": [
                        "Click the üîí lock icon in the address bar",
                        "Select 'Site settings' or 'Permissions'",
                        "Set Microphone to 'Allow'",
                        "Reload the page"
                    ]
                }
            },
            "should_retry": False,
            "skip_restart": True  # CRITICAL: Prevent restart loop
        }
    elif error_code == "audio-capture":
        # Audio capture errors - might be transient
        strategy = {
            "action": {
                "type": "restart_audio_context",
                "params": {"delay": 500}
            },
            "should_retry": retry_count < 3,
            "skip_restart": retry_count >= 3
        }
    elif error_code == "network":
        # Network errors - implement intelligent retry with backoff
        strategy = {
            "action": {
                "type": "network_retry",
                "params": {
                    "delay": min(1000 * (2 ** retry_count), 5000),  # Exponential backoff
                    "max_retries": 3,
                    "retry_count": retry_count
                }
            },
            "should_retry": retry_count < 3,
            "skip_restart": retry_count >= 3
        }
    elif error_code == "no-speech":
        # No speech is normal, not an error
        return {
            "success": True,
            "message": "No speech - normal behavior",
            "strategy": None
        }
    
    return {
        "success": True,
        "error_logged": True,
        "timestamp": datetime.now().isoformat(),
        "strategy": strategy
    }


@app.get("/audio/ml/config")
async def audio_ml_config():
    """
    Get ML audio configuration for frontend.

    Returns configuration settings for the MLAudioHandler including
    retry policies, thresholds, and feature flags.
    """
    return {
        "enableML": True,
        "autoRecovery": True,
        "maxRetries": 3,  # Reduced from 5 to prevent loops
        "retryDelays": [100, 500, 1000],  # Shorter delays
        "anomalyThreshold": 0.8,
        "predictionThreshold": 0.7,
        "features": {
            "permissionStateTracking": True,
            "circuitBreaker": True,
            "adaptiveRetry": True,
            "telemetry": True
        },
        "circuitBreaker": {
            "threshold": 5,  # Max errors before tripping
            "windowMs": 10000,  # Time window
            "cooldownMs": 30000  # Cooldown after trip
        }
    }


# =============================================================================
# Network Recovery & Diagnostics - ML-Enhanced v10.6
# =============================================================================
# Advanced network recovery endpoints with:
# - Intelligent strategy selection based on error patterns
# - Multi-tier circuit breaker pattern
# - Real-time health scoring and telemetry
# - ML-assisted recovery prediction
# - Zero hardcoded endpoints (all dynamic)
# =============================================================================

@app.post("/network/ml/recovery-success")
async def network_ml_recovery_success(request: dict):
    """
    Log successful network recovery for ML learning and analytics.

    This endpoint receives telemetry when a network recovery strategy succeeds,
    allowing the backend to:
    - Track which strategies work best for different error patterns
    - Build ML models for predictive recovery
    - Update circuit breaker states
    - Generate analytics dashboards

    Args:
        request: Contains strategy name, result, connection health metrics, timestamp

    Returns:
        Acknowledgment with updated recommendations
    """
    from datetime import datetime

    strategy = request.get("strategy", "unknown")
    result = request.get("result", {})
    connection_health = request.get("connectionHealth", {})
    timestamp = request.get("timestamp", datetime.now().timestamp())

    logger.info(
        f"üéâ Network recovery success: strategy={strategy}, "
        f"failures={connection_health.get('consecutiveFailures', 0)}, "
        f"latency={connection_health.get('averageLatency', 0)}ms"
    )

    # Store recovery success for ML training (future enhancement)
    # This data can be used to build predictive models
    recovery_event = {
        "strategy": strategy,
        "success": result.get("success", True),
        "message": result.get("message", ""),
        "connection_health": connection_health,
        "timestamp": datetime.fromtimestamp(timestamp / 1000).isoformat(),
        "type": "network_recovery_success"
    }

    # Future: Store in vector DB (ChromaDB) for pattern analysis
    # For now, just log and acknowledge

    # Return recommendations for future recoveries
    recommendations = {
        "preferred_strategies": _get_preferred_strategies(strategy),
        "circuit_breaker_status": "healthy",
        "health_score": _calculate_health_score(connection_health)
    }

    return {
        "success": True,
        "acknowledged": True,
        "timestamp": datetime.now().isoformat(),
        "recommendations": recommendations,
        "message": f"Recovery success logged for strategy: {strategy}"
    }


@app.post("/network/ml/advanced-recovery")
async def network_ml_advanced_recovery(request: dict):
    """
    ML-assisted advanced network recovery endpoint.

    This endpoint analyzes complex network failures and provides intelligent
    recovery strategies based on:
    - Error pattern history
    - Connection health metrics
    - Browser/platform information
    - Recovery attempt history
    - ML predictions (when available)

    Args:
        request: Contains error details, connection health, recovery attempts, browser info

    Returns:
        Advanced recovery strategy with proxy options if needed
    """
    from datetime import datetime
    import asyncio

    error = request.get("error", "unknown")
    connection_health = request.get("connectionHealth", {})
    recovery_attempts = request.get("recoveryAttempts", 0)
    browser_info = request.get("browserInfo", {})

    consecutive_failures = connection_health.get("consecutiveFailures", 0)
    error_patterns = connection_health.get("errorPatterns", [])

    logger.info(
        f"üîß Advanced recovery request: error={error}, attempts={recovery_attempts}, "
        f"failures={consecutive_failures}"
    )

    # Analyze error patterns to determine best strategy
    strategy_recommendation = None

    # Check if we're in a degraded state (many failures)
    if consecutive_failures >= 5 or recovery_attempts >= 3:
        logger.warning("Network severely degraded - enabling proxy mode")

        # Offer backend as speech recognition proxy
        strategy_recommendation = {
            "type": "backend_proxy",
            "proxyEndpoint": "/voice/proxy/recognize",  # Future endpoint
            "fallbackMode": "offline",
            "message": "Severe network issues detected - switching to backend proxy mode",
            "priority": 1
        }

    # Check for specific error patterns
    elif error.lower() in ["network", "service-not-allowed"]:
        # Network connectivity issues - try DNS recovery first
        strategy_recommendation = {
            "type": "dns_recovery",
            "actions": [
                {"type": "flush_dns", "timeout": 2000},
                {"type": "retry_connection", "delay": 1000},
                {"type": "fallback_endpoint", "url": _get_fallback_api_url()}
            ],
            "message": "Network connectivity issue - attempting DNS recovery",
            "priority": 2
        }

    elif "cors" in error.lower() or "cross-origin" in error.lower():
        # CORS issues - suggest backend mediation
        strategy_recommendation = {
            "type": "cors_proxy",
            "proxyEndpoint": "/voice/cors-proxy",
            "message": "CORS issue detected - routing through backend proxy",
            "priority": 2
        }

    else:
        # Generic recovery - service switch or retry
        strategy_recommendation = {
            "type": "service_switch",
            "actions": [
                {"type": "stop_current", "timeout": 500},
                {"type": "create_new_instance", "delay": 300},
                {"type": "start_with_new_config", "timeout": 3000}
            ],
            "message": "Attempting service switch with new configuration",
            "priority": 3
        }

    # Calculate health score for diagnostics
    health_score = _calculate_health_score(connection_health)

    response = {
        "success": True,
        "strategy": strategy_recommendation,
        "health_score": health_score,
        "diagnostics": {
            "consecutive_failures": consecutive_failures,
            "recovery_attempts": recovery_attempts,
            "error_type": error,
            "recommendation_priority": strategy_recommendation.get("priority", 99),
            "browser": browser_info.get("userAgent", "unknown")
        },
        "timestamp": datetime.now().isoformat()
    }

    return response


@app.post("/network/diagnose")
async def network_diagnose(request: dict):
    """
    Network diagnostics endpoint for comprehensive network health checks.

    Performs intelligent diagnostics to identify:
    - DNS resolution issues
    - Connectivity problems
    - Service availability
    - Latency issues
    - Platform-specific problems

    Args:
        request: Contains error details, timestamp, user agent

    Returns:
        Diagnostic results with recovery recommendations
    """
    from datetime import datetime
    import asyncio
    import socket

    error = request.get("error", "unknown")
    user_agent = request.get("userAgent", "unknown")
    timestamp = request.get("timestamp", datetime.now().timestamp())

    logger.info(f"üîç Network diagnostics requested: error={error}")

    diagnostics = {
        "timestamp": datetime.now().isoformat(),
        "error": error,
        "checks": {}
    }

    # Check 1: DNS Resolution
    try:
        socket.gethostbyname("www.google.com")
        diagnostics["checks"]["dns"] = {"status": "ok", "message": "DNS resolution working"}
    except socket.gaierror:
        diagnostics["checks"]["dns"] = {"status": "failed", "message": "DNS resolution failed"}

    # Check 2: Internet Connectivity (basic check)
    try:
        socket.create_connection(("8.8.8.8", 53), timeout=3)
        diagnostics["checks"]["internet"] = {"status": "ok", "message": "Internet connectivity available"}
    except OSError:
        diagnostics["checks"]["internet"] = {"status": "failed", "message": "No internet connectivity"}

    # Check 3: Local Service Health
    diagnostics["checks"]["local_service"] = {
        "status": "ok",
        "message": "Backend service is responsive",
        "port": 8010  # Current backend port
    }

    # Check 4: Browser Compatibility
    browser_check = _check_browser_compatibility(user_agent)
    diagnostics["checks"]["browser"] = browser_check

    # Determine if recovered based on checks
    all_ok = all(check.get("status") == "ok" for check in diagnostics["checks"].values())

    diagnostics["recovered"] = all_ok
    diagnostics["recommendation"] = _get_diagnostic_recommendation(diagnostics["checks"])

    return diagnostics


# =============================================================================
# Helper Functions for Network Recovery
# =============================================================================

def _get_preferred_strategies(successful_strategy: str) -> list:
    """
    Get list of preferred strategies based on what worked.

    Returns strategies in priority order for future recovery attempts.
    """
    # Strategy priority tiers
    strategy_tiers = {
        "connectionCheck": ["quickRetry", "serviceSwitch"],
        "quickRetry": ["connectionCheck", "serviceSwitch"],
        "serviceSwitch": ["quickRetry", "webSocketFallback"],
        "webSocketFallback": ["serviceSwitch", "mlBackendRecovery"],
        "dnsRecovery": ["connectionCheck", "serviceSwitch"],
        "mlBackendRecovery": ["serviceSwitch", "offlineMode"],
        "offlineMode": ["connectionCheck", "edgeProcessing"],
        "edgeProcessing": ["offlineMode", "mlBackendRecovery"]
    }

    return strategy_tiers.get(successful_strategy, ["connectionCheck", "quickRetry", "serviceSwitch"])


def _calculate_health_score(connection_health: dict) -> float:
    """
    Calculate network health score from 0.0 (worst) to 1.0 (best).

    Factors:
    - Consecutive failures (lower is better)
    - Average latency (lower is better)
    - Time since last successful connection (shorter is better)
    """
    consecutive_failures = connection_health.get("consecutiveFailures", 0)
    average_latency = connection_health.get("averageLatency", 0)

    # Base score starts at 1.0
    score = 1.0

    # Reduce score for consecutive failures (0.1 per failure, max 0.5 reduction)
    score -= min(consecutive_failures * 0.1, 0.5)

    # Reduce score for high latency (>200ms is bad)
    if average_latency > 200:
        score -= min((average_latency - 200) / 1000, 0.3)

    # Ensure score stays in valid range
    return max(0.0, min(1.0, score))


def _get_fallback_api_url() -> str:
    """
    Get fallback API URL for degraded network scenarios.

    Returns localhost URL as fallback (always available locally).
    """
    import os
    port = os.getenv("JARVIS_PORT", "8010")
    return f"http://localhost:{port}"


def _check_browser_compatibility(user_agent: str) -> dict:
    """
    Check browser compatibility for speech recognition features.

    Returns compatibility status and warnings.
    """
    ua_lower = user_agent.lower()

    if "chrome" in ua_lower or "chromium" in ua_lower:
        return {
            "status": "ok",
            "browser": "Chrome/Chromium",
            "message": "Full speech recognition support"
        }
    elif "firefox" in ua_lower:
        return {
            "status": "warning",
            "browser": "Firefox",
            "message": "Limited speech recognition support - may require additional permissions"
        }
    elif "safari" in ua_lower:
        return {
            "status": "warning",
            "browser": "Safari",
            "message": "Speech recognition support varies by version"
        }
    else:
        return {
            "status": "unknown",
            "browser": "Unknown",
            "message": "Speech recognition support unknown for this browser"
        }


def _get_diagnostic_recommendation(checks: dict) -> str:
    """
    Get human-readable recommendation based on diagnostic checks.

    Analyzes check results and provides actionable advice.
    """
    if checks.get("dns", {}).get("status") == "failed":
        return "DNS resolution failed - check your internet connection or try changing DNS servers"

    if checks.get("internet", {}).get("status") == "failed":
        return "No internet connectivity detected - check your network connection"

    if checks.get("browser", {}).get("status") == "warning":
        return f"Browser compatibility issue - {checks['browser'].get('message', '')}"

    if all(check.get("status") == "ok" for check in checks.values()):
        return "All diagnostics passed - network should be operational"

    return "Some diagnostic checks failed - try restarting your browser or check network settings"


# Audio endpoints for frontend compatibility - Robust, async, intelligent TTS
@app.post("/audio/speak")
async def audio_speak_post(request: dict):
    """
    Robust audio speak endpoint with intelligent fallback chain.
    Never returns 503 - always provides audio response.

    Fallback chain:
    1. JARVIS Voice API (if available)
    2. Async TTS Handler with caching
    3. Direct macOS `say` command
    4. Silent audio (absolute last resort)
    """
    from fastapi.responses import Response
    import asyncio
    import struct
    import tempfile
    import os

    text = request.get("text", "")
    if not text:
        # Return minimal silent audio for empty text
        return _generate_silent_audio_response()

    # === Strategy 1: Try JARVIS Voice API (primary) ===
    voice = components.get("voice", {})
    jarvis_api = voice.get("jarvis_api")

    if jarvis_api:
        try:
            logger.debug(f"[TTS] Trying JARVIS Voice API for: {text[:50]}...")
            return await asyncio.wait_for(jarvis_api.speak(request), timeout=15.0)
        except asyncio.TimeoutError:
            logger.warning("[TTS] JARVIS Voice API timed out, falling back")
        except Exception as e:
            logger.warning(f"[TTS] JARVIS Voice API failed: {e}, falling back")

    # === Strategy 2: Try Async TTS Handler (cached, fast) ===
    try:
        from api.async_tts_handler import generate_speech_async

        logger.debug(f"[TTS] Trying Async TTS Handler for: {text[:50]}...")
        audio_path, content_type = await asyncio.wait_for(
            generate_speech_async(text, voice="Daniel"),
            timeout=30.0
        )

        # Read and return the audio file
        with open(audio_path, "rb") as f:
            audio_data = f.read()

        return Response(
            content=audio_data,
            media_type=content_type,
            headers={
                "Content-Disposition": f"inline; filename=jarvis_speech.mp3",
                "Cache-Control": "public, max-age=3600",
                "Access-Control-Allow-Origin": "*",
            },
        )
    except ImportError:
        logger.info("[TTS] Async TTS Handler not available, falling back")
    except asyncio.TimeoutError:
        logger.warning("[TTS] Async TTS Handler timed out, falling back")
    except Exception as e:
        logger.warning(f"[TTS] Async TTS Handler failed: {e}, falling back")

    # === Strategy 3: Direct macOS `say` command ===
    try:
        logger.debug(f"[TTS] Trying direct macOS say command for: {text[:50]}...")

        with tempfile.NamedTemporaryFile(suffix=".aiff", delete=False) as tmp:
            tmp_path = tmp.name

        # Use macOS say command with Daniel voice
        proc = await asyncio.create_subprocess_exec(
            "say", "-v", "Daniel", "-r", "160", "-o", tmp_path, text,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        await asyncio.wait_for(proc.communicate(), timeout=30.0)

        if proc.returncode != 0:
            raise Exception("say command failed")

        # Try to convert to MP3 with ffmpeg
        mp3_path = tmp_path.replace(".aiff", ".mp3")
        media_type = "audio/mpeg"

        try:
            ffmpeg_proc = await asyncio.create_subprocess_exec(
                "ffmpeg", "-i", tmp_path, "-acodec", "mp3", "-ab", "96k",
                "-ar", "22050", mp3_path, "-y",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await asyncio.wait_for(ffmpeg_proc.communicate(), timeout=30.0)

            if ffmpeg_proc.returncode == 0:
                with open(mp3_path, "rb") as f:
                    audio_data = f.read()
                os.unlink(tmp_path)
                os.unlink(mp3_path)
            else:
                raise Exception("ffmpeg conversion failed")
        except Exception:
            # Use AIFF directly if conversion fails
            with open(tmp_path, "rb") as f:
                audio_data = f.read()
            media_type = "audio/aiff"
            os.unlink(tmp_path)
            if os.path.exists(mp3_path):
                os.unlink(mp3_path)

        return Response(
            content=audio_data,
            media_type=media_type,
            headers={
                "Content-Disposition": "inline; filename=jarvis_speech.mp3",
                "Cache-Control": "no-cache",
                "Access-Control-Allow-Origin": "*",
            },
        )
    except asyncio.TimeoutError:
        logger.error("[TTS] Direct say command timed out")
    except Exception as e:
        logger.error(f"[TTS] Direct say command failed: {e}")

    # === Strategy 4: Silent audio (absolute last resort) ===
    logger.warning("[TTS] All TTS methods failed, returning silent audio")
    return _generate_silent_audio_response()


def _generate_silent_audio_response():
    """Generate a minimal silent WAV audio response"""
    from fastapi.responses import Response
    import struct

    # Generate a simple WAV header with 0.1 second of silence
    sample_rate = 44100
    duration = 0.1
    num_samples = int(sample_rate * duration)

    # WAV header
    wav_header = struct.pack(
        "<4sI4s4sIHHIIHH4sI",
        b"RIFF",
        36 + num_samples * 2,
        b"WAVE",
        b"fmt ",
        16,
        1,
        1,
        sample_rate,
        sample_rate * 2,
        2,
        16,
        b"data",
        num_samples * 2,
    )

    # Silent audio data (zeros)
    audio_data = wav_header + (b"\x00\x00" * num_samples)

    return Response(
        content=audio_data,
        media_type="audio/wav",
        headers={
            "Content-Disposition": "inline; filename=silence.wav",
            "Access-Control-Allow-Origin": "*",
        },
    )


@app.get("/audio/speak/{text}")
async def audio_speak_get(text: str):
    """GET endpoint for audio speak - decodes URL-encoded text and processes"""
    from urllib.parse import unquote

    # Decode URL-encoded text (handles special characters)
    decoded_text = unquote(text)
    return await audio_speak_post({"text": decoded_text})


# ============================================================
# JARVIS VOICE ENDPOINTS (Frontend Compatibility)
# ============================================================
# These endpoints are required by the frontend JarvisVoice.js component
# They provide status and activation for the voice system

@app.get("/voice/jarvis/status")
async def voice_jarvis_status():
    """Get JARVIS voice system status - required by frontend"""
    voice = components.get("voice", {})
    voice_unlock = components.get("voice_unlock", {})

    return {
        "status": "ready",
        "jarvis_available": voice.get("jarvis_available", False),
        "voice_unlock_available": bool(voice_unlock.get("router")),
        "listening": False,
        "speaking": False,
        "wake_word_enabled": False,
        "message": "JARVIS voice system ready"
    }


@app.post("/voice/jarvis/activate")
async def voice_jarvis_activate(request: Optional[dict] = None):
    """Activate JARVIS voice system"""
    return {
        "status": "activated",
        "message": "JARVIS voice system activated",
        "listening": True
    }


@app.post("/voice/jarvis/speak")
async def voice_jarvis_speak(request: dict):
    """Make JARVIS speak text - uses robust TTS with fallback chain"""
    # Use the robust audio speak implementation which handles all fallbacks
    return await audio_speak_post(request)


# ============================================================
# LAZY LOADING HELPER FOR UAE/SAI/LEARNING DB
# ============================================================


async def ensure_uae_loaded(app_state):
    """
    Lazy-load UAE/SAI/Learning DB on first use with Memory Quantizer integration.
    This saves 8-10GB of RAM at startup and prevents OOM kills.
    """
    # Already loaded?
    if app_state.uae_engine is not None:
        return app_state.uae_engine

    # Already initializing?
    if app_state.uae_initializing:
        # Wait for initialization to complete
        import asyncio

        for _ in range(50):  # Wait up to 5 seconds
            await asyncio.sleep(0.1)
            if app_state.uae_engine is not None:
                return app_state.uae_engine
        logger.warning("[LAZY-UAE] Timeout waiting for UAE initialization")
        return None

    # ============================================================
    # MEMORY QUANTIZER INTEGRATION - Intelligent Load Prevention
    # ============================================================
    try:
        from core.memory_quantizer import MemoryQuantizer, MemoryTier

        # Get memory quantizer instance
        quantizer = MemoryQuantizer()
        metrics = quantizer.get_current_metrics()  # Synchronous call

        # Log current memory state
        logger.info(f"[LAZY-UAE] Memory check before loading:")
        logger.info(f"[LAZY-UAE]   ‚Ä¢ Tier: {metrics.tier.value}")
        logger.info(f"[LAZY-UAE]   ‚Ä¢ Pressure: {metrics.pressure.value}")
        logger.info(f"[LAZY-UAE]   ‚Ä¢ Available: {metrics.system_memory_available_gb:.2f} GB")
        logger.info(f"[LAZY-UAE]   ‚Ä¢ Usage: {metrics.system_memory_percent:.1f}%")

        # Estimated UAE/SAI/Learning DB memory requirement
        REQUIRED_MEMORY_GB = 10.0

        # Check if we have enough memory available
        if metrics.system_memory_available_gb < REQUIRED_MEMORY_GB:
            logger.error(f"[LAZY-UAE] ‚ùå Insufficient memory for intelligence components")
            logger.error(f"[LAZY-UAE]    Required: {REQUIRED_MEMORY_GB:.1f} GB")
            logger.error(f"[LAZY-UAE]    Available: {metrics.system_memory_available_gb:.2f} GB")
            logger.error(
                f"[LAZY-UAE]    Deficit: {REQUIRED_MEMORY_GB - metrics.system_memory_available_gb:.2f} GB"
            )

            # Provide fallback recommendation
            logger.info(f"[LAZY-UAE] üí° Falling back to basic multi-space detection (Yabai only)")
            return None

        # Check memory tier - refuse to load in dangerous tiers
        dangerous_tiers = {MemoryTier.CRITICAL, MemoryTier.EMERGENCY, MemoryTier.CONSTRAINED}
        if metrics.tier in dangerous_tiers:
            logger.warning(
                f"[LAZY-UAE] ‚ö†Ô∏è  Memory tier is {metrics.tier.value} - postponing intelligence loading"
            )
            logger.warning(f"[LAZY-UAE]    Current tier: {metrics.tier.value}")
            logger.warning(f"[LAZY-UAE]    Required tier: ELEVATED or better")
            logger.info(f"[LAZY-UAE] üí° Using lightweight mode until memory pressure reduces")
            return None

        # Predictive check - will loading cause OOM?
        predicted_usage = metrics.system_memory_percent + (
            REQUIRED_MEMORY_GB / metrics.system_memory_gb * 100
        )
        if predicted_usage > 90:
            logger.warning(
                f"[LAZY-UAE] ‚ö†Ô∏è  Loading would push usage to {predicted_usage:.1f}% (OOM risk)"
            )
            logger.warning(f"[LAZY-UAE]    Current: {metrics.system_memory_percent:.1f}%")
            logger.warning(f"[LAZY-UAE]    After load: ~{predicted_usage:.1f}%")
            logger.warning(f"[LAZY-UAE]    Safe threshold: <85%")
            return None

        # Memory check PASSED - safe to load
        logger.info(f"[LAZY-UAE] ‚úÖ Memory check PASSED - safe to load intelligence")
        logger.info(f"[LAZY-UAE]    Predicted usage after load: {predicted_usage:.1f}%")

    except Exception as e:
        logger.warning(f"[LAZY-UAE] Memory Quantizer check failed: {e}")
        logger.warning(f"[LAZY-UAE] Proceeding with loading (no safety check)")

    # Start initialization
    app_state.uae_initializing = True
    logger.info("[LAZY-UAE] üß† Initializing UAE/SAI/Learning DB on first use...")

    try:
        from intelligence.uae_integration import get_learning_db, initialize_uae

        config = app_state.uae_lazy_config

        # Create voice callback
        async def voice_callback(text: str):
            """Voice callback for proactive suggestions"""
            try:
                voice = components.get("voice", {})
                jarvis_api = voice.get("jarvis_api")
                if jarvis_api:
                    await jarvis_api.speak({"text": text})
                    logger.debug(f"[PROACTIVE-VOICE] Spoke: {text}")
            except Exception as e:
                logger.error(f"[PROACTIVE-VOICE] Error: {e}")

        # Create notification callback
        async def notification_callback(title: str, message: str, priority: str = "low"):
            """Notification callback for proactive suggestions"""
            try:
                logger.info(f"[PROACTIVE-NOTIFY] [{priority.upper()}] {title}: {message}")
            except Exception as e:
                logger.error(f"[PROACTIVE-NOTIFY] Error: {e}")

        # Initialize UAE
        uae = await initialize_uae(
            vision_analyzer=config["vision_analyzer"],
            sai_monitoring_interval=config["sai_monitoring_interval"],
            enable_auto_start=config["enable_auto_start"],
            enable_learning_db=config["enable_learning_db"],
            enable_yabai=config["enable_yabai"],
            enable_proactive_intelligence=config["enable_proactive_intelligence"],
            voice_callback=voice_callback,
            notification_callback=notification_callback,
        )

        if uae and uae.is_active:
            app_state.uae_engine = uae
            app_state.learning_db = get_learning_db()
            logger.info("[LAZY-UAE] ‚úÖ UAE/SAI/Learning DB loaded successfully")
            return uae
        else:
            logger.warning("[LAZY-UAE] ‚ö†Ô∏è  UAE initialized but not active")
            return None

    except Exception as e:
        logger.error(f"[LAZY-UAE] ‚ùå Failed to load UAE: {e}")
        return None
    finally:
        app_state.uae_initializing = False


# Add more endpoints based on loaded components...
# (The rest of your API endpoints would go here)

if __name__ == "__main__":
    import argparse

    # ==========================================================================
    # v85.0: Entry Point Detection - Know who launched us
    # ==========================================================================
    # This provides context for downstream components about the launch chain.
    # Coordinates with run_supervisor.py and start_system.py via env vars.
    # ==========================================================================
    try:
        from core.trinity_integrator import TrinityEntryPointDetector

        _entry_info = TrinityEntryPointDetector.detect_entry_point()
        _entry_point = _entry_info.get("entry_point", "unknown")
        _parent_entry = _entry_info.get("parent_entry_point", "unknown")

        logger.info(f"[v85.0] JARVIS backend launched - entry: {_entry_point}")

        # Set environment variables for downstream components
        if _entry_point == "run_supervisor" or os.environ.get("JARVIS_SUPERVISED") == "1":
            os.environ["JARVIS_MANAGED_BY_SUPERVISOR"] = "1"
            logger.info("[v85.0] Running under supervisor management")
        elif _entry_point == "start_system":
            os.environ["JARVIS_MANAGED_BY_START_SYSTEM"] = "1"
            logger.info("[v85.0] Running via start_system.py")
        elif _entry_point == "main_direct" or _entry_point == "unknown":
            os.environ["JARVIS_DIRECT_LAUNCH"] = "1"
            logger.info("[v85.0] Direct launch (no coordinator)")

        # Check if externally managed (by another coordinator)
        if os.environ.get("JARVIS_MANAGED_EXTERNALLY") == "1":
            _manager_pid = os.environ.get("JARVIS_MANAGER_PID", "unknown")
            _manager_entry = os.environ.get("JARVIS_MANAGER_ENTRY", "unknown")
            logger.info(
                f"[v85.0] Externally managed by {_manager_entry} (PID: {_manager_pid})"
            )

    except ImportError:
        logger.debug("[v85.0] Entry point detection not available")
    except Exception as e:
        logger.debug(f"[v85.0] Entry point detection failed (non-fatal): {e}")

    # ==========================================================================
    # HYPER-RUNTIME v9.0: Rust-First Server Architecture
    # ==========================================================================
    # Intelligent runtime selection:
    #   Level 3 (HYPER):    Granian (Rust/Tokio) - 3-5x faster
    #   Level 2 (FAST):     uvloop (C/libuv)     - 2-4x faster
    #   Level 1 (STANDARD): asyncio              - Python baseline
    # ==========================================================================
    try:
        from core.hyper_runtime import (
            start_hyper_server,
            get_runtime_engine,
            ServerConfig,
        )
        HYPER_RUNTIME_AVAILABLE = True
        _runtime = get_runtime_engine()
        print(f"‚ö° [HYPER-RUNTIME] {_runtime.name} engine ready (Level {_runtime.level.value}/3)")
    except ImportError:
        HYPER_RUNTIME_AVAILABLE = False
        import uvicorn
        print("üêç [RUNTIME] Using standard uvicorn (hyper_runtime not available)")

    # Parse command line arguments
    parser = argparse.ArgumentParser(description="JARVIS Backend Server")
    parser.add_argument(
        "--port",
        type=int,
        default=int(os.getenv("BACKEND_PORT", "8010")),
        help="Port to run the server on",
    )
    args = parser.parse_args()

    # Print startup information
    print(f"\nüöÄ Starting JARVIS Backend")
    print(f"   HTTP:      http://localhost:{args.port}")
    print(f"   WebSocket: ws://localhost:{args.port}/ws")
    print(f"   API Docs:  http://localhost:{args.port}/docs")
    print("=" * 60)

    # Register cleanup hook on exit
    import atexit
    def cleanup_on_exit():
        try:
            # We can't use async here easily, but we can try to run the sync cleanup logic
            # or just log that we are exiting. The async hook is better handled by start_system.py
            # But this is a fallback.
            pass
        except Exception:
            pass
    atexit.register(cleanup_on_exit)

    # Start server with best available runtime
    if HYPER_RUNTIME_AVAILABLE:
        # Use Hyper-Runtime (Granian/Rust or uvloop fallback)
        config = ServerConfig(
            host="0.0.0.0",
            port=args.port,
            log_level="info" if OPTIMIZE_STARTUP else "warning",
            websockets=True,
        )
        # OPTIMIZATION: Pass app object directly in single-worker mode
        # This prevents module reimport and double-initialization
        # String format "main:app" is only needed for multi-worker/reload modes
        if config.workers <= 1:
            # Single worker: Pass app object directly (no reimport)
            start_hyper_server(app, config=config)
        else:
            # Multi-worker: Use string format for proper process forking
            start_hyper_server("main:app", config=config)
    elif OPTIMIZE_STARTUP:
        # Fallback: uvicorn with optimized settings
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=args.port,
            log_level="info",
            access_log=False,  # Disable access logs for performance
            loop="asyncio",  # Use asyncio (uvloop disabled due to speechbrain/torch compatibility)
        )
    else:
        # Fallback: standard uvicorn
        uvicorn.run(app, host="0.0.0.0", port=args.port)
