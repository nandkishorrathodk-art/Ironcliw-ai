# RAG + RLHF Intelligence Learning System

## Overview

Ironcliw v5.0 introduces an advanced **Intelligence Learning Coordinator** that integrates **RAG (Retrieval-Augmented Generation)** and **RLHF (Reinforcement Learning from Human Feedback)** with multi-factor authentication intelligence for continuous improvement and context-aware decision making.

### Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                    Authentication Request                        │
└────────────────────────┬────────────────────────────────────────┘
                         │
                ┌────────┴────────┐
                │                 │
         Voice Biometric   Multi-Factor
         Intelligence      Intelligence
                │                 │
                │  ┌──────────────┴──────────────┐
                │  │                             │
                │  v                             v
                │ Network  Temporal  Device  Drift
                │ Context  Patterns  State   Detector
                │        │      │      │         │
                └────────┴──────┴──────┴─────────┘
                                 │
                        ┌────────┴────────┐
                        │                 │
                        v                 v
                 ┌──────────────┐  ┌──────────────┐
                 │ RAG Engine   │  │ RLHF Loop    │
                 │              │  │              │
                 │ • Retrieve   │  │ • Feedback   │
                 │   similar    │  │ • Correction │
                 │   contexts   │  │ • Adjust     │
                 └──────┬───────┘  └──────┬───────┘
                        │                 │
                        └────────┬────────┘
                                 │
                                 v
                    ┌────────────────────────┐
                    │ Learning Database      │
                    │ (SQLite / Cloud SQL)   │
                    │                        │
                    │ • Auth History         │
                    │ • Voice Samples        │
                    │ • Context Patterns     │
                    │ • Feedback Scores      │
                    └────────────────────────┘
```

## Key Features

### 1. **RAG (Retrieval-Augmented Generation)**

Retrieves similar authentication contexts from historical data to inform current decisions.

**How it works:**
1. **Context Vector Creation**: Current authentication context (network, temporal, device) is converted to a vector
2. **Similarity Search**: Cosine similarity is calculated against historical authentications
3. **K-Nearest Neighbors**: Top-5 most similar contexts are retrieved
4. **Confidence Boost**: Average confidence from similar contexts informs current decision

**Example:**
```
Current Context:
- Network: Home WiFi (trusted)
- Time: 7:15 AM (typical)
- Device: Docked workstation

RAG Retrieval:
→ Found 5 similar contexts:
  1. [98% similar] 7:12 AM, Home WiFi, Docked → 94% confidence ✅
  2. [96% similar] 7:20 AM, Home WiFi, Docked → 96% confidence ✅
  3. [95% similar] 7:08 AM, Home WiFi, Stationary → 93% confidence ✅
  4. [94% similar] 7:30 AM, Home WiFi, Docked → 95% confidence ✅
  5. [92% similar] 7:00 AM, Home WiFi, Docked → 92% confidence ✅

RAG Insight: Avg confidence 94%, 100% success rate
Recommendation: High confidence - proceed with authentication
```

### 2. **RLHF (Reinforcement Learning from Human Feedback)**

Learns from corrections and feedback to continuously improve.

**How it works:**
1. **Record Authentication**: Every authentication attempt is recorded with full context
2. **User Feedback**: User (or system) provides feedback on accuracy
3. **Learn from Mistakes**: False positives and false negatives are learned from
4. **Adaptive Thresholds**: Thresholds automatically adjust to minimize errors

**Feedback Types:**
- ✅ **Correct Decision** (feedback_score: 1.0) - Reinforce this pattern
- ❌ **False Positive** (feedback_score: 0.0) - Authenticated wrong person
- ❌ **False Negative** (feedback_score: 0.0) - Denied legitimate user
- ⚠️ **Borderline** (feedback_score: 0.5) - Correct but uncertain

**Example:**
```python
# User corrects a false positive
await learning_coordinator.apply_rlhf_feedback(
    record_id=12345,
    was_correct=False,  # This was wrong
    feedback_score=0.0,  # False positive
    feedback_notes="Sister's voice was mistaken for mine"
)

# System learns:
# → Voice similarity alone insufficient for this case
# → Need stronger multi-factor corroboration
# → Adjust threshold up by +5%
```

### 3. **Cross-Intelligence Correlation**

Discovers patterns across multiple intelligence signals.

**Learned Correlations:**
- "Docked + Home WiFi + Morning" → 98% confidence pattern
- "Unknown network + Late night" → High risk pattern
- "Just woke + Stationary" → Morning voice expected
- "Moving device + Voice drift" → Equipment change likely

### 4. **Predictive Authentication**

Anticipates unlock needs based on learned patterns.

**Predictions:**
- Next unlock time (based on schedule patterns)
- Typical confidence for current context
- Anomaly detection for unusual patterns

**Example:**
```
Learned Pattern:
- Typical unlocks: 7:15 AM, 12:30 PM, 6:00 PM
- Current time: 7:13 AM
- Prediction: High probability of unlock in 2 minutes
- Pre-warm: Start loading voice models now
```

### 5. **Adaptive Threshold Tuning**

Self-optimizes thresholds based on performance.

**Metrics:**
- **FPR (False Positive Rate)**: Target 1% - Too many = increase threshold
- **FNR (False Negative Rate)**: Target 5% - Too many = decrease threshold
- **Automatic Adjustment**: ±5% increments every 7 days if needed

## Integration with Multi-Factor Authentication

### Before Learning (Baseline)

```
Authentication Decision:
├─ Voice: 82% (moderate match)
├─ Network: 95% (trusted)
├─ Temporal: 88% (typical time)
└─ Device: 92% (stationary)

Multi-Factor Fusion: 87% → ✅ AUTHENTICATE
```

### After Learning (Enhanced with RAG + RLHF)

```
Authentication Decision:
├─ Voice: 82% (moderate match)
├─ Network: 95% (trusted)
├─ Temporal: 88% (typical time)
└─ Device: 92% (stationary)

RAG Context:
├─ Similar contexts: 5 found
├─ Avg confidence: 94%
├─ Success rate: 100%
└─ Recommendation: "High confidence based on similar patterns"

RLHF Adjustments:
├─ This context previously → 94% success
├─ No false positives in history
└─ Confidence boost: +5%

Enhanced Decision: 92% → ✅ AUTHENTICATE (High Confidence)
```

## Configuration

### Environment Variables

```bash
# Learning Coordinator
Ironcliw_LEARNING_ENABLE_RAG=true
Ironcliw_LEARNING_ENABLE_RLHF=true
Ironcliw_LEARNING_ENABLE_PREDICTION=true
Ironcliw_LEARNING_ADAPTIVE_THRESHOLDS=true

# RAG Configuration
Ironcliw_RAG_K_NEIGHBORS=5                 # Top-K similar contexts
Ironcliw_RAG_SIMILARITY_THRESHOLD=0.75     # Minimum similarity

# RLHF Configuration
Ironcliw_RLHF_LEARNING_RATE=0.1
Ironcliw_RLHF_MIN_SAMPLES=10               # Min samples before learning

# Prediction Configuration
Ironcliw_PREDICTION_WINDOW_DAYS=30
Ironcliw_PREDICTION_MIN_SAMPLES=20

# Adaptive Thresholds
Ironcliw_ADAPTIVE_WINDOW_DAYS=7
Ironcliw_TARGET_FALSE_POSITIVE_RATE=0.01   # 1% FPR target
Ironcliw_TARGET_FALSE_NEGATIVE_RATE=0.05   # 5% FNR target
```

## Usage Examples

### Automatic (No Code Changes Required)

The learning system is **fully automatic** and integrates seamlessly:

```python
# Your existing authentication code works automatically
vbi = await get_voice_biometric_intelligence()
result = await vbi.verify_and_announce(audio_data, context)

# Behind the scenes:
# 1. RAG retrieves similar contexts
# 2. Multi-factor fusion with RAG insights
# 3. Authentication recorded for learning
# 4. RLHF feedback loop ready
```

### Manual RLHF Feedback

Provide explicit feedback for corrections:

```python
from intelligence.intelligence_learning_coordinator import get_learning_coordinator

coordinator = await get_learning_coordinator()

# After authentication
if user_reports_error:
    await coordinator.apply_rlhf_feedback(
        record_id=result.learning_record_id,
        was_correct=False,
        feedback_score=0.0,
        feedback_notes="False positive - authenticated wrong person"
    )
```

### Get RAG Context Manually

```python
coordinator = await get_learning_coordinator()

rag_context = await coordinator.get_rag_context(
    user_id="derek",
    network_context={'ssid_hash': 'abc123', 'trust_score': 0.95},
    temporal_context={'confidence': 0.88},
    device_context={'state': 'stationary', 'trust_score': 0.92}
)

print(f"Similar contexts: {len(rag_context['similar_contexts'])}")
print(f"Avg confidence: {rag_context['avg_confidence']:.1%}")
print(f"Recommendation: {rag_context['recommendation']}")
```

### Predictive Authentication

```python
coordinator = await get_learning_coordinator()

# Predict next unlock
next_unlock = await coordinator.predict_next_unlock("derek")
if next_unlock:
    print(f"Predicted next unlock: {next_unlock}")
    # Could pre-warm models here
```

### Get Learning Insights

```python
coordinator = await get_learning_coordinator()

insights = await coordinator.get_learning_insights("derek")

print(f"Typical network trust: {insights.typical_network_trust:.1%}")
print(f"Typical device state: {insights.typical_device_state}")
print(f"Avg successful confidence: {insights.avg_successful_confidence:.1%}")
print(f"\nReasoning:")
for reason in insights.reasoning:
    print(f"  - {reason}")
```

## Data Storage

### Authentication Records

Stored in: `~/.jarvis/intelligence/authentication_learning.json`

**Record Structure:**
```json
{
  "timestamp": "2025-12-22T10:30:45",
  "user_id": "derek",
  "outcome": "success",
  "voice_confidence": 0.94,
  "network_ssid_hash": "abc123",
  "network_trust": 0.95,
  "temporal_confidence": 0.88,
  "device_state": "stationary",
  "device_trust": 0.92,
  "drift_adjustment": 0.02,
  "final_confidence": 0.96,
  "risk_score": 0.08,
  "decision": "authenticate",
  "was_correct": true,
  "feedback_score": 1.0,
  "feedback_notes": "Perfect authentication"
}
```

### Voice Samples (Learning Database)

Stored in: Learning Database (`~/.jarvis/learning/jarvis_learning.db` or Cloud SQL)

**Integrated with:**
- SQLite for local development
- Cloud SQL (PostgreSQL) for production
- Automatic sync between local and cloud

## Learning Workflow

### Phase 1: Initial Collection (Days 1-7)

```
Day 1: Record all authentications
→ 15 successful unlocks recorded
→ Build baseline patterns

Day 3: First RAG retrieval available
→ 45 authentications recorded
→ Similar contexts start appearing

Day 7: Full RAG + prediction active
→ 100+ authentications recorded
→ Patterns clearly established
→ Predictions 70% accurate
```

### Phase 2: Active Learning (Days 8-30)

```
Week 2: RLHF feedback incorporated
→ User corrects 2 false positives
→ Thresholds adjusted automatically
→ FPR drops from 3% to 1%

Week 3: Predictive accuracy improves
→ Next unlock predictions 85% accurate
→ Pre-warming reduces latency 50%

Week 4: Fully optimized
→ Authentication 95% confident
→ Zero false positives in 7 days
→ Adaptive thresholds stable
```

### Phase 3: Continuous Improvement (Ongoing)

```
Monthly:
→ Review RLHF feedback patterns
→ Adjust target FPR/FNR if needed
→ Archive old patterns (>90 days)

Quarterly:
→ Retrain models with accumulated data
→ Update baseline patterns
→ Performance audit
```

## Performance Impact

### Latency

- **RAG Lookup**: +20-50ms (parallelized)
- **RLHF Recording**: +10-20ms (async, non-blocking)
- **Prediction**: +5-10ms (cached)
- **Total Impact**: +35-80ms average

### Memory

- **Learning Coordinator**: ~5MB
- **Authentication History**: ~100KB per 1000 records
- **RAG Cache**: ~2MB

### Storage

- **Local**: ~1MB per month per user
- **Cloud SQL**: Compressed, ~500KB per month per user

## Best Practices

### 1. Let the System Learn

- **Don't intervene early** - Need 7-14 days to establish patterns
- **Trust the process** - RAG needs historical data
- **Provide feedback** - But only when genuinely wrong

### 2. Monitor Metrics

```python
coordinator = await get_learning_coordinator()
stats = coordinator.get_stats()

print(f"Total Records: {stats['total_records']}")
print(f"RLHF Feedbacks: {stats['rlhf_feedbacks']}")
print(f"Threshold Adjustments: {stats['threshold_adjustments']}")
print(f"Prediction Accuracy: {stats['prediction_accuracy']:.1%}")
```

### 3. Balanced Feedback

- **Positive feedback** (correct decisions) → Reinforces patterns
- **Negative feedback** (mistakes) → Triggers learning
- **Neutral feedback** (borderline) → Fine-tunes thresholds

### 4. Gradual Threshold Tuning

```
Target FPR: 1% (very strict security)
Target FNR: 5% (some false negatives acceptable)

If FPR > 1%: Increase threshold by 5%
If FNR > 5%: Decrease threshold by 5%

Auto-adjust every 7 days
```

## Troubleshooting

### Issue: RAG not finding similar contexts

**Symptoms**: `similar_contexts: []`

**Solutions:**
1. Need more historical data (wait 3-7 days)
2. Lower similarity threshold: `Ironcliw_RAG_SIMILARITY_THRESHOLD=0.65`
3. Check that authentication is being recorded

### Issue: RLHF not improving accuracy

**Symptoms**: Same errors recurring

**Solutions:**
1. Ensure feedback is being applied correctly
2. Check `rlhf_feedbacks` stat is increasing
3. Verify learning_record_id is set in results
4. Need minimum 10 samples before adjustments apply

### Issue: Predictions always wrong

**Symptoms**: Predicted times don't match reality

**Solutions:**
1. Need more data (minimum 20 successful unlocks)
2. Irregular schedule = predictions unreliable
3. Check `predictions_made` stat
4. Disable prediction if schedule too variable

### Issue: Thresholds adjusting too aggressively

**Symptoms**: Threshold changes every week

**Solutions:**
1. Increase adaptive window: `Ironcliw_ADAPTIVE_WINDOW_DAYS=14`
2. Relax targets: `Ironcliw_TARGET_FALSE_POSITIVE_RATE=0.02`
3. Increase min samples: `Ironcliw_RLHF_MIN_SAMPLES=20`

## Security Considerations

### Privacy

- **SSID Hashing**: Network SSIDs are SHA-256 hashed before storage
- **No Raw Audio**: Only embeddings stored, never raw audio
- **Local First**: Data stored locally by default
- **Opt-in Cloud**: Cloud SQL sync optional

### Data Retention

- **Authentication Records**: Last 10,000 records kept
- **Voice Samples**: Unlimited (for continuous learning)
- **Automatic Cleanup**: Records >90 days archived

### RLHF Feedback Security

- **Authenticated Feedback Only**: Must be from legitimate user
- **Rate Limiting**: Max 10 feedback corrections per hour
- **Audit Trail**: All feedback logged with timestamp
- **Reversible**: Feedback can be reviewed and corrected

## Integration with Existing Systems

### Voice Biometric Intelligence (VBI)

✅ **Fully Integrated** - Automatic RAG retrieval and learning recording

### Multi-Factor Fusion Engine

✅ **Fully Integrated** - RAG context used in fusion decisions

### Learning Database

✅ **Fully Integrated** - Voice samples and RLHF feedback stored

### Network Context Provider

✅ **Fully Integrated** - Network patterns learned and retrieved

### Temporal Pattern Tracker

✅ **Fully Integrated** - Temporal predictions enhanced with RAG

### Device State Monitor

✅ **Fully Integrated** - Device patterns learned and correlated

## Advanced Features

### Cross-User Learning (Future)

```python
# Learn from patterns across multiple users
# while maintaining privacy

coordinator.enable_federated_learning(
    users=["derek", "other_user"],
    privacy_preserving=True
)
```

### Transfer Learning (Future)

```python
# Transfer learned patterns to new devices
coordinator.export_learned_patterns("derek")
coordinator.import_learned_patterns("derek", new_device=True)
```

### Explainable AI

```python
# Get detailed explanation of authentication decision
explanation = await coordinator.explain_decision(record_id)

print(f"Decision: {explanation.decision}")
print(f"Key Factors:")
for factor, contribution in explanation.factors.items():
    print(f"  {factor}: {contribution:.1%} contribution")
```

## Monitoring Dashboard (Future)

```
╔══════════════════════════════════════════════════════════════╗
║  Ironcliw Learning Intelligence Dashboard                      ║
╠══════════════════════════════════════════════════════════════╣
║                                                              ║
║  Authentication Performance:                                 ║
║  ├─ Total Authentications: 1,247                            ║
║  ├─ Success Rate: 96.2%                                     ║
║  ├─ False Positive Rate: 0.8% ✅ (Target: 1%)              ║
║  └─ False Negative Rate: 3.0% ✅ (Target: 5%)              ║
║                                                              ║
║  Learning System:                                            ║
║  ├─ RAG Retrievals: 1,153                                   ║
║  ├─ RLHF Feedbacks: 23                                      ║
║  ├─ Threshold Adjustments: 4                                ║
║  └─ Prediction Accuracy: 87%                                ║
║                                                              ║
║  Recent Patterns:                                            ║
║  ├─ Most Common: "Morning + Home WiFi + Docked"             ║
║  ├─ Highest Confidence: "Afternoon + Office + Stationary"   ║
║  └─ Most Challenging: "Late Night + Unknown Network"        ║
║                                                              ║
╚══════════════════════════════════════════════════════════════╝
```

## Summary

The **RAG + RLHF Intelligence Learning System** transforms Ironcliw voice authentication from a static system into an **adaptive, self-improving intelligence** that:

✅ Learns from every authentication
✅ Retrieves relevant historical context
✅ Corrects mistakes through feedback
✅ Predicts future authentication needs
✅ Self-tunes thresholds for optimal performance
✅ Maintains privacy and security
✅ Improves accuracy over time

**Result**: More secure, more reliable, more intelligent authentication that gets better every day.

---

**Ironcliw Intelligence Learning Coordinator v5.0**
*RAG + RLHF + Multi-Factor Intelligence*
*Adaptive • Intelligent • Self-Improving*
