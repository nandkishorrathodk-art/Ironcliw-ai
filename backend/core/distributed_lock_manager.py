"""
Distributed Lock Manager for Cross-Repo Coordination v3.0
===========================================================

Production-grade distributed lock manager for coordinating operations across
JARVIS, JARVIS-Prime, and Reactor-Core repositories.

Features:
- File-based locks with automatic expiration
- Stale lock detection and cleanup
- Lock timeout and retry logic
- Deadlock prevention with TTL (time-to-live)
- Lock health monitoring
- Zero-config operation with sensible defaults
- Async-first API
- v2.0: UUID-based temp files (fixes race conditions)
- v2.0: Per-lock-name async semaphores (prevents concurrent write collisions)
- v2.0: Exponential backoff with jitter (reduces contention)
- v2.0: Orphaned temp file cleanup (prevents disk clutter)
- v2.0: Cross-repo coordination improvements
- v3.0: Redis backend support for distributed environments
- v3.0: Automatic backend selection (Redis → File fallback)
- v3.0: Cross-repo lock bridge for JARVIS-Prime and Reactor-Core
- v3.0: Fencing tokens for monotonic ordering
- v3.0: Lock lease extension (keepalive)

Problem Solved:
    Before: Process crashes while holding lock → other processes blocked forever
    After: Locks auto-expire after TTL → stale locks cleaned up automatically

    Before (v2.0): Only file-based locks, limited to single machine
    After (v3.0): Redis + File hybrid, works across VMs, GCP instances, Docker

Example Usage:
    ```python
    lock_manager = DistributedLockManager()

    # Acquire lock with auto-expiration
    async with lock_manager.acquire("vbia_events", timeout=5.0, ttl=10.0) as acquired:
        if acquired:
            # Perform critical operation
            await update_vbia_events()
        else:
            logger.warning("Could not acquire lock")
    ```

Architecture:
    v3.0 Unified Cross-Repo Lock System:

    ┌─────────────────────────────────────────────────────────────────────────┐
    │                      UNIFIED LOCK MANAGER v3.0                          │
    │                                                                         │
    │  ┌───────────────────┐       ┌───────────────────┐                      │
    │  │  Redis Backend    │  OR   │   File Backend    │                      │
    │  │  (Distributed)    │ ←──→  │   (Local/NFS)     │                      │
    │  │                   │       │                   │                      │
    │  │  - Fencing tokens │       │  - JSON metadata  │                      │
    │  │  - Keepalive      │       │  - PID validation │                      │
    │  │  - SETNX + TTL    │       │  - Atomic rename  │                      │
    │  └───────────────────┘       └───────────────────┘                      │
    │           ↑                           ↑                                 │
    │           │   Automatic Fallback      │                                 │
    │           └───────────┬───────────────┘                                 │
    │                       │                                                 │
    │  ┌────────────────────┴────────────────────┐                            │
    │  │         Lock Coordinator                │                            │
    │  │  - Backend selection                    │                            │
    │  │  - Cross-repo coordination              │                            │
    │  │  - Health monitoring                    │                            │
    │  └────────────────────┬────────────────────┘                            │
    │                       │                                                 │
    │  ┌────────────┬───────┴───────┬────────────┐                            │
    │  │  JARVIS    │  JARVIS-Prime │ Reactor    │                            │
    │  │  (Body)    │  (Mind)       │ Core       │                            │
    │  └────────────┴───────────────┴────────────┘                            │
    └─────────────────────────────────────────────────────────────────────────┘

    File-based locks (fallback):
    ~/.jarvis/cross_repo/locks/
    ├── vbia_events.dlm.lock     (Lock file with metadata)
    │   {
    │     "acquired_at": 1736895345.123,
    │     "expires_at": 1736895355.123,
    │     "owner": "jarvis-12345-1736895300.5",
    │     "token": "f47ac10b-58cc-4372-a567-0e02b2c3d479",
    │     "fencing_token": 42,
    │     "backend": "file"
    │   }
    ├── prime_state.dlm.lock
    └── reactor_training.dlm.lock

Author: JARVIS AI System
Version: 3.0.0

v2.0.0 CRITICAL FIX (Race Condition Resolution):
    Before: Multiple async tasks used same temp file path `{lock}.tmp.{pid}`
            This caused "Temp file size mismatch" and "No such file" errors
            when concurrent tasks stepped on each other's temp files.
    After: Each write uses UUID-based temp file + per-lock semaphore
           Eliminates all inter-task race conditions.

v3.0.0 CROSS-REPO UNIFICATION:
    Before: JARVIS used file locks, Reactor-Core used Redis locks, no coordination.
    After: Unified lock manager with automatic backend selection:
           - Redis available → Use Redis (distributed across machines)
           - Redis unavailable → Fall back to file locks (single machine)
           - Both repos can import this module for consistent locking
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
import random
import socket
import threading
import time
from contextlib import asynccontextmanager
from dataclasses import asdict, dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, AsyncIterator, Callable, Dict, List, Optional, Tuple
from uuid import uuid4

import aiofiles
import aiofiles.os

logger = logging.getLogger(__name__)


# =============================================================================
# v3.0: Backend Configuration
# =============================================================================

# Redis configuration from environment
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", "6379"))
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", None)
REDIS_LOCK_DB = int(os.getenv("REDIS_LOCK_DB", "1"))  # Separate DB for locks

# Lock configuration from environment
DEFAULT_LOCK_TTL = float(os.getenv("DISTRIBUTED_LOCK_TTL", "10.0"))
DEFAULT_ACQUIRE_TIMEOUT = float(os.getenv("DISTRIBUTED_LOCK_TIMEOUT", "5.0"))
DEFAULT_RETRY_INTERVAL = float(os.getenv("DISTRIBUTED_LOCK_RETRY_INTERVAL", "0.1"))
KEEPALIVE_INTERVAL = float(os.getenv("DISTRIBUTED_LOCK_KEEPALIVE", "3.0"))

# Cross-repo lock key prefixes
LOCK_PREFIX = "jarvis:lock:"
TRINITY_LOCK_PREFIX = f"{LOCK_PREFIX}trinity:"


class LockBackend(Enum):
    """v3.0: Available lock backends."""
    REDIS = "redis"
    FILE = "file"
    AUTO = "auto"  # Automatic selection: Redis if available, else file


class LockState(Enum):
    """v3.0: States of a distributed lock."""
    RELEASED = "released"
    ACQUIRING = "acquiring"
    ACQUIRED = "acquired"
    EXTENDING = "extending"
    FAILED = "failed"


# =============================================================================
# v3.0: Redis Client Wrapper
# =============================================================================

class AsyncRedisLockClient:
    """
    v3.0: Async Redis client wrapper for distributed locks.

    Falls back gracefully when Redis is unavailable.
    Thread-safe singleton pattern.
    """

    _instance: Optional["AsyncRedisLockClient"] = None
    _lock = threading.Lock()

    def __new__(cls) -> "AsyncRedisLockClient":
        with cls._lock:
            if cls._instance is None:
                cls._instance = super().__new__(cls)
                cls._instance._initialized = False
            return cls._instance

    def __init__(self):
        if self._initialized:
            return

        self.host = REDIS_HOST
        self.port = REDIS_PORT
        self.password = REDIS_PASSWORD
        self.db = REDIS_LOCK_DB
        self._client = None
        self._available = False
        self._connect_lock = asyncio.Lock()
        self._last_check = 0.0
        self._check_interval = 30.0  # Re-check availability every 30s
        self._initialized = True

    async def connect(self) -> bool:
        """Connect to Redis. Returns True if successful."""
        async with self._connect_lock:
            if self._client is not None and self._available:
                return True

            # Rate limit connection attempts
            now = time.time()
            if now - self._last_check < self._check_interval and not self._available:
                return False
            self._last_check = now

            try:
                # Try importing redis-py async (preferred) or aioredis (legacy)
                redis_module = None
                try:
                    import redis.asyncio as redis_module  # type: ignore
                except ImportError:
                    try:
                        import aioredis as redis_module  # type: ignore
                    except ImportError:
                        logger.debug("[v3.0] Redis client not available (redis/aioredis not installed)")
                        self._available = False
                        return False

                url = f"redis://{self.host}:{self.port}/{self.db}"
                self._client = await redis_module.from_url(
                    url,
                    password=self.password,
                    encoding="utf-8",
                    decode_responses=True,
                )

                # Test connection
                await self._client.ping()
                self._available = True
                logger.info(f"[v3.0] Connected to Redis at {self.host}:{self.port}")
                return True

            except Exception as e:
                logger.debug(f"[v3.0] Redis connection failed: {e}")
                self._available = False
                self._client = None
                return False

    async def disconnect(self) -> None:
        """Disconnect from Redis."""
        async with self._connect_lock:
            if self._client:
                try:
                    await self._client.close()
                except Exception:
                    pass
                self._client = None
                self._available = False

    @property
    def is_available(self) -> bool:
        return self._available and self._client is not None

    async def set_nx(self, key: str, value: str, ttl_seconds: float) -> bool:
        """Set if not exists with TTL (SETNX + EXPIRE)."""
        if not self.is_available or self._client is None:
            return False

        try:
            result = await self._client.set(key, value, nx=True, ex=int(ttl_seconds))
            return result is not None
        except Exception as e:
            logger.debug(f"[v3.0] Redis SETNX failed: {e}")
            self._available = False
            return False

    async def get(self, key: str) -> Optional[str]:
        """Get value by key."""
        if not self.is_available or self._client is None:
            return None

        try:
            result = await self._client.get(key)
            return str(result) if result else None
        except Exception as e:
            logger.debug(f"[v3.0] Redis GET failed: {e}")
            return None

    async def delete(self, key: str) -> bool:
        """Delete key."""
        if not self.is_available or self._client is None:
            return False

        try:
            result = await self._client.delete(key)
            return int(result) > 0
        except Exception as e:
            logger.debug(f"[v3.0] Redis DELETE failed: {e}")
            return False

    async def expire(self, key: str, ttl_seconds: int) -> bool:
        """Set TTL on key."""
        if not self.is_available or self._client is None:
            return False

        try:
            result = await self._client.expire(key, ttl_seconds)
            return bool(result)
        except Exception as e:
            logger.debug(f"[v3.0] Redis EXPIRE failed: {e}")
            return False

    async def incr(self, key: str) -> int:
        """Increment and return new value (for fencing tokens)."""
        if not self.is_available or self._client is None:
            return 0

        try:
            result = await self._client.incr(key)
            return int(result)
        except Exception as e:
            logger.debug(f"[v3.0] Redis INCR failed: {e}")
            return 0


# Global Redis client instance
_redis_client: Optional[AsyncRedisLockClient] = None


def get_redis_client() -> AsyncRedisLockClient:
    """Get the global Redis client instance."""
    global _redis_client
    if _redis_client is None:
        _redis_client = AsyncRedisLockClient()
    return _redis_client


# =============================================================================
# Configuration
# =============================================================================

@dataclass
class LockConfig:
    """
    Configuration for distributed lock manager.

    v96.2: Lock files now use `.dlm.lock` extension to avoid collision with
    flock-based locks from other systems (e.g., AtomicFileWriter in unified_loop_manager.py).

    v3.0: Added backend selection, Redis configuration, and cross-repo settings.

    Lock File Naming:
    - DistributedLockManager: {lock_name}.dlm.lock (JSON metadata)
    - Other systems (flock): {name}.lock (empty flock files)

    This prevents "corrupted lock file" errors when both systems share the same lock directory.
    """
    # Lock directory
    lock_dir: Path = field(default_factory=lambda: Path.home() / ".jarvis" / "cross_repo" / "locks")

    # v96.2: Lock file extension - distinct from flock-based locks
    lock_extension: str = ".dlm.lock"

    # Default lock timeout (how long to wait for lock acquisition)
    default_timeout_seconds: float = DEFAULT_ACQUIRE_TIMEOUT

    # Default lock TTL (how long lock is valid before auto-expiration)
    default_ttl_seconds: float = DEFAULT_LOCK_TTL

    # Retry settings
    retry_delay_seconds: float = DEFAULT_RETRY_INTERVAL

    # Stale lock cleanup settings
    cleanup_enabled: bool = True
    cleanup_interval_seconds: float = 30.0

    # Lock health check
    health_check_enabled: bool = True

    # v3.0: Backend selection
    backend: LockBackend = LockBackend.AUTO  # AUTO tries Redis first, falls back to file

    # v3.0: Redis configuration
    redis_host: str = REDIS_HOST
    redis_port: int = REDIS_PORT
    redis_db: int = REDIS_LOCK_DB
    redis_password: Optional[str] = REDIS_PASSWORD

    # v3.0: Cross-repo identification
    repo_source: str = field(default_factory=lambda: os.getenv("JARVIS_REPO_SOURCE", "jarvis"))

    # v3.0: Keepalive settings (for long-running operations)
    keepalive_enabled: bool = True
    keepalive_interval_seconds: float = KEEPALIVE_INTERVAL

    # v3.0: Fencing token counter file (for file-based backend)
    fencing_counter_file: Path = field(
        default_factory=lambda: Path.home() / ".jarvis" / "cross_repo" / "locks" / ".fencing_counter"
    )


# =============================================================================
# Lock Metadata
# =============================================================================

@dataclass
class LockMetadata:
    """
    v3.0: Enhanced metadata stored in lock file.

    CRITICAL FIX for Problem 19 (Startup lock not released on crash):
    - Added process_start_time for PID reuse detection
    - Added process_name and process_cmdline for identity validation
    - Added machine_id for distributed environments
    - Enhanced stale detection with multiple signals

    v3.0 Additions:
    - backend: Which backend was used to acquire the lock (redis/file)
    - fencing_token: Monotonically increasing token for ordering
    - repo_source: Which repo acquired the lock (jarvis/jarvis-prime/reactor-core)
    - extensions: Number of times the lock TTL was extended

    Lock File Format (v3.0):
    {
        "acquired_at": 1736895345.123,
        "expires_at": 1736895355.123,
        "owner": "jarvis-12345-1736895300.5",
        "token": "f47ac10b-58cc-4372-a567...",
        "lock_name": "vbia_events",
        "process_start_time": 1736895300.5,
        "process_name": "python3",
        "process_cmdline": "python3 -m ...",
        "machine_id": "darwin-hostname",
        "backend": "redis",                   # v3.0
        "fencing_token": 42,                  # v3.0
        "repo_source": "jarvis",              # v3.0
        "extensions": 0                       # v3.0
    }
    """
    acquired_at: float  # Timestamp when lock was acquired
    expires_at: float  # Timestamp when lock expires (acquired_at + TTL)
    owner: str  # Process identifier (e.g., "jarvis-{pid}-{start_time}")
    token: str  # Unique token for this lock instance
    lock_name: str  # Name of the locked resource

    # v96.0: Enhanced process tracking for PID reuse detection
    process_start_time: float = 0.0    # Process creation timestamp
    process_name: str = ""              # Process name (e.g., "python3")
    process_cmdline: str = ""           # Command line (truncated)
    machine_id: str = ""                # Machine identifier

    # v3.0: Cross-repo and backend tracking
    backend: str = "file"               # Which backend: "redis" or "file"
    fencing_token: int = 0              # Monotonically increasing token
    repo_source: str = "jarvis"         # Which repo: jarvis, jarvis-prime, reactor-core
    extensions: int = 0                 # Number of TTL extensions

    def is_expired(self) -> bool:
        """Check if lock has expired."""
        return time.time() >= self.expires_at

    def is_stale(self) -> bool:
        """Check if lock is stale (expired for significant time)."""
        return time.time() >= self.expires_at + 5.0

    def time_remaining(self) -> float:
        """Get remaining time before expiration (negative if expired)."""
        return self.expires_at - time.time()

    def get_pid(self) -> int:
        """
        v96.0: Extract PID from owner string.

        Supports both old format "jarvis-{pid}" and new format "jarvis-{pid}-{start_time}".
        """
        try:
            parts = self.owner.split("-")
            if len(parts) >= 2:
                return int(parts[1])
        except (ValueError, IndexError):
            pass
        return 0

    def get_owner_start_time(self) -> float:
        """
        v96.0: Extract process start time from owner string.

        Returns 0.0 if using old format without start time.
        """
        try:
            parts = self.owner.split("-")
            if len(parts) >= 3:
                return float(parts[2])
        except (ValueError, IndexError):
            pass
        # Fall back to stored process_start_time
        return self.process_start_time


# =============================================================================
# Distributed Lock Manager
# =============================================================================

class DistributedLockManager:
    """
    v3.0: Production-grade distributed lock manager for cross-repo coordination.

    Features:
    - Automatic lock expiration (TTL-based)
    - Stale lock detection and cleanup
    - Deadlock prevention
    - Lock renewal support
    - Process-safe across multiple repos
    - v93.0: Aggressive pre-cleanup on initialization
    - v93.0: PID validation for stale lock detection
    - v93.0: Faster acquisition with optimistic locking
    - v96.0: Process start time tracking for PID reuse detection (Problem 19 fix)
    - v96.0: Process fingerprint in lock metadata
    - v96.0: Machine ID tracking for distributed environments
    - v96.0: Enhanced owner ID format: "jarvis-{pid}-{start_time}"
    - v3.0: Redis backend support with automatic fallback
    - v3.0: Fencing tokens for monotonic ordering
    - v3.0: Lock keepalive for long-running operations
    - v3.0: Cross-repo identification (jarvis/jarvis-prime/reactor-core)

    CRITICAL FIX (v96.0) - Problem 19: Startup lock not released on crash
    ════════════════════════════════════════════════════════════════════════
    Before: Process crash → PID reused → stale lock appears valid → deadlock
    After: Process start time validated → PID reuse detected → stale lock cleaned

    v3.0 CROSS-REPO UNIFICATION:
    ════════════════════════════════════════════════════════════════════════
    Before: Each repo had its own locking mechanism
    After: Unified lock manager with Redis (distributed) + File (fallback)
    """

    def __init__(self, config: Optional[LockConfig] = None):
        """Initialize distributed lock manager."""
        self.config = config or LockConfig()
        self._cleanup_task: Optional[asyncio.Task] = None

        # v96.0: Enhanced owner ID with process start time for PID reuse detection
        self._process_start_time = self._get_own_process_start_time()
        self._owner_id = f"{self.config.repo_source}-{os.getpid()}-{self._process_start_time:.1f}"
        self._process_name = self._get_own_process_name()
        self._process_cmdline = self._get_own_process_cmdline()
        self._machine_id = self._get_machine_id()

        self._initialized = False
        self._init_lock = asyncio.Lock()

        # v2.0: Per-lock-name semaphores to prevent concurrent write collisions
        # Within the same process, only one task can write to a specific lock at a time
        self._lock_semaphores: Dict[str, asyncio.Semaphore] = {}
        self._semaphore_lock = asyncio.Lock()

        # v2.0: Counter for unique temp file naming (backup to UUID)
        self._write_counter = 0

        # v3.0: Backend selection state
        self._active_backend: LockBackend = LockBackend.FILE
        self._redis_client: Optional[AsyncRedisLockClient] = None
        self._redis_available = False

        # v3.0: Fencing token counter (monotonically increasing)
        self._fencing_token_counter = 0
        self._fencing_lock = asyncio.Lock()

        # v3.0: Active keepalive tasks (lock_name -> task)
        self._keepalive_tasks: Dict[str, asyncio.Task] = {}

        # v3.0: Callbacks for lock events
        self._on_lock_acquired: List[Callable[[str, LockMetadata], None]] = []
        self._on_lock_released: List[Callable[[str], None]] = []
        self._on_lock_failed: List[Callable[[str, str], None]] = []

        logger.info(
            f"Distributed Lock Manager v3.0 initialized "
            f"(owner: {self._owner_id}, backend: {self.config.backend.value})"
        )

    def _get_own_process_start_time(self) -> float:
        """v96.0: Get our own process start time for owner ID."""
        try:
            import psutil
            return psutil.Process(os.getpid()).create_time()
        except Exception:
            return time.time()  # Fallback to current time

    def _get_own_process_name(self) -> str:
        """v96.0: Get our own process name for lock metadata."""
        try:
            import psutil
            return psutil.Process(os.getpid()).name()
        except Exception:
            return "unknown"

    def _get_own_process_cmdline(self) -> str:
        """v96.0: Get our own command line for lock metadata (truncated)."""
        try:
            import psutil
            cmdline = psutil.Process(os.getpid()).cmdline()
            return " ".join(cmdline)[:500] if cmdline else ""
        except Exception:
            return ""

    def _get_machine_id(self) -> str:
        """v96.0: Get machine identifier for distributed environments."""
        try:
            import platform
            import socket
            return f"{platform.system().lower()}-{socket.gethostname()}"
        except Exception:
            return "unknown"

    async def _get_lock_semaphore(self, lock_name: str) -> asyncio.Semaphore:
        """
        v2.0: Get or create a semaphore for a specific lock name.

        This ensures only one async task within this process can write
        to a particular lock file at a time, preventing the race condition
        where multiple tasks use the same temp file path.
        """
        async with self._semaphore_lock:
            if lock_name not in self._lock_semaphores:
                self._lock_semaphores[lock_name] = asyncio.Semaphore(1)
            return self._lock_semaphores[lock_name]

    def _generate_temp_file_path(self, lock_file: Path, token: str) -> Path:
        """
        v2.0: Generate a truly unique temp file path.

        Uses UUID + PID + counter to guarantee uniqueness even with:
        - Multiple async tasks in same process
        - Multiple processes with same PID (after restart)
        - Rapid consecutive writes

        Format: {lock_file}.tmp.{pid}.{counter}.{token_prefix}
        Example: vbia_state.dlm.lock.tmp.12345.1.f47ac10b
        """
        self._write_counter += 1
        token_prefix = token[:8] if len(token) >= 8 else token
        return lock_file.parent / f"{lock_file.name}.tmp.{os.getpid()}.{self._write_counter}.{token_prefix}"

    async def initialize(self) -> None:
        """
        v2.0: Initialize lock manager with aggressive pre-cleanup.

        This method ensures any stale locks from crashed processes are
        cleaned up BEFORE we start operations, preventing the common
        "Lock acquisition timeout" issue.

        v2.0 Enhancements:
        - Cleans up orphaned temp files from crashed/interrupted writes
        - Validates lock directory permissions
        """
        async with self._init_lock:
            if self._initialized:
                return

            # Create lock directory
            try:
                await aiofiles.os.makedirs(self.config.lock_dir, exist_ok=True)
                logger.info(f"Lock directory initialized: {self.config.lock_dir}")
            except Exception as e:
                logger.error(f"Failed to create lock directory: {e}")
                raise

            # v2.0: Clean up orphaned temp files from crashed/interrupted writes
            temp_cleaned = await self._cleanup_orphaned_temp_files()
            if temp_cleaned > 0:
                logger.info(f"[v2.0] Cleaned {temp_cleaned} orphaned temp file(s) at startup")

            # v93.0: CRITICAL - Aggressive pre-cleanup of stale locks
            # This runs SYNCHRONOUSLY before anything else to ensure clean state
            cleaned = await self._aggressive_stale_cleanup()
            if cleaned > 0:
                logger.info(f"[v93.0] Pre-cleaned {cleaned} stale lock(s) at startup")

            # Start cleanup task
            if self.config.cleanup_enabled:
                self._cleanup_task = asyncio.create_task(
                    self._cleanup_loop(),
                    name="lock_cleanup_loop"
                )
                logger.info("Stale lock cleanup task started")

            self._initialized = True

    async def _cleanup_orphaned_temp_files(self) -> int:
        """
        v2.0: Clean up orphaned temp files from crashed/interrupted writes.

        Temp files have the pattern: *.tmp.{pid}.{counter}.{token}
        We clean up any that:
        1. Are older than 60 seconds (stale)
        2. OR belong to a dead process

        Returns:
            Number of temp files cleaned
        """
        cleaned = 0

        try:
            if not await aiofiles.os.path.exists(self.config.lock_dir):
                return 0

            # Find all temp files
            all_files = await aiofiles.os.listdir(self.config.lock_dir)
            temp_files = [f for f in all_files if '.tmp.' in f]

            current_time = time.time()

            for temp_name in temp_files:
                temp_path = self.config.lock_dir / temp_name

                try:
                    stat_info = await aiofiles.os.stat(temp_path)
                    age_seconds = current_time - stat_info.st_mtime

                    should_remove = False
                    reason = ""

                    # Check 1: File is older than 60 seconds (definitely stale)
                    if age_seconds > 60:
                        should_remove = True
                        reason = f"stale ({age_seconds:.0f}s old)"

                    # Check 2: Try to extract PID and check if process is alive
                    else:
                        # Format: {name}.tmp.{pid}.{counter}.{token}
                        parts = temp_name.split('.tmp.')
                        if len(parts) >= 2:
                            remainder = parts[1]  # {pid}.{counter}.{token}
                            pid_parts = remainder.split('.')
                            if len(pid_parts) >= 1:
                                try:
                                    temp_pid = int(pid_parts[0])
                                    # Check if this PID is alive
                                    try:
                                        os.kill(temp_pid, 0)
                                        # Process exists - don't remove unless very old
                                        if age_seconds > 10:
                                            # 10 seconds is long for a write - likely stuck
                                            should_remove = True
                                            reason = f"stuck write ({age_seconds:.0f}s)"
                                    except ProcessLookupError:
                                        should_remove = True
                                        reason = f"owner process {temp_pid} dead"
                                    except PermissionError:
                                        # Process exists, can't check - leave it
                                        pass
                                except ValueError:
                                    # Can't parse PID - remove if old
                                    if age_seconds > 30:
                                        should_remove = True
                                        reason = "unparseable and old"

                    if should_remove:
                        await aiofiles.os.remove(temp_path)
                        logger.debug(f"[v2.0] Cleaned temp file: {temp_name} ({reason})")
                        cleaned += 1

                except FileNotFoundError:
                    pass  # Already gone
                except Exception as e:
                    logger.debug(f"[v2.0] Error checking temp file {temp_name}: {e}")

        except Exception as e:
            logger.error(f"[v2.0] Error in temp file cleanup: {e}")

        return cleaned

    async def shutdown(self) -> None:
        """Shutdown lock manager and cleanup resources."""
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass

        logger.info("Distributed Lock Manager shut down")

    # =========================================================================
    # Lock Acquisition
    # =========================================================================

    @asynccontextmanager
    async def acquire(
        self,
        lock_name: str,
        timeout: Optional[float] = None,
        ttl: Optional[float] = None
    ) -> AsyncIterator[bool]:
        """
        Acquire distributed lock with automatic expiration.

        Args:
            lock_name: Name of the lock (e.g., "vbia_events", "prime_state")
            timeout: Max time to wait for lock acquisition (seconds)
            ttl: Lock time-to-live - auto-expires after this duration (seconds)

        Yields:
            bool: True if lock acquired, False if timeout

        Example:
            async with lock_manager.acquire("my_resource", timeout=5.0, ttl=10.0) as acquired:
                if acquired:
                    # Critical section - you have the lock
                    await do_work()
                else:
                    # Could not acquire lock
                    logger.warning("Lock acquisition failed")
        """
        timeout = timeout or self.config.default_timeout_seconds
        ttl = ttl or self.config.default_ttl_seconds

        # v96.2: Use .dlm.lock extension to avoid collision with flock-based locks
        lock_file = self.config.lock_dir / f"{lock_name}{self.config.lock_extension}"
        token = str(uuid4())
        acquired = False

        start_time = time.time()

        attempt = 0
        try:
            # Try to acquire lock with timeout
            while time.time() - start_time < timeout:
                if await self._try_acquire_lock(lock_file, lock_name, token, ttl):
                    acquired = True
                    logger.debug(f"Lock acquired: {lock_name} (token: {token[:8]}...)")
                    break

                # v2.0: Wait before retry with jitter to reduce contention
                # Jitter prevents multiple processes from retrying at exactly the same time
                attempt += 1
                base_delay = self.config.retry_delay_seconds
                # Add jitter: up to 50% random variation
                jitter = random.uniform(0, base_delay * 0.5)
                # Small exponential component for sustained contention (capped)
                exp_factor = min(1.5, 1.0 + (attempt * 0.1))
                delay = (base_delay * exp_factor) + jitter
                await asyncio.sleep(delay)

            if not acquired:
                logger.warning(f"Lock acquisition timeout: {lock_name} (waited {timeout}s, {attempt} attempts)")

            # Yield control to caller
            yield acquired

        finally:
            # Always release lock when exiting context
            if acquired:
                await self._release_lock(lock_file, token)
                logger.debug(f"Lock released: {lock_name} (token: {token[:8]}...)")

    async def _try_acquire_lock(
        self,
        lock_file: Path,
        lock_name: str,
        token: str,
        ttl: float
    ) -> bool:
        """
        v93.0: Try to acquire lock atomically with dead process detection.

        Enhancements:
        - Immediately removes locks from expired TTL
        - Immediately removes locks from dead processes (no waiting!)
        - Optimistic locking with verification

        Returns:
            True if lock acquired, False otherwise
        """
        try:
            # Check if lock file exists
            if await aiofiles.os.path.exists(lock_file):
                # Read existing lock metadata
                existing_lock = await self._read_lock_metadata(lock_file)

                if existing_lock:
                    should_remove = False
                    remove_reason = ""

                    # Check 1: Lock is expired
                    if existing_lock.is_expired():
                        should_remove = True
                        remove_reason = f"expired {-existing_lock.time_remaining():.1f}s ago"

                    # v93.0: Check 2: Owner process is dead - DON'T WAIT!
                    elif not self._is_process_alive(existing_lock.owner):
                        should_remove = True
                        remove_reason = f"owner process {existing_lock.owner} is dead"

                    if should_remove:
                        logger.info(
                            f"[v93.0] Removing lock: {lock_name} ({remove_reason})"
                        )
                        await self._remove_lock_file(lock_file)
                    else:
                        # Lock is still valid AND owner is alive
                        logger.debug(
                            f"Lock held by another process: {lock_name} "
                            f"(owner: {existing_lock.owner}, expires in {existing_lock.time_remaining():.1f}s)"
                        )
                        return False

            # v96.0: Create new lock with enhanced metadata for PID reuse detection
            now = time.time()
            metadata = LockMetadata(
                acquired_at=now,
                expires_at=now + ttl,
                owner=self._owner_id,
                token=token,
                lock_name=lock_name,
                # v96.0: Process fingerprint for identity validation
                process_start_time=self._process_start_time,
                process_name=self._process_name,
                process_cmdline=self._process_cmdline,
                machine_id=self._machine_id,
            )

            # Write lock file atomically (with fsync for durability)
            await self._write_lock_metadata(lock_file, metadata)

            # v93.0: Verify we actually got the lock with exponential backoff
            # This handles slow filesystems (NFS, cloud storage) that may have delays
            verification_delays = [0.005, 0.01, 0.02, 0.05]  # Exponential backoff
            for delay in verification_delays:
                await asyncio.sleep(delay)
                verify_lock = await self._read_lock_metadata(lock_file)

                if verify_lock is None:
                    # Lock file disappeared - race condition, we lost
                    logger.debug(f"Lock file disappeared during verification: {lock_name}")
                    return False

                if verify_lock.token == token:
                    # We successfully own the lock
                    return True

                # Someone else's token - continue checking in case of read timing issue
                continue

            # After all retries, still not our token - we lost the race
            logger.debug(f"Lock race condition detected: {lock_name} (lost to another process)")
            return False

        except Exception as e:
            logger.error(f"Error acquiring lock {lock_name}: {e}")
            return False

    async def _release_lock(self, lock_file: Path, token: str) -> None:
        """
        Release lock if we own it.
        
        v2.1.0: Improved logging - only warns for actual race conditions,
        not for natural lock expiration (which is expected behavior).

        Args:
            lock_file: Path to lock file
            token: Our lock token
        """
        try:
            # Read current lock
            current_lock = await self._read_lock_metadata(lock_file)

            if current_lock is None:
                # v2.1.0: Lock file doesn't exist - this is normal if:
                # - Lock expired and was cleaned up by background task
                # - Lock was already released
                # This is NOT a warning condition - it's expected behavior
                logger.debug(
                    f"Lock already released or expired: {lock_file.name} "
                    f"(token: {token[:8]}... - no action needed)"
                )
            elif current_lock.token == token:
                # We own this lock, remove it
                await self._remove_lock_file(lock_file)
            else:
                # v2.1.0: Someone else owns the lock - this IS concerning
                # It indicates a potential race condition or TTL issue
                # where our lock expired and someone else acquired it
                logger.warning(
                    f"Cannot release lock - owned by different token: {lock_file.name} "
                    f"(our token: {token[:8]}..., current owner: {current_lock.token[:8]}..., "
                    f"time remaining: {current_lock.time_remaining():.1f}s)"
                )
        except Exception as e:
            logger.error(f"Error releasing lock {lock_file.name}: {e}")

    # =========================================================================
    # Lock Metadata I/O
    # =========================================================================

    async def _read_lock_metadata(self, lock_file: Path) -> Optional[LockMetadata]:
        """
        v96.1: Read and parse lock metadata from file with robust error handling.

        Handles both old format (without v96.0 fields) and new format (with process fingerprint).

        v96.1 Enhancements:
        - Pre-check for empty files before JSON parsing
        - Better error categorization for diagnostics
        - More resilient against filesystem race conditions
        """
        try:
            # v96.1: Check file size before reading to detect empty/corrupted files early
            try:
                file_size = (await aiofiles.os.stat(lock_file)).st_size
                if file_size == 0:
                    logger.debug(f"Empty lock file detected: {lock_file} (will remove)")
                    await self._remove_lock_file(lock_file)
                    return None
            except FileNotFoundError:
                return None
            except OSError:
                pass  # Fall through to normal read, will handle errors there

            async with aiofiles.open(lock_file, 'r') as f:
                data = await f.read()

                # v96.1: Additional empty check after read (race condition protection)
                if not data or not data.strip():
                    logger.debug(f"Lock file has no content: {lock_file} (will remove)")
                    await self._remove_lock_file(lock_file)
                    return None

                metadata_dict = json.loads(data)

                # v96.0: Handle legacy lock files without new fields
                # Set default values for missing fields to maintain backward compatibility
                if "process_start_time" not in metadata_dict:
                    metadata_dict["process_start_time"] = 0.0
                if "process_name" not in metadata_dict:
                    metadata_dict["process_name"] = ""
                if "process_cmdline" not in metadata_dict:
                    metadata_dict["process_cmdline"] = ""
                if "machine_id" not in metadata_dict:
                    metadata_dict["machine_id"] = ""

                return LockMetadata(**metadata_dict)
        except FileNotFoundError:
            return None
        except json.JSONDecodeError:
            logger.error(f"Corrupted lock file: {lock_file} (will remove)")
            await self._remove_lock_file(lock_file)
            return None
        except TypeError as e:
            # v96.0: Handle unknown fields from newer lock format gracefully
            logger.debug(f"Lock file has unknown fields: {lock_file} - {e}")
            try:
                # Try reading with only known fields
                async with aiofiles.open(lock_file, 'r') as f:
                    data = await f.read()
                    metadata_dict = json.loads(data)
                    # Extract only the fields we know about
                    return LockMetadata(
                        acquired_at=metadata_dict.get("acquired_at", 0.0),
                        expires_at=metadata_dict.get("expires_at", 0.0),
                        owner=metadata_dict.get("owner", ""),
                        token=metadata_dict.get("token", ""),
                        lock_name=metadata_dict.get("lock_name", ""),
                        process_start_time=metadata_dict.get("process_start_time", 0.0),
                        process_name=metadata_dict.get("process_name", ""),
                        process_cmdline=metadata_dict.get("process_cmdline", ""),
                        machine_id=metadata_dict.get("machine_id", ""),
                    )
            except Exception:
                return None
        except Exception as e:
            logger.error(f"Error reading lock metadata {lock_file}: {e}")
            return None

    async def _write_lock_metadata(self, lock_file: Path, metadata: LockMetadata) -> None:
        """
        v2.0: Write lock metadata to file atomically with enhanced durability.

        CRITICAL FIX (v2.0): Uses UUID-based temp files + per-lock semaphores
        to prevent race conditions when multiple async tasks try to write
        to the same lock concurrently.

        Previous bug: All tasks used temp_file = {lock}.tmp.{pid}
        Fix: Each task uses temp_file = {lock}.tmp.{pid}.{counter}.{token}

        v1.1: Ensures parent directory exists before writing.
        v93.0: Added fsync to ensure filesystem consistency before verification.
        v96.1: Added content verification after write to detect silent corruption.
        v2.0: Per-lock semaphore + unique temp file path per write attempt.
        """
        max_retries = 3
        last_error: Optional[Exception] = None

        # v2.0: Get semaphore for this specific lock to serialize writes
        # This prevents multiple async tasks from stepping on each other's temp files
        lock_semaphore = await self._get_lock_semaphore(metadata.lock_name)

        async with lock_semaphore:
            for attempt in range(max_retries):
                # v2.0: Generate truly unique temp file path for this attempt
                temp_file = self._generate_temp_file_path(lock_file, metadata.token)

                try:
                    # Ensure directory exists (resilient to race conditions)
                    lock_dir = lock_file.parent
                    try:
                        await aiofiles.os.makedirs(lock_dir, exist_ok=True)
                    except FileExistsError:
                        pass  # Another process created it - that's fine

                    # v96.1: Serialize the content first to catch any serialization errors
                    content = json.dumps(asdict(metadata), indent=2)
                    content_bytes = content.encode('utf-8')
                    expected_size = len(content_bytes)

                    if not content or expected_size < 10:
                        raise ValueError(f"Invalid metadata serialization: {content[:50]}")

                    try:
                        # v2.0: Write content in binary mode for exact size control
                        async with aiofiles.open(temp_file, 'wb') as f:
                            await f.write(content_bytes)
                            await f.flush()
                            # v93.0: Force write to disk before rename
                            os.fsync(f.fileno())

                        # v96.1: Verify temp file was written correctly before rename
                        try:
                            temp_stat = await aiofiles.os.stat(temp_file)
                            temp_size = temp_stat.st_size
                            if temp_size != expected_size:
                                raise IOError(
                                    f"Temp file size mismatch: expected {expected_size}, got {temp_size}"
                                )
                        except FileNotFoundError:
                            # Another process/task may have already renamed it - rare but possible
                            raise IOError(f"Temp file disappeared: {temp_file}")

                        # Atomic rename
                        await aiofiles.os.rename(temp_file, lock_file)

                    except Exception:
                        # Clean up temp file on failure
                        try:
                            await aiofiles.os.remove(temp_file)
                        except (FileNotFoundError, OSError):
                            pass
                        raise

                    # v93.0: Sync directory to ensure rename is durable
                    try:
                        dir_fd = os.open(str(lock_dir), os.O_RDONLY | os.O_DIRECTORY)
                        try:
                            os.fsync(dir_fd)
                        finally:
                            os.close(dir_fd)
                    except (OSError, AttributeError):
                        # O_DIRECTORY not supported on some platforms - fall back to brief sleep
                        await asyncio.sleep(0.001)

                    # v96.1: Verify final file was written correctly
                    final_size = (await aiofiles.os.stat(lock_file)).st_size
                    if final_size == 0:
                        raise IOError("Lock file is empty after write - filesystem issue detected")

                    # Success - exit retry loop
                    return

                except Exception as e:
                    last_error = e
                    if attempt < max_retries - 1:
                        # v2.0: Exponential backoff with jitter to reduce contention
                        base_delay = 0.01 * (2 ** attempt)  # 0.01, 0.02, 0.04
                        jitter = random.uniform(0, base_delay * 0.5)  # Up to 50% jitter
                        delay = base_delay + jitter
                        logger.warning(
                            f"Lock write attempt {attempt + 1} failed: {e}, retrying in {delay:.3f}s..."
                        )
                        await asyncio.sleep(delay)
                    continue

            # All retries failed
            logger.error(f"Error writing lock metadata {lock_file} after {max_retries} attempts: {last_error}")
            raise last_error or IOError(f"Failed to write lock file: {lock_file}")

    async def _remove_lock_file(self, lock_file: Path) -> None:
        """
        Remove lock file safely with race condition handling.

        v1.1: Made robust against TOCTOU race conditions - if file disappears
        between exists check and remove, we treat it as successful removal.
        """
        try:
            await aiofiles.os.remove(lock_file)
        except FileNotFoundError:
            # File already gone - treat as successful removal (race condition handled)
            logger.debug(f"Lock file already removed (race condition OK): {lock_file}")
        except OSError as e:
            # Check if it's "No such file or directory" error
            import errno
            if e.errno == errno.ENOENT:
                logger.debug(f"Lock file already removed: {lock_file}")
            else:
                logger.error(f"Error removing lock file {lock_file}: {e}")

    # =========================================================================
    # v96.0: Advanced Cleanup with PID Reuse Detection
    # =========================================================================

    def _is_process_alive(
        self,
        owner_id: str,
        lock_metadata: Optional[LockMetadata] = None,
    ) -> bool:
        """
        v96.0: Check if the process that owns a lock is still alive.

        CRITICAL FIX for Problem 19 (PID reuse detection):
        - Extracts PID from owner ID
        - Validates process start time against stored value
        - Detects PID reuse and treats as stale lock

        Args:
            owner_id: Lock owner identifier (format: "jarvis-{pid}-{start_time}")
            lock_metadata: Optional metadata for additional validation

        Returns:
            True if process is alive AND matches stored identity
            False if dead, PID reused, or unable to determine

        Validation Phases:
        ┌─────────────────────────────────────────────────────────────────────┐
        │ Phase 1: PID Extraction                                             │
        │   - Parse owner_id for PID                                          │
        │   - Handle both old "jarvis-{pid}" and new "jarvis-{pid}-{start}"  │
        ├─────────────────────────────────────────────────────────────────────┤
        │ Phase 2: Process Existence Check                                    │
        │   - Use signal 0 to check if PID exists                            │
        │   - Handle permission errors                                        │
        ├─────────────────────────────────────────────────────────────────────┤
        │ Phase 3: PID Reuse Detection (v96.0 CRITICAL FIX)                  │
        │   - Compare process start time with stored value                    │
        │   - If mismatch > 1s, PID was reused → treat as stale             │
        │   - Optionally validate process name/cmdline                        │
        └─────────────────────────────────────────────────────────────────────┘
        """
        import psutil

        try:
            # ═══════════════════════════════════════════════════════════════
            # Phase 1: PID Extraction
            # ═══════════════════════════════════════════════════════════════
            if "-" not in owner_id:
                logger.debug(f"Cannot extract PID from owner_id: {owner_id}")
                return True  # Can't validate, assume alive (conservative)

            parts = owner_id.split("-")

            # Extract PID (always second part)
            try:
                pid = int(parts[1]) if len(parts) >= 2 else 0
            except ValueError:
                logger.debug(f"Invalid PID in owner_id: {owner_id}")
                return False  # Invalid PID format = dead process

            if pid <= 0:
                logger.debug(f"Invalid PID value: {pid}")
                return False  # Invalid PID = dead process

            # v96.0: Extract stored start time from owner_id (third part)
            stored_start_time = 0.0
            if len(parts) >= 3:
                try:
                    stored_start_time = float(parts[2])
                except ValueError:
                    pass

            # Fall back to metadata if owner_id doesn't have start time
            if stored_start_time == 0.0 and lock_metadata:
                stored_start_time = lock_metadata.process_start_time

            # ═══════════════════════════════════════════════════════════════
            # Phase 2: Process Existence Check
            # ═══════════════════════════════════════════════════════════════
            try:
                os.kill(pid, 0)
                # Process exists - continue to Phase 3
            except ProcessLookupError:
                # Process doesn't exist - definitely dead
                logger.debug(f"[v96.0] Process {pid} does not exist (ProcessLookupError)")
                return False
            except PermissionError:
                # Process exists but we can't signal it (different user)
                # Continue to Phase 3 for start time validation
                pass
            except OSError as e:
                import errno
                if e.errno == errno.ESRCH:  # No such process
                    return False
                elif e.errno == errno.EPERM:  # Permission denied
                    pass  # Process exists, continue to Phase 3
                else:
                    logger.debug(f"OSError checking PID {pid}: {e}")
                    return False  # Conservative: assume dead

            # ═══════════════════════════════════════════════════════════════
            # Phase 3: PID Reuse Detection (v96.0 CRITICAL FIX)
            # ═══════════════════════════════════════════════════════════════
            if stored_start_time > 0.0:
                try:
                    process = psutil.Process(pid)
                    current_start_time = process.create_time()

                    # Allow 1 second tolerance for timing differences
                    time_diff = abs(current_start_time - stored_start_time)
                    if time_diff > 1.0:
                        logger.info(
                            f"[v96.0] PID REUSE DETECTED for {owner_id}: "
                            f"stored_start={stored_start_time:.1f}, "
                            f"current_start={current_start_time:.1f}, "
                            f"diff={time_diff:.1f}s"
                        )
                        return False  # PID was reused → stale lock

                    # v96.0: Optional process name validation
                    if lock_metadata and lock_metadata.process_name:
                        current_name = process.name()
                        if current_name != lock_metadata.process_name:
                            logger.info(
                                f"[v96.0] Process NAME MISMATCH for PID {pid}: "
                                f"expected={lock_metadata.process_name}, "
                                f"actual={current_name}"
                            )
                            return False  # Different process → stale lock

                except psutil.NoSuchProcess:
                    logger.debug(f"[v96.0] Process {pid} gone during validation")
                    return False
                except psutil.AccessDenied:
                    # Can't validate start time, assume process is valid
                    logger.debug(f"[v96.0] Access denied validating PID {pid}")
                    pass
                except Exception as e:
                    logger.debug(f"[v96.0] Error validating PID {pid}: {e}")
                    pass

            # Process exists and passed all validation checks
            return True

        except Exception as e:
            # v93.0: Log unexpected errors instead of silently assuming alive
            logger.warning(f"Unexpected error checking process {owner_id}: {e}")
            # Conservative approach: assume dead to prevent stale lock accumulation
            return False

    def _validate_lock_owner_identity(
        self,
        metadata: LockMetadata,
    ) -> Tuple[bool, str]:
        """
        v96.0: Comprehensive validation of lock owner identity.

        This is used during lock acquisition to determine if an existing lock
        is still valid or should be removed as stale.

        Args:
            metadata: Lock metadata to validate

        Returns:
            Tuple of (is_valid: bool, reason: str)
        """
        import psutil

        pid = metadata.get_pid()
        if pid <= 0:
            return False, "invalid_pid"

        # Check if process exists
        try:
            process = psutil.Process(pid)
            if not process.is_running():
                return False, "process_not_running"

            if process.status() == psutil.STATUS_ZOMBIE:
                return False, "zombie_process"

        except psutil.NoSuchProcess:
            return False, "process_not_found"
        except psutil.AccessDenied:
            # Process exists but can't inspect - assume valid
            return True, "access_denied_assumed_valid"

        # Validate start time
        stored_start_time = metadata.get_owner_start_time()
        if stored_start_time > 0.0:
            try:
                current_start_time = process.create_time()
                time_diff = abs(current_start_time - stored_start_time)
                if time_diff > 1.0:
                    return False, f"pid_reused:time_diff={time_diff:.1f}s"
            except (psutil.AccessDenied, psutil.NoSuchProcess):
                pass

        # Validate process name
        if metadata.process_name:
            try:
                current_name = process.name()
                if current_name != metadata.process_name:
                    return False, f"name_mismatch:{metadata.process_name}!={current_name}"
            except (psutil.AccessDenied, psutil.NoSuchProcess):
                pass

        # Validate machine ID (for distributed setups)
        if metadata.machine_id and metadata.machine_id != self._machine_id:
            # Different machine - can't validate PID, but lock may be valid
            # Check TTL instead
            if metadata.is_expired():
                return False, "expired_remote_lock"
            # Assume valid if not expired (distributed scenario)
            return True, "remote_lock_not_expired"

        return True, "identity_verified"

    async def _aggressive_stale_cleanup(self) -> int:
        """
        v93.0: Aggressive cleanup of stale locks at startup.

        This is more aggressive than the normal cleanup:
        1. Cleans ALL expired locks (not just stale ones)
        2. Validates PIDs for locks that aren't expired yet
        3. Removes locks from dead processes immediately

        This prevents the "Lock acquisition timeout" issue when starting
        after a crash.

        Returns:
            Number of locks cleaned
        """
        cleaned_count = 0

        try:
            if not await aiofiles.os.path.exists(self.config.lock_dir):
                return 0

            # v96.2: Only process our .dlm.lock files, ignore other lock types
            ext = self.config.lock_extension
            lock_files = [
                f for f in await aiofiles.os.listdir(self.config.lock_dir)
                if f.endswith(ext)
            ]

            for lock_file_name in lock_files:
                lock_file = self.config.lock_dir / lock_file_name
                metadata = await self._read_lock_metadata(lock_file)

                if not metadata:
                    # Corrupted lock file - remove it
                    await self._remove_lock_file(lock_file)
                    cleaned_count += 1
                    continue

                should_clean = False
                reason = ""

                # Check 1: Lock is expired
                if metadata.is_expired():
                    should_clean = True
                    reason = f"expired {-metadata.time_remaining():.1f}s ago"

                # Check 2: Owner process is dead (even if not expired yet)
                # v96.0: Pass metadata for enhanced PID reuse detection
                elif not self._is_process_alive(metadata.owner, metadata):
                    should_clean = True
                    reason = f"owner process dead (was {metadata.owner})"

                # Check 3: v96.0 Enhanced - Validate lock owner identity comprehensively
                else:
                    is_valid, validation_reason = self._validate_lock_owner_identity(metadata)
                    if not is_valid:
                        should_clean = True
                        reason = f"identity validation failed: {validation_reason}"
                    # Also check for our own stale lock from previous run
                    elif metadata.owner == self._owner_id:
                        # Same owner ID as us - check if this is truly our lock
                        # or a stale lock from a previous run that got same PID + start_time collision
                        if metadata.acquired_at < time.time() - 60:  # More than 1 min old
                            # If lock was acquired more than 60s ago but we just started,
                            # this is definitely stale
                            should_clean = True
                            reason = "own stale lock from previous run"

                if should_clean:
                    logger.info(
                        f"[v93.0] Cleaning lock: {metadata.lock_name} ({reason})"
                    )
                    await self._remove_lock_file(lock_file)
                    cleaned_count += 1

        except Exception as e:
            logger.error(f"[v93.0] Error in aggressive cleanup: {e}")

        return cleaned_count

    async def _cleanup_loop(self) -> None:
        """Background task to clean up stale locks."""
        logger.info("Stale lock cleanup loop started")

        while True:
            try:
                await asyncio.sleep(self.config.cleanup_interval_seconds)
                await self._cleanup_stale_locks()
            except asyncio.CancelledError:
                logger.info("Stale lock cleanup loop cancelled")
                break
            except Exception as e:
                logger.error(f"Error in cleanup loop: {e}", exc_info=True)

    async def _cleanup_stale_locks(self) -> None:
        """
        v93.0: Enhanced cleanup with PID validation.

        Cleans locks that are:
        1. Past their TTL (expired/stale)
        2. Owned by dead processes (PID validation)
        """
        try:
            if not await aiofiles.os.path.exists(self.config.lock_dir):
                return

            # v96.2: Only process our .dlm.lock files, ignore other lock types
            ext = self.config.lock_extension
            lock_files = [
                f for f in await aiofiles.os.listdir(self.config.lock_dir)
                if f.endswith(ext)
            ]

            cleaned_count = 0

            for lock_file_name in lock_files:
                lock_file = self.config.lock_dir / lock_file_name
                metadata = await self._read_lock_metadata(lock_file)

                if not metadata:
                    continue

                should_clean = False

                # Check TTL expiration
                if metadata.is_stale():
                    logger.warning(
                        f"Cleaning stale lock: {metadata.lock_name} "
                        f"(owner: {metadata.owner}, expired {-metadata.time_remaining():.1f}s ago)"
                    )
                    should_clean = True

                # v96.0: Enhanced - Check if owner process is dead with PID reuse detection
                elif not self._is_process_alive(metadata.owner, metadata):
                    logger.warning(
                        f"Cleaning orphaned lock: {metadata.lock_name} "
                        f"(owner process {metadata.owner} is dead)"
                    )
                    should_clean = True

                # v96.0: Also validate lock owner identity comprehensively
                else:
                    is_valid, validation_reason = self._validate_lock_owner_identity(metadata)
                    if not is_valid:
                        logger.warning(
                            f"Cleaning invalid lock: {metadata.lock_name} "
                            f"(identity validation failed: {validation_reason})"
                        )
                        should_clean = True

                if should_clean:
                    await self._remove_lock_file(lock_file)
                    cleaned_count += 1

            if cleaned_count > 0:
                logger.info(f"Cleaned up {cleaned_count} stale/orphaned lock(s)")

        except Exception as e:
            logger.error(f"Error during stale lock cleanup: {e}", exc_info=True)

    # =========================================================================
    # Monitoring & Health
    # =========================================================================

    async def get_lock_status(self, lock_name: str) -> Optional[dict]:
        """
        Get current status of a lock.

        Returns:
            dict with lock status or None if lock not held
        """
        # v96.2: Use .dlm.lock extension
        lock_file = self.config.lock_dir / f"{lock_name}{self.config.lock_extension}"
        metadata = await self._read_lock_metadata(lock_file)

        if not metadata:
            return None

        # v96.0: Include identity validation in status
        is_valid, validation_reason = self._validate_lock_owner_identity(metadata)
        is_alive = self._is_process_alive(metadata.owner, metadata)

        return {
            "lock_name": metadata.lock_name,
            "owner": metadata.owner,
            "acquired_at": metadata.acquired_at,
            "expires_at": metadata.expires_at,
            "time_remaining": metadata.time_remaining(),
            "is_expired": metadata.is_expired(),
            "is_stale": metadata.is_stale(),
            # v96.0: Enhanced validation fields
            "owner_alive": is_alive,
            "identity_valid": is_valid,
            "validation_reason": validation_reason,
            "process_name": metadata.process_name,
            "machine_id": metadata.machine_id,
        }

    async def list_all_locks(self) -> list[dict]:
        """List all current locks with their status."""
        locks = []

        try:
            if not await aiofiles.os.path.exists(self.config.lock_dir):
                return locks

            # v96.2: Only list our .dlm.lock files, ignore other lock types (e.g., flock .lock files)
            ext = self.config.lock_extension
            lock_files = [
                f for f in await aiofiles.os.listdir(self.config.lock_dir)
                if f.endswith(ext)
            ]

            for lock_file_name in lock_files:
                # v96.2: Remove our extension to get the lock name
                lock_name = lock_file_name[:-len(ext)]
                status = await self.get_lock_status(lock_name)
                if status:
                    locks.append(status)

            return locks

        except Exception as e:
            logger.error(f"Error listing locks: {e}")
            return locks

    # =========================================================================
    # v3.0: Redis Backend Support
    # =========================================================================

    async def _select_backend(self) -> LockBackend:
        """
        v3.0: Select the best available lock backend.

        Priority: Redis (if available) > File (fallback)
        """
        if self.config.backend == LockBackend.FILE:
            return LockBackend.FILE

        if self.config.backend == LockBackend.REDIS:
            self._redis_client = get_redis_client()
            if await self._redis_client.connect():
                self._redis_available = True
                return LockBackend.REDIS
            logger.warning("[v3.0] Redis backend requested but unavailable")
            return LockBackend.REDIS

        # AUTO mode: Try Redis first, fall back to file
        if self.config.backend == LockBackend.AUTO:
            self._redis_client = get_redis_client()
            if await self._redis_client.connect():
                self._redis_available = True
                logger.info("[v3.0] Using Redis backend (distributed mode)")
                return LockBackend.REDIS
            else:
                logger.info("[v3.0] Redis unavailable, using file backend (local mode)")
                return LockBackend.FILE

        return LockBackend.FILE

    async def _get_next_fencing_token(self, lock_name: str) -> int:
        """v3.0: Get the next monotonically increasing fencing token."""
        async with self._fencing_lock:
            if self._redis_available and self._redis_client:
                token = await self._redis_client.incr(f"{TRINITY_LOCK_PREFIX}fencing:{lock_name}")
                if token > 0:
                    return token

            # Fall back to file-based counter
            self._fencing_token_counter += 1
            return self._fencing_token_counter

    async def _try_acquire_redis(
        self,
        lock_name: str,
        token: str,
        ttl: float,
    ) -> Optional[LockMetadata]:
        """v3.0: Try to acquire lock via Redis backend."""
        if not self._redis_available or not self._redis_client:
            return None

        redis_key = f"{TRINITY_LOCK_PREFIX}{lock_name}"
        fencing_token = await self._get_next_fencing_token(lock_name)
        now = time.time()

        metadata = LockMetadata(
            acquired_at=now,
            expires_at=now + ttl,
            owner=self._owner_id,
            token=token,
            lock_name=lock_name,
            process_start_time=self._process_start_time,
            process_name=self._process_name,
            process_cmdline=self._process_cmdline,
            machine_id=self._machine_id,
            backend="redis",
            fencing_token=fencing_token,
            repo_source=self.config.repo_source,
        )

        value = json.dumps(asdict(metadata))

        if await self._redis_client.set_nx(redis_key, value, ttl):
            logger.debug(f"[v3.0] Redis lock acquired: {lock_name} (fencing={fencing_token})")
            return metadata

        # Check if existing lock is stale
        existing_value = await self._redis_client.get(redis_key)
        if existing_value:
            try:
                existing = LockMetadata(**json.loads(existing_value))
                if existing.is_expired() or not self._is_process_alive(existing.owner, existing):
                    if await self._redis_client.delete(redis_key):
                        if await self._redis_client.set_nx(redis_key, value, ttl):
                            logger.info(f"[v3.0] Redis lock acquired after stale cleanup: {lock_name}")
                            return metadata
            except (json.JSONDecodeError, TypeError):
                await self._redis_client.delete(redis_key)
                if await self._redis_client.set_nx(redis_key, value, ttl):
                    return metadata

        return None

    async def _release_redis(self, lock_name: str, token: str) -> bool:
        """v3.0: Release lock from Redis backend."""
        if not self._redis_available or not self._redis_client:
            return False

        redis_key = f"{TRINITY_LOCK_PREFIX}{lock_name}"
        value = await self._redis_client.get(redis_key)

        if not value:
            return True

        try:
            metadata = LockMetadata(**json.loads(value))
            if metadata.token == token:
                return await self._redis_client.delete(redis_key)
            return False
        except (json.JSONDecodeError, TypeError):
            return await self._redis_client.delete(redis_key)

    @asynccontextmanager
    async def acquire_unified(
        self,
        lock_name: str,
        timeout: Optional[float] = None,
        ttl: Optional[float] = None,
        enable_keepalive: bool = True,
    ) -> AsyncIterator[Tuple[bool, Optional[LockMetadata]]]:
        """
        v3.0: Acquire lock using best available backend with optional keepalive.

        Yields:
            Tuple of (acquired: bool, metadata: Optional[LockMetadata])
        """
        timeout = timeout or self.config.default_timeout_seconds
        ttl = ttl or self.config.default_ttl_seconds
        token = str(uuid4())

        if not self._initialized:
            await self.initialize()

        if self._active_backend == LockBackend.FILE:
            self._active_backend = await self._select_backend()

        acquired = False
        metadata: Optional[LockMetadata] = None
        keepalive_task: Optional[asyncio.Task] = None

        start_time = time.time()
        attempt = 0

        try:
            while time.time() - start_time < timeout:
                # Try Redis first if available
                if self._redis_available and self._redis_client:
                    metadata = await self._try_acquire_redis(lock_name, token, ttl)
                    if metadata:
                        acquired = True
                        break

                # Fall back to file-based lock
                lock_file = self.config.lock_dir / f"{lock_name}{self.config.lock_extension}"
                if await self._try_acquire_lock(lock_file, lock_name, token, ttl):
                    acquired = True
                    fencing_token = await self._get_next_fencing_token(lock_name)
                    now = time.time()
                    metadata = LockMetadata(
                        acquired_at=now,
                        expires_at=now + ttl,
                        owner=self._owner_id,
                        token=token,
                        lock_name=lock_name,
                        process_start_time=self._process_start_time,
                        process_name=self._process_name,
                        process_cmdline=self._process_cmdline,
                        machine_id=self._machine_id,
                        backend="file",
                        fencing_token=fencing_token,
                        repo_source=self.config.repo_source,
                    )
                    break

                attempt += 1
                base_delay = self.config.retry_delay_seconds
                jitter = random.uniform(0, base_delay * 0.5)
                await asyncio.sleep(base_delay + jitter)

            if acquired and metadata:
                logger.debug(
                    f"[v3.0] Lock acquired: {lock_name} "
                    f"(backend={metadata.backend}, fencing={metadata.fencing_token})"
                )

                # Start keepalive if enabled
                if enable_keepalive and self.config.keepalive_enabled:
                    keepalive_task = asyncio.create_task(
                        self._keepalive_loop(lock_name, token, ttl, metadata.backend),
                        name=f"keepalive_{lock_name}"
                    )
                    self._keepalive_tasks[lock_name] = keepalive_task
            else:
                logger.warning(f"[v3.0] Lock acquisition timeout: {lock_name}")

            yield acquired, metadata

        finally:
            if keepalive_task and not keepalive_task.done():
                keepalive_task.cancel()
                try:
                    await asyncio.wait_for(keepalive_task, timeout=1.0)
                except (asyncio.CancelledError, asyncio.TimeoutError):
                    pass

            if lock_name in self._keepalive_tasks:
                del self._keepalive_tasks[lock_name]

            if acquired:
                if metadata and metadata.backend == "redis":
                    await self._release_redis(lock_name, token)
                else:
                    lock_file = self.config.lock_dir / f"{lock_name}{self.config.lock_extension}"
                    await self._release_lock(lock_file, token)
                logger.debug(f"[v3.0] Lock released: {lock_name}")

    async def _keepalive_loop(
        self,
        lock_name: str,
        token: str,
        ttl: float,
        backend: str,
    ) -> None:
        """v3.0: Background task to extend lock TTL periodically."""
        interval = self.config.keepalive_interval_seconds

        while True:
            try:
                await asyncio.sleep(interval)

                if backend == "redis" and self._redis_client:
                    redis_key = f"{TRINITY_LOCK_PREFIX}{lock_name}"
                    value = await self._redis_client.get(redis_key)
                    if value:
                        try:
                            meta = LockMetadata(**json.loads(value))
                            if meta.token == token:
                                await self._redis_client.expire(redis_key, int(ttl))
                                logger.debug(f"[v3.0] Redis lock TTL extended: {lock_name}")
                        except (json.JSONDecodeError, TypeError):
                            break
                else:
                    lock_file = self.config.lock_dir / f"{lock_name}{self.config.lock_extension}"
                    metadata = await self._read_lock_metadata(lock_file)
                    if metadata and metadata.token == token:
                        metadata.expires_at = time.time() + ttl
                        metadata.extensions += 1
                        await self._write_lock_metadata(lock_file, metadata)
                        logger.debug(f"[v3.0] File lock TTL extended: {lock_name}")
                    else:
                        break

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.debug(f"[v3.0] Keepalive error for {lock_name}: {e}")
                break

    def get_active_backend(self) -> LockBackend:
        """v3.0: Get the currently active lock backend."""
        return self._active_backend

    def is_redis_available(self) -> bool:
        """v3.0: Check if Redis backend is available."""
        return self._redis_available

    async def get_cross_repo_status(self) -> Dict[str, Any]:
        """v3.0: Get comprehensive cross-repo lock status."""
        status: Dict[str, Any] = {
            "backend": self._active_backend.value,
            "redis_available": self._redis_available,
            "repo_source": self.config.repo_source,
            "owner_id": self._owner_id,
            "machine_id": self._machine_id,
            "active_keepalives": list(self._keepalive_tasks.keys()),
            "locks": [],
        }

        file_locks = await self.list_all_locks()
        for lock in file_locks:
            lock["backend"] = "file"
        status["locks"].extend(file_locks)

        return status


# =============================================================================
# Global Instance (Singleton Pattern)
# =============================================================================

_lock_manager_instance: Optional[DistributedLockManager] = None


async def get_lock_manager(config: Optional[LockConfig] = None) -> DistributedLockManager:
    """
    Get or create global lock manager instance.

    v3.0: Supports optional config for cross-repo initialization.
    """
    global _lock_manager_instance

    if _lock_manager_instance is None:
        _lock_manager_instance = DistributedLockManager(config)
        await _lock_manager_instance.initialize()

    return _lock_manager_instance


async def shutdown_lock_manager() -> None:
    """Shutdown global lock manager instance."""
    global _lock_manager_instance

    if _lock_manager_instance:
        await _lock_manager_instance.shutdown()
        _lock_manager_instance = None


# =============================================================================
# v3.0: Cross-Repo Convenience Functions
# =============================================================================

@asynccontextmanager
async def acquire_cross_repo_lock(
    lock_name: str,
    repo_source: str = "jarvis",
    timeout: float = 5.0,
    ttl: float = 10.0,
) -> AsyncIterator[Tuple[bool, Optional[LockMetadata]]]:
    """
    v3.0: Convenience function for cross-repo lock acquisition.

    Makes it easy for JARVIS-Prime and Reactor-Core to acquire locks consistently.

    Example (from JARVIS-Prime):
        from backend.core.distributed_lock_manager import acquire_cross_repo_lock

        async with acquire_cross_repo_lock("training_job", "jarvis-prime") as (acquired, meta):
            if acquired:
                await run_training()
    """
    config = LockConfig(repo_source=repo_source)
    manager = await get_lock_manager(config)

    async with manager.acquire_unified(lock_name, timeout, ttl) as result:
        yield result


def get_trinity_lock_prefix() -> str:
    """v3.0: Get the Trinity lock prefix for Redis keys."""
    return TRINITY_LOCK_PREFIX
