"""
Ironcliw Cognitive Architecture v2.0
===================================

Advanced AGI capabilities addressing critical reasoning gaps.

COMPONENTS:
    1. CausalReasoningEngine     - Interventional calculus, do-calculus, causal discovery
    2. WorldModelSimulator       - Physics understanding, predictive simulation
    3. TheoryOfMindEngine        - Mental state modeling, intention inference
    4. AbstractReasoningEngine   - Formal logic, mathematical proofs, symbolic reasoning
    5. LongTermPlanner           - Hierarchical goal decomposition, multi-step planning
    6. CounterfactualReasoner    - "What-if" analysis, alternative outcome exploration
    7. CreativeProblemSolver     - Divergent thinking, novel solution generation
    8. MetaLearner               - Self-improvement, strategy optimization, experience analysis
    9. EthicsFramework           - Value alignment, moral reasoning, ethical constraints

UNIFIED INTERFACE:
    CognitiveSystem              - Single interface to all cognitive modules with auto-routing

DESIGN PRINCIPLES:
    - Zero hardcoding: All parameters from environment
    - Fully async: Non-blocking cognitive operations
    - Observable: Full introspection and explanation
    - Composable: Cognitive modules work together
    - Learnable: Improves through experience
    - Self-improving: Meta-learning from past performance

Author: Ironcliw System
Version: 2.0.0
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
import math
import os
import random
import time
import uuid
from abc import ABC, abstractmethod
from collections import defaultdict, deque
from dataclasses import dataclass, field
from enum import Enum, auto
from functools import lru_cache
from heapq import heappush, heappop
from typing import (
    Any,
    Awaitable,
    Callable,
    Deque,
    Dict,
    FrozenSet,
    Generic,
    Iterable,
    Iterator,
    List,
    Mapping,
    Optional,
    Protocol,
    Sequence,
    Set,
    Tuple,
    Type,
    TypeVar,
    Union,
)

from backend.core.async_safety import LazyAsyncLock

logger = logging.getLogger(__name__)

T = TypeVar("T")
S = TypeVar("S")  # State type
A = TypeVar("A")  # Action type


# =============================================================================
# CONFIGURATION
# =============================================================================

def _env(key: str, default: str) -> str:
    return os.getenv(key, default)

def _env_float(key: str, default: float) -> float:
    try:
        return float(os.getenv(key, str(default)))
    except ValueError:
        return default

def _env_int(key: str, default: int) -> int:
    try:
        return int(os.getenv(key, str(default)))
    except ValueError:
        return default

def _env_bool(key: str, default: bool) -> bool:
    return os.getenv(key, str(default)).lower() in ("true", "1", "yes")


# =============================================================================
# UNCERTAINTY QUANTIFICATION (UQ)
# =============================================================================

@dataclass
class CredibleInterval:
    """
    Bayesian credible interval for uncertainty quantification.

    Unlike frequentist confidence intervals, credible intervals
    represent our actual belief about where the true value lies.
    """
    lower: float
    upper: float
    probability: float = 0.95  # 95% credible interval by default
    point_estimate: Optional[float] = None

    @property
    def width(self) -> float:
        """Interval width (measure of uncertainty)."""
        return self.upper - self.lower

    @property
    def midpoint(self) -> float:
        """Midpoint of the interval."""
        return (self.lower + self.upper) / 2

    def contains(self, value: float) -> bool:
        """Check if value is within the interval."""
        return self.lower <= value <= self.upper


@dataclass
class UncertainValue:
    """
    A value with associated uncertainty.

    Represents epistemic (knowledge) and aleatoric (inherent) uncertainty.
    """
    value: float
    uncertainty: float  # Standard deviation or similar measure
    credible_interval: Optional[CredibleInterval] = None
    uncertainty_type: str = "combined"  # "epistemic", "aleatoric", or "combined"

    @classmethod
    def from_samples(cls, samples: List[float], confidence: float = 0.95) -> "UncertainValue":
        """Create UncertainValue from a list of samples (e.g., from Monte Carlo)."""
        if not samples:
            return cls(value=0.0, uncertainty=float("inf"))

        mean = sum(samples) / len(samples)
        variance = sum((x - mean) ** 2 for x in samples) / len(samples)
        std = variance ** 0.5

        # Calculate credible interval
        sorted_samples = sorted(samples)
        n = len(sorted_samples)
        alpha = 1 - confidence
        lower_idx = int(n * alpha / 2)
        upper_idx = int(n * (1 - alpha / 2))

        return cls(
            value=mean,
            uncertainty=std,
            credible_interval=CredibleInterval(
                lower=sorted_samples[lower_idx] if lower_idx < n else sorted_samples[-1],
                upper=sorted_samples[upper_idx] if upper_idx < n else sorted_samples[-1],
                probability=confidence,
                point_estimate=mean,
            ),
        )

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        result = {
            "value": self.value,
            "uncertainty": self.uncertainty,
            "uncertainty_type": self.uncertainty_type,
        }
        if self.credible_interval:
            result["credible_interval"] = {
                "lower": self.credible_interval.lower,
                "upper": self.credible_interval.upper,
                "probability": self.credible_interval.probability,
            }
        return result


@dataclass
class ReasoningResult:
    """
    Unified result structure for cognitive reasoning with uncertainty.

    All cognitive modules should return this structure for consistency.
    """
    reasoning_type: str
    conclusion: Any
    confidence: UncertainValue
    explanation: str
    evidence: List[str] = field(default_factory=list)
    alternatives: List[Dict[str, Any]] = field(default_factory=list)
    reasoning_steps: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

    @property
    def is_high_confidence(self) -> bool:
        """Check if result has high confidence (>0.8)."""
        return self.confidence.value >= 0.8

    @property
    def is_uncertain(self) -> bool:
        """Check if result has high uncertainty."""
        return self.confidence.uncertainty > 0.2

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            "reasoning_type": self.reasoning_type,
            "conclusion": self.conclusion,
            "confidence": self.confidence.to_dict(),
            "explanation": self.explanation,
            "evidence": self.evidence,
            "alternatives": self.alternatives,
            "reasoning_steps": self.reasoning_steps,
            "is_high_confidence": self.is_high_confidence,
            "is_uncertain": self.is_uncertain,
            "metadata": self.metadata,
        }


class BayesianUpdater:
    """
    Simple Bayesian updater for belief revision.

    Uses conjugate priors for efficient updating.
    Supports Beta-Binomial (binary outcomes) and Normal-Normal (continuous).
    """

    def __init__(self):
        # Beta distribution parameters (prior: Beta(1,1) = uniform)
        self._beta_priors: Dict[str, Tuple[float, float]] = {}  # key -> (alpha, beta)

        # Normal distribution parameters (prior: N(0, 1))
        self._normal_priors: Dict[str, Tuple[float, float]] = {}  # key -> (mean, precision)

    def update_beta(
        self,
        key: str,
        successes: int,
        failures: int,
        prior_alpha: float = 1.0,
        prior_beta: float = 1.0,
    ) -> Tuple[float, float]:
        """
        Update Beta distribution with new observations.

        Args:
            key: Identifier for the belief
            successes: Number of successes
            failures: Number of failures
            prior_alpha: Prior alpha (pseudo-successes)
            prior_beta: Prior beta (pseudo-failures)

        Returns:
            (posterior_mean, posterior_std)
        """
        # Get or initialize prior
        if key not in self._beta_priors:
            self._beta_priors[key] = (prior_alpha, prior_beta)

        alpha, beta = self._beta_priors[key]

        # Bayesian update: posterior = prior + data
        alpha_post = alpha + successes
        beta_post = beta + failures

        self._beta_priors[key] = (alpha_post, beta_post)

        # Calculate posterior statistics
        mean = alpha_post / (alpha_post + beta_post)
        variance = (alpha_post * beta_post) / ((alpha_post + beta_post) ** 2 * (alpha_post + beta_post + 1))
        std = variance ** 0.5

        return mean, std

    def get_beta_credible_interval(
        self,
        key: str,
        probability: float = 0.95,
    ) -> CredibleInterval:
        """Get credible interval for a Beta belief."""
        if key not in self._beta_priors:
            return CredibleInterval(lower=0.0, upper=1.0, probability=probability)

        alpha, beta = self._beta_priors[key]
        mean = alpha / (alpha + beta)

        # Approximate credible interval using normal approximation
        variance = (alpha * beta) / ((alpha + beta) ** 2 * (alpha + beta + 1))
        std = variance ** 0.5

        # 95% CI ≈ mean ± 1.96*std
        z = 1.96 if probability == 0.95 else 2.58  # 99%
        lower = max(0.0, mean - z * std)
        upper = min(1.0, mean + z * std)

        return CredibleInterval(
            lower=lower,
            upper=upper,
            probability=probability,
            point_estimate=mean,
        )

    def update_normal(
        self,
        key: str,
        observation: float,
        observation_precision: float = 1.0,
        prior_mean: float = 0.0,
        prior_precision: float = 1.0,
    ) -> Tuple[float, float]:
        """
        Update Normal distribution with new observation.

        Args:
            key: Identifier for the belief
            observation: Observed value
            observation_precision: Precision (1/variance) of observation
            prior_mean: Prior mean
            prior_precision: Prior precision

        Returns:
            (posterior_mean, posterior_std)
        """
        if key not in self._normal_priors:
            self._normal_priors[key] = (prior_mean, prior_precision)

        mu, tau = self._normal_priors[key]

        # Bayesian update for Normal-Normal
        tau_post = tau + observation_precision
        mu_post = (tau * mu + observation_precision * observation) / tau_post

        self._normal_priors[key] = (mu_post, tau_post)

        # Calculate posterior statistics
        std_post = (1.0 / tau_post) ** 0.5

        return mu_post, std_post


# Global Bayesian updater instance
_bayesian_updater: Optional[BayesianUpdater] = None


def get_bayesian_updater() -> BayesianUpdater:
    """Get singleton BayesianUpdater instance."""
    global _bayesian_updater
    if _bayesian_updater is None:
        _bayesian_updater = BayesianUpdater()
    return _bayesian_updater


# =============================================================================
# 1. CAUSAL REASONING ENGINE
# =============================================================================

class CausalRelationType(Enum):
    """Types of causal relationships."""
    DIRECT_CAUSE = "direct_cause"          # A directly causes B
    INDIRECT_CAUSE = "indirect_cause"      # A causes B through intermediaries
    COMMON_CAUSE = "common_cause"          # C causes both A and B
    COMMON_EFFECT = "common_effect"        # Both A and B cause C
    CONFOUNDER = "confounder"              # Hidden variable affecting both
    MEDIATOR = "mediator"                  # Variable on causal path
    MODERATOR = "moderator"                # Affects strength of relationship
    INSTRUMENTAL = "instrumental"           # Affects cause but not effect directly
    SELECTION = "selection"                # Conditioning on collider


@dataclass
class CausalEdge:
    """Edge in a causal graph."""
    source: str
    target: str
    relation_type: CausalRelationType
    strength: float = 1.0  # Effect size
    confidence: float = 1.0  # How confident we are
    mechanism: Optional[str] = None  # Description of causal mechanism
    time_lag: float = 0.0  # Temporal delay
    conditions: List[str] = field(default_factory=list)  # Required conditions

    def __hash__(self):
        return hash((self.source, self.target))


@dataclass
class CausalNode:
    """Node in a causal graph."""
    name: str
    node_type: str = "observable"  # observable, latent, intervention
    domain: Optional[List[Any]] = None  # Possible values
    current_value: Optional[Any] = None
    distribution: Optional[Dict[str, float]] = None  # Prior distribution

    def __hash__(self):
        return hash(self.name)


@dataclass
class InterventionResult:
    """Result of a do-calculus intervention."""
    target_variable: str
    intervention_value: Any
    affected_variables: Dict[str, Any]
    causal_effect: float
    confidence: float
    explanation: str
    counterfactual_outcomes: Dict[str, Any] = field(default_factory=dict)


class CausalGraph:
    """
    Causal graphical model supporting do-calculus.

    Implements Pearl's causal hierarchy:
        Level 1: Association (seeing) - P(Y|X)
        Level 2: Intervention (doing) - P(Y|do(X))
        Level 3: Counterfactual (imagining) - P(Y_x|X', Y')
    """

    def __init__(self):
        self._nodes: Dict[str, CausalNode] = {}
        self._edges: Dict[str, List[CausalEdge]] = defaultdict(list)
        self._reverse_edges: Dict[str, List[CausalEdge]] = defaultdict(list)
        self._lock = asyncio.Lock()

    def add_node(self, node: CausalNode) -> None:
        """Add a node to the graph."""
        self._nodes[node.name] = node

    def add_edge(self, edge: CausalEdge) -> None:
        """Add a causal edge."""
        self._edges[edge.source].append(edge)
        self._reverse_edges[edge.target].append(edge)

    def get_parents(self, node: str) -> List[str]:
        """Get direct causes of a node."""
        return [e.source for e in self._reverse_edges.get(node, [])]

    def get_children(self, node: str) -> List[str]:
        """Get direct effects of a node."""
        return [e.target for e in self._edges.get(node, [])]

    def get_ancestors(self, node: str, visited: Optional[Set[str]] = None) -> Set[str]:
        """Get all causes (direct and indirect) of a node."""
        if visited is None:
            visited = set()

        ancestors = set()
        for parent in self.get_parents(node):
            if parent not in visited:
                visited.add(parent)
                ancestors.add(parent)
                ancestors.update(self.get_ancestors(parent, visited))

        return ancestors

    def get_descendants(self, node: str, visited: Optional[Set[str]] = None) -> Set[str]:
        """Get all effects (direct and indirect) of a node."""
        if visited is None:
            visited = set()

        descendants = set()
        for child in self.get_children(node):
            if child not in visited:
                visited.add(child)
                descendants.add(child)
                descendants.update(self.get_descendants(child, visited))

        return descendants

    def is_d_separated(
        self,
        x: str,
        y: str,
        z: Set[str],
    ) -> bool:
        """
        Check if X and Y are d-separated given Z.

        Uses the Bayes-Ball algorithm for efficient d-separation.
        """
        # Simplified d-separation check
        # Full implementation would use Bayes-Ball algorithm

        # Find all paths from X to Y
        paths = self._find_all_paths(x, y)

        # Check if all paths are blocked by Z
        for path in paths:
            if not self._is_path_blocked(path, z):
                return False

        return True

    def _find_all_paths(
        self,
        start: str,
        end: str,
        path: Optional[List[str]] = None,
        visited: Optional[Set[str]] = None,
    ) -> List[List[str]]:
        """Find all paths between two nodes."""
        if path is None:
            path = [start]
        if visited is None:
            visited = {start}

        if start == end:
            return [path]

        paths = []

        # Check both directions (edges and reverse edges)
        neighbors = set(self.get_children(start)) | set(self.get_parents(start))

        for neighbor in neighbors:
            if neighbor not in visited:
                new_path = path + [neighbor]
                visited.add(neighbor)
                paths.extend(self._find_all_paths(neighbor, end, new_path, visited))
                visited.remove(neighbor)

        return paths

    def _is_path_blocked(self, path: List[str], z: Set[str]) -> bool:
        """Check if a path is blocked by conditioning set Z."""
        if len(path) < 3:
            return False

        # Check each triple in the path
        for i in range(len(path) - 2):
            a, b, c = path[i], path[i + 1], path[i + 2]

            # Determine the type of triple
            a_to_b = b in self.get_children(a)
            b_to_c = c in self.get_children(b)

            if a_to_b and b_to_c:
                # Chain: A → B → C (blocked if B in Z)
                if b in z:
                    return True
            elif not a_to_b and not b_to_c:
                # Collider: A → B ← C (blocked if B NOT in Z and no descendant in Z)
                descendants = self.get_descendants(b)
                if b not in z and not any(d in z for d in descendants):
                    return True
            else:
                # Fork or inverted (blocked if B in Z)
                if b in z:
                    return True

        return False


class CausalReasoningEngine:
    """
    Advanced causal reasoning with do-calculus and interventional analysis.

    Capabilities:
        - Causal discovery from observational data
        - Do-calculus for interventional queries
        - Counterfactual reasoning
        - Confounding bias detection
        - Causal effect estimation

    Environment Variables:
        CAUSAL_MAX_INTERVENTIONS: Max simultaneous interventions
        CAUSAL_CONFIDENCE_THRESHOLD: Min confidence for causal claims
        CAUSAL_DISCOVERY_ALPHA: Significance level for discovery
    """

    def __init__(self):
        self._graph = CausalGraph()
        self._observation_history: Deque[Dict[str, Any]] = deque(
            maxlen=_env_int("CAUSAL_HISTORY_SIZE", 10000)
        )
        self._max_interventions = _env_int("CAUSAL_MAX_INTERVENTIONS", 5)
        self._confidence_threshold = _env_float("CAUSAL_CONFIDENCE_THRESHOLD", 0.7)
        self._discovery_alpha = _env_float("CAUSAL_DISCOVERY_ALPHA", 0.05)
        self._lock = asyncio.Lock()

        # Causal effect cache
        self._effect_cache: Dict[str, InterventionResult] = {}

    async def add_observation(self, observation: Dict[str, Any]) -> None:
        """Record an observation for causal learning."""
        async with self._lock:
            self._observation_history.append({
                **observation,
                "_timestamp": time.time(),
                "_id": str(uuid.uuid4()),
            })

    async def discover_causal_structure(
        self,
        variables: List[str],
        max_edges: Optional[int] = None,
    ) -> CausalGraph:
        """
        Discover causal structure from observations.

        Uses a combination of:
            - PC algorithm for conditional independence tests
            - FCI for handling latent confounders
            - Score-based search for optimization

        Args:
            variables: Variables to analyze
            max_edges: Maximum edges to discover

        Returns:
            CausalGraph with discovered structure
        """
        async with self._lock:
            observations = list(self._observation_history)

        if len(observations) < 10:
            logger.warning("[Causal] Insufficient observations for discovery")
            return self._graph

        # Phase 1: Build complete graph
        graph = CausalGraph()
        for var in variables:
            graph.add_node(CausalNode(name=var))

        # Phase 2: Remove edges based on conditional independence
        # (Simplified - full PC algorithm would be more sophisticated)
        for i, var1 in enumerate(variables):
            for var2 in variables[i + 1:]:
                # Test for conditional independence
                independent = await self._test_conditional_independence(
                    var1, var2, observations
                )

                if not independent:
                    # Add edge (direction determined by temporal order or domain knowledge)
                    edge = await self._orient_edge(var1, var2, observations)
                    if edge:
                        graph.add_edge(edge)

        # Phase 3: Orient edges using d-separation
        await self._orient_remaining_edges(graph, observations)

        self._graph = graph
        return graph

    async def _test_conditional_independence(
        self,
        x: str,
        y: str,
        observations: List[Dict[str, Any]],
    ) -> bool:
        """Test if X and Y are conditionally independent."""
        # Extract values
        x_vals = [obs.get(x) for obs in observations if x in obs]
        y_vals = [obs.get(y) for obs in observations if y in obs]

        if not x_vals or not y_vals:
            return True  # No data, assume independent

        # Simple correlation test (full implementation would use partial correlation)
        if len(x_vals) != len(y_vals):
            return True

        try:
            # Calculate Pearson correlation for numeric values
            if all(isinstance(v, (int, float)) for v in x_vals + y_vals):
                n = len(x_vals)
                mean_x = sum(x_vals) / n
                mean_y = sum(y_vals) / n

                cov = sum((x_vals[i] - mean_x) * (y_vals[i] - mean_y) for i in range(n))
                var_x = sum((v - mean_x) ** 2 for v in x_vals)
                var_y = sum((v - mean_y) ** 2 for v in y_vals)

                if var_x == 0 or var_y == 0:
                    return True

                correlation = cov / (math.sqrt(var_x) * math.sqrt(var_y))

                # Use Fisher's z-transformation for significance
                z = 0.5 * math.log((1 + correlation) / (1 - correlation + 1e-10))
                se = 1 / math.sqrt(n - 3) if n > 3 else 1
                z_stat = abs(z / se)

                # Approximate p-value
                p_value = 2 * (1 - self._normal_cdf(z_stat))

                return p_value > self._discovery_alpha
        except Exception:
            pass

        return False

    def _normal_cdf(self, x: float) -> float:
        """Approximate normal CDF."""
        return 0.5 * (1 + math.erf(x / math.sqrt(2)))

    async def _orient_edge(
        self,
        var1: str,
        var2: str,
        observations: List[Dict[str, Any]],
    ) -> Optional[CausalEdge]:
        """Determine the direction of a causal edge."""
        # Use temporal ordering if timestamps available
        var1_times = [obs.get("_timestamp", 0) for obs in observations if var1 in obs]
        var2_times = [obs.get("_timestamp", 0) for obs in observations if var2 in obs]

        if var1_times and var2_times:
            avg_var1 = sum(var1_times) / len(var1_times)
            avg_var2 = sum(var2_times) / len(var2_times)

            if avg_var1 < avg_var2:
                return CausalEdge(
                    source=var1,
                    target=var2,
                    relation_type=CausalRelationType.DIRECT_CAUSE,
                    confidence=0.7,
                )
            elif avg_var2 < avg_var1:
                return CausalEdge(
                    source=var2,
                    target=var1,
                    relation_type=CausalRelationType.DIRECT_CAUSE,
                    confidence=0.7,
                )

        # Default: bidirectional (unknown)
        return CausalEdge(
            source=var1,
            target=var2,
            relation_type=CausalRelationType.DIRECT_CAUSE,
            confidence=0.5,
        )

    async def _orient_remaining_edges(
        self,
        graph: CausalGraph,
        observations: List[Dict[str, Any]],
    ) -> None:
        """Orient remaining edges using Meek's rules."""
        # Meek's Rule 1: If A → B and no edge between A and C, and B - C, then B → C
        # (Simplified implementation)
        pass

    async def do(
        self,
        intervention: Dict[str, Any],
        target: str,
    ) -> InterventionResult:
        """
        Perform do-calculus intervention: P(target | do(intervention)).

        This answers "What would happen to target if we set intervention?"

        Args:
            intervention: Variables to intervene on with their values
            target: Target variable to query

        Returns:
            InterventionResult with causal effect estimate
        """
        cache_key = f"{json.dumps(intervention, sort_keys=True)}_{target}"
        if cache_key in self._effect_cache:
            return self._effect_cache[cache_key]

        async with self._lock:
            observations = list(self._observation_history)

        # Create mutilated graph (remove edges into intervened variables)
        mutilated_graph = CausalGraph()
        for name, node in self._graph._nodes.items():
            mutilated_graph.add_node(node)

        intervened_vars = set(intervention.keys())

        for source, edges in self._graph._edges.items():
            for edge in edges:
                # Remove edges INTO intervened variables
                if edge.target not in intervened_vars:
                    mutilated_graph.add_edge(edge)

        # Estimate causal effect using adjustment formula
        # E[Y|do(X=x)] = Σ_z P(Y|X=x,Z=z)P(Z=z)

        # Find adjustment set (backdoor criterion)
        adjustment_set = await self._find_adjustment_set(
            list(intervention.keys())[0] if intervention else "",
            target,
        )

        # Estimate effect
        effect = await self._estimate_causal_effect(
            intervention,
            target,
            adjustment_set,
            observations,
        )

        # Find affected downstream variables
        affected = {}
        for var in self._graph.get_descendants(target):
            affected[var] = await self._propagate_effect(intervention, var, observations)

        result = InterventionResult(
            target_variable=target,
            intervention_value=intervention,
            affected_variables=affected,
            causal_effect=effect,
            confidence=self._calculate_confidence(observations, intervention),
            explanation=self._generate_causal_explanation(intervention, target, effect),
        )

        self._effect_cache[cache_key] = result
        return result

    async def _find_adjustment_set(self, treatment: str, outcome: str) -> Set[str]:
        """Find valid adjustment set using backdoor criterion."""
        # A set Z satisfies backdoor criterion if:
        # 1. No node in Z is a descendant of treatment
        # 2. Z blocks all backdoor paths from treatment to outcome

        descendants = self._graph.get_descendants(treatment)
        ancestors = self._graph.get_ancestors(treatment)

        # Candidates are ancestors of treatment that are not descendants
        candidates = ancestors - descendants

        # Find minimal sufficient adjustment set
        adjustment_set = set()
        for candidate in candidates:
            # Check if adding this variable blocks more paths
            if not self._graph.is_d_separated(treatment, outcome, adjustment_set):
                adjustment_set.add(candidate)
                if self._graph.is_d_separated(treatment, outcome, adjustment_set):
                    break

        return adjustment_set

    async def _estimate_causal_effect(
        self,
        intervention: Dict[str, Any],
        target: str,
        adjustment_set: Set[str],
        observations: List[Dict[str, Any]],
    ) -> float:
        """Estimate causal effect using adjustment formula."""
        if not observations:
            return 0.0

        # Filter observations matching intervention
        matching_obs = [
            obs for obs in observations
            if all(obs.get(k) == v for k, v in intervention.items())
        ]

        if not matching_obs:
            return 0.0

        # Calculate average target value under intervention
        target_values = [obs.get(target, 0) for obs in matching_obs]
        if not target_values:
            return 0.0

        # Simple average (full implementation would use IPW or g-computation)
        intervention_mean = sum(target_values) / len(target_values)

        # Calculate baseline
        all_target_values = [obs.get(target, 0) for obs in observations]
        baseline_mean = sum(all_target_values) / len(all_target_values) if all_target_values else 0

        return intervention_mean - baseline_mean

    async def _propagate_effect(
        self,
        intervention: Dict[str, Any],
        variable: str,
        observations: List[Dict[str, Any]],
    ) -> Any:
        """Propagate causal effect to downstream variable."""
        # Find path from intervention to variable
        for int_var in intervention:
            if variable in self._graph.get_descendants(int_var):
                # Estimate indirect effect
                effect = await self._estimate_causal_effect(
                    intervention, variable, set(), observations
                )
                return effect
        return None

    def _calculate_confidence(
        self,
        observations: List[Dict[str, Any]],
        intervention: Dict[str, Any],
    ) -> float:
        """Calculate confidence in causal estimate."""
        n = len(observations)
        if n < 10:
            return 0.3
        elif n < 100:
            return 0.5
        elif n < 1000:
            return 0.7
        else:
            return 0.9

    def _generate_causal_explanation(
        self,
        intervention: Dict[str, Any],
        target: str,
        effect: float,
    ) -> str:
        """Generate human-readable causal explanation."""
        int_str = ", ".join(f"{k}={v}" for k, v in intervention.items())
        direction = "increases" if effect > 0 else "decreases" if effect < 0 else "does not affect"
        magnitude = abs(effect)

        return (
            f"Setting {int_str} {direction} {target} by approximately {magnitude:.2f} units. "
            f"This is based on causal analysis of the relationship between these variables."
        )

    async def counterfactual(
        self,
        factual: Dict[str, Any],
        intervention: Dict[str, Any],
        target: str,
    ) -> Dict[str, Any]:
        """
        Answer counterfactual query: "What would target have been if intervention,
        given that we observed factual?"

        Level 3 of Pearl's causal hierarchy (imagining).

        Args:
            factual: What we actually observed
            intervention: What we're imagining changed
            target: Variable we're querying about

        Returns:
            Counterfactual outcome and explanation
        """
        # Step 1: Abduction - infer latent variable values from factual
        latent_values = await self._abduct_latents(factual)

        # Step 2: Action - apply intervention to mutilated graph
        mutilated_world = {**factual, **intervention, **latent_values}

        # Step 3: Prediction - compute target in counterfactual world
        cf_target = await self._predict_counterfactual(mutilated_world, target)

        # Compare to actual
        actual = factual.get(target)

        return {
            "counterfactual_value": cf_target,
            "actual_value": actual,
            "difference": cf_target - actual if isinstance(cf_target, (int, float)) and isinstance(actual, (int, float)) else None,
            "intervention": intervention,
            "explanation": self._explain_counterfactual(factual, intervention, actual, cf_target),
        }

    async def _abduct_latents(self, factual: Dict[str, Any]) -> Dict[str, Any]:
        """Infer latent variable values from observations."""
        # In a full implementation, this would solve for latent U such that
        # the structural equations produce the observed factual values
        return {}

    async def _predict_counterfactual(
        self,
        world: Dict[str, Any],
        target: str,
    ) -> Any:
        """Predict target value in counterfactual world."""
        # Use structural equations to propagate values
        # For now, use simple interpolation from observations

        async with self._lock:
            observations = list(self._observation_history)

        # Find most similar observations and interpolate
        similarities = []
        for obs in observations:
            sim = self._calculate_observation_similarity(world, obs)
            similarities.append((sim, obs.get(target)))

        if not similarities:
            return None

        # Weighted average
        total_weight = sum(s[0] for s in similarities)
        if total_weight == 0:
            return similarities[0][1]

        weighted_sum = sum(s[0] * (s[1] or 0) for s in similarities)
        return weighted_sum / total_weight

    def _calculate_observation_similarity(
        self,
        world: Dict[str, Any],
        observation: Dict[str, Any],
    ) -> float:
        """Calculate similarity between world state and observation."""
        common_keys = set(world.keys()) & set(observation.keys())
        if not common_keys:
            return 0.0

        matches = sum(1 for k in common_keys if world.get(k) == observation.get(k))
        return matches / len(common_keys)

    def _explain_counterfactual(
        self,
        factual: Dict[str, Any],
        intervention: Dict[str, Any],
        actual: Any,
        counterfactual: Any,
    ) -> str:
        """Generate explanation for counterfactual reasoning."""
        int_str = ", ".join(f"{k}={v}" for k, v in intervention.items())

        if counterfactual is None:
            return f"Unable to determine counterfactual outcome for intervention {int_str}."

        if actual == counterfactual:
            return f"Even if {int_str}, the outcome would have been the same ({actual})."

        return (
            f"If {int_str} had been different, the outcome would have been "
            f"{counterfactual} instead of {actual}."
        )


# =============================================================================
# 2. WORLD MODEL SIMULATOR
# =============================================================================

class PhysicsType(Enum):
    """Types of physical phenomena."""
    GRAVITY = "gravity"
    MOMENTUM = "momentum"
    COLLISION = "collision"
    FRICTION = "friction"
    FLUID_DYNAMICS = "fluid_dynamics"
    THERMODYNAMICS = "thermodynamics"
    ELECTROMAGNETISM = "electromagnetism"
    OPTICS = "optics"


@dataclass
class PhysicalObject:
    """Object in the world model."""
    id: str
    position: Tuple[float, float, float] = (0.0, 0.0, 0.0)
    velocity: Tuple[float, float, float] = (0.0, 0.0, 0.0)
    acceleration: Tuple[float, float, float] = (0.0, 0.0, 0.0)
    mass: float = 1.0
    shape: str = "sphere"  # sphere, cube, plane
    dimensions: Tuple[float, float, float] = (1.0, 1.0, 1.0)
    material: str = "default"
    temperature: float = 293.15  # Kelvin
    charge: float = 0.0
    properties: Dict[str, Any] = field(default_factory=dict)

    def apply_force(self, force: Tuple[float, float, float], dt: float) -> None:
        """Apply force to object for time dt."""
        # F = ma, a = F/m
        ax = force[0] / self.mass
        ay = force[1] / self.mass
        az = force[2] / self.mass

        # Update velocity
        self.velocity = (
            self.velocity[0] + ax * dt,
            self.velocity[1] + ay * dt,
            self.velocity[2] + az * dt,
        )

        # Update position
        self.position = (
            self.position[0] + self.velocity[0] * dt,
            self.position[1] + self.velocity[1] * dt,
            self.position[2] + self.velocity[2] * dt,
        )


@dataclass
class WorldState:
    """Complete state of the simulated world."""
    objects: Dict[str, PhysicalObject]
    time: float = 0.0
    gravity: Tuple[float, float, float] = (0.0, -9.81, 0.0)
    air_density: float = 1.225  # kg/m³
    temperature: float = 293.15  # Kelvin
    constraints: List[Dict[str, Any]] = field(default_factory=list)


@dataclass
class SimulationResult:
    """Result of a world simulation."""
    initial_state: WorldState
    final_state: WorldState
    trajectory: List[WorldState]
    events: List[Dict[str, Any]]
    predictions: Dict[str, Any]
    confidence: float


class WorldModelSimulator:
    """
    Physics-based world model for intuitive understanding.

    Capabilities:
        - Newtonian mechanics simulation
        - Collision detection and response
        - Thermodynamics
        - Simple fluid dynamics
        - Object permanence and tracking
        - Future state prediction

    Environment Variables:
        WORLD_SIM_DT: Simulation timestep
        WORLD_SIM_MAX_STEPS: Maximum simulation steps
        WORLD_SIM_COLLISION_TOLERANCE: Collision detection tolerance
    """

    def __init__(self):
        self._dt = _env_float("WORLD_SIM_DT", 0.01)  # 10ms timestep
        self._max_steps = _env_int("WORLD_SIM_MAX_STEPS", 10000)
        self._collision_tolerance = _env_float("WORLD_SIM_COLLISION_TOLERANCE", 0.01)
        self._current_state: Optional[WorldState] = None
        self._history: Deque[WorldState] = deque(maxlen=1000)
        self._lock = asyncio.Lock()

    async def create_world(
        self,
        objects: Optional[List[PhysicalObject]] = None,
        gravity: Tuple[float, float, float] = (0.0, -9.81, 0.0),
    ) -> WorldState:
        """Create a new world state."""
        async with self._lock:
            self._current_state = WorldState(
                objects={obj.id: obj for obj in (objects or [])},
                gravity=gravity,
            )
            return self._current_state

    async def add_object(self, obj: PhysicalObject) -> None:
        """Add object to current world."""
        async with self._lock:
            if self._current_state:
                self._current_state.objects[obj.id] = obj

    async def simulate(
        self,
        duration: float,
        initial_state: Optional[WorldState] = None,
    ) -> SimulationResult:
        """
        Simulate world physics for given duration.

        Args:
            duration: Time to simulate in seconds
            initial_state: Starting state (uses current if None)

        Returns:
            SimulationResult with trajectory and predictions
        """
        state = initial_state or self._current_state
        if not state:
            raise ValueError("No world state to simulate")

        # Deep copy for initial state
        initial = WorldState(
            objects={k: PhysicalObject(**{f: getattr(v, f) for f in v.__dataclass_fields__})
                    for k, v in state.objects.items()},
            time=state.time,
            gravity=state.gravity,
        )

        trajectory = []
        events = []
        steps = int(duration / self._dt)
        steps = min(steps, self._max_steps)

        for step in range(steps):
            # Store trajectory snapshot periodically
            if step % 10 == 0:
                trajectory.append(self._copy_state(state))

            # Apply physics
            step_events = await self._physics_step(state)
            events.extend(step_events)

            state.time += self._dt

        # Final state
        final = self._copy_state(state)

        # Generate predictions
        predictions = await self._generate_predictions(initial, final, trajectory)

        return SimulationResult(
            initial_state=initial,
            final_state=final,
            trajectory=trajectory,
            events=events,
            predictions=predictions,
            confidence=self._calculate_sim_confidence(steps, events),
        )

    async def _physics_step(self, state: WorldState) -> List[Dict[str, Any]]:
        """Perform one physics simulation step."""
        events = []

        for obj_id, obj in state.objects.items():
            # Apply gravity
            gravity_force = (
                state.gravity[0] * obj.mass,
                state.gravity[1] * obj.mass,
                state.gravity[2] * obj.mass,
            )

            # Apply air resistance (simplified)
            drag = self._calculate_drag(obj, state.air_density)

            # Net force
            net_force = (
                gravity_force[0] + drag[0],
                gravity_force[1] + drag[1],
                gravity_force[2] + drag[2],
            )

            obj.apply_force(net_force, self._dt)

        # Check for collisions
        collision_events = await self._detect_collisions(state)
        events.extend(collision_events)

        # Handle collisions
        for event in collision_events:
            await self._handle_collision(state, event)

        return events

    def _calculate_drag(
        self,
        obj: PhysicalObject,
        air_density: float,
    ) -> Tuple[float, float, float]:
        """Calculate air resistance force."""
        # Simplified drag: F = -0.5 * rho * v² * Cd * A
        cd = 0.47  # Drag coefficient for sphere

        # Approximate cross-sectional area
        area = 3.14159 * (obj.dimensions[0] / 2) ** 2

        speed_sq = sum(v ** 2 for v in obj.velocity)

        if speed_sq < 1e-10:
            return (0.0, 0.0, 0.0)

        speed = math.sqrt(speed_sq)
        drag_magnitude = 0.5 * air_density * speed_sq * cd * area

        # Direction opposite to velocity
        return (
            -drag_magnitude * obj.velocity[0] / speed,
            -drag_magnitude * obj.velocity[1] / speed,
            -drag_magnitude * obj.velocity[2] / speed,
        )

    async def _detect_collisions(
        self,
        state: WorldState,
    ) -> List[Dict[str, Any]]:
        """Detect collisions between objects."""
        events = []
        objects = list(state.objects.values())

        for i, obj1 in enumerate(objects):
            # Ground collision
            if obj1.position[1] < 0:
                events.append({
                    "type": "collision",
                    "objects": [obj1.id, "ground"],
                    "position": obj1.position,
                    "time": state.time,
                })

            # Object-object collisions
            for obj2 in objects[i + 1:]:
                if self._objects_colliding(obj1, obj2):
                    events.append({
                        "type": "collision",
                        "objects": [obj1.id, obj2.id],
                        "position": self._collision_point(obj1, obj2),
                        "time": state.time,
                    })

        return events

    def _objects_colliding(
        self,
        obj1: PhysicalObject,
        obj2: PhysicalObject,
    ) -> bool:
        """Check if two objects are colliding."""
        # Simple sphere collision detection
        distance = math.sqrt(sum(
            (obj1.position[i] - obj2.position[i]) ** 2
            for i in range(3)
        ))

        min_distance = (obj1.dimensions[0] + obj2.dimensions[0]) / 2
        return distance < min_distance + self._collision_tolerance

    def _collision_point(
        self,
        obj1: PhysicalObject,
        obj2: PhysicalObject,
    ) -> Tuple[float, float, float]:
        """Calculate collision point between two objects."""
        return (
            (obj1.position[0] + obj2.position[0]) / 2,
            (obj1.position[1] + obj2.position[1]) / 2,
            (obj1.position[2] + obj2.position[2]) / 2,
        )

    async def _handle_collision(
        self,
        state: WorldState,
        event: Dict[str, Any],
    ) -> None:
        """Handle collision response."""
        objects = event["objects"]

        if "ground" in objects:
            # Ground collision - bounce
            obj_id = [o for o in objects if o != "ground"][0]
            obj = state.objects[obj_id]

            # Reflect velocity with energy loss
            restitution = 0.7  # Coefficient of restitution
            obj.position = (obj.position[0], 0.0, obj.position[2])
            obj.velocity = (
                obj.velocity[0] * 0.9,  # Friction
                -obj.velocity[1] * restitution,
                obj.velocity[2] * 0.9,
            )
        else:
            # Object-object collision
            obj1 = state.objects[objects[0]]
            obj2 = state.objects[objects[1]]

            # Simple elastic collision (simplified)
            # Exchange velocities weighted by mass
            m1, m2 = obj1.mass, obj2.mass
            total_mass = m1 + m2

            new_v1 = tuple(
                (m1 - m2) / total_mass * obj1.velocity[i] +
                (2 * m2) / total_mass * obj2.velocity[i]
                for i in range(3)
            )

            new_v2 = tuple(
                (2 * m1) / total_mass * obj1.velocity[i] +
                (m2 - m1) / total_mass * obj2.velocity[i]
                for i in range(3)
            )

            obj1.velocity = new_v1
            obj2.velocity = new_v2

    async def _generate_predictions(
        self,
        initial: WorldState,
        final: WorldState,
        trajectory: List[WorldState],
    ) -> Dict[str, Any]:
        """Generate predictions from simulation."""
        predictions = {
            "duration": final.time - initial.time,
            "object_states": {},
        }

        for obj_id in initial.objects:
            if obj_id in final.objects:
                init_obj = initial.objects[obj_id]
                final_obj = final.objects[obj_id]

                displacement = tuple(
                    final_obj.position[i] - init_obj.position[i]
                    for i in range(3)
                )

                predictions["object_states"][obj_id] = {
                    "final_position": final_obj.position,
                    "final_velocity": final_obj.velocity,
                    "displacement": displacement,
                    "will_be_at_rest": sum(v ** 2 for v in final_obj.velocity) < 0.01,
                }

        return predictions

    def _copy_state(self, state: WorldState) -> WorldState:
        """Create a copy of world state."""
        return WorldState(
            objects={k: PhysicalObject(**{f: getattr(v, f) for f in v.__dataclass_fields__})
                    for k, v in state.objects.items()},
            time=state.time,
            gravity=state.gravity,
            air_density=state.air_density,
            temperature=state.temperature,
        )

    def _calculate_sim_confidence(
        self,
        steps: int,
        events: List[Dict[str, Any]],
    ) -> float:
        """Calculate confidence in simulation results."""
        # More steps = potentially more error accumulation
        step_penalty = 1.0 - (steps / self._max_steps) * 0.2

        # Many collisions = more uncertainty
        collision_count = len([e for e in events if e.get("type") == "collision"])
        collision_penalty = 1.0 - min(collision_count * 0.05, 0.3)

        return max(0.3, step_penalty * collision_penalty)

    async def predict_outcome(
        self,
        action: str,
        objects: List[PhysicalObject],
    ) -> Dict[str, Any]:
        """
        Predict outcome of an action in the physical world.

        Examples:
            - "drop ball" → predicts where ball lands
            - "push box" → predicts box trajectory
            - "pour water" → predicts water flow
        """
        await self.create_world(objects)

        # Parse action and apply initial conditions
        if "drop" in action.lower():
            # Apply initial velocity downward
            for obj in objects:
                obj.velocity = (0.0, -1.0, 0.0)
        elif "push" in action.lower():
            # Apply initial velocity forward
            for obj in objects:
                obj.velocity = (5.0, 0.0, 0.0)
        elif "throw" in action.lower():
            # Apply initial velocity at angle
            for obj in objects:
                obj.velocity = (10.0, 5.0, 0.0)

        # Simulate for 10 seconds
        result = await self.simulate(10.0)

        return {
            "action": action,
            "predictions": result.predictions,
            "events": result.events,
            "confidence": result.confidence,
            "explanation": self._explain_prediction(action, result),
        }

    def _explain_prediction(
        self,
        action: str,
        result: SimulationResult,
    ) -> str:
        """Generate explanation for prediction."""
        events = result.events
        predictions = result.predictions

        explanations = []

        if events:
            collision_count = len([e for e in events if e.get("type") == "collision"])
            if collision_count > 0:
                explanations.append(f"There will be {collision_count} collision(s).")

        for obj_id, state in predictions.get("object_states", {}).items():
            if state.get("will_be_at_rest"):
                pos = state["final_position"]
                explanations.append(
                    f"Object {obj_id} will come to rest at position ({pos[0]:.1f}, {pos[1]:.1f}, {pos[2]:.1f})."
                )

        return " ".join(explanations) if explanations else "Simulation complete."


# =============================================================================
# 3. THEORY OF MIND ENGINE
# =============================================================================

class MentalStateType(Enum):
    """Types of mental states."""
    BELIEF = "belief"
    DESIRE = "desire"
    INTENTION = "intention"
    EMOTION = "emotion"
    KNOWLEDGE = "knowledge"
    GOAL = "goal"
    PREFERENCE = "preference"
    EXPECTATION = "expectation"


@dataclass
class MentalState:
    """Representation of an agent's mental state."""
    state_type: MentalStateType
    content: Any
    confidence: float = 1.0
    source: str = "inferred"  # observed, inferred, communicated
    timestamp: float = field(default_factory=time.time)
    evidence: List[str] = field(default_factory=list)


@dataclass
class AgentModel:
    """Model of another agent's mind."""
    agent_id: str
    beliefs: Dict[str, MentalState] = field(default_factory=dict)
    desires: Dict[str, MentalState] = field(default_factory=dict)
    intentions: Dict[str, MentalState] = field(default_factory=dict)
    emotions: Dict[str, MentalState] = field(default_factory=dict)
    knowledge: Dict[str, MentalState] = field(default_factory=dict)
    goals: List[MentalState] = field(default_factory=list)
    personality_traits: Dict[str, float] = field(default_factory=dict)
    interaction_history: Deque[Dict[str, Any]] = field(default_factory=lambda: deque(maxlen=100))
    trust_level: float = 0.5
    relationship_type: str = "neutral"


class TheoryOfMindEngine:
    """
    Models mental states of other agents for social intelligence.

    Capabilities:
        - Belief attribution (what does X believe?)
        - Desire inference (what does X want?)
        - Intention prediction (what will X do?)
        - Emotion recognition (how does X feel?)
        - False belief understanding
        - Perspective taking
        - Social reasoning

    Environment Variables:
        TOM_MAX_AGENTS: Maximum agents to model
        TOM_INFERENCE_DEPTH: Levels of recursive reasoning
        TOM_BELIEF_DECAY: Rate of belief confidence decay
    """

    def __init__(self):
        self._agents: Dict[str, AgentModel] = {}
        self._max_agents = _env_int("TOM_MAX_AGENTS", 100)
        self._inference_depth = _env_int("TOM_INFERENCE_DEPTH", 3)
        self._belief_decay = _env_float("TOM_BELIEF_DECAY", 0.01)
        self._lock = asyncio.Lock()

        # Action-intention mappings
        self._action_intentions: Dict[str, List[str]] = {
            "look": ["observe", "search", "check"],
            "move": ["approach", "avoid", "explore"],
            "speak": ["inform", "request", "persuade"],
            "take": ["acquire", "steal", "borrow"],
            "give": ["share", "gift", "trade"],
        }

    async def get_or_create_agent(self, agent_id: str) -> AgentModel:
        """Get existing agent model or create new one."""
        async with self._lock:
            if agent_id not in self._agents:
                if len(self._agents) >= self._max_agents:
                    # Remove oldest interacted agent
                    oldest = min(
                        self._agents.values(),
                        key=lambda a: a.interaction_history[-1]["timestamp"] if a.interaction_history else 0
                    )
                    del self._agents[oldest.agent_id]

                self._agents[agent_id] = AgentModel(agent_id=agent_id)

            return self._agents[agent_id]

    async def observe_action(
        self,
        agent_id: str,
        action: str,
        context: Dict[str, Any],
    ) -> Dict[str, MentalState]:
        """
        Observe an agent's action and infer mental states.

        Args:
            agent_id: ID of the observed agent
            action: Action performed
            context: Contextual information

        Returns:
            Inferred mental states
        """
        agent = await self.get_or_create_agent(agent_id)

        # Record observation
        agent.interaction_history.append({
            "type": "observation",
            "action": action,
            "context": context,
            "timestamp": time.time(),
        })

        inferred = {}

        # Infer intentions from actions
        intentions = await self._infer_intentions(action, context)
        for intent_name, confidence in intentions.items():
            state = MentalState(
                state_type=MentalStateType.INTENTION,
                content=intent_name,
                confidence=confidence,
                source="inferred",
                evidence=[f"observed action: {action}"],
            )
            agent.intentions[intent_name] = state
            inferred[f"intention:{intent_name}"] = state

        # Infer beliefs from actions
        beliefs = await self._infer_beliefs_from_action(action, context)
        for belief_name, (content, confidence) in beliefs.items():
            state = MentalState(
                state_type=MentalStateType.BELIEF,
                content=content,
                confidence=confidence,
                source="inferred",
                evidence=[f"action implies belief: {action}"],
            )
            agent.beliefs[belief_name] = state
            inferred[f"belief:{belief_name}"] = state

        # Infer emotions from actions
        emotions = await self._infer_emotions(action, context)
        for emotion_name, intensity in emotions.items():
            state = MentalState(
                state_type=MentalStateType.EMOTION,
                content={"emotion": emotion_name, "intensity": intensity},
                confidence=0.7,
                source="inferred",
                evidence=[f"action suggests emotion: {action}"],
            )
            agent.emotions[emotion_name] = state
            inferred[f"emotion:{emotion_name}"] = state

        return inferred

    async def _infer_intentions(
        self,
        action: str,
        context: Dict[str, Any],
    ) -> Dict[str, float]:
        """Infer intentions from observed action."""
        intentions = {}

        # Parse action to get base verb
        words = action.lower().split()
        if not words:
            return intentions

        verb = words[0]

        # Look up possible intentions for this action type
        possible_intents = self._action_intentions.get(verb, [])

        # Weight intentions by context
        for intent in possible_intents:
            confidence = 0.5  # Base confidence

            # Adjust based on context
            if context.get("target") and intent in ["acquire", "approach"]:
                confidence += 0.2
            if context.get("urgency") and intent in ["escape", "help"]:
                confidence += 0.3

            intentions[intent] = min(1.0, confidence)

        # Default intention based on action
        if not intentions:
            intentions[f"perform_{verb}"] = 0.6

        return intentions

    async def _infer_beliefs_from_action(
        self,
        action: str,
        context: Dict[str, Any],
    ) -> Dict[str, Tuple[Any, float]]:
        """Infer beliefs from observed action."""
        beliefs = {}

        # If agent looks for something, they believe it exists
        if "look" in action.lower() or "search" in action.lower():
            target = context.get("target")
            if target:
                beliefs["existence"] = (f"{target} exists somewhere", 0.8)

        # If agent avoids something, they believe it's dangerous/undesirable
        if "avoid" in action.lower():
            target = context.get("target")
            if target:
                beliefs["danger"] = (f"{target} is undesirable", 0.7)

        return beliefs

    async def _infer_emotions(
        self,
        action: str,
        context: Dict[str, Any],
    ) -> Dict[str, float]:
        """Infer emotional state from action."""
        emotions = {}

        # Simple emotion inference
        if any(word in action.lower() for word in ["run", "flee", "escape"]):
            emotions["fear"] = 0.7
        elif any(word in action.lower() for word in ["attack", "hit", "yell"]):
            emotions["anger"] = 0.6
        elif any(word in action.lower() for word in ["smile", "laugh", "play"]):
            emotions["joy"] = 0.7
        elif any(word in action.lower() for word in ["cry", "sigh"]):
            emotions["sadness"] = 0.6

        return emotions

    async def predict_action(
        self,
        agent_id: str,
        situation: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Predict what action an agent will take in a situation.

        Uses beliefs, desires, and intentions to predict behavior.
        """
        agent = await self.get_or_create_agent(agent_id)

        predictions = []

        # Consider goals
        for goal in agent.goals:
            actions = await self._actions_toward_goal(goal.content, situation)
            for action, probability in actions.items():
                predictions.append({
                    "action": action,
                    "probability": probability * goal.confidence,
                    "motivation": f"pursuing goal: {goal.content}",
                })

        # Consider intentions
        for intent_name, intent in agent.intentions.items():
            actions = await self._actions_for_intention(intent.content, situation)
            for action, probability in actions.items():
                predictions.append({
                    "action": action,
                    "probability": probability * intent.confidence,
                    "motivation": f"intention: {intent.content}",
                })

        # Aggregate predictions
        action_probs = defaultdict(float)
        motivations = defaultdict(list)

        for pred in predictions:
            action_probs[pred["action"]] += pred["probability"]
            motivations[pred["action"]].append(pred["motivation"])

        # Normalize
        total = sum(action_probs.values())
        if total > 0:
            action_probs = {k: v / total for k, v in action_probs.items()}

        # Get most likely action
        if action_probs:
            best_action = max(action_probs.items(), key=lambda x: x[1])
            return {
                "predicted_action": best_action[0],
                "confidence": best_action[1],
                "motivations": motivations[best_action[0]],
                "alternatives": [
                    {"action": a, "probability": p}
                    for a, p in sorted(action_probs.items(), key=lambda x: -x[1])[:5]
                ],
            }

        return {
            "predicted_action": "unknown",
            "confidence": 0.0,
            "motivations": [],
            "alternatives": [],
        }

    async def _actions_toward_goal(
        self,
        goal: str,
        situation: Dict[str, Any],
    ) -> Dict[str, float]:
        """Determine actions that could achieve a goal."""
        # Simple goal-action mapping
        actions = {}

        goal_lower = goal.lower()

        if "find" in goal_lower:
            actions["search"] = 0.8
            actions["ask"] = 0.5
        elif "obtain" in goal_lower or "get" in goal_lower:
            actions["take"] = 0.7
            actions["buy"] = 0.6
            actions["ask_for"] = 0.4
        elif "avoid" in goal_lower:
            actions["move_away"] = 0.8
            actions["hide"] = 0.5
        elif "help" in goal_lower:
            actions["assist"] = 0.8
            actions["give"] = 0.6

        return actions

    async def _actions_for_intention(
        self,
        intention: str,
        situation: Dict[str, Any],
    ) -> Dict[str, float]:
        """Determine actions that fulfill an intention."""
        actions = {}

        intention_lower = intention.lower()

        if intention_lower in ["inform", "communicate"]:
            actions["speak"] = 0.8
            actions["write"] = 0.4
        elif intention_lower in ["acquire", "obtain"]:
            actions["take"] = 0.7
            actions["reach_for"] = 0.6
        elif intention_lower in ["avoid", "escape"]:
            actions["move_away"] = 0.8
            actions["run"] = 0.5

        return actions

    async def understand_perspective(
        self,
        agent_id: str,
        topic: str,
    ) -> Dict[str, Any]:
        """
        Understand an agent's perspective on a topic.

        Level 2 Theory of Mind: "What does X think about Y?"
        """
        agent = await self.get_or_create_agent(agent_id)

        relevant_beliefs = {
            name: belief for name, belief in agent.beliefs.items()
            if topic.lower() in name.lower() or topic.lower() in str(belief.content).lower()
        }

        relevant_desires = {
            name: desire for name, desire in agent.desires.items()
            if topic.lower() in name.lower() or topic.lower() in str(desire.content).lower()
        }

        return {
            "agent": agent_id,
            "topic": topic,
            "beliefs": {k: {"content": v.content, "confidence": v.confidence}
                       for k, v in relevant_beliefs.items()},
            "desires": {k: {"content": v.content, "confidence": v.confidence}
                       for k, v in relevant_desires.items()},
            "emotional_stance": self._summarize_emotions(agent.emotions),
            "likely_opinion": await self._infer_opinion(agent, topic),
        }

    def _summarize_emotions(self, emotions: Dict[str, MentalState]) -> Dict[str, float]:
        """Summarize current emotional state."""
        return {
            name: state.content.get("intensity", 0.5)
            for name, state in emotions.items()
        }

    async def _infer_opinion(
        self,
        agent: AgentModel,
        topic: str,
    ) -> Dict[str, Any]:
        """Infer agent's opinion on a topic."""
        # Combine beliefs and emotions to form opinion
        positive_indicators = 0
        negative_indicators = 0

        for belief in agent.beliefs.values():
            content_str = str(belief.content).lower()
            if topic.lower() in content_str:
                if any(word in content_str for word in ["good", "helpful", "positive"]):
                    positive_indicators += belief.confidence
                elif any(word in content_str for word in ["bad", "harmful", "negative"]):
                    negative_indicators += belief.confidence

        # Factor in emotions
        if "joy" in agent.emotions:
            positive_indicators += 0.2
        if "anger" in agent.emotions or "fear" in agent.emotions:
            negative_indicators += 0.2

        total = positive_indicators + negative_indicators
        if total == 0:
            return {"stance": "neutral", "confidence": 0.3}

        positivity = positive_indicators / total

        if positivity > 0.6:
            return {"stance": "positive", "confidence": positivity}
        elif positivity < 0.4:
            return {"stance": "negative", "confidence": 1 - positivity}
        else:
            return {"stance": "neutral", "confidence": 0.5}


# =============================================================================
# 4. ABSTRACT REASONING ENGINE
# =============================================================================

class LogicalOperator(Enum):
    """Logical operators for formal reasoning."""
    AND = "and"
    OR = "or"
    NOT = "not"
    IMPLIES = "implies"
    IFF = "iff"  # If and only if
    FORALL = "forall"
    EXISTS = "exists"


@dataclass
class LogicalProposition:
    """A logical proposition."""
    name: str
    value: Optional[bool] = None
    variables: List[str] = field(default_factory=list)
    predicate: Optional[str] = None  # For first-order logic


@dataclass
class LogicalExpression:
    """A logical expression combining propositions."""
    operator: LogicalOperator
    operands: List[Union["LogicalExpression", LogicalProposition]]
    bound_variable: Optional[str] = None  # For quantifiers


@dataclass
class ProofStep:
    """A step in a logical proof."""
    statement: Union[LogicalExpression, LogicalProposition]
    justification: str
    rule_applied: str
    from_steps: List[int] = field(default_factory=list)


@dataclass
class Proof:
    """A complete logical proof."""
    goal: Union[LogicalExpression, LogicalProposition]
    premises: List[Union[LogicalExpression, LogicalProposition]]
    steps: List[ProofStep]
    is_valid: bool
    explanation: str


class AbstractReasoningEngine:
    """
    Formal logic and abstract reasoning capabilities.

    Capabilities:
        - Propositional logic
        - First-order logic
        - Proof construction
        - Pattern recognition
        - Analogical reasoning
        - Mathematical reasoning

    Environment Variables:
        ABSTRACT_MAX_PROOF_STEPS: Maximum steps in proof search
        ABSTRACT_TIMEOUT: Reasoning timeout in seconds
    """

    def __init__(self):
        self._max_proof_steps = _env_int("ABSTRACT_MAX_PROOF_STEPS", 100)
        self._timeout = _env_float("ABSTRACT_TIMEOUT", 30.0)
        self._lock = asyncio.Lock()

        # Knowledge base of known facts and rules
        self._facts: Dict[str, LogicalProposition] = {}
        self._rules: List[LogicalExpression] = []

        # Inference rules
        self._inference_rules = {
            "modus_ponens": self._modus_ponens,
            "modus_tollens": self._modus_tollens,
            "hypothetical_syllogism": self._hypothetical_syllogism,
            "disjunctive_syllogism": self._disjunctive_syllogism,
            "conjunction": self._conjunction,
            "simplification": self._simplification,
        }

    async def add_fact(self, name: str, value: bool = True) -> None:
        """Add a fact to the knowledge base."""
        async with self._lock:
            self._facts[name] = LogicalProposition(name=name, value=value)

    async def add_rule(
        self,
        if_clause: Union[str, LogicalExpression],
        then_clause: Union[str, LogicalExpression],
    ) -> None:
        """Add an inference rule: IF if_clause THEN then_clause."""
        if isinstance(if_clause, str):
            if_clause = LogicalProposition(name=if_clause)
        if isinstance(then_clause, str):
            then_clause = LogicalProposition(name=then_clause)

        rule = LogicalExpression(
            operator=LogicalOperator.IMPLIES,
            operands=[if_clause, then_clause],
        )

        async with self._lock:
            self._rules.append(rule)

    async def prove(
        self,
        goal: Union[str, LogicalExpression],
    ) -> Proof:
        """
        Attempt to prove a goal from known facts and rules.

        Uses forward and backward chaining.
        """
        if isinstance(goal, str):
            goal = LogicalProposition(name=goal)

        steps = []

        # Start with known facts as premises
        premises = list(self._facts.values())

        # Try forward chaining first
        derived = await self._forward_chain(premises, goal, steps)

        if derived:
            return Proof(
                goal=goal,
                premises=premises,
                steps=steps,
                is_valid=True,
                explanation=self._explain_proof(steps),
            )

        # Try backward chaining
        steps = []
        proved = await self._backward_chain(goal, premises, steps, set())

        return Proof(
            goal=goal,
            premises=premises,
            steps=steps,
            is_valid=proved,
            explanation=self._explain_proof(steps) if proved else "Could not prove goal.",
        )

    async def _forward_chain(
        self,
        facts: List[LogicalProposition],
        goal: LogicalProposition,
        steps: List[ProofStep],
    ) -> bool:
        """Forward chaining inference."""
        known = {f.name: f for f in facts}

        for i, fact in enumerate(facts):
            steps.append(ProofStep(
                statement=fact,
                justification="Given premise",
                rule_applied="premise",
            ))

        changed = True
        iterations = 0

        while changed and iterations < self._max_proof_steps:
            changed = False
            iterations += 1

            for rule in self._rules:
                if rule.operator == LogicalOperator.IMPLIES:
                    antecedent, consequent = rule.operands

                    # Check if antecedent is satisfied
                    if await self._is_satisfied(antecedent, known):
                        cons_name = getattr(consequent, "name", str(consequent))

                        if cons_name not in known:
                            known[cons_name] = consequent
                            steps.append(ProofStep(
                                statement=consequent,
                                justification=f"Derived from rule: {antecedent} → {consequent}",
                                rule_applied="modus_ponens",
                            ))
                            changed = True

                            if cons_name == getattr(goal, "name", str(goal)):
                                return True

        return getattr(goal, "name", str(goal)) in known

    async def _backward_chain(
        self,
        goal: LogicalProposition,
        facts: List[LogicalProposition],
        steps: List[ProofStep],
        visited: Set[str],
    ) -> bool:
        """Backward chaining inference."""
        goal_name = getattr(goal, "name", str(goal))

        if goal_name in visited:
            return False
        visited.add(goal_name)

        # Check if goal is already known
        if goal_name in {f.name for f in facts}:
            steps.append(ProofStep(
                statement=goal,
                justification="Known fact",
                rule_applied="fact",
            ))
            return True

        # Find rules that conclude the goal
        for rule in self._rules:
            if rule.operator == LogicalOperator.IMPLIES:
                antecedent, consequent = rule.operands
                cons_name = getattr(consequent, "name", str(consequent))

                if cons_name == goal_name:
                    # Try to prove antecedent
                    if await self._backward_chain(antecedent, facts, steps, visited):
                        steps.append(ProofStep(
                            statement=goal,
                            justification=f"Derived from: {antecedent} → {goal}",
                            rule_applied="modus_ponens",
                        ))
                        return True

        return False

    async def _is_satisfied(
        self,
        expr: Union[LogicalExpression, LogicalProposition],
        known: Dict[str, LogicalProposition],
    ) -> bool:
        """Check if expression is satisfied by known facts."""
        if isinstance(expr, LogicalProposition):
            if expr.name in known:
                fact = known[expr.name]
                return fact.value is True
            return False

        if expr.operator == LogicalOperator.AND:
            return all(await self._is_satisfied(op, known) for op in expr.operands)

        if expr.operator == LogicalOperator.OR:
            return any(await self._is_satisfied(op, known) for op in expr.operands)

        if expr.operator == LogicalOperator.NOT:
            return not await self._is_satisfied(expr.operands[0], known)

        return False

    def _explain_proof(self, steps: List[ProofStep]) -> str:
        """Generate human-readable proof explanation."""
        if not steps:
            return "No proof steps."

        lines = ["Proof:"]
        for i, step in enumerate(steps, 1):
            statement = getattr(step.statement, "name", str(step.statement))
            lines.append(f"{i}. {statement} [{step.rule_applied}]: {step.justification}")

        return "\n".join(lines)

    # Inference rule implementations
    async def _modus_ponens(
        self,
        p: LogicalProposition,
        p_implies_q: LogicalExpression,
    ) -> Optional[LogicalProposition]:
        """If P and P→Q, then Q."""
        if p_implies_q.operator == LogicalOperator.IMPLIES:
            antecedent, consequent = p_implies_q.operands
            if getattr(antecedent, "name", "") == p.name:
                return consequent
        return None

    async def _modus_tollens(
        self,
        not_q: LogicalProposition,
        p_implies_q: LogicalExpression,
    ) -> Optional[LogicalProposition]:
        """If ¬Q and P→Q, then ¬P."""
        if p_implies_q.operator == LogicalOperator.IMPLIES:
            antecedent, consequent = p_implies_q.operands
            if getattr(consequent, "name", "") == not_q.name[4:]:  # Remove "not_"
                return LogicalProposition(name=f"not_{antecedent.name}")
        return None

    async def _hypothetical_syllogism(
        self,
        p_implies_q: LogicalExpression,
        q_implies_r: LogicalExpression,
    ) -> Optional[LogicalExpression]:
        """If P→Q and Q→R, then P→R."""
        if (p_implies_q.operator == LogicalOperator.IMPLIES and
            q_implies_r.operator == LogicalOperator.IMPLIES):
            _, q1 = p_implies_q.operands
            q2, r = q_implies_r.operands
            if getattr(q1, "name", "") == getattr(q2, "name", ""):
                return LogicalExpression(
                    operator=LogicalOperator.IMPLIES,
                    operands=[p_implies_q.operands[0], r],
                )
        return None

    async def _disjunctive_syllogism(
        self,
        p_or_q: LogicalExpression,
        not_p: LogicalProposition,
    ) -> Optional[LogicalProposition]:
        """If P∨Q and ¬P, then Q."""
        if p_or_q.operator == LogicalOperator.OR:
            p, q = p_or_q.operands
            if f"not_{p.name}" == not_p.name:
                return q
        return None

    async def _conjunction(
        self,
        p: LogicalProposition,
        q: LogicalProposition,
    ) -> LogicalExpression:
        """If P and Q, then P∧Q."""
        return LogicalExpression(
            operator=LogicalOperator.AND,
            operands=[p, q],
        )

    async def _simplification(
        self,
        p_and_q: LogicalExpression,
    ) -> List[LogicalProposition]:
        """If P∧Q, then P and Q."""
        if p_and_q.operator == LogicalOperator.AND:
            return list(p_and_q.operands)
        return []

    async def find_analogy(
        self,
        source_domain: Dict[str, Any],
        target_domain: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Find analogical mapping between domains.

        Uses structure mapping theory.
        """
        mappings = {}

        # Extract relations from both domains
        source_relations = self._extract_relations(source_domain)
        target_relations = self._extract_relations(target_domain)

        # Find structural correspondences
        for s_rel, s_args in source_relations.items():
            for t_rel, t_args in target_relations.items():
                # Relations with same arity might correspond
                if len(s_args) == len(t_args):
                    similarity = self._calculate_relation_similarity(s_rel, t_rel)
                    if similarity > 0.5:
                        mappings[s_rel] = {
                            "target_relation": t_rel,
                            "argument_mapping": dict(zip(s_args, t_args)),
                            "similarity": similarity,
                        }

        return {
            "mappings": mappings,
            "confidence": self._calculate_analogy_confidence(mappings),
            "inferences": await self._generate_analogical_inferences(mappings, source_domain, target_domain),
        }

    def _extract_relations(self, domain: Dict[str, Any]) -> Dict[str, List[str]]:
        """Extract relations from domain description."""
        relations = {}

        for key, value in domain.items():
            if isinstance(value, dict) and "relation" in value:
                relations[value["relation"]] = value.get("arguments", [])
            elif isinstance(value, list):
                relations[f"has_{key}"] = value

        return relations

    def _calculate_relation_similarity(self, rel1: str, rel2: str) -> float:
        """Calculate similarity between relation names."""
        # Simple string similarity
        words1 = set(rel1.lower().split("_"))
        words2 = set(rel2.lower().split("_"))

        intersection = len(words1 & words2)
        union = len(words1 | words2)

        return intersection / union if union > 0 else 0

    def _calculate_analogy_confidence(self, mappings: Dict[str, Any]) -> float:
        """Calculate overall confidence in analogy."""
        if not mappings:
            return 0.0

        similarities = [m.get("similarity", 0) for m in mappings.values()]
        return sum(similarities) / len(similarities)

    async def _generate_analogical_inferences(
        self,
        mappings: Dict[str, Any],
        source: Dict[str, Any],
        target: Dict[str, Any],
    ) -> List[Dict[str, Any]]:
        """Generate inferences from analogy."""
        inferences = []

        for source_rel, mapping in mappings.items():
            target_rel = mapping["target_relation"]
            arg_map = mapping["argument_mapping"]

            # If source has a property, predict target has analogous property
            for key, value in source.items():
                if key not in target and key in arg_map:
                    inferences.append({
                        "type": "property_transfer",
                        "source_property": key,
                        "predicted_target_property": arg_map.get(key, key),
                        "confidence": mapping["similarity"] * 0.7,
                    })

        return inferences


# =============================================================================
# 5. LONG-TERM PLANNER
# =============================================================================

class GoalStatus(Enum):
    """Status of a goal."""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    ACHIEVED = "achieved"
    FAILED = "failed"
    ABANDONED = "abandoned"


@dataclass
class Goal:
    """A goal in the planning system."""
    id: str
    description: str
    priority: float = 0.5
    status: GoalStatus = GoalStatus.PENDING
    parent_goal: Optional[str] = None
    subgoals: List[str] = field(default_factory=list)
    preconditions: List[str] = field(default_factory=list)
    effects: List[str] = field(default_factory=list)
    deadline: Optional[float] = None
    created_at: float = field(default_factory=time.time)
    achieved_at: Optional[float] = None
    progress: float = 0.0


@dataclass
class Action:
    """An action that can be taken."""
    id: str
    name: str
    preconditions: List[str]
    effects: List[str]
    cost: float = 1.0
    duration: float = 1.0
    parameters: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Plan:
    """A plan to achieve a goal."""
    goal_id: str
    actions: List[Action]
    total_cost: float
    expected_duration: float
    confidence: float
    alternatives: List["Plan"] = field(default_factory=list)


class LongTermPlanner:
    """
    Hierarchical goal decomposition and multi-step planning.

    Capabilities:
        - Goal-subgoal hierarchies
        - STRIPS-style planning
        - Partial-order planning
        - Plan monitoring and replanning
        - Resource-aware scheduling

    Environment Variables:
        PLANNER_MAX_DEPTH: Maximum goal hierarchy depth
        PLANNER_MAX_PLAN_LENGTH: Maximum actions in a plan
        PLANNER_SEARCH_TIMEOUT: Planning search timeout
    """

    def __init__(self):
        self._goals: Dict[str, Goal] = {}
        self._actions: Dict[str, Action] = {}
        self._current_state: Set[str] = set()
        self._max_depth = _env_int("PLANNER_MAX_DEPTH", 10)
        self._max_plan_length = _env_int("PLANNER_MAX_PLAN_LENGTH", 50)
        self._search_timeout = _env_float("PLANNER_SEARCH_TIMEOUT", 30.0)
        self._lock = asyncio.Lock()

    async def add_goal(
        self,
        description: str,
        priority: float = 0.5,
        parent_goal: Optional[str] = None,
        preconditions: Optional[List[str]] = None,
        effects: Optional[List[str]] = None,
        deadline: Optional[float] = None,
    ) -> Goal:
        """Add a new goal to the planner."""
        goal_id = str(uuid.uuid4())

        goal = Goal(
            id=goal_id,
            description=description,
            priority=priority,
            parent_goal=parent_goal,
            preconditions=preconditions or [],
            effects=effects or [description],  # Default effect is the goal itself
            deadline=deadline,
        )

        async with self._lock:
            self._goals[goal_id] = goal

            if parent_goal and parent_goal in self._goals:
                self._goals[parent_goal].subgoals.append(goal_id)

        return goal

    async def register_action(
        self,
        name: str,
        preconditions: List[str],
        effects: List[str],
        cost: float = 1.0,
        duration: float = 1.0,
        parameters: Optional[Dict[str, Any]] = None,
    ) -> Action:
        """Register an available action."""
        action_id = str(uuid.uuid4())

        action = Action(
            id=action_id,
            name=name,
            preconditions=preconditions,
            effects=effects,
            cost=cost,
            duration=duration,
            parameters=parameters or {},
        )

        async with self._lock:
            self._actions[action_id] = action

        return action

    async def update_state(self, propositions: Set[str]) -> None:
        """Update the current world state."""
        async with self._lock:
            self._current_state = propositions.copy()

    async def add_to_state(self, proposition: str) -> None:
        """Add a proposition to current state."""
        async with self._lock:
            self._current_state.add(proposition)

    async def remove_from_state(self, proposition: str) -> None:
        """Remove a proposition from current state."""
        async with self._lock:
            self._current_state.discard(proposition)

    async def plan(self, goal_id: str) -> Optional[Plan]:
        """
        Create a plan to achieve a goal.

        Uses A* search with goal decomposition.
        """
        async with self._lock:
            goal = self._goals.get(goal_id)
            if not goal:
                return None

            initial_state = self._current_state.copy()
            goal_conditions = set(goal.effects)

            # Check if goal already achieved
            if goal_conditions.issubset(initial_state):
                return Plan(
                    goal_id=goal_id,
                    actions=[],
                    total_cost=0,
                    expected_duration=0,
                    confidence=1.0,
                )

            # A* search
            plan = await self._astar_search(
                initial_state,
                goal_conditions,
            )

            if plan:
                return Plan(
                    goal_id=goal_id,
                    actions=plan,
                    total_cost=sum(a.cost for a in plan),
                    expected_duration=sum(a.duration for a in plan),
                    confidence=self._calculate_plan_confidence(plan),
                )

        return None

    async def _astar_search(
        self,
        initial_state: Set[str],
        goal_conditions: Set[str],
    ) -> Optional[List[Action]]:
        """A* search for a plan."""
        # Priority queue: (f_score, counter, state, actions)
        counter = 0
        start_state = frozenset(initial_state)

        open_set = [(0, counter, start_state, [])]
        closed_set = set()
        g_scores = {start_state: 0}

        start_time = time.time()

        while open_set:
            if time.time() - start_time > self._search_timeout:
                logger.warning("[Planner] Search timeout")
                break

            f_score, _, current_state, actions = heappop(open_set)

            # Goal check
            if goal_conditions.issubset(current_state):
                return actions

            if current_state in closed_set:
                continue

            closed_set.add(current_state)

            # Expand
            for action in self._actions.values():
                # Check preconditions
                if not set(action.preconditions).issubset(current_state):
                    continue

                # Apply action
                new_state = set(current_state)
                # Remove negative effects (simplified - just add effects)
                new_state.update(action.effects)
                new_state = frozenset(new_state)

                new_g = g_scores[current_state] + action.cost

                if new_state in closed_set:
                    continue

                if new_state not in g_scores or new_g < g_scores[new_state]:
                    g_scores[new_state] = new_g
                    f_score = new_g + self._heuristic(new_state, goal_conditions)
                    counter += 1
                    heappush(open_set, (f_score, counter, new_state, actions + [action]))

            if len(actions) >= self._max_plan_length:
                continue

        return None

    def _heuristic(
        self,
        state: FrozenSet[str],
        goal: Set[str],
    ) -> float:
        """Heuristic: count unsatisfied goal conditions."""
        unsatisfied = goal - set(state)
        return len(unsatisfied)

    def _calculate_plan_confidence(self, plan: List[Action]) -> float:
        """Calculate confidence in plan success."""
        if not plan:
            return 1.0

        # Longer plans have lower confidence
        length_penalty = 1.0 - (len(plan) / self._max_plan_length) * 0.3

        # Higher cost plans have lower confidence
        total_cost = sum(a.cost for a in plan)
        avg_cost = total_cost / len(plan)
        cost_penalty = 1.0 / (1.0 + avg_cost * 0.1)

        return max(0.3, length_penalty * cost_penalty)

    async def decompose_goal(
        self,
        goal_id: str,
    ) -> List[Goal]:
        """
        Decompose a goal into subgoals.

        Uses goal decomposition heuristics.
        """
        async with self._lock:
            goal = self._goals.get(goal_id)
            if not goal:
                return []

            subgoals = []

            # Check if there are preconditions to satisfy first
            for precondition in goal.preconditions:
                if precondition not in self._current_state:
                    subgoal = await self.add_goal(
                        description=f"Achieve: {precondition}",
                        priority=goal.priority * 0.9,  # Slightly lower priority
                        parent_goal=goal_id,
                        effects=[precondition],
                    )
                    subgoals.append(subgoal)

            # Add main goal effects as subgoals if multiple
            if len(goal.effects) > 1:
                for effect in goal.effects:
                    if effect not in self._current_state:
                        subgoal = await self.add_goal(
                            description=f"Achieve: {effect}",
                            priority=goal.priority * 0.95,
                            parent_goal=goal_id,
                            effects=[effect],
                        )
                        subgoals.append(subgoal)

            return subgoals

    async def monitor_and_replan(
        self,
        goal_id: str,
        current_plan: Plan,
    ) -> Tuple[bool, Optional[Plan]]:
        """
        Monitor plan execution and replan if needed.

        Returns:
            (plan_still_valid, new_plan_if_needed)
        """
        async with self._lock:
            goal = self._goals.get(goal_id)
            if not goal:
                return False, None

            # Check if goal already achieved
            if set(goal.effects).issubset(self._current_state):
                goal.status = GoalStatus.ACHIEVED
                goal.achieved_at = time.time()
                return True, None

            # Check if remaining plan is still valid
            remaining_actions = []
            simulated_state = self._current_state.copy()

            for action in current_plan.actions:
                # Check if action is still applicable
                if not set(action.preconditions).issubset(simulated_state):
                    # Plan is invalid, need to replan
                    new_plan = await self.plan(goal_id)
                    return False, new_plan

                # Simulate action
                simulated_state.update(action.effects)
                remaining_actions.append(action)

            return True, None


# =============================================================================
# 6. COUNTERFACTUAL REASONING
# =============================================================================

@dataclass
class CounterfactualScenario:
    """A counterfactual scenario for 'what-if' analysis."""
    id: str
    original_state: Dict[str, Any]
    intervention: Dict[str, Any]
    hypothetical_state: Dict[str, Any]
    probability: float
    explanation: str
    consequences: List[str] = field(default_factory=list)


@dataclass
class CounterfactualResult:
    """Result of counterfactual reasoning."""
    query: str
    scenarios: List[CounterfactualScenario]
    most_likely_outcome: Optional[CounterfactualScenario]
    confidence: float
    reasoning_chain: List[str]


class CounterfactualReasoner:
    """
    'What-if' analysis and alternative outcome exploration.

    Capabilities:
        - Counterfactual scenario generation
        - Alternative history simulation
        - Regret analysis
        - Opportunity cost evaluation
        - Hindsight analysis

    Environment Variables:
        COUNTERFACTUAL_MAX_SCENARIOS: Maximum scenarios to generate
        COUNTERFACTUAL_DEPTH: How deep to explore alternatives
    """

    def __init__(self):
        self._max_scenarios = _env_int("COUNTERFACTUAL_MAX_SCENARIOS", 5)
        self._depth = _env_int("COUNTERFACTUAL_DEPTH", 3)
        self._scenario_cache: Dict[str, CounterfactualResult] = {}
        self._lock = asyncio.Lock()

    async def imagine(
        self,
        query: str,
        actual_state: Dict[str, Any],
        intervention: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None,
    ) -> CounterfactualResult:
        """
        Imagine what would have happened with a different intervention.

        Args:
            query: The counterfactual question
            actual_state: What actually happened
            intervention: The hypothetical change
            context: Additional context

        Returns:
            CounterfactualResult with explored scenarios
        """
        context = context or {}
        scenarios: List[CounterfactualScenario] = []
        reasoning_chain: List[str] = []

        reasoning_chain.append(f"Counterfactual query: {query}")
        reasoning_chain.append(f"Actual state: {list(actual_state.keys())}")
        reasoning_chain.append(f"Intervention: {intervention}")

        # Generate primary scenario
        primary_scenario = await self._generate_scenario(
            actual_state=actual_state,
            intervention=intervention,
            context=context,
        )
        scenarios.append(primary_scenario)
        reasoning_chain.append(f"Primary scenario confidence: {primary_scenario.probability:.2f}")

        # Generate alternative scenarios with variations
        for i in range(min(self._max_scenarios - 1, 4)):
            variation = await self._generate_variation(
                base_scenario=primary_scenario,
                variation_index=i,
                context=context,
            )
            if variation:
                scenarios.append(variation)

        # Determine most likely outcome
        most_likely = max(scenarios, key=lambda s: s.probability) if scenarios else None

        # Calculate overall confidence
        if scenarios:
            confidence = sum(s.probability for s in scenarios) / len(scenarios)
        else:
            confidence = 0.0

        return CounterfactualResult(
            query=query,
            scenarios=scenarios,
            most_likely_outcome=most_likely,
            confidence=confidence,
            reasoning_chain=reasoning_chain,
        )

    async def _generate_scenario(
        self,
        actual_state: Dict[str, Any],
        intervention: Dict[str, Any],
        context: Dict[str, Any],
    ) -> CounterfactualScenario:
        """Generate a single counterfactual scenario."""
        # Apply intervention to create hypothetical state
        hypothetical = actual_state.copy()
        hypothetical.update(intervention)

        # Trace consequences
        consequences = await self._trace_consequences(
            original=actual_state,
            modified=hypothetical,
            context=context,
        )

        # Estimate probability based on intervention magnitude
        intervention_magnitude = len(intervention) / max(len(actual_state), 1)
        probability = max(0.1, 1.0 - intervention_magnitude * 0.3)

        return CounterfactualScenario(
            id=str(uuid.uuid4())[:8],
            original_state=actual_state,
            intervention=intervention,
            hypothetical_state=hypothetical,
            probability=probability,
            explanation=f"If {list(intervention.keys())} had been different",
            consequences=consequences,
        )

    async def _generate_variation(
        self,
        base_scenario: CounterfactualScenario,
        variation_index: int,
        context: Dict[str, Any],
    ) -> Optional[CounterfactualScenario]:
        """Generate a variation of the base scenario."""
        # Create slight variations in the intervention
        varied_intervention = base_scenario.intervention.copy()

        # Modify one aspect
        keys = list(varied_intervention.keys())
        if keys:
            key = keys[variation_index % len(keys)]
            value = varied_intervention[key]
            if isinstance(value, (int, float)):
                varied_intervention[key] = value * (0.8 + variation_index * 0.1)
            elif isinstance(value, bool):
                varied_intervention[key] = not value
            elif isinstance(value, str):
                varied_intervention[key] = f"{value}_v{variation_index}"

        hypothetical = base_scenario.original_state.copy()
        hypothetical.update(varied_intervention)

        return CounterfactualScenario(
            id=str(uuid.uuid4())[:8],
            original_state=base_scenario.original_state,
            intervention=varied_intervention,
            hypothetical_state=hypothetical,
            probability=base_scenario.probability * (0.9 - variation_index * 0.1),
            explanation=f"Variation {variation_index + 1}: modified {keys[0] if keys else 'unknown'}",
            consequences=[],
        )

    async def _trace_consequences(
        self,
        original: Dict[str, Any],
        modified: Dict[str, Any],
        context: Dict[str, Any],
    ) -> List[str]:
        """Trace the downstream consequences of the intervention."""
        consequences = []

        # Identify changed variables
        changed = set(modified.keys()) - set(original.keys())
        for key in original:
            if key in modified and original[key] != modified[key]:
                changed.add(key)

        # Generate consequence descriptions
        for key in changed:
            old_val = original.get(key, "N/A")
            new_val = modified.get(key, "N/A")
            consequences.append(f"{key}: {old_val} → {new_val}")

        return consequences[:10]  # Limit consequences

    async def regret_analysis(
        self,
        chosen_action: str,
        outcome: Dict[str, Any],
        alternative_actions: List[str],
        context: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Analyze regret for a decision.

        Compares actual outcome with hypothetical outcomes of alternatives.
        """
        context = context or {}
        regret_scores: Dict[str, float] = {}

        for alt_action in alternative_actions:
            # Imagine alternative outcome
            counterfactual = await self.imagine(
                query=f"What if we had chosen {alt_action} instead of {chosen_action}?",
                actual_state=outcome,
                intervention={"action": alt_action},
                context=context,
            )

            # Calculate regret as opportunity cost
            if counterfactual.most_likely_outcome:
                hypo = counterfactual.most_likely_outcome.hypothetical_state
                # Simple regret: difference in key metrics
                regret = 0.0
                for key in outcome:
                    if key in hypo:
                        try:
                            actual = float(outcome[key])
                            hypothetical = float(hypo[key])
                            regret += max(0, hypothetical - actual)
                        except (ValueError, TypeError):
                            pass
                regret_scores[alt_action] = regret

        return {
            "chosen_action": chosen_action,
            "actual_outcome": outcome,
            "regret_scores": regret_scores,
            "max_regret_action": max(regret_scores, key=regret_scores.get) if regret_scores else None,
            "recommendation": "No regret" if all(r == 0 for r in regret_scores.values()) else "Consider alternatives",
        }


# =============================================================================
# 7. CREATIVE PROBLEM SOLVING
# =============================================================================

class CreativityTechnique(Enum):
    """Creative thinking techniques."""
    BRAINSTORMING = "brainstorming"
    LATERAL_THINKING = "lateral_thinking"
    SCAMPER = "scamper"
    MORPHOLOGICAL_ANALYSIS = "morphological_analysis"
    ANALOGICAL_REASONING = "analogical_reasoning"
    RANDOM_STIMULATION = "random_stimulation"
    REVERSE_BRAINSTORMING = "reverse_brainstorming"


@dataclass
class CreativeIdea:
    """A creative solution idea."""
    id: str
    description: str
    technique: CreativityTechnique
    novelty_score: float  # 0-1, how novel
    feasibility_score: float  # 0-1, how feasible
    impact_score: float  # 0-1, potential impact
    inspiration: Optional[str] = None
    variations: List[str] = field(default_factory=list)


@dataclass
class CreativeSolution:
    """Result of creative problem solving."""
    problem: str
    ideas: List[CreativeIdea]
    best_idea: Optional[CreativeIdea]
    synthesis: Optional[str]  # Combined solution
    reasoning_process: List[str]


class CreativeProblemSolver:
    """
    Divergent thinking and novel solution generation.

    Capabilities:
        - Multiple creative techniques
        - Idea combination and synthesis
        - Novelty evaluation
        - Feasibility assessment
        - Analogical transfer

    Environment Variables:
        CREATIVE_MIN_IDEAS: Minimum ideas to generate
        CREATIVE_NOVELTY_THRESHOLD: Minimum novelty score
    """

    def __init__(self):
        self._min_ideas = _env_int("CREATIVE_MIN_IDEAS", 5)
        self._novelty_threshold = _env_float("CREATIVE_NOVELTY_THRESHOLD", 0.3)
        self._analogy_bank: List[Dict[str, Any]] = []
        self._lock = asyncio.Lock()

        # Initialize analogy bank with domain patterns
        self._init_analogy_bank()

    def _init_analogy_bank(self):
        """Initialize bank of analogies for transfer."""
        self._analogy_bank = [
            {"domain": "nature", "pattern": "swarm_intelligence", "example": "ant colonies optimize paths"},
            {"domain": "nature", "pattern": "evolution", "example": "mutation and selection"},
            {"domain": "physics", "pattern": "equilibrium", "example": "systems seek balance"},
            {"domain": "architecture", "pattern": "modularity", "example": "interchangeable components"},
            {"domain": "economics", "pattern": "incentives", "example": "rewards drive behavior"},
            {"domain": "biology", "pattern": "feedback_loops", "example": "homeostasis"},
            {"domain": "music", "pattern": "composition", "example": "themes and variations"},
            {"domain": "games", "pattern": "strategy", "example": "multi-move planning"},
        ]

    async def solve(
        self,
        problem: str,
        constraints: Optional[List[str]] = None,
        context: Optional[Dict[str, Any]] = None,
    ) -> CreativeSolution:
        """
        Generate creative solutions to a problem.

        Args:
            problem: Problem description
            constraints: Known constraints
            context: Additional context

        Returns:
            CreativeSolution with multiple ideas
        """
        constraints = constraints or []
        context = context or {}
        ideas: List[CreativeIdea] = []
        reasoning: List[str] = []

        reasoning.append(f"Problem: {problem}")
        reasoning.append(f"Constraints: {constraints}")

        # Apply multiple techniques
        techniques_to_use = [
            CreativityTechnique.BRAINSTORMING,
            CreativityTechnique.ANALOGICAL_REASONING,
            CreativityTechnique.LATERAL_THINKING,
            CreativityTechnique.SCAMPER,
        ]

        for technique in techniques_to_use:
            technique_ideas = await self._apply_technique(
                problem=problem,
                technique=technique,
                constraints=constraints,
                context=context,
            )
            ideas.extend(technique_ideas)
            reasoning.append(f"{technique.value}: generated {len(technique_ideas)} ideas")

        # Score and rank ideas
        for idea in ideas:
            idea.novelty_score = await self._evaluate_novelty(idea, ideas)
            idea.feasibility_score = await self._evaluate_feasibility(idea, constraints)
            idea.impact_score = await self._evaluate_impact(idea, context)

        # Filter by novelty threshold
        ideas = [i for i in ideas if i.novelty_score >= self._novelty_threshold]

        # Select best idea by composite score
        if ideas:
            best = max(ideas, key=lambda i: (
                i.novelty_score * 0.3 + i.feasibility_score * 0.4 + i.impact_score * 0.3
            ))
        else:
            best = None

        # Synthesize top ideas
        synthesis = await self._synthesize_ideas(ideas[:3]) if len(ideas) >= 2 else None

        return CreativeSolution(
            problem=problem,
            ideas=ideas,
            best_idea=best,
            synthesis=synthesis,
            reasoning_process=reasoning,
        )

    async def _apply_technique(
        self,
        problem: str,
        technique: CreativityTechnique,
        constraints: List[str],
        context: Dict[str, Any],
    ) -> List[CreativeIdea]:
        """Apply a specific creative technique."""
        ideas: List[CreativeIdea] = []

        if technique == CreativityTechnique.BRAINSTORMING:
            ideas = await self._brainstorm(problem, constraints)
        elif technique == CreativityTechnique.ANALOGICAL_REASONING:
            ideas = await self._analogical_reasoning(problem, context)
        elif technique == CreativityTechnique.LATERAL_THINKING:
            ideas = await self._lateral_thinking(problem)
        elif technique == CreativityTechnique.SCAMPER:
            ideas = await self._scamper(problem)
        elif technique == CreativityTechnique.REVERSE_BRAINSTORMING:
            ideas = await self._reverse_brainstorm(problem)

        return ideas

    async def _brainstorm(
        self,
        problem: str,
        constraints: List[str],
    ) -> List[CreativeIdea]:
        """Generate ideas through free association."""
        ideas = []
        keywords = problem.lower().split()

        # Generate ideas based on problem keywords
        for i, keyword in enumerate(keywords[:5]):
            idea = CreativeIdea(
                id=f"brain_{i}",
                description=f"Solution focusing on: {keyword}",
                technique=CreativityTechnique.BRAINSTORMING,
                novelty_score=0.5,
                feasibility_score=0.5,
                impact_score=0.5,
            )
            ideas.append(idea)

        return ideas

    async def _analogical_reasoning(
        self,
        problem: str,
        context: Dict[str, Any],
    ) -> List[CreativeIdea]:
        """Generate ideas by transferring solutions from other domains."""
        ideas = []

        for analogy in self._analogy_bank:
            # Check if analogy pattern might apply
            pattern = analogy["pattern"]
            example = analogy["example"]
            domain = analogy["domain"]

            idea = CreativeIdea(
                id=f"analog_{domain}_{pattern}",
                description=f"Apply {pattern} pattern from {domain}: {example}",
                technique=CreativityTechnique.ANALOGICAL_REASONING,
                novelty_score=0.7,  # Analogies tend to be novel
                feasibility_score=0.4,  # May need adaptation
                impact_score=0.6,
                inspiration=f"{domain}/{pattern}",
            )
            ideas.append(idea)

        return ideas[:3]  # Return top 3 analogies

    async def _lateral_thinking(
        self,
        problem: str,
    ) -> List[CreativeIdea]:
        """Generate ideas by challenging assumptions."""
        ideas = []

        # Challenge common assumptions
        challenges = [
            ("assumption_reversal", "What if we did the opposite?"),
            ("constraint_removal", "What if there were no constraints?"),
            ("perspective_shift", "How would a child/expert/alien solve this?"),
            ("time_shift", "How would this be solved in 100 years?"),
        ]

        for challenge_id, challenge in challenges:
            idea = CreativeIdea(
                id=f"lateral_{challenge_id}",
                description=f"{challenge} Applied to: {problem[:50]}",
                technique=CreativityTechnique.LATERAL_THINKING,
                novelty_score=0.8,
                feasibility_score=0.3,
                impact_score=0.5,
            )
            ideas.append(idea)

        return ideas

    async def _scamper(
        self,
        problem: str,
    ) -> List[CreativeIdea]:
        """Apply SCAMPER technique (Substitute, Combine, Adapt, etc.)."""
        scamper_ops = [
            ("S", "Substitute", "Replace a component"),
            ("C", "Combine", "Merge with something else"),
            ("A", "Adapt", "Adjust for a different use"),
            ("M", "Modify", "Change attributes"),
            ("P", "Put to other use", "Use for something else"),
            ("E", "Eliminate", "Remove unnecessary parts"),
            ("R", "Reverse/Rearrange", "Change order or direction"),
        ]

        ideas = []
        for letter, name, description in scamper_ops:
            idea = CreativeIdea(
                id=f"scamper_{letter}",
                description=f"{name}: {description}",
                technique=CreativityTechnique.SCAMPER,
                novelty_score=0.6,
                feasibility_score=0.6,
                impact_score=0.5,
            )
            ideas.append(idea)

        return ideas

    async def _reverse_brainstorm(
        self,
        problem: str,
    ) -> List[CreativeIdea]:
        """Generate ideas by thinking how to make the problem worse."""
        ideas = []

        # Think of ways to worsen, then invert
        idea = CreativeIdea(
            id="reverse_1",
            description=f"Inversion: Think of ways to worsen, then do opposite",
            technique=CreativityTechnique.REVERSE_BRAINSTORMING,
            novelty_score=0.7,
            feasibility_score=0.5,
            impact_score=0.6,
        )
        ideas.append(idea)

        return ideas

    async def _evaluate_novelty(
        self,
        idea: CreativeIdea,
        all_ideas: List[CreativeIdea],
    ) -> float:
        """Evaluate how novel an idea is compared to others."""
        # Simple novelty: based on technique diversity
        technique_count = len(set(i.technique for i in all_ideas))
        base_novelty = idea.novelty_score

        # Ideas from less common techniques are more novel
        technique_frequency = sum(1 for i in all_ideas if i.technique == idea.technique)
        frequency_bonus = 1.0 - (technique_frequency / max(len(all_ideas), 1))

        return min(1.0, base_novelty * 0.7 + frequency_bonus * 0.3)

    async def _evaluate_feasibility(
        self,
        idea: CreativeIdea,
        constraints: List[str],
    ) -> float:
        """Evaluate how feasible an idea is given constraints."""
        # Base feasibility from idea
        feasibility = idea.feasibility_score

        # Reduce for each constraint (simplified)
        constraint_penalty = len(constraints) * 0.1
        return max(0.1, feasibility - constraint_penalty)

    async def _evaluate_impact(
        self,
        idea: CreativeIdea,
        context: Dict[str, Any],
    ) -> float:
        """Evaluate potential impact of an idea."""
        return idea.impact_score

    async def _synthesize_ideas(
        self,
        ideas: List[CreativeIdea],
    ) -> str:
        """Combine multiple ideas into a synthesized solution."""
        if not ideas:
            return ""

        descriptions = [idea.description for idea in ideas]
        return f"Combined solution incorporating: {'; '.join(descriptions[:3])}"


# =============================================================================
# 8. SELF-IMPROVEMENT / META-LEARNING
# =============================================================================

@dataclass
class LearningExperience:
    """A learning experience for meta-learning."""
    id: str
    task_type: str
    input_features: Dict[str, Any]
    strategy_used: str
    outcome: float  # 0-1 success
    timestamp: float
    insights: List[str] = field(default_factory=list)


@dataclass
class StrategyPerformance:
    """Performance metrics for a strategy."""
    strategy: str
    attempts: int
    successes: int
    average_outcome: float
    confidence: float
    context_patterns: List[str] = field(default_factory=list)


class MetaLearner:
    """
    Self-improvement through experience analysis with persistent storage.

    Capabilities:
        - Strategy selection optimization
        - Performance tracking
        - Weakness identification
        - Improvement recommendations
        - Adaptive behavior
        - PERSISTENT STORAGE (survives restarts)

    Note: This is meta-learning (learning to learn), not code self-modification.

    Environment Variables:
        META_LEARNING_WINDOW: Number of experiences to analyze
        META_MIN_SAMPLES: Minimum samples before adapting
        META_PERSISTENCE_DIR: Directory for persistent storage
        META_AUTOSAVE_INTERVAL: Auto-save interval in experiences
    """

    def __init__(self):
        self._learning_window = _env_int("META_LEARNING_WINDOW", 1000)  # Increased for persistence
        self._min_samples = _env_int("META_MIN_SAMPLES", 5)
        self._autosave_interval = _env_int("META_AUTOSAVE_INTERVAL", 10)
        self._experiences: Deque[LearningExperience] = deque(maxlen=self._learning_window)
        self._strategy_stats: Dict[str, StrategyPerformance] = {}
        self._lock = asyncio.Lock()

        # Persistence
        self._persistence_dir = Path(
            os.getenv("META_PERSISTENCE_DIR", str(Path.home() / ".jarvis" / "meta_learner"))
        )
        self._persistence_dir.mkdir(parents=True, exist_ok=True)
        self._experiences_since_save = 0
        self._loaded = False

    async def _ensure_loaded(self) -> None:
        """Ensure persistent state is loaded."""
        if self._loaded:
            return
        await self.load_state()
        self._loaded = True

    async def save_state(self) -> bool:
        """
        Save current state to persistent storage.

        Returns:
            True if saved successfully
        """
        async with self._lock:
            try:
                # Save experiences
                experiences_file = self._persistence_dir / "experiences.jsonl"
                with open(experiences_file, "w") as f:
                    for exp in self._experiences:
                        line = json.dumps({
                            "id": exp.id,
                            "task_type": exp.task_type,
                            "input_features": exp.input_features,
                            "strategy_used": exp.strategy_used,
                            "outcome": exp.outcome,
                            "timestamp": exp.timestamp,
                            "insights": exp.insights,
                        }, default=str)
                        f.write(line + "\n")

                # Save strategy stats
                stats_file = self._persistence_dir / "strategy_stats.json"
                stats_data = {
                    name: {
                        "strategy": s.strategy,
                        "attempts": s.attempts,
                        "successes": s.successes,
                        "average_outcome": s.average_outcome,
                        "confidence": s.confidence,
                        "context_patterns": s.context_patterns,
                    }
                    for name, s in self._strategy_stats.items()
                }
                temp_file = stats_file.with_suffix(".tmp")
                temp_file.write_text(json.dumps(stats_data, indent=2))
                temp_file.rename(stats_file)

                # Save metadata
                meta_file = self._persistence_dir / "metadata.json"
                meta_data = {
                    "total_experiences": len(self._experiences),
                    "strategies_count": len(self._strategy_stats),
                    "saved_at": time.time(),
                    "version": "2.0",
                }
                meta_file.write_text(json.dumps(meta_data, indent=2))

                self._experiences_since_save = 0
                logger.debug(f"[MetaLearner] Saved {len(self._experiences)} experiences")
                return True

            except Exception as e:
                logger.error(f"[MetaLearner] Save failed: {e}")
                return False

    async def load_state(self) -> bool:
        """
        Load state from persistent storage.

        Returns:
            True if loaded successfully
        """
        async with self._lock:
            try:
                # Load experiences
                experiences_file = self._persistence_dir / "experiences.jsonl"
                if experiences_file.exists():
                    with open(experiences_file, "r") as f:
                        for line in f:
                            if line.strip():
                                data = json.loads(line)
                                exp = LearningExperience(
                                    id=data["id"],
                                    task_type=data["task_type"],
                                    input_features=data["input_features"],
                                    strategy_used=data["strategy_used"],
                                    outcome=data["outcome"],
                                    timestamp=data["timestamp"],
                                    insights=data.get("insights", []),
                                )
                                self._experiences.append(exp)

                # Load strategy stats
                stats_file = self._persistence_dir / "strategy_stats.json"
                if stats_file.exists():
                    stats_data = json.loads(stats_file.read_text())
                    for name, s in stats_data.items():
                        self._strategy_stats[name] = StrategyPerformance(
                            strategy=s["strategy"],
                            attempts=s["attempts"],
                            successes=s["successes"],
                            average_outcome=s["average_outcome"],
                            confidence=s["confidence"],
                            context_patterns=s.get("context_patterns", []),
                        )

                logger.info(
                    f"[MetaLearner] Loaded {len(self._experiences)} experiences, "
                    f"{len(self._strategy_stats)} strategies"
                )
                return True

            except Exception as e:
                logger.warning(f"[MetaLearner] Load failed (starting fresh): {e}")
                return False

    async def record_experience(
        self,
        task_type: str,
        input_features: Dict[str, Any],
        strategy_used: str,
        outcome: float,
        insights: Optional[List[str]] = None,
    ) -> None:
        """Record a learning experience with auto-persistence."""
        await self._ensure_loaded()

        async with self._lock:
            experience = LearningExperience(
                id=str(uuid.uuid4())[:8],
                task_type=task_type,
                input_features=input_features,
                strategy_used=strategy_used,
                outcome=outcome,
                timestamp=time.time(),
                insights=insights or [],
            )
            self._experiences.append(experience)
            self._experiences_since_save += 1

            # Update strategy statistics (without lock, we're already in lock)
            await self._update_strategy_stats_unlocked(experience)

        # Auto-save periodically (outside lock)
        if self._experiences_since_save >= self._autosave_interval:
            await self.save_state()

    async def _update_strategy_stats_unlocked(self, exp: LearningExperience) -> None:
        """Update strategy performance statistics (caller must hold lock)."""
        strategy = exp.strategy_used

        if strategy not in self._strategy_stats:
            self._strategy_stats[strategy] = StrategyPerformance(
                strategy=strategy,
                attempts=0,
                successes=0,
                average_outcome=0.0,
                confidence=0.0,
            )

        stats = self._strategy_stats[strategy]
        stats.attempts += 1
        if exp.outcome >= 0.5:
            stats.successes += 1

        # Running average
        stats.average_outcome = (
            (stats.average_outcome * (stats.attempts - 1) + exp.outcome) / stats.attempts
        )

        # Confidence based on sample size
        stats.confidence = min(1.0, stats.attempts / self._min_samples)

    async def recommend_strategy(
        self,
        task_type: str,
        input_features: Dict[str, Any],
        available_strategies: List[str],
    ) -> Tuple[str, float]:
        """
        Recommend the best strategy for a task based on past performance.

        Returns:
            (recommended_strategy, confidence)
        """
        await self._ensure_loaded()

        async with self._lock:
            best_strategy = available_strategies[0] if available_strategies else "default"
            best_score = 0.0

            for strategy in available_strategies:
                if strategy in self._strategy_stats:
                    stats = self._strategy_stats[strategy]
                    if stats.attempts >= self._min_samples:
                        score = stats.average_outcome * stats.confidence
                        if score > best_score:
                            best_score = score
                            best_strategy = strategy

            return best_strategy, best_score

    async def identify_weaknesses(self) -> List[Dict[str, Any]]:
        """Identify areas where performance is weak."""
        weaknesses = []

        for strategy, stats in self._strategy_stats.items():
            if stats.attempts >= self._min_samples and stats.average_outcome < 0.5:
                weaknesses.append({
                    "strategy": strategy,
                    "average_outcome": stats.average_outcome,
                    "attempts": stats.attempts,
                    "recommendation": f"Consider improving or replacing strategy: {strategy}",
                })

        return sorted(weaknesses, key=lambda w: w["average_outcome"])

    async def get_improvement_insights(self) -> Dict[str, Any]:
        """Get insights for self-improvement."""
        # Analyze recent experiences
        recent = list(self._experiences)[-20:]
        if not recent:
            return {"message": "Not enough experiences to analyze"}

        # Calculate success rate
        success_rate = sum(1 for e in recent if e.outcome >= 0.5) / len(recent)

        # Identify patterns in failures
        failures = [e for e in recent if e.outcome < 0.5]
        failure_strategies = [e.strategy_used for e in failures]

        # Strategy rankings
        strategy_rankings = sorted(
            self._strategy_stats.values(),
            key=lambda s: s.average_outcome,
            reverse=True,
        )

        return {
            "recent_success_rate": success_rate,
            "total_experiences": len(self._experiences),
            "strategies_analyzed": len(self._strategy_stats),
            "top_strategies": [
                {"name": s.strategy, "score": s.average_outcome}
                for s in strategy_rankings[:3]
            ],
            "weak_strategies": [
                {"name": s.strategy, "score": s.average_outcome}
                for s in strategy_rankings[-3:]
                if s.average_outcome < 0.5
            ],
            "failure_pattern": {
                "count": len(failures),
                "common_strategies": list(set(failure_strategies)),
            },
        }


# =============================================================================
# 9. ETHICS FRAMEWORK
# =============================================================================

class EthicalPrinciple(Enum):
    """Core ethical principles."""
    BENEFICENCE = "beneficence"          # Do good
    NON_MALEFICENCE = "non_maleficence"  # Do no harm
    AUTONOMY = "autonomy"                 # Respect user autonomy
    JUSTICE = "justice"                   # Fair treatment
    TRANSPARENCY = "transparency"         # Be honest and clear
    PRIVACY = "privacy"                   # Protect privacy
    SAFETY = "safety"                     # Ensure safety
    ACCOUNTABILITY = "accountability"     # Take responsibility


@dataclass
class EthicalConstraint:
    """A specific ethical constraint."""
    id: str
    principle: EthicalPrinciple
    description: str
    severity: float  # 0 = advisory, 1 = absolute
    conditions: List[str] = field(default_factory=list)
    exceptions: List[str] = field(default_factory=list)


@dataclass
class EthicalEvaluation:
    """Result of ethical evaluation."""
    action: str
    is_permissible: bool
    confidence: float
    violated_constraints: List[EthicalConstraint]
    satisfied_constraints: List[EthicalConstraint]
    recommendations: List[str]
    explanation: str


class EthicsFramework:
    """
    Value alignment and moral reasoning framework.

    Capabilities:
        - Ethical constraint checking
        - Value-based decision making
        - Harm assessment
        - Privacy protection
        - Fairness evaluation
        - Moral dilemma resolution

    Environment Variables:
        ETHICS_STRICT_MODE: Enforce all constraints strictly
        ETHICS_HARM_THRESHOLD: Threshold for harm rejection
        ETHICS_LOGGING_ENABLED: Log ethical decisions
    """

    def __init__(self):
        self._constraints: Dict[str, EthicalConstraint] = {}
        self._strict_mode = _env_bool("ETHICS_STRICT_MODE", True)
        self._harm_threshold = _env_float("ETHICS_HARM_THRESHOLD", 0.3)
        self._logging_enabled = _env_bool("ETHICS_LOGGING_ENABLED", True)
        self._lock = asyncio.Lock()
        self._decision_history: Deque[EthicalEvaluation] = deque(maxlen=1000)

        # Initialize core constraints
        asyncio.create_task(self._init_core_constraints())

    async def _init_core_constraints(self) -> None:
        """Initialize core ethical constraints."""
        core_constraints = [
            EthicalConstraint(
                id="no_harm",
                principle=EthicalPrinciple.NON_MALEFICENCE,
                description="Do not cause harm to users or others",
                severity=1.0,
                conditions=["involves_harm", "causes_damage"],
            ),
            EthicalConstraint(
                id="respect_privacy",
                principle=EthicalPrinciple.PRIVACY,
                description="Protect user privacy and data",
                severity=0.9,
                conditions=["accesses_private_data", "shares_personal_info"],
                exceptions=["user_consent"],
            ),
            EthicalConstraint(
                id="be_honest",
                principle=EthicalPrinciple.TRANSPARENCY,
                description="Do not deceive users",
                severity=0.9,
                conditions=["deception", "misleading"],
            ),
            EthicalConstraint(
                id="respect_autonomy",
                principle=EthicalPrinciple.AUTONOMY,
                description="Respect user's right to make their own decisions",
                severity=0.8,
                conditions=["overrides_user_choice", "forces_action"],
            ),
            EthicalConstraint(
                id="ensure_safety",
                principle=EthicalPrinciple.SAFETY,
                description="Prioritize user safety",
                severity=1.0,
                conditions=["unsafe_action", "dangerous_output"],
            ),
            EthicalConstraint(
                id="fair_treatment",
                principle=EthicalPrinciple.JUSTICE,
                description="Treat all users fairly and without bias",
                severity=0.8,
                conditions=["discriminatory", "biased"],
            ),
        ]

        for constraint in core_constraints:
            await self.add_constraint(constraint)

    async def add_constraint(self, constraint: EthicalConstraint) -> None:
        """Add an ethical constraint."""
        async with self._lock:
            self._constraints[constraint.id] = constraint

    async def evaluate_action(
        self,
        action: str,
        context: Dict[str, Any],
    ) -> EthicalEvaluation:
        """
        Evaluate whether an action is ethically permissible.

        Args:
            action: Description of the proposed action
            context: Contextual information

        Returns:
            EthicalEvaluation with permission decision and explanation
        """
        violated = []
        satisfied = []
        recommendations = []

        async with self._lock:
            for constraint in self._constraints.values():
                is_violated = await self._check_violation(action, context, constraint)

                if is_violated:
                    violated.append(constraint)

                    # Check for exceptions
                    exception_applies = await self._check_exceptions(context, constraint)
                    if exception_applies:
                        satisfied.append(constraint)
                        violated.remove(constraint)
                else:
                    satisfied.append(constraint)

        # Determine if action is permissible
        is_permissible = True
        if violated:
            if self._strict_mode:
                is_permissible = False
            else:
                # Check severity
                max_severity = max(c.severity for c in violated)
                is_permissible = max_severity < 0.8

        # Calculate confidence
        confidence = self._calculate_ethics_confidence(violated, satisfied)

        # Generate recommendations
        for constraint in violated:
            recommendations.append(
                f"Consider: {constraint.description} ({constraint.principle.value})"
            )

        # Generate explanation
        explanation = self._generate_ethics_explanation(
            action, is_permissible, violated, satisfied
        )

        evaluation = EthicalEvaluation(
            action=action,
            is_permissible=is_permissible,
            confidence=confidence,
            violated_constraints=violated,
            satisfied_constraints=satisfied,
            recommendations=recommendations,
            explanation=explanation,
        )

        # Log decision
        if self._logging_enabled:
            self._decision_history.append(evaluation)
            if not is_permissible:
                logger.warning(f"[Ethics] Action rejected: {action[:50]}... Violations: {[c.id for c in violated]}")

        return evaluation

    async def _check_violation(
        self,
        action: str,
        context: Dict[str, Any],
        constraint: EthicalConstraint,
    ) -> bool:
        """Check if action violates a constraint."""
        action_lower = action.lower()

        # Check if any condition keywords are present
        for condition in constraint.conditions:
            # Direct keyword match
            if condition.replace("_", " ") in action_lower:
                return True

            # Check context flags
            if context.get(condition, False):
                return True

        # Semantic checks based on principle
        if constraint.principle == EthicalPrinciple.NON_MALEFICENCE:
            harm_words = ["harm", "damage", "hurt", "destroy", "kill", "attack"]
            if any(word in action_lower for word in harm_words):
                return True

        elif constraint.principle == EthicalPrinciple.PRIVACY:
            privacy_words = ["password", "private", "personal", "secret", "confidential"]
            action_words = ["share", "send", "expose", "leak", "reveal"]
            if any(p in action_lower for p in privacy_words) and any(a in action_lower for a in action_words):
                return True

        elif constraint.principle == EthicalPrinciple.TRANSPARENCY:
            deception_words = ["lie", "deceive", "trick", "mislead", "pretend"]
            if any(word in action_lower for word in deception_words):
                return True

        elif constraint.principle == EthicalPrinciple.SAFETY:
            unsafe_words = ["unsafe", "dangerous", "risky", "hazardous"]
            if any(word in action_lower for word in unsafe_words):
                return True

        return False

    async def _check_exceptions(
        self,
        context: Dict[str, Any],
        constraint: EthicalConstraint,
    ) -> bool:
        """Check if an exception applies to the constraint."""
        for exception in constraint.exceptions:
            if context.get(exception, False):
                return True
        return False

    def _calculate_ethics_confidence(
        self,
        violated: List[EthicalConstraint],
        satisfied: List[EthicalConstraint],
    ) -> float:
        """Calculate confidence in ethical evaluation."""
        total = len(violated) + len(satisfied)
        if total == 0:
            return 0.5

        # Weight by severity
        violated_weight = sum(c.severity for c in violated)
        satisfied_weight = sum(c.severity for c in satisfied)

        total_weight = violated_weight + satisfied_weight
        if total_weight == 0:
            return 0.5

        return satisfied_weight / total_weight

    def _generate_ethics_explanation(
        self,
        action: str,
        is_permissible: bool,
        violated: List[EthicalConstraint],
        satisfied: List[EthicalConstraint],
    ) -> str:
        """Generate human-readable ethics explanation."""
        if is_permissible:
            if not violated:
                return f"Action '{action[:50]}...' is ethically permissible. All ethical constraints satisfied."
            else:
                return (
                    f"Action '{action[:50]}...' is permissible with caution. "
                    f"Minor concerns: {', '.join(c.principle.value for c in violated)}"
                )
        else:
            return (
                f"Action '{action[:50]}...' is NOT permissible. "
                f"Violated principles: {', '.join(c.principle.value for c in violated)}. "
                f"Please reconsider or modify the action."
            )

    async def assess_harm(
        self,
        action: str,
        stakeholders: List[str],
    ) -> Dict[str, Any]:
        """
        Assess potential harm to stakeholders.

        Returns harm assessment for each stakeholder.
        """
        assessments = {}

        for stakeholder in stakeholders:
            harm_score = await self._estimate_harm(action, stakeholder)

            assessments[stakeholder] = {
                "harm_score": harm_score,
                "risk_level": self._categorize_risk(harm_score),
                "mitigations": await self._suggest_mitigations(action, stakeholder, harm_score),
            }

        overall_harm = sum(a["harm_score"] for a in assessments.values()) / len(stakeholders) if stakeholders else 0

        return {
            "stakeholder_assessments": assessments,
            "overall_harm": overall_harm,
            "is_acceptable": overall_harm < self._harm_threshold,
            "recommendation": "proceed" if overall_harm < self._harm_threshold else "reconsider",
        }

    async def _estimate_harm(self, action: str, stakeholder: str) -> float:
        """Estimate harm score for a stakeholder."""
        harm_score = 0.0
        action_lower = action.lower()

        # Check for harmful keywords
        harm_keywords = {
            "delete": 0.3,
            "remove": 0.2,
            "modify": 0.1,
            "access": 0.1,
            "share": 0.2,
            "send": 0.1,
            "harm": 0.8,
            "damage": 0.7,
            "destroy": 0.9,
        }

        for keyword, weight in harm_keywords.items():
            if keyword in action_lower:
                harm_score += weight

        # Cap at 1.0
        return min(1.0, harm_score)

    def _categorize_risk(self, harm_score: float) -> str:
        """Categorize risk level from harm score."""
        if harm_score < 0.2:
            return "low"
        elif harm_score < 0.5:
            return "medium"
        elif harm_score < 0.8:
            return "high"
        else:
            return "critical"

    async def _suggest_mitigations(
        self,
        action: str,
        stakeholder: str,
        harm_score: float,
    ) -> List[str]:
        """Suggest mitigations for potential harm."""
        mitigations = []

        if harm_score > 0.3:
            mitigations.append("Obtain explicit consent before proceeding")
        if harm_score > 0.5:
            mitigations.append("Implement safeguards and reversibility")
        if harm_score > 0.7:
            mitigations.append("Consider alternative approaches")
            mitigations.append("Require human oversight for this action")

        return mitigations


# =============================================================================
# UNIFIED COGNITIVE SYSTEM
# =============================================================================

class CognitiveSystem:
    """
    Unified cognitive system integrating all reasoning capabilities.

    Provides a single interface to:
        - Causal reasoning
        - World simulation
        - Theory of mind
        - Abstract reasoning
        - Long-term planning
        - Counterfactual reasoning
        - Creative problem solving
        - Meta-learning / self-improvement
        - Ethical evaluation
    """

    def __init__(self):
        # Core reasoning modules
        self.causal = CausalReasoningEngine()
        self.world = WorldModelSimulator()
        self.tom = TheoryOfMindEngine()
        self.abstract = AbstractReasoningEngine()
        self.planner = LongTermPlanner()

        # Extended cognitive capabilities
        self.counterfactual = CounterfactualReasoner()
        self.creative = CreativeProblemSolver()
        self.meta_learner = MetaLearner()

        # Ethics (always last to govern decisions)
        self.ethics = EthicsFramework()

        self._initialized = False
        self._lock = asyncio.Lock()

    async def initialize(self) -> None:
        """Initialize all cognitive components."""
        if self._initialized:
            return

        async with self._lock:
            if self._initialized:
                return

            logger.info("[Cognitive] Initializing cognitive system...")

            # Initialize components that need setup
            # Most are initialized on first use

            self._initialized = True
            logger.info("[Cognitive] Cognitive system initialized")

    async def reason(
        self,
        query: str,
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        High-level reasoning entry point.

        Automatically routes to appropriate cognitive module.
        """
        await self.initialize()

        query_lower = query.lower()

        # Determine reasoning type needed
        if any(word in query_lower for word in ["why", "cause", "because", "effect"]):
            return await self._causal_reasoning(query, context)

        elif any(word in query_lower for word in ["what if", "would happen", "predict"]):
            return await self._predictive_reasoning(query, context)

        elif any(word in query_lower for word in ["think", "believe", "want", "feel"]):
            return await self._social_reasoning(query, context)

        elif any(word in query_lower for word in ["prove", "therefore", "if then"]):
            return await self._logical_reasoning(query, context)

        elif any(word in query_lower for word in ["plan", "goal", "achieve", "steps"]):
            return await self._planning_reasoning(query, context)

        elif any(word in query_lower for word in ["should", "ethical", "right", "wrong"]):
            return await self._ethical_reasoning(query, context)

        elif any(word in query_lower for word in ["what if had", "instead", "alternative", "regret"]):
            return await self._counterfactual_reasoning(query, context)

        elif any(word in query_lower for word in ["creative", "idea", "brainstorm", "solve", "novel"]):
            return await self._creative_reasoning(query, context)

        else:
            # Default to general reasoning
            return await self._general_reasoning(query, context)

    async def _causal_reasoning(
        self,
        query: str,
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle causal reasoning queries."""
        # Record observation if provided
        if "observation" in context:
            await self.causal.add_observation(context["observation"])

        # Extract variables and intervention if specified
        intervention = context.get("intervention", {})
        target = context.get("target", "outcome")

        if intervention:
            result = await self.causal.do(intervention, target)
            return {
                "reasoning_type": "causal",
                "intervention": result.intervention_value,
                "effect": result.causal_effect,
                "confidence": result.confidence,
                "explanation": result.explanation,
                "affected_variables": result.affected_variables,
            }

        return {
            "reasoning_type": "causal",
            "message": "Provide intervention and target for causal analysis",
        }

    async def _predictive_reasoning(
        self,
        query: str,
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle predictive/counterfactual queries."""
        objects = context.get("objects", [])
        action = context.get("action", query)

        if objects:
            # Create physical objects
            phys_objects = [
                PhysicalObject(
                    id=obj.get("id", str(uuid.uuid4())),
                    position=tuple(obj.get("position", [0, 0, 0])),
                    velocity=tuple(obj.get("velocity", [0, 0, 0])),
                    mass=obj.get("mass", 1.0),
                )
                for obj in objects
            ]

            result = await self.world.predict_outcome(action, phys_objects)
            return {
                "reasoning_type": "predictive",
                "predictions": result["predictions"],
                "events": result["events"],
                "confidence": result["confidence"],
                "explanation": result["explanation"],
            }

        # Counterfactual reasoning
        factual = context.get("factual", {})
        intervention = context.get("intervention", {})
        target = context.get("target", "outcome")

        if factual and intervention:
            result = await self.causal.counterfactual(factual, intervention, target)
            return {
                "reasoning_type": "counterfactual",
                **result,
            }

        return {
            "reasoning_type": "predictive",
            "message": "Provide objects for simulation or factual/intervention for counterfactual",
        }

    async def _social_reasoning(
        self,
        query: str,
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle theory of mind queries."""
        agent_id = context.get("agent", "default_agent")
        action = context.get("action")
        topic = context.get("topic")
        situation = context.get("situation", {})

        if action:
            inferred = await self.tom.observe_action(agent_id, action, context)
            return {
                "reasoning_type": "theory_of_mind",
                "inferred_states": {k: {"type": v.state_type.value, "content": v.content, "confidence": v.confidence}
                                   for k, v in inferred.items()},
            }

        if topic:
            perspective = await self.tom.understand_perspective(agent_id, topic)
            return {
                "reasoning_type": "theory_of_mind",
                "perspective": perspective,
            }

        if situation:
            prediction = await self.tom.predict_action(agent_id, situation)
            return {
                "reasoning_type": "theory_of_mind",
                "prediction": prediction,
            }

        return {
            "reasoning_type": "theory_of_mind",
            "message": "Provide action, topic, or situation for social reasoning",
        }

    async def _logical_reasoning(
        self,
        query: str,
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle logical/abstract reasoning queries."""
        facts = context.get("facts", [])
        rules = context.get("rules", [])
        goal = context.get("goal")

        # Add facts
        for fact in facts:
            await self.abstract.add_fact(fact, True)

        # Add rules
        for rule in rules:
            if "if" in rule and "then" in rule:
                parts = rule.split("then")
                if_part = parts[0].replace("if", "").strip()
                then_part = parts[1].strip()
                await self.abstract.add_rule(if_part, then_part)

        if goal:
            proof = await self.abstract.prove(goal)
            return {
                "reasoning_type": "logical",
                "goal": goal,
                "proven": proof.is_valid,
                "explanation": proof.explanation,
            }

        return {
            "reasoning_type": "logical",
            "message": "Provide facts, rules, and goal for logical reasoning",
        }

    async def _planning_reasoning(
        self,
        query: str,
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle planning queries."""
        goal_description = context.get("goal", query)
        current_state = context.get("state", set())
        actions = context.get("actions", [])

        # Update state
        if current_state:
            await self.planner.update_state(set(current_state))

        # Register actions
        for action in actions:
            await self.planner.register_action(
                name=action["name"],
                preconditions=action.get("preconditions", []),
                effects=action.get("effects", []),
                cost=action.get("cost", 1.0),
            )

        # Create goal and plan
        goal = await self.planner.add_goal(
            description=goal_description,
            effects=context.get("goal_conditions", [goal_description]),
        )

        plan = await self.planner.plan(goal.id)

        if plan:
            return {
                "reasoning_type": "planning",
                "goal": goal_description,
                "plan": [{"action": a.name, "cost": a.cost} for a in plan.actions],
                "total_cost": plan.total_cost,
                "confidence": plan.confidence,
            }

        return {
            "reasoning_type": "planning",
            "goal": goal_description,
            "message": "Could not find a plan",
        }

    async def _ethical_reasoning(
        self,
        query: str,
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle ethical reasoning queries."""
        action = context.get("action", query)
        stakeholders = context.get("stakeholders", ["user"])

        evaluation = await self.ethics.evaluate_action(action, context)
        harm_assessment = await self.ethics.assess_harm(action, stakeholders)

        return {
            "reasoning_type": "ethical",
            "action": action,
            "is_permissible": evaluation.is_permissible,
            "confidence": evaluation.confidence,
            "explanation": evaluation.explanation,
            "recommendations": evaluation.recommendations,
            "harm_assessment": harm_assessment,
        }

    async def _counterfactual_reasoning(
        self,
        query: str,
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle counterfactual 'what-if' reasoning queries."""
        actual_state = context.get("actual_state", {})
        intervention = context.get("intervention", {})
        alternatives = context.get("alternatives", [])

        # If we have alternatives, do regret analysis
        if alternatives and actual_state:
            chosen_action = context.get("chosen_action", "unknown")
            regret = await self.counterfactual.regret_analysis(
                chosen_action=chosen_action,
                outcome=actual_state,
                alternative_actions=alternatives,
                context=context,
            )
            return {
                "reasoning_type": "counterfactual",
                "analysis_type": "regret",
                **regret,
            }

        # Otherwise, imagine alternative scenarios
        result = await self.counterfactual.imagine(
            query=query,
            actual_state=actual_state or {"query": query},
            intervention=intervention or {"change": "hypothetical"},
            context=context,
        )

        return {
            "reasoning_type": "counterfactual",
            "analysis_type": "scenario",
            "query": result.query,
            "scenarios_count": len(result.scenarios),
            "most_likely": {
                "explanation": result.most_likely_outcome.explanation,
                "probability": result.most_likely_outcome.probability,
                "consequences": result.most_likely_outcome.consequences,
            } if result.most_likely_outcome else None,
            "confidence": result.confidence,
            "reasoning_chain": result.reasoning_chain,
        }

    async def _creative_reasoning(
        self,
        query: str,
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle creative problem solving queries."""
        problem = context.get("problem", query)
        constraints = context.get("constraints", [])

        solution = await self.creative.solve(
            problem=problem,
            constraints=constraints,
            context=context,
        )

        return {
            "reasoning_type": "creative",
            "problem": solution.problem,
            "ideas_count": len(solution.ideas),
            "best_idea": {
                "description": solution.best_idea.description,
                "technique": solution.best_idea.technique.value,
                "novelty": solution.best_idea.novelty_score,
                "feasibility": solution.best_idea.feasibility_score,
                "impact": solution.best_idea.impact_score,
            } if solution.best_idea else None,
            "synthesis": solution.synthesis,
            "all_ideas": [
                {
                    "description": idea.description,
                    "technique": idea.technique.value,
                    "score": idea.novelty_score * 0.3 + idea.feasibility_score * 0.4 + idea.impact_score * 0.3,
                }
                for idea in solution.ideas[:5]  # Top 5
            ],
            "reasoning_process": solution.reasoning_process,
        }

    async def _general_reasoning(
        self,
        query: str,
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle general reasoning queries."""
        return {
            "reasoning_type": "general",
            "query": query,
            "context": context,
            "available_capabilities": [
                "causal (why/cause/effect)",
                "predictive (what if/predict)",
                "social (think/believe/want)",
                "logical (prove/therefore)",
                "planning (plan/goal/achieve)",
                "ethical (should/right/wrong)",
                "counterfactual (what if had/alternative/regret)",
                "creative (idea/brainstorm/solve/novel)",
            ],
        }


# =============================================================================
# SINGLETON ACCESS
# =============================================================================

_cognitive_system: Optional[CognitiveSystem] = None
_cognitive_lock = LazyAsyncLock()  # v100.1: Lazy initialization to avoid "no running event loop" error


async def get_cognitive_system() -> CognitiveSystem:
    """Get the singleton CognitiveSystem instance."""
    global _cognitive_system
    if _cognitive_system is None:
        async with _cognitive_lock:
            if _cognitive_system is None:
                _cognitive_system = CognitiveSystem()
                await _cognitive_system.initialize()
    return _cognitive_system


async def shutdown_cognitive_system() -> None:
    """Shutdown the cognitive system."""
    global _cognitive_system
    if _cognitive_system is not None:
        _cognitive_system = None
        logger.info("[Cognitive] System shutdown complete")
