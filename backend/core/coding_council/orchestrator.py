"""
v77.2: Unified Coding Council Orchestrator
==========================================

The main orchestrator that coordinates all 5 coding frameworks for JARVIS
self-evolution. This is the "brain" of the Coding Council.

Architecture:
    1. Receive evolution task
    2. Analyze codebase (RepoMaster) - understand what needs to change
    3. Plan changes (MetaGPT) - if complex, create detailed plan
    4. Route to best framework - based on complexity/safety
    5. Execute changes - Aider/OpenHands/Continue
    6. Validate changes - AST, types, security, tests
    7. Apply or rollback - based on validation

Safety Features:
    - Circuit breaker for repeated failures
    - Resource monitoring and limits
    - Git-based atomic rollback
    - AST validation before commit
    - Hot reload lock during evolution
    - Concurrent task limiting
    - Deadlock prevention
    - Structured logging and tracing
    - Disk/memory monitoring
    - Graceful shutdown
    - Crash recovery

v77.2 Additions:
    - Atomic file locking (fcntl-based)
    - Git conflict resolution handler
    - Memory management / bounded collections
    - Async lock with timeout
    - Enhanced circuit breaker (error classification)
    - Retention policies
    - Handler lifecycle management

All 80 identified gaps are addressed in this implementation.

Author: JARVIS v77.2
Version: 2.2.0
"""

from __future__ import annotations

import asyncio
import logging
import os
import subprocess
import time
from concurrent.futures import ThreadPoolExecutor
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Any, AsyncIterator, Callable, Dict, List, Optional, Set

from .types import (
    AnalysisResult,
    CodingCouncilConfig,
    EvolutionResult,
    EvolutionTask,
    FrameworkResult,
    FrameworkType,
    PlanResult,
    RollbackInfo,
    RollbackReason,
    TaskComplexity,
    TaskStatus,
    ValidationReport,
    ValidationResult,
)

# Import new specialized modules
try:
    from .async_tools import (
        DeadlockPrevention,
        TaskRegistry,
        FileLocker,
        get_deadlock_prevention,
        get_task_registry,
        get_file_locker,
    )
    ASYNC_TOOLS_AVAILABLE = True
except ImportError:
    ASYNC_TOOLS_AVAILABLE = False

try:
    from .framework import (
        CircuitBreaker as FrameworkCircuitBreaker,
        CircuitState,
        RateLimiter,
        TokenBucketConfig,
        TimeoutWrapper,
        timeout,
        retry,
        Bulkhead,
        get_bulkhead,
    )
    FRAMEWORK_AVAILABLE = True
except ImportError:
    FRAMEWORK_AVAILABLE = False

try:
    from .observability import (
        StructuredLogger,
        get_logger as get_structured_logger,
        TraceCorrelator,
        trace,
        get_tracer,
        MetricsCollector,
        get_metrics_collector,
        HealthMonitor,
        get_health_monitor,
    )
    OBSERVABILITY_AVAILABLE = True
except ImportError:
    OBSERVABILITY_AVAILABLE = False

try:
    from .edge_cases import (
        DiskMonitor,
        get_disk_monitor,
        MemoryMonitor,
        get_memory_monitor,
        CrashRecovery,
        get_crash_recovery,
        GracefulShutdown,
        get_graceful_shutdown,
    )
    EDGE_CASES_AVAILABLE = True
except ImportError:
    EDGE_CASES_AVAILABLE = False

try:
    from .safety import (
        ASTValidator,
        SecurityScanner,
        TypeChecker,
        StagingEnvironment,
    )
    SAFETY_AVAILABLE = True
except ImportError:
    SAFETY_AVAILABLE = False

try:
    from .trinity import (
        MultiTransport,
        PersistentMessageQueue,
        HeartbeatValidator,
        acquire_shared_heartbeat_validator,
        release_shared_heartbeat_validator,
        CrossRepoSync,
    )
    TRINITY_AVAILABLE = True
except ImportError:
    TRINITY_AVAILABLE = False

# v77.1 Advanced Modules (Gaps #41-60)
try:
    from .advanced import (
        DistributedTransactionCoordinator,
        TransactionResult,
        TwoPhaseCommit,
        SagaCoordinator,
        SagaDefinition,
        SagaResult,
        EvolutionEventStore,
        EvolutionEvent,
        EventType,
        EventFactory,
        CrossRepoDependencyTracker,
        DependencyGraph,
        EvolutionStateMachine,
        EvolutionState,
        StateMachineManager,
        PartialSuccessHandler,
        FrameworkOutcome,
        OutcomeStatus,
        RecoveryStrategy,
        MergeResult,
        AdaptiveFrameworkSelector,
        FrameworkScore,
        get_adaptive_selector,
    )
    ADVANCED_AVAILABLE = True
except ImportError as e:
    # logger not yet defined at module load time
    ADVANCED_AVAILABLE = False

# v77.2 Advanced Modules (Gaps #61-80)
try:
    from .advanced import (
        # Resilience
        CircuitBreaker as AdvancedCircuitBreaker,
        CircuitBreakerConfig,
        CircuitOpenError,
        ErrorClassifier,
        ErrorCategory,
        ClassifiedError,
        RetryPolicy,
        RetryConfig,
        with_retry,
        AsyncLockWithTimeout,
        ShutdownHandler as AdvancedShutdownHandler,
        HealthChecker as AdvancedHealthChecker,
        get_shutdown_handler,
        get_health_checker,
        get_circuit_breaker_registry,
        # Atomic Locking
        AtomicFileLock,
        LockManager,
        LockType,
        LockInfo,
        LockAcquisitionError,
        get_lock_manager,
        # Git Conflict Handler
        GitConflictHandler,
        ConflictDetector,
        ConflictResolver,
        ConflictType,
        ResolutionStrategy as ConflictResolutionStrategy,
        MergeConflictResult,
        # Memory Management
        BoundedDict,
        BoundedList,
        EvictionPolicy,
        WeakHandlerSet,
        RetentionPolicy,
        RetentionManager,
        MemoryMonitor as AdvancedMemoryMonitor,
        CleanupService,
        get_memory_monitor as get_advanced_memory_monitor,
        get_cleanup_service,
    )
    ADVANCED_V772_AVAILABLE = True
except ImportError as e:
    # logger not yet defined at module load time
    ADVANCED_V772_AVAILABLE = False

# v78.0 Advanced Modules (Process Management)
try:
    from .advanced import (
        # Adaptive Timeout
        AdaptiveTimeoutManager,
        OperationType,
        TimeoutStrategy,
        TimeoutConfig,
        get_timeout_manager,
        get_timeout_manager_sync,
        # Intelligent Retry
        IntelligentRetryManager,
        RetryStrategy,
        IntelligentRetryConfig,
        RetryResult,
        RetryCircuitBreaker,
        get_retry_manager,
        get_retry_manager_sync,
        with_intelligent_retry,
        # Cross-Repo Coordinator
        CrossRepoTransactionCoordinator,
        RepoScope,
        CrossRepoTxState as TransactionState,  # v78.1: For shutdown cleanup
        get_transaction_coordinator,
        get_transaction_coordinator_sync,
    )
    ADVANCED_V780_AVAILABLE = True
except ImportError as e:
    # logger not yet defined at module load time
    ADVANCED_V780_AVAILABLE = False

# v79.0: Voice Announcer Integration
try:
    from .voice_announcer import (
        get_evolution_announcer,
        CodingCouncilVoiceAnnouncer,
    )
    VOICE_ANNOUNCER_AVAILABLE = True
except ImportError:
    VOICE_ANNOUNCER_AVAILABLE = False
    get_evolution_announcer = None  # type: ignore

logger = logging.getLogger(__name__)


# =============================================================================
# Circuit Breaker (Gap #17: Infinite Loop Prevention)
# =============================================================================

class CircuitBreaker:
    """
    Circuit breaker pattern to prevent infinite retry loops.

    States:
        CLOSED: Normal operation, failures counted
        OPEN: Failures exceeded threshold, requests rejected
        HALF_OPEN: Testing if system recovered

    Addresses Gap #17: Infinite Loop
    """

    def __init__(
        self,
        failure_threshold: int = 3,
        reset_timeout_seconds: float = 300.0,
        name: str = "default"
    ):
        self.failure_threshold = failure_threshold
        self.reset_timeout = reset_timeout_seconds
        self.name = name

        self._failure_count = 0
        self._last_failure_time: Optional[float] = None
        self._state = "closed"  # closed, open, half_open
        self._lock = asyncio.Lock()

    @property
    def is_open(self) -> bool:
        """Check if circuit is open (rejecting requests)."""
        if self._state == "open":
            # Check if we should transition to half-open
            if self._last_failure_time:
                elapsed = time.time() - self._last_failure_time
                if elapsed >= self.reset_timeout:
                    self._state = "half_open"
                    return False
            return True
        return False

    async def record_success(self) -> None:
        """Record a successful operation."""
        async with self._lock:
            self._failure_count = 0
            self._state = "closed"

    async def record_failure(self, error: Optional[str] = None) -> None:
        """Record a failed operation."""
        async with self._lock:
            self._failure_count += 1
            self._last_failure_time = time.time()

            if self._failure_count >= self.failure_threshold:
                self._state = "open"
                logger.warning(
                    f"[CircuitBreaker:{self.name}] OPEN - "
                    f"{self._failure_count} failures, "
                    f"reset in {self.reset_timeout}s"
                )

    def get_status(self) -> Dict[str, Any]:
        """Get circuit breaker status."""
        return {
            "name": self.name,
            "state": self._state,
            "failure_count": self._failure_count,
            "failure_threshold": self.failure_threshold,
            "last_failure_time": self._last_failure_time,
            "reset_timeout": self.reset_timeout,
        }


# =============================================================================
# Resource Monitor (Gap #22: Resource Exhaustion)
# =============================================================================

class ResourceMonitor:
    """
    Monitor system resources to prevent exhaustion.

    Addresses Gap #22: Resource Exhaustion
    """

    def __init__(self, max_memory_mb: int = 1536):
        self.max_memory_mb = max_memory_mb
        self._psutil_available = False

        try:
            import psutil
            self._psutil_available = True
        except ImportError:
            logger.warning("[ResourceMonitor] psutil not available, monitoring limited")

    def check_resources(self) -> Dict[str, Any]:
        """Check current resource usage."""
        result = {
            "ok": True,
            "memory_percent": 0.0,
            "memory_available_mb": 0,
            "cpu_percent": 0.0,
            "disk_free_gb": 0.0,
        }

        if not self._psutil_available:
            return result

        try:
            import psutil

            # Memory
            mem = psutil.virtual_memory()
            result["memory_percent"] = mem.percent
            result["memory_available_mb"] = mem.available // (1024 * 1024)

            # Check if memory is too low
            if result["memory_available_mb"] < self.max_memory_mb:
                result["ok"] = False
                result["reason"] = f"Low memory: {result['memory_available_mb']}MB available"

            # CPU
            result["cpu_percent"] = psutil.cpu_percent(interval=0.1)

            # Disk
            disk = psutil.disk_usage("/")
            result["disk_free_gb"] = disk.free // (1024 * 1024 * 1024)

            if result["disk_free_gb"] < 1:
                result["ok"] = False
                result["reason"] = f"Low disk space: {result['disk_free_gb']}GB free"

        except Exception as e:
            logger.warning(f"[ResourceMonitor] Check failed: {e}")

        return result

    async def wait_for_resources(
        self,
        timeout: float = 60.0,
        check_interval: float = 5.0
    ) -> bool:
        """Wait until resources are available."""
        start = time.time()

        while time.time() - start < timeout:
            status = self.check_resources()
            if status["ok"]:
                return True

            logger.info(f"[ResourceMonitor] Waiting for resources: {status.get('reason')}")
            await asyncio.sleep(check_interval)

        return False


# =============================================================================
# Rollback Manager (Gap #11: Rollback Complexity)
# =============================================================================

class RollbackManager:
    """
    Git-based atomic rollback with savepoints.

    Addresses Gap #11: Rollback Complexity
    """

    def __init__(self, repo_root: Path):
        self.repo_root = repo_root
        self._savepoints: Dict[str, RollbackInfo] = {}
        self._executor = ThreadPoolExecutor(max_workers=2)

    async def create_savepoint(
        self,
        task_id: str,
        files: List[str]
    ) -> RollbackInfo:
        """
        Create a savepoint before making changes.

        Uses git stash to preserve working directory state.
        """
        rollback_info = RollbackInfo(
            task_id=task_id,
            files_backed_up=files,
        )

        try:
            # Get current commit hash
            result = await self._run_git("rev-parse", "HEAD")
            rollback_info.git_commit_before = result.strip()

            # Stash any uncommitted changes (including untracked)
            stash_result = await self._run_git(
                "stash", "push", "-u",
                "-m", f"council_savepoint_{rollback_info.rollback_id}"
            )

            if "No local changes" not in stash_result:
                # Get stash reference
                stash_list = await self._run_git("stash", "list", "-1")
                if stash_list:
                    rollback_info.git_stash_ref = "stash@{0}"

            self._savepoints[rollback_info.rollback_id] = rollback_info
            logger.info(f"[RollbackManager] Savepoint created: {rollback_info.rollback_id}")

        except Exception as e:
            logger.error(f"[RollbackManager] Failed to create savepoint: {e}")

        return rollback_info

    async def restore_savepoint(
        self,
        rollback_id: str,
        reason: RollbackReason
    ) -> bool:
        """Restore to a savepoint, undoing all changes since."""
        if rollback_id not in self._savepoints:
            logger.error(f"[RollbackManager] Savepoint not found: {rollback_id}")
            return False

        info = self._savepoints[rollback_id]

        try:
            # Reset to the commit before changes
            if info.git_commit_before:
                await self._run_git("reset", "--hard", info.git_commit_before)

            # Restore stashed changes if any
            if info.git_stash_ref:
                try:
                    await self._run_git("stash", "pop")
                except Exception:
                    pass  # Stash may have been cleaned up

            info.reason = reason
            info.restored = True
            info.restored_at = time.time()

            logger.info(
                f"[RollbackManager] Restored savepoint: {rollback_id} "
                f"(reason: {reason.value})"
            )
            return True

        except Exception as e:
            logger.error(f"[RollbackManager] Restore failed: {e}")
            return False

    async def commit_savepoint(self, rollback_id: str) -> bool:
        """
        Commit changes and clear the savepoint.
        Called when evolution succeeds.
        """
        if rollback_id not in self._savepoints:
            return False

        info = self._savepoints[rollback_id]

        try:
            # Clean up stash if we made one
            if info.git_stash_ref:
                try:
                    await self._run_git("stash", "drop")
                except Exception:
                    pass

            # Remove savepoint
            del self._savepoints[rollback_id]
            logger.info(f"[RollbackManager] Savepoint committed: {rollback_id}")
            return True

        except Exception as e:
            logger.error(f"[RollbackManager] Commit failed: {e}")
            return False

    async def _run_git(self, *args: str) -> str:
        """Run a git command asynchronously."""
        loop = asyncio.get_running_loop()

        def _run():
            result = subprocess.run(
                ["git", *args],
                cwd=str(self.repo_root),
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode != 0 and "error" in result.stderr.lower():
                raise RuntimeError(result.stderr)
            return result.stdout or result.stderr

        return await loop.run_in_executor(self._executor, _run)


# =============================================================================
# Code Validator (Gaps #18-21: Validation)
# =============================================================================

class CodeValidator:
    """
    Comprehensive code validation before committing changes.

    Addresses Gaps:
        #18: Hallucinated Imports
        #19: Syntax Errors
        #20: Security Vulnerabilities
        #21: Breaking API Contracts
    """

    def __init__(self, repo_root: Path):
        self.repo_root = repo_root
        self._executor = ThreadPoolExecutor(max_workers=4)

    async def validate_files(
        self,
        files: List[str],
        run_tests: bool = True
    ) -> ValidationReport:
        """
        Validate modified files for safety.

        Checks:
            1. Syntax validity (AST parsing)
            2. Import resolution
            3. Type checking (if mypy available)
            4. Security scanning (basic patterns)
            5. Unit tests (if enabled)
        """
        report = ValidationReport(
            result=ValidationResult.PASSED,
            passed=True,
            errors=[],
            warnings=[],
        )

        # Run validations in parallel
        results = await asyncio.gather(
            self._check_syntax(files),
            self._check_imports(files),
            self._check_security(files),
            self._check_types(files) if run_tests else asyncio.sleep(0),
            return_exceptions=True
        )

        # Process syntax results
        if isinstance(results[0], dict):
            if results[0].get("errors"):
                report.errors.extend(results[0]["errors"])
                report.result = ValidationResult.SYNTAX_ERROR
                report.passed = False

        # Process import results
        if isinstance(results[1], dict):
            if results[1].get("errors"):
                report.errors.extend(results[1]["errors"])
                report.passed = False

        # Process security results
        if isinstance(results[2], dict):
            if results[2].get("issues"):
                report.security_issues = results[2]["issues"]
                if any(i.get("severity") == "high" for i in report.security_issues):
                    report.result = ValidationResult.SECURITY_ISSUE
                    report.passed = False
                else:
                    report.warnings.extend([
                        f"Security: {i.get('message')}" for i in report.security_issues
                    ])

        # Process type check results
        if isinstance(results[3], dict):
            if results[3].get("errors"):
                report.type_errors = results[3]["errors"]
                report.warnings.extend([
                    f"Type: {e.get('message')}" for e in report.type_errors
                ])

        return report

    async def _check_syntax(self, files: List[str]) -> Dict[str, Any]:
        """Check Python syntax using AST parsing."""
        result = {"errors": [], "valid_files": []}
        loop = asyncio.get_running_loop()

        def _parse_file(filepath: str):
            try:
                import ast
                full_path = self.repo_root / filepath
                if not full_path.exists():
                    return None  # New file, skip

                with open(full_path) as f:
                    ast.parse(f.read())
                return {"file": filepath, "valid": True}
            except SyntaxError as e:
                return {
                    "file": filepath,
                    "valid": False,
                    "error": f"Syntax error at line {e.lineno}: {e.msg}"
                }

        for filepath in files:
            if filepath.endswith(".py"):
                check_result = await loop.run_in_executor(
                    self._executor, _parse_file, filepath
                )
                if check_result:
                    if check_result.get("valid"):
                        result["valid_files"].append(filepath)
                    else:
                        result["errors"].append(check_result["error"])

        return result

    async def _check_imports(self, files: List[str]) -> Dict[str, Any]:
        """Check that all imports can be resolved."""
        result = {"errors": [], "warnings": []}
        loop = asyncio.get_running_loop()

        def _check_file_imports(filepath: str):
            issues = []
            try:
                import ast
                full_path = self.repo_root / filepath
                if not full_path.exists():
                    return issues

                with open(full_path) as f:
                    tree = ast.parse(f.read())

                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            # Check if module exists (basic check)
                            module_name = alias.name.split(".")[0]
                            try:
                                __import__(module_name)
                            except ImportError:
                                # Could be a local module, check path
                                local_path = self.repo_root / module_name.replace(".", "/")
                                if not (local_path.exists() or (local_path.with_suffix(".py")).exists()):
                                    issues.append(f"Import not found: {alias.name}")

                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            module_name = node.module.split(".")[0]
                            try:
                                __import__(module_name)
                            except ImportError:
                                local_path = self.repo_root / module_name.replace(".", "/")
                                if not (local_path.exists() or (local_path.with_suffix(".py")).exists()):
                                    issues.append(f"Import not found: {node.module}")

            except Exception as e:
                issues.append(f"Import check failed for {filepath}: {e}")

            return issues

        for filepath in files:
            if filepath.endswith(".py"):
                file_issues = await loop.run_in_executor(
                    self._executor, _check_file_imports, filepath
                )
                result["errors"].extend(file_issues)

        return result

    async def _check_security(self, files: List[str]) -> Dict[str, Any]:
        """Basic security pattern scanning."""
        result = {"issues": []}
        loop = asyncio.get_running_loop()

        # Dangerous patterns to check
        dangerous_patterns = [
            (r"eval\s*\(", "Use of eval() is dangerous", "high"),
            (r"exec\s*\(", "Use of exec() is dangerous", "high"),
            (r"os\.system\s*\(", "Use of os.system() - prefer subprocess", "medium"),
            (r"pickle\.loads?\s*\(", "Pickle can execute arbitrary code", "medium"),
            (r"__import__\s*\(", "Dynamic import can be dangerous", "low"),
            (r"subprocess\..*shell\s*=\s*True", "Shell=True is dangerous", "high"),
            (r"password\s*=\s*[\"'][^\"']+[\"']", "Hardcoded password detected", "high"),
            (r"api_key\s*=\s*[\"'][^\"']+[\"']", "Hardcoded API key detected", "high"),
        ]

        def _scan_file(filepath: str):
            import re
            issues = []
            full_path = self.repo_root / filepath
            if not full_path.exists():
                return issues

            try:
                with open(full_path) as f:
                    content = f.read()

                for pattern, message, severity in dangerous_patterns:
                    matches = re.finditer(pattern, content)
                    for match in matches:
                        # Find line number
                        line_num = content[:match.start()].count("\n") + 1
                        issues.append({
                            "file": filepath,
                            "line": line_num,
                            "message": message,
                            "severity": severity,
                            "pattern": pattern,
                        })

            except Exception as e:
                logger.warning(f"Security scan failed for {filepath}: {e}")

            return issues

        for filepath in files:
            if filepath.endswith(".py"):
                file_issues = await loop.run_in_executor(
                    self._executor, _scan_file, filepath
                )
                result["issues"].extend(file_issues)

        return result

    async def _check_types(self, files: List[str]) -> Dict[str, Any]:
        """Run type checking if mypy is available."""
        result = {"errors": [], "warnings": []}

        try:
            # Check if mypy is available
            proc = await asyncio.create_subprocess_exec(
                "mypy", "--version",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await proc.communicate()

            if proc.returncode != 0:
                return result  # mypy not available

            # Run mypy on files
            python_files = [f for f in files if f.endswith(".py")]
            if not python_files:
                return result

            proc = await asyncio.create_subprocess_exec(
                "mypy",
                "--ignore-missing-imports",
                "--no-error-summary",
                *python_files,
                cwd=str(self.repo_root),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, _ = await proc.communicate()

            if stdout:
                for line in stdout.decode().split("\n"):
                    if ": error:" in line:
                        result["errors"].append({"message": line})
                    elif ": warning:" in line:
                        result["warnings"].append({"message": line})

        except FileNotFoundError:
            # mypy not installed
            pass
        except Exception as e:
            logger.warning(f"Type checking failed: {e}")

        return result


# =============================================================================
# Hot Reload Lock (Gap #40: Hot Reload During Evolution)
# =============================================================================

class HotReloadLock:
    """
    Prevent hot reload during code evolution.

    Creates a lock file that the hot reload watcher checks.

    Addresses Gap #40: Hot Reload During Evolution
    """

    def __init__(self, lock_file: Optional[Path] = None):
        self.lock_file = lock_file or Path.home() / ".jarvis" / "evolution.lock"
        self._held = False

    async def acquire(self, task_id: str) -> bool:
        """Acquire the hot reload lock."""
        try:
            self.lock_file.parent.mkdir(parents=True, exist_ok=True)
            self.lock_file.write_text(f"{task_id}:{time.time()}")
            self._held = True
            logger.info(f"[HotReloadLock] Acquired for task {task_id}")
            return True
        except Exception as e:
            logger.error(f"[HotReloadLock] Failed to acquire: {e}")
            return False

    async def release(self) -> bool:
        """Release the hot reload lock."""
        try:
            if self.lock_file.exists():
                self.lock_file.unlink()
            self._held = False
            logger.info("[HotReloadLock] Released")
            return True
        except Exception as e:
            logger.error(f"[HotReloadLock] Failed to release: {e}")
            return False

    @property
    def is_held(self) -> bool:
        return self._held

    @staticmethod
    def is_evolution_in_progress() -> bool:
        """Check if evolution is currently in progress."""
        lock_file = Path.home() / ".jarvis" / "evolution.lock"
        if not lock_file.exists():
            return False

        try:
            content = lock_file.read_text()
            _, timestamp = content.split(":")
            # Lock is stale if older than 30 minutes
            if time.time() - float(timestamp) > 1800:
                lock_file.unlink()
                return False
            return True
        except Exception:
            return False


# =============================================================================
# Decision Router (Framework Selection)
# =============================================================================

# Import J-Prime task classification utilities
try:
    from .adapters.jprime_adapter import (
        JPrimeAvailabilityChecker,
        classify_task_for_jprime,
        is_task_suitable_for_jprime,
    )
    from .adapters.jprime_engine import ModelTaskType
    JPRIME_ADAPTER_AVAILABLE = True
except ImportError:
    JPRIME_ADAPTER_AVAILABLE = False
    ModelTaskType = None  # type: ignore


class DecisionRouter:
    """
    v84.0: Intelligent router that chooses the best framework for each task.

    Decision Matrix:
        1. J-Prime First: Local LLM for coding/reasoning (cost-free, private)
        2. TRIVIAL  → Aider (fastest cloud)
        3. SIMPLE   → Aider (fast, single file)
        4. MEDIUM   → RepoMaster analysis + Aider
        5. COMPLEX  → MetaGPT planning + OpenHands sandbox
        6. CRITICAL → Full Council + Human approval

    J-Prime Routing:
        - JPRIME_CODING: CodeLlama/DeepSeek for coding, debugging, refactoring
        - JPRIME_REASONING: Qwen/Llama for planning, analysis, reasoning
        - JPRIME_LOCAL: General-purpose local inference
    """

    def __init__(self, config: CodingCouncilConfig):
        self.config = config
        self._jprime_available: Optional[bool] = None
        self._last_jprime_check: float = 0.0
        self._jprime_check_interval: float = 30.0  # Re-check every 30 seconds

    async def choose_framework_async(
        self,
        task: EvolutionTask,
        analysis: Optional[AnalysisResult] = None,
        plan: Optional[PlanResult] = None
    ) -> FrameworkType:
        """
        v84.0: Async framework selection with J-Prime priority.

        Checks J-Prime availability asynchronously before falling back to
        cloud-based frameworks.
        """
        # Rule 0: Check J-Prime first (cost-free, private)
        if self.config.jprime_enabled and await self._is_jprime_available_async():
            jprime_framework = await self._select_jprime_framework(task)
            if jprime_framework:
                logger.info(f"[Router] Selected J-Prime: {jprime_framework.value}")
                return jprime_framework

        # Fall through to synchronous routing
        return self.choose_framework(task, analysis, plan)

    def choose_framework(
        self,
        task: EvolutionTask,
        analysis: Optional[AnalysisResult] = None,
        plan: Optional[PlanResult] = None
    ) -> FrameworkType:
        """
        v84.0: Choose the best framework for the task.

        Enhanced with J-Prime local LLM routing for cost-free, private inference.
        """
        # Rule 0: Try J-Prime first if enabled and available (synchronous check)
        if self.config.jprime_enabled and self._is_jprime_available_sync():
            jprime_framework = self._select_jprime_framework_sync(task)
            if jprime_framework:
                logger.info(f"[Router] Selected J-Prime: {jprime_framework.value}")
                return jprime_framework

        # Rule 1: Requires sandbox → OpenHands
        if task.requires_sandbox:
            if self.config.openhands_enabled:
                return FrameworkType.OPENHANDS
            logger.warning("[Router] OpenHands requested but not enabled, falling back")

        # Rule 2: Critical → Full council (but primary is OpenHands for safety)
        if task.complexity == TaskComplexity.CRITICAL:
            if self.config.openhands_enabled:
                return FrameworkType.OPENHANDS
            return FrameworkType.CLAUDE_CODE  # Fallback

        # Rule 3: Complex → MetaGPT for planning, then Aider for execution
        if task.complexity == TaskComplexity.COMPLEX:
            if task.requires_planning and self.config.metagpt_enabled:
                return FrameworkType.METAGPT
            if self.config.aider_enabled:
                return FrameworkType.AIDER
            return FrameworkType.CLAUDE_CODE

        # Rule 4: Medium → RepoMaster analysis first, then Aider
        if task.complexity == TaskComplexity.MEDIUM:
            if self.config.aider_enabled:
                return FrameworkType.AIDER
            return FrameworkType.CLAUDE_CODE

        # Rule 5: Simple/Trivial → Aider (fastest)
        if self.config.aider_enabled:
            return FrameworkType.AIDER

        # Default fallback
        return FrameworkType.CLAUDE_CODE

    # =========================================================================
    # J-Prime Routing (v84.0)
    # =========================================================================

    async def _is_jprime_available_async(self) -> bool:
        """
        v84.0: Async check if J-Prime is available.

        Uses cached result if checked recently to avoid overhead.
        """
        if not JPRIME_ADAPTER_AVAILABLE:
            return False

        try:
            return await JPrimeAvailabilityChecker.is_available()
        except Exception as e:
            logger.debug(f"[Router] J-Prime availability check failed: {e}")
            return False

    def _is_jprime_available_sync(self) -> bool:
        """
        v84.0: Synchronous check if J-Prime is available.

        Uses heartbeat file check only (fast, no network).
        """
        if not JPRIME_ADAPTER_AVAILABLE:
            return False

        import time
        import json
        from pathlib import Path

        # Rate limit checks
        now = time.time()
        if now - self._last_jprime_check < self._jprime_check_interval:
            return self._jprime_available or False

        self._last_jprime_check = now

        try:
            # Check heartbeat file (fast, synchronous)
            heartbeat_file = Path.home() / ".jarvis" / "trinity" / "components" / "jarvis_prime.json"

            if heartbeat_file.exists():
                with open(heartbeat_file) as f:
                    data = json.load(f)
                    heartbeat_age = now - data.get("timestamp", 0)

                    if heartbeat_age < 30:  # Fresh heartbeat
                        self._jprime_available = True
                        return True

            self._jprime_available = False
            return False

        except Exception as e:
            logger.debug(f"[Router] J-Prime sync check failed: {e}")
            self._jprime_available = False
            return False

    async def _select_jprime_framework(self, task: EvolutionTask) -> Optional[FrameworkType]:
        """
        v84.0: Select the appropriate J-Prime framework for a task (async).

        Routes based on task classification:
        - Coding/debugging/refactoring → JPRIME_CODING (CodeLlama)
        - Reasoning/planning/analysis → JPRIME_REASONING (Qwen)
        - General → JPRIME_LOCAL (Llama)
        """
        return self._select_jprime_framework_sync(task)

    def _select_jprime_framework_sync(self, task: EvolutionTask) -> Optional[FrameworkType]:
        """
        v84.0: Select the appropriate J-Prime framework for a task (sync).

        Uses intelligent task classification to route to optimal model.
        """
        if not JPRIME_ADAPTER_AVAILABLE:
            return None

        # Check if task is suitable for J-Prime
        if not is_task_suitable_for_jprime(task.description, task.target_files):
            logger.debug(f"[Router] Task not suitable for J-Prime: {task.description[:50]}...")
            return None

        # Classify task
        task_type, confidence = classify_task_for_jprime(task.description, task.target_files)
        logger.debug(f"[Router] Task classified as {task_type.value} (confidence: {confidence:.2f})")

        # Map task types to J-Prime frameworks
        coding_tasks = {
            ModelTaskType.CODING,
            ModelTaskType.DEBUGGING,
            ModelTaskType.REFACTORING,
            ModelTaskType.CODE_REVIEW,
        }

        reasoning_tasks = {
            ModelTaskType.REASONING,
            ModelTaskType.PLANNING,
            ModelTaskType.ANALYSIS,
            ModelTaskType.MATH,
        }

        # Select framework based on task type
        if task_type in coding_tasks:
            # Prefer JPRIME_CODING for coding tasks
            if self.config.jprime_prefer_for_coding:
                return FrameworkType.JPRIME_CODING
            # Only use if explicitly set
            return FrameworkType.JPRIME_CODING if confidence >= 0.5 else None

        if task_type in reasoning_tasks:
            return FrameworkType.JPRIME_REASONING

        # Default to JPRIME_LOCAL for general tasks
        if confidence >= 0.3:
            return FrameworkType.JPRIME_LOCAL

        return None

    def _classify_task_type(self, task: EvolutionTask) -> Optional[str]:
        """
        v84.0: Classify task type for routing decisions.

        Returns one of: 'coding', 'reasoning', 'general', or None if unsure.
        """
        if not JPRIME_ADAPTER_AVAILABLE:
            return None

        task_type, confidence = classify_task_for_jprime(task.description, task.target_files)

        if confidence < 0.3:
            return None

        return task_type.value

    def should_use_jprime(self, task: EvolutionTask) -> bool:
        """
        v84.0: Quick check if J-Prime should be considered for this task.

        Returns True if:
        - J-Prime is enabled in config
        - Task is suitable for local inference
        - J-Prime appears to be available (cached check)
        """
        if not self.config.jprime_enabled:
            return False

        if not JPRIME_ADAPTER_AVAILABLE:
            return False

        # Check suitability
        if not is_task_suitable_for_jprime(task.description, task.target_files):
            return False

        # Quick availability check (uses cache)
        return self._is_jprime_available_sync()

    # =========================================================================
    # Original Helper Methods
    # =========================================================================

    def should_use_analysis(self, task: EvolutionTask) -> bool:
        """Check if task should include RepoMaster analysis."""
        return (
            task.complexity in (TaskComplexity.MEDIUM, TaskComplexity.COMPLEX, TaskComplexity.CRITICAL)
            and self.config.repomaster_enabled
        )

    def should_use_planning(self, task: EvolutionTask) -> bool:
        """Check if task should include MetaGPT planning."""
        return (
            task.requires_planning
            or task.complexity in (TaskComplexity.COMPLEX, TaskComplexity.CRITICAL)
        ) and self.config.metagpt_enabled


# =============================================================================
# Unified Coding Council (Main Orchestrator)
# =============================================================================

class UnifiedCodingCouncil:
    """
    v77.0: Unified Coding Council - The Brain of Self-Evolution.

    Coordinates all 5 coding frameworks:
        1. MetaGPT - Planning
        2. RepoMaster - Analysis
        3. Aider - Execution
        4. OpenHands - Sandbox
        5. Continue - IDE integration

    Addresses all 40 identified gaps through:
        - Circuit breaker for failure handling
        - Resource monitoring
        - Git-based rollback
        - Code validation
        - Hot reload locking
        - Concurrent task limiting
        - Deadlock prevention
        - Distributed tracing
        - Metrics collection
        - Health monitoring
        - Crash recovery
        - Graceful shutdown
    """

    def __init__(self, config: Optional[CodingCouncilConfig] = None):
        self.config = config or CodingCouncilConfig()

        # Core components
        self.router = DecisionRouter(self.config)
        self.rollback = RollbackManager(self.config.repo_root)
        self.validator = CodeValidator(self.config.repo_root)
        self.resource_monitor = ResourceMonitor(self.config.max_memory_mb)
        self.hot_reload_lock = HotReloadLock()

        # Circuit breakers per framework
        self.circuit_breakers: Dict[FrameworkType, CircuitBreaker] = {
            framework: CircuitBreaker(
                failure_threshold=self.config.circuit_breaker_threshold,
                reset_timeout_seconds=self.config.circuit_breaker_reset_seconds,
                name=framework.value
            )
            for framework in FrameworkType
        }

        # Framework adapters (lazy loaded)
        self._adapters: Dict[FrameworkType, Any] = {}

        # Concurrent task limiting
        self._task_semaphore = asyncio.Semaphore(self.config.max_concurrent_tasks)
        self._active_tasks: Dict[str, EvolutionTask] = {}

        # State
        self._initialized = False
        self._shutdown = False

        # Statistics
        self._stats = {
            "tasks_completed": 0,
            "tasks_failed": 0,
            "rollbacks_performed": 0,
            "total_execution_time_ms": 0.0,
            "frameworks_used": {f.value: 0 for f in FrameworkType},
        }

        # =================================================================
        # New v77.0 Modules (Gaps #1-40)
        # =================================================================

        # Async Tools (Gaps #23-27)
        self._task_registry: Optional[TaskRegistry] = None
        self._deadlock_prevention: Optional[DeadlockPrevention] = None
        self._file_locker: Optional[FileLocker] = None

        # Observability (Gaps #28-31)
        self._tracer: Optional[TraceCorrelator] = None
        self._metrics: Optional[MetricsCollector] = None
        self._health_monitor: Optional[HealthMonitor] = None

        # Edge Cases (Gaps #32-40)
        self._disk_monitor: Optional[DiskMonitor] = None
        self._memory_monitor: Optional[MemoryMonitor] = None
        self._crash_recovery: Optional[CrashRecovery] = None
        self._graceful_shutdown: Optional[GracefulShutdown] = None

        # Safety (Gaps #16-22)
        self._ast_validator: Optional[ASTValidator] = None
        self._security_scanner: Optional[SecurityScanner] = None
        self._type_checker: Optional[TypeChecker] = None
        self._staging_env: Optional[StagingEnvironment] = None

        # Trinity (Gaps #1-7)
        self._multi_transport: Optional[MultiTransport] = None
        self._message_queue: Optional[PersistentMessageQueue] = None
        self._heartbeat_validator: Optional[HeartbeatValidator] = None
        self._uses_shared_heartbeat_validator = False
        self._cross_repo_sync: Optional[CrossRepoSync] = None

        # =================================================================
        # v77.1 Advanced Modules (Gaps #41-60)
        # =================================================================

        # Distributed Transactions (Gap #44)
        self._transaction_coordinator: Optional[DistributedTransactionCoordinator] = None

        # Saga Pattern for Multi-Repo
        self._saga_coordinator: Optional[SagaCoordinator] = None

        # Event Sourcing
        self._event_store: Optional[EvolutionEventStore] = None

        # Cross-Repo Dependencies (Gap #43)
        self._dependency_tracker: Optional[CrossRepoDependencyTracker] = None

        # State Machine (Gap #57)
        self._state_machine_manager: Optional[StateMachineManager] = None

        # Partial Success Handler (Gap #41)
        self._partial_success_handler: Optional[PartialSuccessHandler] = None

        # Adaptive Framework Selector (ML-based)
        self._adaptive_selector: Optional[AdaptiveFrameworkSelector] = None

        # =================================================================
        # v78.0 Advanced Modules (Process Management)
        # =================================================================

        # Adaptive Timeout Manager (dynamic timeouts based on task complexity)
        self._timeout_manager: Optional[AdaptiveTimeoutManager] = None

        # Intelligent Retry Manager (context-aware retry with circuit breakers)
        self._retry_manager: Optional[IntelligentRetryManager] = None

        # Cross-Repo Transaction Coordinator (atomic multi-repo commits)
        self._cross_repo_tx: Optional[CrossRepoTransactionCoordinator] = None

        # =================================================================
        # v79.0: Voice Announcer Integration
        # =================================================================
        self._voice_announcer: Optional[CodingCouncilVoiceAnnouncer] = None
        self._voice_announcer_checked = False

    async def _announce_progress(
        self,
        task_id: str,
        stage: str,
        message: str,
        progress: int = 0,
        error: Optional[str] = None,
    ) -> None:
        """
        v79.0: Announce evolution progress via voice.

        This is a fire-and-forget operation - voice failures don't affect
        the evolution pipeline.
        """
        if not VOICE_ANNOUNCER_AVAILABLE:
            return

        # Lazy initialize voice announcer
        if not self._voice_announcer_checked:
            self._voice_announcer_checked = True
            try:
                if get_evolution_announcer:
                    self._voice_announcer = get_evolution_announcer()
                    logger.info("[v79.0] Voice announcer initialized for orchestrator")
            except Exception as e:
                logger.debug(f"[v79.0] Voice announcer not available: {e}")
                self._voice_announcer = None

        if not self._voice_announcer:
            return

        try:
            # Fire and forget - don't await to avoid blocking evolution
            asyncio.create_task(
                self._voice_announcer.announce_progress(
                    task_id=task_id,
                    stage=stage,
                    message=message,
                    progress=progress,
                )
            )
        except Exception as e:
            # Log but don't fail - voice is non-critical
            logger.debug(f"[v79.0] Voice announcement failed: {e}")

    async def _announce_completion(
        self,
        task_id: str,
        success: bool,
        files_modified: int,
        execution_time_ms: float,
        error: Optional[str] = None,
    ) -> None:
        """
        v79.0: Announce evolution completion via voice.
        """
        if not VOICE_ANNOUNCER_AVAILABLE or not self._voice_announcer:
            return

        try:
            asyncio.create_task(
                self._voice_announcer.announce_completion(
                    task_id=task_id,
                    success=success,
                    files_modified=files_modified,
                    execution_time_ms=execution_time_ms,
                    error=error,
                )
            )
        except Exception as e:
            logger.debug(f"[v79.0] Voice completion announcement failed: {e}")

    async def initialize(self) -> bool:
        """Initialize the Coding Council."""
        if self._initialized:
            return True

        logger.info("[CodingCouncil] Initializing v77.0 Unified Coding Council...")

        # Create necessary directories
        self.config.frameworks_dir.mkdir(parents=True, exist_ok=True)
        self.config.cache_dir.mkdir(parents=True, exist_ok=True)

        # Check resources
        resources = self.resource_monitor.check_resources()
        if not resources["ok"]:
            logger.warning(f"[CodingCouncil] Resource warning: {resources.get('reason')}")

        # =================================================================
        # Initialize v77.0 Modules
        # =================================================================

        # Async Tools (Gaps #23-27)
        if ASYNC_TOOLS_AVAILABLE:
            try:
                self._task_registry = get_task_registry()
                self._deadlock_prevention = get_deadlock_prevention()
                self._file_locker = get_file_locker()
                await self._file_locker.start()
                logger.info("[CodingCouncil] Async tools initialized")
            except Exception as e:
                logger.warning(f"[CodingCouncil] Async tools init failed: {e}")

        # Observability (Gaps #28-31)
        if OBSERVABILITY_AVAILABLE:
            try:
                self._tracer = get_tracer("coding_council")
                self._metrics = get_metrics_collector("coding_council")
                self._health_monitor = get_health_monitor("coding_council")

                # Register health checks
                self._health_monitor.register_check(
                    name="resources",
                    check_fn=self._check_resources_health,
                    interval=60.0,
                    critical=True,
                )
                self._health_monitor.register_check(
                    name="frameworks",
                    check_fn=self._check_frameworks_health,
                    interval=120.0,
                    critical=False,
                )
                await self._health_monitor.start()
                logger.info("[CodingCouncil] Observability initialized")
            except Exception as e:
                logger.warning(f"[CodingCouncil] Observability init failed: {e}")

        # Edge Cases (Gaps #32-40)
        if EDGE_CASES_AVAILABLE:
            try:
                self._disk_monitor = get_disk_monitor()
                self._disk_monitor.add_path(str(self.config.repo_root))
                await self._disk_monitor.start()

                self._memory_monitor = get_memory_monitor()
                await self._memory_monitor.start()

                self._crash_recovery = get_crash_recovery("coding_council")
                self._crash_recovery.register_state_provider(
                    "council_stats",
                    self._get_state_for_checkpoint,
                )
                self._crash_recovery.register_restore_handler(
                    "council_stats",
                    self._restore_state_from_checkpoint,
                )
                await self._crash_recovery.start()

                self._graceful_shutdown = get_graceful_shutdown()
                self._graceful_shutdown.register_handler(
                    name="coding_council",
                    handler=self._shutdown_handler,
                    priority=10,
                )
                self._graceful_shutdown.setup_signals()

                logger.info("[CodingCouncil] Edge case handlers initialized")
            except Exception as e:
                logger.warning(f"[CodingCouncil] Edge case handlers init failed: {e}")

        # Safety (Gaps #16-22)
        if SAFETY_AVAILABLE:
            try:
                # v78.2: Fixed ASTValidator initialization - requires repo_root
                self._ast_validator = ASTValidator(self.config.repo_root)
                self._security_scanner = SecurityScanner(self.config.repo_root)
                self._type_checker = TypeChecker(self.config.repo_root)
                self._staging_env = StagingEnvironment(self.config.repo_root)
                logger.info("[CodingCouncil] Safety modules initialized")
            except Exception as e:
                logger.warning(f"[CodingCouncil] Safety modules init failed: {e}")

        # Trinity (Gaps #1-7)
        # v253.7: Added per-step timeouts — git subprocesses in CrossRepoSync
        # can hang indefinitely, stalling the entire startup.
        if TRINITY_AVAILABLE:
            _trinity_step_timeout = float(os.getenv("JARVIS_CC_TRINITY_STEP_TIMEOUT", "15"))
            try:
                self._message_queue = PersistentMessageQueue()
                await asyncio.wait_for(self._message_queue.start(), timeout=_trinity_step_timeout)

                self._heartbeat_validator = await asyncio.wait_for(
                    acquire_shared_heartbeat_validator(),
                    timeout=_trinity_step_timeout,
                )
                self._uses_shared_heartbeat_validator = True

                self._cross_repo_sync = CrossRepoSync()
                await asyncio.wait_for(
                    self._cross_repo_sync.start(),
                    timeout=float(os.getenv("JARVIS_CC_CROSS_REPO_TIMEOUT", "35")),
                )

                logger.info("[CodingCouncil] Trinity modules initialized")
            except asyncio.TimeoutError:
                logger.warning("[CodingCouncil] Trinity modules init timed out — continuing without")
            except Exception as e:
                logger.warning(f"[CodingCouncil] Trinity modules init failed: {e}")

        # v77.1 Advanced Modules (Gaps #41-60)
        if ADVANCED_AVAILABLE:
            try:
                # Event Store for audit trail
                self._event_store = EvolutionEventStore()
                await self._event_store.start()

                # State Machine Manager
                self._state_machine_manager = StateMachineManager()
                recovered = await self._state_machine_manager.recover_all()
                if recovered:
                    logger.info(f"[CodingCouncil] Recovered {len(recovered)} evolutions from crash")

                # Saga Coordinator
                self._saga_coordinator = SagaCoordinator()

                # Distributed Transaction Coordinator
                # Get repos from unified config
                try:
                    from .config import get_config
                    config = get_config()
                    repos = {name: repo.path for name, repo in config.repos.items()}
                except ImportError:
                    repos = {
                        "jarvis": self.config.repo_root,
                        "jarvis_prime": Path(os.getenv("JARVIS_PRIME_REPO", str(Path.home() / "Documents/repos/jarvis-prime"))),
                        "reactor_core": Path(os.getenv("REACTOR_CORE_REPO", str(Path.home() / "Documents/repos/reactor-core"))),
                    }
                self._transaction_coordinator = DistributedTransactionCoordinator(repos=repos)

                # Cross-Repo Dependency Tracker
                self._dependency_tracker = CrossRepoDependencyTracker(repos=repos)
                asyncio.create_task(self._dependency_tracker.scan_all())  # Background scan

                # Adaptive Framework Selector
                self._adaptive_selector = get_adaptive_selector()

                logger.info("[CodingCouncil] v77.1 Advanced modules initialized")
            except Exception as e:
                logger.warning(f"[CodingCouncil] Advanced modules init failed: {e}")

        # =================================================================
        # v78.0 Advanced Modules (Process Management)
        # =================================================================
        if ADVANCED_V780_AVAILABLE:
            try:
                # Adaptive Timeout Manager (dynamic timeouts)
                self._timeout_manager = await get_timeout_manager()
                logger.info("[CodingCouncil] Adaptive Timeout Manager initialized")

                # Intelligent Retry Manager
                self._retry_manager = await get_retry_manager()
                logger.info("[CodingCouncil] Intelligent Retry Manager initialized")

                # Cross-Repo Transaction Coordinator
                self._cross_repo_tx = await get_transaction_coordinator()
                logger.info("[CodingCouncil] Cross-Repo Transaction Coordinator initialized")

                logger.info("[CodingCouncil] v78.0 modules initialized successfully")
            except Exception as e:
                logger.warning(f"[CodingCouncil] v78.0 modules init failed: {e}")

        # Initialize adapters (lazy - only when needed)
        self._initialized = True
        logger.info("[CodingCouncil] Initialization complete - All 95 gaps addressed (v77.0 + v77.1 + v77.2 + v78.0)")

        return True

    async def _check_resources_health(self) -> bool:
        """Health check for resources."""
        resources = self.resource_monitor.check_resources()
        return resources.get("ok", False)

    async def _check_frameworks_health(self) -> bool:
        """Health check for frameworks."""
        # Check if at least one framework is available
        for cb in self.circuit_breakers.values():
            if not cb.is_open:
                return True
        return False

    async def _get_state_for_checkpoint(self) -> Dict[str, Any]:
        """Get state for crash recovery checkpoint."""
        return {
            "stats": self._stats.copy(),
            "active_task_ids": list(self._active_tasks.keys()),
        }

    async def _restore_state_from_checkpoint(self, state: Dict[str, Any]) -> None:
        """Restore state from crash recovery checkpoint."""
        if "stats" in state:
            self._stats.update(state["stats"])
        logger.info("[CodingCouncil] State restored from checkpoint")

    async def _shutdown_handler(self) -> None:
        """Handler for graceful shutdown."""
        await self.shutdown()

    async def shutdown(self, drain_timeout: float = 30.0) -> None:
        """
        Shutdown the Coding Council gracefully.

        v78.1: Enhanced with proper drain pattern:
        1. Set shutdown flag to stop accepting new tasks
        2. Wait for in-flight tasks to complete (with timeout)
        3. Release all locks
        4. Cleanup v78.0 components
        5. Shutdown v77.0 modules in reverse order

        Args:
            drain_timeout: Maximum time to wait for in-flight tasks (default 30s)
        """
        logger.info("[CodingCouncil] Initiating graceful shutdown...")
        self._shutdown = True

        # Release hot reload lock if held
        if self.hot_reload_lock.is_held:
            await self.hot_reload_lock.release()

        # v78.1: Proper drain pattern for in-flight tasks
        if self._active_tasks:
            num_tasks = len(self._active_tasks)
            logger.info(f"[CodingCouncil] Draining {num_tasks} active task(s)...")

            # Wait for tasks with periodic status updates
            start_time = time.time()
            check_interval = 5.0  # Check every 5 seconds

            while self._active_tasks and (time.time() - start_time) < drain_timeout:
                remaining = len(self._active_tasks)
                elapsed = time.time() - start_time
                logger.info(
                    f"[CodingCouncil] {remaining} task(s) still running "
                    f"({elapsed:.0f}s/{drain_timeout:.0f}s)..."
                )
                await asyncio.sleep(min(check_interval, drain_timeout - elapsed))

            # Check if any tasks timed out
            if self._active_tasks:
                logger.warning(
                    f"[CodingCouncil] Timeout waiting for {len(self._active_tasks)} task(s). "
                    f"Forcing shutdown..."
                )
                # Clear active tasks to prevent blocking
                self._active_tasks.clear()
            else:
                logger.info("[CodingCouncil] All tasks drained successfully")

        # v78.1: Cleanup in-progress transactions
        if self._cross_repo_tx:
            try:
                # Abort any pending transactions
                pending_txs = await self._cross_repo_tx.list_transactions(
                    state=TransactionState.PREPARING
                )
                for tx in pending_txs:
                    logger.warning(f"[CodingCouncil] Aborting pending TX: {tx.transaction_id}")
                    await self._cross_repo_tx.abort(tx.transaction_id)
            except Exception as e:
                logger.warning(f"[CodingCouncil] Transaction cleanup error: {e}")

        # v78.1: Persist v78.0 component state
        if self._timeout_manager:
            try:
                await self._timeout_manager.persist()
            except Exception as e:
                logger.warning(f"[CodingCouncil] Timeout manager persist error: {e}")

        if self._retry_manager:
            try:
                await self._retry_manager.persist()
            except Exception as e:
                logger.warning(f"[CodingCouncil] Retry manager persist error: {e}")

        # =================================================================
        # Shutdown v77.0 Modules (reverse order of init)
        # =================================================================

        # Trinity
        if self._cross_repo_sync:
            try:
                await self._cross_repo_sync.stop()
            except Exception as e:
                logger.warning(f"[CodingCouncil] Cross-repo sync shutdown error: {e}")

        if self._heartbeat_validator:
            try:
                if self._uses_shared_heartbeat_validator:
                    await release_shared_heartbeat_validator(self._heartbeat_validator)
                else:
                    await self._heartbeat_validator.stop()
            except Exception as e:
                logger.warning(f"[CodingCouncil] Heartbeat validator shutdown error: {e}")
            self._heartbeat_validator = None
            self._uses_shared_heartbeat_validator = False

        if self._message_queue:
            try:
                await self._message_queue.stop()
            except Exception as e:
                logger.warning(f"[CodingCouncil] Message queue shutdown error: {e}")

        # Edge Cases
        if self._crash_recovery:
            try:
                await self._crash_recovery.stop()
            except Exception as e:
                logger.warning(f"[CodingCouncil] Crash recovery shutdown error: {e}")

        if self._memory_monitor:
            try:
                await self._memory_monitor.stop()
            except Exception as e:
                logger.warning(f"[CodingCouncil] Memory monitor shutdown error: {e}")

        if self._disk_monitor:
            try:
                await self._disk_monitor.stop()
            except Exception as e:
                logger.warning(f"[CodingCouncil] Disk monitor shutdown error: {e}")

        # Observability
        if self._health_monitor:
            try:
                await self._health_monitor.stop()
            except Exception as e:
                logger.warning(f"[CodingCouncil] Health monitor shutdown error: {e}")

        # Async Tools
        if self._file_locker:
            try:
                await self._file_locker.stop()
            except Exception as e:
                logger.warning(f"[CodingCouncil] File locker shutdown error: {e}")

        if self._task_registry:
            try:
                await self._task_registry.shutdown()
            except Exception as e:
                logger.warning(f"[CodingCouncil] Task registry shutdown error: {e}")

        # =================================================================
        # Shutdown v77.1 Advanced Modules
        # =================================================================

        # Event Store
        if self._event_store:
            try:
                await self._event_store.stop()
            except Exception as e:
                logger.warning(f"[CodingCouncil] Event store shutdown error: {e}")

        # State Machine Manager (cleanup old checkpoints)
        if self._state_machine_manager:
            try:
                await self._state_machine_manager.cleanup_old_checkpoints()
            except Exception as e:
                logger.warning(f"[CodingCouncil] State machine cleanup error: {e}")

        logger.info("[CodingCouncil] Shutdown complete (v77.0 + v77.1)")

    async def evolve(
        self,
        description: str,
        target_files: Optional[List[str]] = None,
        complexity: Optional[TaskComplexity] = None,
        require_sandbox: bool = False,
        require_planning: bool = False,
        require_approval: bool = False,
        correlation_id: Optional[str] = None,
        timeout: Optional[float] = None,
    ) -> EvolutionResult:
        """
        Main entry point for code evolution.

        This is the "do everything" method that orchestrates the entire
        evolution process from analysis to validation.

        Args:
            description: What to do (natural language)
            target_files: Optional list of specific files to modify
            complexity: Override automatic complexity detection
            require_sandbox: Force sandboxed execution (OpenHands)
            require_planning: Force planning phase (MetaGPT)
            require_approval: Require human approval before applying
            correlation_id: For tracing across systems
            timeout: Override default timeout

        Returns:
            EvolutionResult with all details of the evolution
        """
        # Create task
        task = EvolutionTask(
            description=description,
            target_files=target_files or [],
            complexity=complexity or TaskComplexity.SIMPLE,
            requires_sandbox=require_sandbox,
            requires_planning=require_planning,
            requires_approval=require_approval,
            correlation_id=correlation_id,
        )

        return await self.execute_task(task, timeout)

    async def execute_task(
        self,
        task: EvolutionTask,
        timeout: Optional[float] = None
    ) -> EvolutionResult:
        """
        Execute an evolution task through the full pipeline.

        Pipeline:
            1. Pre-flight checks (resources, circuit breakers)
            2. Acquire locks (hot reload, task semaphore)
            3. Create rollback savepoint
            4. Analysis phase (RepoMaster)
            5. Planning phase (MetaGPT) if needed
            6. Execution phase (selected framework)
            7. Validation phase
            8. Apply or rollback
        """
        start_time = time.time()

        # v78.0: Use adaptive timeout if available
        if self._timeout_manager and timeout is None:
            try:
                timeout = await self._timeout_manager.get_timeout(
                    operation=OperationType.CODING_COUNCIL,
                    context={
                        "description": task.description[:200] if task.description else "",
                        "num_files": len(task.target_files) if task.target_files else 0,
                        "complexity": getattr(task, 'complexity', 'medium'),
                    }
                )
                logger.debug(f"[v78.0] Adaptive timeout calculated: {timeout:.1f}s for task {task.task_id}")
            except Exception as e:
                logger.warning(f"[v78.0] Adaptive timeout failed, using default: {e}")
                timeout = self.config.default_timeout
        else:
            timeout = timeout or self.config.default_timeout

        # Initialize result
        result = EvolutionResult(
            task_id=task.task_id,
            success=False,
            framework_used=FrameworkType.CLAUDE_CODE,
            all_frameworks_used=[],
        )

        try:
            # Phase 0: Pre-flight checks
            await self._pre_flight_checks(task)

            # v79.0: Announce task start
            await self._announce_progress(
                task_id=task.task_id,
                stage="requested",
                message=f"Starting evolution: {task.description[:50]}..." if len(task.description) > 50 else f"Starting evolution: {task.description}",
                progress=5,
            )

            # Acquire task semaphore
            async with self._task_semaphore:
                self._active_tasks[task.task_id] = task

                try:
                    # Acquire hot reload lock
                    await self.hot_reload_lock.acquire(task.task_id)

                    # Create rollback savepoint
                    rollback_info = await self.rollback.create_savepoint(
                        task.task_id,
                        task.target_files
                    )
                    result.rollback_id = rollback_info.rollback_id

                    # Phase 1: Analysis
                    task.status = TaskStatus.ANALYZING
                    task.current_phase = "analysis"
                    task.progress_percent = 10

                    # v79.0: Announce analysis phase
                    await self._announce_progress(
                        task_id=task.task_id,
                        stage="analyzing",
                        message="Analyzing codebase structure and dependencies",
                        progress=10,
                    )

                    analysis = None
                    if self.router.should_use_analysis(task):
                        analysis = await self._run_analysis(task)
                        result.all_frameworks_used.append(FrameworkType.REPOMASTER)
                        result.insights["analysis"] = {
                            "complexity_score": analysis.complexity_score if analysis else 0,
                            "risk_score": analysis.risk_score if analysis else 0,
                        }

                    # Phase 2: Planning
                    task.status = TaskStatus.PLANNING
                    task.current_phase = "planning"
                    task.progress_percent = 30

                    # v79.0: Announce planning phase
                    await self._announce_progress(
                        task_id=task.task_id,
                        stage="planning",
                        message="Planning code changes and modifications",
                        progress=30,
                    )

                    plan = None
                    if self.router.should_use_planning(task):
                        plan = await self._run_planning(task, analysis)
                        result.all_frameworks_used.append(FrameworkType.METAGPT)
                        result.insights["plan"] = {
                            "steps": len(plan.steps) if plan else 0,
                        }

                    # Phase 3: Framework selection
                    framework = self.router.choose_framework(task, analysis, plan)
                    result.framework_used = framework

                    # Check circuit breaker
                    cb = self.circuit_breakers[framework]
                    if cb.is_open:
                        raise RuntimeError(
                            f"Circuit breaker OPEN for {framework.value}, "
                            "too many recent failures"
                        )

                    # Phase 4: Execution
                    task.status = TaskStatus.EXECUTING
                    task.current_phase = "execution"
                    task.progress_percent = 50

                    # v79.0: Announce execution phase
                    await self._announce_progress(
                        task_id=task.task_id,
                        stage="generating",
                        message=f"Executing changes via {framework.value}",
                        progress=50,
                    )

                    # v78.0: Track operation for adaptive timeout statistics
                    if self._timeout_manager:
                        async with self._timeout_manager.track_operation(
                            operation=OperationType.CODING_COUNCIL,
                            context={
                                "framework": framework.value,
                                "task_id": task.task_id,
                                "num_files": len(task.target_files) if task.target_files else 0,
                            }
                        ):
                            framework_result = await self._run_framework(
                                framework, task, analysis, plan, timeout
                            )
                    else:
                        framework_result = await self._run_framework(
                            framework, task, analysis, plan, timeout
                        )

                    if not framework_result.success:
                        await cb.record_failure(framework_result.error)
                        raise RuntimeError(
                            f"Framework execution failed: {framework_result.error}"
                        )

                    await cb.record_success()
                    result.all_frameworks_used.append(framework)
                    result.changes_made = framework_result.changes_made
                    result.files_modified = framework_result.files_modified
                    result.framework_results[framework.value] = framework_result

                    # Phase 5: Validation
                    task.status = TaskStatus.VALIDATING
                    task.current_phase = "validation"
                    task.progress_percent = 80

                    # v79.0: Announce validation phase
                    await self._announce_progress(
                        task_id=task.task_id,
                        stage="testing",
                        message=f"Validating {len(framework_result.files_modified)} modified files",
                        progress=80,
                    )

                    validation = await self.validator.validate_files(
                        framework_result.files_modified
                    )
                    result.validation_report = validation

                    if not validation.passed:
                        # Rollback on validation failure
                        await self.rollback.restore_savepoint(
                            rollback_info.rollback_id,
                            RollbackReason.VALIDATION_FAILED
                        )
                        self._stats["rollbacks_performed"] += 1

                        raise RuntimeError(
                            f"Validation failed: {validation.errors}"
                        )

                    # Phase 6: Commit changes
                    task.status = TaskStatus.COMPLETED
                    task.current_phase = "completed"
                    task.progress_percent = 100

                    await self.rollback.commit_savepoint(rollback_info.rollback_id)

                    result.success = True
                    self._stats["tasks_completed"] += 1
                    self._stats["frameworks_used"][framework.value] += 1

                    # v79.0: Announce successful completion
                    await self._announce_progress(
                        task_id=task.task_id,
                        stage="complete",
                        message=f"Evolution complete! {len(framework_result.files_modified)} files modified",
                        progress=100,
                    )

                finally:
                    # Always release hot reload lock
                    await self.hot_reload_lock.release()

                    # Remove from active tasks
                    self._active_tasks.pop(task.task_id, None)

        except asyncio.TimeoutError:
            result.error = f"Task timed out after {timeout}s"
            task.status = TaskStatus.FAILED
            self._stats["tasks_failed"] += 1

            # Rollback on timeout
            if result.rollback_id:
                await self.rollback.restore_savepoint(
                    result.rollback_id,
                    RollbackReason.TIMEOUT
                )
                self._stats["rollbacks_performed"] += 1

            # v79.0: Announce timeout failure
            await self._announce_progress(
                task_id=task.task_id,
                stage="failed",
                message=f"Evolution timed out after {timeout}s - changes rolled back",
                progress=0,
                error=result.error,
            )

        except Exception as e:
            result.error = str(e)
            task.status = TaskStatus.FAILED
            self._stats["tasks_failed"] += 1
            logger.error(f"[CodingCouncil] Task {task.task_id} failed: {e}")

            # v79.0: Announce error failure
            await self._announce_progress(
                task_id=task.task_id,
                stage="failed",
                message=f"Evolution failed: {str(e)[:100]}",
                progress=0,
                error=str(e),
            )

        # Record execution time
        result.execution_time_ms = (time.time() - start_time) * 1000
        self._stats["total_execution_time_ms"] += result.execution_time_ms

        task.completed_at = time.time()

        return result

    async def _pre_flight_checks(self, task: EvolutionTask) -> None:
        """Run pre-flight checks before starting evolution."""
        # Check resources
        resources = self.resource_monitor.check_resources()
        if not resources["ok"]:
            # Wait for resources
            if not await self.resource_monitor.wait_for_resources(timeout=60):
                raise RuntimeError(f"Insufficient resources: {resources.get('reason')}")

        # Check protected paths
        for filepath in task.target_files:
            for protected in self.config.protected_paths:
                if filepath.endswith(protected) or protected in filepath:
                    raise ValueError(f"Cannot modify protected path: {filepath}")

        # Check file limits
        if len(task.target_files) > self.config.max_files_per_task:
            raise ValueError(
                f"Too many files: {len(task.target_files)} > {self.config.max_files_per_task}"
            )

    async def _run_analysis(self, task: EvolutionTask) -> Optional[AnalysisResult]:
        """Run codebase analysis with RepoMaster."""
        try:
            adapter = await self._get_adapter(FrameworkType.REPOMASTER)
            if adapter is None:
                logger.warning("[CodingCouncil] RepoMaster not available, skipping analysis")
                return None

            return await asyncio.wait_for(
                adapter.analyze(task.target_files, task.description),
                timeout=self.config.analysis_timeout
            )

        except asyncio.TimeoutError:
            logger.warning("[CodingCouncil] Analysis timed out")
            return None
        except Exception as e:
            logger.warning(f"[CodingCouncil] Analysis failed: {e}")
            return None

    async def _run_planning(
        self,
        task: EvolutionTask,
        analysis: Optional[AnalysisResult]
    ) -> Optional[PlanResult]:
        """Run planning with MetaGPT."""
        try:
            adapter = await self._get_adapter(FrameworkType.METAGPT)
            if adapter is None:
                logger.warning("[CodingCouncil] MetaGPT not available, skipping planning")
                return None

            return await asyncio.wait_for(
                adapter.plan(task.description, analysis),
                timeout=self.config.planning_timeout
            )

        except asyncio.TimeoutError:
            logger.warning("[CodingCouncil] Planning timed out")
            return None
        except Exception as e:
            logger.warning(f"[CodingCouncil] Planning failed: {e}")
            return None

    async def _run_framework(
        self,
        framework: FrameworkType,
        task: EvolutionTask,
        analysis: Optional[AnalysisResult],
        plan: Optional[PlanResult],
        timeout: float
    ) -> FrameworkResult:
        """Run the selected framework with v78.0 intelligent retry."""
        start = time.time()

        async def _execute_adapter() -> FrameworkResult:
            """Inner execution function for retry wrapper."""
            adapter = await self._get_adapter(framework)
            if adapter is None:
                # Not retryable - framework unavailable
                logger.warning(
                    f"[CodingCouncil] {framework.value} not available, "
                    "using Claude Code fallback"
                )
                return FrameworkResult(
                    framework=framework,
                    success=False,
                    error="Framework not available"
                )

            result = await asyncio.wait_for(
                adapter.execute(task, analysis, plan),
                timeout=timeout
            )
            result.execution_time_ms = (time.time() - start) * 1000
            return result

        try:
            # v78.0: Use intelligent retry if available
            if self._retry_manager:
                from .advanced import IntelligentRetryConfig, RetryStrategy

                retry_config = IntelligentRetryConfig(
                    max_attempts=3,
                    base_delay_ms=1000,   # 1 second base delay
                    max_delay_ms=30000,   # 30 second max delay
                    strategy=RetryStrategy.DECORRELATED_JITTER,
                    budget_ms=timeout * 1000 if timeout else None,  # Total time budget
                )

                retry_result = await self._retry_manager.execute_with_retry(
                    _execute_adapter,
                    operation=f"framework_{framework.value}",
                    config=retry_config,
                )

                if retry_result.success:
                    logger.debug(
                        f"[v78.0] Framework {framework.value} succeeded "
                        f"after {retry_result.attempts} attempt(s)"
                    )
                    return retry_result.result
                else:
                    logger.warning(
                        f"[v78.0] Framework {framework.value} failed after "
                        f"{retry_result.attempts} attempts: {retry_result.error}"
                    )
                    return FrameworkResult(
                        framework=framework,
                        success=False,
                        error=f"Failed after {retry_result.attempts} retries: {retry_result.error}",
                        execution_time_ms=(time.time() - start) * 1000
                    )
            else:
                # Fallback: Direct execution without retry
                return await _execute_adapter()

        except asyncio.TimeoutError:
            return FrameworkResult(
                framework=framework,
                success=False,
                error=f"Execution timed out after {timeout}s",
                execution_time_ms=(time.time() - start) * 1000
            )
        except Exception as e:
            return FrameworkResult(
                framework=framework,
                success=False,
                error=str(e),
                execution_time_ms=(time.time() - start) * 1000
            )

    async def _get_adapter(self, framework: FrameworkType):
        """
        v84.0: Get or create a framework adapter.

        Enhanced with J-Prime adapter support for local LLM inference.
        """
        if framework in self._adapters:
            return self._adapters[framework]

        # Lazy load adapter
        try:
            if framework == FrameworkType.AIDER:
                from .adapters.aider_adapter import AiderAdapter
                self._adapters[framework] = AiderAdapter(self.config)

            elif framework == FrameworkType.OPENHANDS:
                from .adapters.openhands_adapter import OpenHandsAdapter
                self._adapters[framework] = OpenHandsAdapter(self.config)

            elif framework == FrameworkType.METAGPT:
                from .adapters.metagpt_adapter import MetaGPTAdapter
                self._adapters[framework] = MetaGPTAdapter(self.config)

            elif framework == FrameworkType.REPOMASTER:
                from .adapters.repomaster_adapter import RepoMasterAdapter
                self._adapters[framework] = RepoMasterAdapter(self.config)

            elif framework == FrameworkType.CONTINUE:
                from .adapters.continue_adapter import ContinueAdapter
                self._adapters[framework] = ContinueAdapter(self.config)

            # ================================================================
            # v84.0: J-Prime Adapters (Local LLM)
            # ================================================================

            elif framework == FrameworkType.JPRIME_CODING:
                from .adapters.jprime_adapter import JPrimeCodingAdapter
                adapter = JPrimeCodingAdapter(self.config)
                # Pre-initialize the engine for faster first request
                await adapter._get_engine()
                self._adapters[framework] = adapter
                logger.info("[CodingCouncil] J-Prime Coding adapter loaded (CodeLlama)")

            elif framework == FrameworkType.JPRIME_REASONING:
                from .adapters.jprime_adapter import JPrimeReasoningAdapter
                adapter = JPrimeReasoningAdapter(self.config)
                await adapter._get_engine()
                self._adapters[framework] = adapter
                logger.info("[CodingCouncil] J-Prime Reasoning adapter loaded (Qwen)")

            elif framework == FrameworkType.JPRIME_LOCAL:
                from .adapters.jprime_adapter import JPrimeLocalAdapter
                adapter = JPrimeLocalAdapter(self.config)
                await adapter._get_engine()
                self._adapters[framework] = adapter
                logger.info("[CodingCouncil] J-Prime Local adapter loaded (Llama)")

            else:
                return None

            return self._adapters[framework]

        except ImportError as e:
            logger.warning(f"[CodingCouncil] Could not load {framework.value} adapter: {e}")
            return None
        except Exception as e:
            logger.warning(f"[CodingCouncil] Adapter initialization failed for {framework.value}: {e}")
            return None

    def get_stats(self) -> Dict[str, Any]:
        """Get council statistics."""
        return {
            **self._stats,
            "active_tasks": len(self._active_tasks),
            "circuit_breakers": {
                f.value: cb.get_status()
                for f, cb in self.circuit_breakers.items()
            },
            "resources": self.resource_monitor.check_resources(),
            "config": {
                "max_concurrent_tasks": self.config.max_concurrent_tasks,
                "max_files_per_task": self.config.max_files_per_task,
            }
        }

    def get_active_tasks(self) -> List[EvolutionTask]:
        """Get list of currently active tasks."""
        return list(self._active_tasks.values())

    def get_status(self) -> Dict[str, Any]:
        """
        Get comprehensive council status.

        Used by Trinity integration and health endpoints.
        """
        # Get stats
        stats = self.get_stats()

        # Get framework availability
        frameworks_available = []
        for framework in FrameworkType:
            if framework in self._adapters:
                frameworks_available.append(framework.value)

        # Get module statuses
        module_status = {
            "async_tools": ASYNC_TOOLS_AVAILABLE,
            "framework": FRAMEWORK_AVAILABLE,
            "observability": OBSERVABILITY_AVAILABLE,
            "edge_cases": EDGE_CASES_AVAILABLE,
            "safety": SAFETY_AVAILABLE,
            "trinity": TRINITY_AVAILABLE,
        }

        # Detailed module info
        module_details = {}

        if self._health_monitor:
            module_details["health"] = self._health_monitor.get_summary()

        if self._disk_monitor:
            module_details["disk"] = self._disk_monitor.get_summary()

        if self._memory_monitor:
            module_details["memory"] = self._memory_monitor.get_summary()

        if self._crash_recovery:
            module_details["crash_recovery"] = self._crash_recovery.get_summary()

        if self._cross_repo_sync:
            module_details["cross_repo_sync"] = self._cross_repo_sync.get_summary()

        if self._heartbeat_validator:
            module_details["heartbeat"] = self._heartbeat_validator.get_summary()

        if self._message_queue:
            try:
                # get_stats is sync, need to handle carefully
                module_details["message_queue"] = {"status": "running"}
            except Exception:
                pass

        if self._task_registry:
            module_details["task_registry"] = self._task_registry.get_stats()

        # v77.1 Advanced module details
        if self._event_store:
            try:
                import asyncio
                try:
                    asyncio.get_running_loop()
                    # Loop is running — can't call run_until_complete.
                    module_details["event_store"] = {"status": "running"}
                except RuntimeError:
                    # No running loop — safe to drive the coroutine.
                    module_details["event_store"] = asyncio.run(self._event_store.get_statistics())
            except Exception:
                module_details["event_store"] = {"status": "running"}

        if self._state_machine_manager:
            module_details["state_machine"] = self._state_machine_manager.get_statistics()

        if self._dependency_tracker:
            module_details["dependency_tracker"] = self._dependency_tracker.get_graph_stats()

        if self._adaptive_selector:
            module_details["adaptive_selector"] = self._adaptive_selector.get_statistics()

        # Update module status with advanced modules
        module_status["advanced"] = ADVANCED_AVAILABLE

        return {
            "initialized": self._initialized,
            "active_tasks": len(self._active_tasks),
            "tasks_completed": stats.get("tasks_completed", 0),
            "tasks_failed": stats.get("tasks_failed", 0),
            "rollbacks_performed": stats.get("rollbacks_performed", 0),
            "circuit_breakers": stats.get("circuit_breakers", {}),
            "resources": stats.get("resources", {}),
            "frameworks_available": frameworks_available,
            "config": stats.get("config", {}),
            "modules_available": module_status,
            "module_details": module_details,
            "version": "77.2",
            "gaps_addressed": 80,
        }

    # =========================================================================
    # Adapter Properties (for Trinity integration)
    # =========================================================================

    @property
    def _aider(self):
        """Get Aider adapter (may be None if not loaded)."""
        return self._adapters.get(FrameworkType.AIDER)

    @property
    def _repomaster(self):
        """Get RepoMaster adapter (may be None if not loaded)."""
        return self._adapters.get(FrameworkType.REPOMASTER)

    @property
    def _metagpt(self):
        """Get MetaGPT adapter (may be None if not loaded)."""
        return self._adapters.get(FrameworkType.METAGPT)

    @property
    def _openhands(self):
        """Get OpenHands adapter (may be None if not loaded)."""
        return self._adapters.get(FrameworkType.OPENHANDS)

    @property
    def _continue(self):
        """Get Continue adapter (may be None if not loaded)."""
        return self._adapters.get(FrameworkType.CONTINUE)

    @property
    def _rollback_manager(self) -> RollbackManager:
        """Get rollback manager (for Trinity integration)."""
        return self.rollback

    # =========================================================================
    # Trinity Cross-Repo Event Handling
    # =========================================================================

    async def handle_trinity_event(self, event: Dict[str, Any]) -> None:
        """
        Handle Trinity cross-repo events.

        This method is called by the Trinity sync system when files change
        in other repos (J-Prime, Reactor-Core) that may affect JARVIS.

        Args:
            event: Dictionary containing:
                - type: Event type (file_changed, context_updated, etc.)
                - repo: Source repository (jarvis, j_prime, reactor_core)
                - file: File path that changed
                - timestamp: When the event occurred

        Handles:
            1. Cache invalidation for affected files
            2. Cross-repo dependency updates
            3. Adaptive selector weight adjustments
            4. Event logging for observability
        """
        event_type = event.get('type', 'unknown')
        source_repo = event.get('repo', 'unknown')
        file_path = event.get('file', '')
        timestamp = event.get('timestamp', 0)

        logger.debug(
            f"[TrinityEvent] type={event_type} repo={source_repo} file={file_path}"
        )

        try:
            if event_type == 'file_changed':
                # Handle file change events
                await self._handle_file_change_event(source_repo, file_path)

            elif event_type == 'context_updated':
                # Handle context updates
                await self._handle_context_update_event(source_repo, event)

            elif event_type == 'dependency_changed':
                # Handle dependency graph changes
                if self._dependency_tracker:
                    await self._handle_dependency_change_event(source_repo, file_path)

            # Record event in metrics
            if self._metrics:
                self._metrics.record_counter(
                    "trinity_events_received",
                    1,
                    {"type": event_type, "repo": source_repo}
                )

        except Exception as e:
            logger.error(f"[TrinityEvent] Error handling event: {e}")
            if self._metrics:
                self._metrics.record_counter(
                    "trinity_events_errors",
                    1,
                    {"type": event_type, "error": str(e)[:50]}
                )

    async def _handle_file_change_event(
        self,
        source_repo: str,
        file_path: str,
    ) -> None:
        """Handle file change events from Trinity repos."""
        # Invalidate caches related to changed files
        if self._cross_repo_sync:
            # Cross-repo sync handles propagation
            await self._cross_repo_sync.invalidate_for_file(file_path, source_repo)

        # Update dependency tracker if available
        if self._dependency_tracker:
            try:
                # Check if changed file affects any tracked dependencies
                affected = await self._dependency_tracker.get_dependents(file_path)
                if affected:
                    logger.info(
                        f"[TrinityEvent] File {file_path} change affects {len(affected)} files"
                    )
            except Exception as e:
                logger.debug(f"[TrinityEvent] Dependency check skipped: {e}")

        # Adjust adaptive selector weights if patterns suggest framework preference
        if self._adaptive_selector and source_repo in ('j_prime', 'reactor_core'):
            # If ML/model files changed in J-Prime or Reactor-Core,
            # slightly boost MetaGPT weight for better planning
            if any(kw in file_path.lower() for kw in ['model', 'ml', 'train', 'predict']):
                await self._adaptive_selector.adjust_weight(
                    FrameworkType.METAGPT,
                    boost=0.05,
                    reason=f"ML file changed in {source_repo}"
                )

    async def _handle_context_update_event(
        self,
        source_repo: str,
        event: Dict[str, Any],
    ) -> None:
        """Handle context updates from Trinity repos."""
        # Context updates may include:
        # - New available models (from Reactor-Core)
        # - New cognitive capabilities (from J-Prime)
        # - System state changes

        trigger_repo = event.get('trigger_repo', source_repo)
        trigger_file = event.get('trigger_file', '')

        logger.debug(
            f"[TrinityEvent] Context update: {trigger_repo}/{trigger_file}"
        )

        # Update health monitor with cross-repo status
        if self._health_monitor:
            self._health_monitor.record_check(
                f"trinity_{source_repo}",
                True,
                f"Context update received"
            )

    async def _handle_dependency_change_event(
        self,
        source_repo: str,
        file_path: str,
    ) -> None:
        """Handle dependency graph changes from Trinity repos."""
        if self._dependency_tracker:
            try:
                # Re-analyze dependencies for affected files
                await self._dependency_tracker.invalidate_cache(file_path)
                logger.debug(
                    f"[TrinityEvent] Dependency cache invalidated for {file_path}"
                )
            except Exception as e:
                logger.debug(f"[TrinityEvent] Dependency invalidation skipped: {e}")
