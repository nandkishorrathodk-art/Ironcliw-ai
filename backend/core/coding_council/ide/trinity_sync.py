"""
v77.3: Trinity Cross-Repo IDE Synchronization
==============================================

Advanced synchronization layer that connects the IDE bridge across
JARVIS, J-Prime, and Reactor Core repositories.

Features:
- Real-time file change propagation
- Cross-repo context sharing
- Distributed suggestion caching
- Event mesh for multi-repo awareness
- Conflict detection and resolution
- Delta-based incremental sync

This enables the IDE to understand the full Trinity ecosystem,
providing intelligent suggestions that span all three repositories.

Author: JARVIS v77.3
Version: 1.0.0
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
import os
import time
from dataclasses import dataclass, field
from enum import Enum, auto
from pathlib import Path
from typing import (
    Any,
    Callable,
    Dict,
    List,
    Optional,
    Set,
    Tuple,
    TypeVar,
    Union,
)
import weakref

logger = logging.getLogger(__name__)

T = TypeVar("T")

# =============================================================================
# ARM64 SIMD Acceleration (40-50x faster hash operations)
# =============================================================================

try:
    from ..acceleration import (
        UnifiedAccelerator,
        get_accelerator,
        get_acceleration_registry,
    )
    _ACCELERATOR: Optional[UnifiedAccelerator] = None

    def _get_accelerator() -> Optional[UnifiedAccelerator]:
        """Get or create accelerator instance (lazy initialization)."""
        global _ACCELERATOR
        if _ACCELERATOR is None:
            try:
                _ACCELERATOR = get_accelerator()
                # Register this component
                registry = get_acceleration_registry()
                registry.register(
                    component_name="trinity_sync",
                    repo="jarvis",
                    operations={"fast_hash"}
                )
                logger.debug("[TrinitySynchronizer] ARM64 acceleration enabled")
            except Exception as e:
                logger.debug(f"[TrinitySynchronizer] Acceleration init failed: {e}")
        return _ACCELERATOR

    ACCELERATION_AVAILABLE = True
except ImportError:
    ACCELERATION_AVAILABLE = False
    _ACCELERATOR = None

    def _get_accelerator():
        return None


# =============================================================================
# Configuration (Dynamic via unified config)
# =============================================================================

def _get_config():
    """Get unified configuration."""
    try:
        from ..config import get_config
        return get_config()
    except ImportError:
        return None


def _get_trinity_repos() -> Dict[str, Path]:
    """Get Trinity repos from unified config or fallback."""
    config = _get_config()
    if config:
        return {name: repo.path for name, repo in config.repos.items()}

    # Fallback to environment variables
    return {
        "jarvis": Path(os.getenv("JARVIS_REPO", Path(__file__).parent.parent.parent.parent.parent)),
        "j_prime": Path(os.getenv("J_PRIME_REPO", Path.home() / "Documents/repos/jarvis-prime")),
        "reactor_core": Path(os.getenv("REACTOR_CORE_REPO", Path.home() / "Documents/repos/reactor-core")),
    }


def _get_sync_dir() -> Path:
    """Get Trinity sync directory from config or fallback."""
    config = _get_config()
    if config:
        return config.trinity_dir / "ide_sync"
    return Path(os.getenv("TRINITY_SYNC_DIR", os.path.expanduser("~/.jarvis/trinity/ide_sync")))


def _get_sync_interval() -> float:
    """Get sync interval from config or fallback."""
    config = _get_config()
    if config:
        return config.sync_interval
    return float(os.getenv("TRINITY_SYNC_INTERVAL", "5.0"))


def _get_heartbeat_interval() -> float:
    """Get heartbeat interval from config or fallback."""
    config = _get_config()
    if config:
        return config.heartbeat_interval
    return float(os.getenv("TRINITY_HEARTBEAT_INTERVAL", "10.0"))


# Dynamic property accessors for backward compatibility
TRINITY_REPOS = property(lambda self: _get_trinity_repos())
TRINITY_SYNC_DIR = property(lambda self: _get_sync_dir())
TRINITY_SYNC_INTERVAL = property(lambda self: _get_sync_interval())
TRINITY_HEARTBEAT_INTERVAL = property(lambda self: _get_heartbeat_interval())


# =============================================================================
# Enums and Constants
# =============================================================================

class SyncEventType(Enum):
    """Types of sync events."""
    FILE_CREATED = auto()
    FILE_MODIFIED = auto()
    FILE_DELETED = auto()
    FILE_RENAMED = auto()
    CONTEXT_UPDATED = auto()
    SUGGESTION_CACHED = auto()
    DIAGNOSTIC_ADDED = auto()
    DIAGNOSTIC_CLEARED = auto()
    CURSOR_MOVED = auto()
    SELECTION_CHANGED = auto()


class RepoType(Enum):
    """Trinity repository types."""
    JARVIS = "jarvis"
    J_PRIME = "j_prime"
    REACTOR_CORE = "reactor_core"


class SyncPriority(Enum):
    """Priority levels for sync operations."""
    CRITICAL = 0  # Immediate sync
    HIGH = 1      # Next sync cycle
    NORMAL = 2    # Standard queue
    LOW = 3       # Background sync
    DEFERRED = 4  # Batch with others


# =============================================================================
# Data Classes
# =============================================================================

@dataclass
class SyncEvent:
    """Represents a synchronization event."""
    event_type: SyncEventType
    source_repo: RepoType
    file_path: str
    timestamp: float = field(default_factory=time.time)
    priority: SyncPriority = SyncPriority.NORMAL
    payload: Dict[str, Any] = field(default_factory=dict)
    event_id: str = field(default="")

    def __post_init__(self):
        if not self.event_id:
            content = f"{self.event_type.name}:{self.source_repo.value}:{self.file_path}:{self.timestamp}"
            self.event_id = hashlib.md5(content.encode()).hexdigest()[:12]

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "event_id": self.event_id,
            "event_type": self.event_type.name,
            "source_repo": self.source_repo.value,
            "file_path": self.file_path,
            "timestamp": self.timestamp,
            "priority": self.priority.name,
            "payload": self.payload,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "SyncEvent":
        """Create from dictionary."""
        return cls(
            event_type=SyncEventType[data["event_type"]],
            source_repo=RepoType(data["source_repo"]),
            file_path=data["file_path"],
            timestamp=data.get("timestamp", time.time()),
            priority=SyncPriority[data.get("priority", "NORMAL")],
            payload=data.get("payload", {}),
            event_id=data.get("event_id", ""),
        )


@dataclass
class RepoContext:
    """Context from a single repository."""
    repo_type: RepoType
    root_path: Path
    active_files: Set[str] = field(default_factory=set)
    recent_changes: List[SyncEvent] = field(default_factory=list)
    diagnostics: Dict[str, List[Dict[str, Any]]] = field(default_factory=dict)
    cursor_positions: Dict[str, Tuple[int, int]] = field(default_factory=dict)
    last_sync: float = 0.0
    healthy: bool = True

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "repo_type": self.repo_type.value,
            "root_path": str(self.root_path),
            "active_files": list(self.active_files),
            "recent_changes_count": len(self.recent_changes),
            "diagnostics_count": sum(len(d) for d in self.diagnostics.values()),
            "last_sync": self.last_sync,
            "healthy": self.healthy,
        }


@dataclass
class CrossRepoReference:
    """Represents a cross-repo code reference."""
    source_repo: RepoType
    source_file: str
    source_line: int
    target_repo: RepoType
    target_file: str
    target_line: int
    reference_type: str  # import, call, inherit, etc.
    confidence: float = 1.0


@dataclass
class SuggestionCacheEntry:
    """Cached suggestion that can be shared across repos."""
    suggestion_id: str
    trigger_context: str
    suggestion_text: str
    source_repo: RepoType
    applicable_repos: Set[RepoType]
    created_at: float = field(default_factory=time.time)
    use_count: int = 0
    acceptance_rate: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "suggestion_id": self.suggestion_id,
            "trigger_context": self.trigger_context[:100],
            "source_repo": self.source_repo.value,
            "applicable_repos": [r.value for r in self.applicable_repos],
            "created_at": self.created_at,
            "use_count": self.use_count,
            "acceptance_rate": self.acceptance_rate,
        }


# =============================================================================
# Event Queue with Priority
# =============================================================================

class PrioritySyncQueue:
    """Priority queue for sync events with deduplication."""

    def __init__(self, max_size: int = 10000):
        self._queues: Dict[SyncPriority, asyncio.Queue] = {
            priority: asyncio.Queue(maxsize=max_size // 5)
            for priority in SyncPriority
        }
        self._seen_events: Set[str] = set()
        self._seen_lock = asyncio.Lock()
        self._max_seen = 50000

    async def put(self, event: SyncEvent) -> bool:
        """Add event to queue with deduplication."""
        async with self._seen_lock:
            if event.event_id in self._seen_events:
                return False

            # Prune seen set if too large
            if len(self._seen_events) > self._max_seen:
                self._seen_events = set(list(self._seen_events)[-self._max_seen // 2:])

            self._seen_events.add(event.event_id)

        queue = self._queues[event.priority]
        try:
            queue.put_nowait(event)
            return True
        except asyncio.QueueFull:
            # Drop lowest priority events if queue full
            if event.priority.value < SyncPriority.LOW.value:
                try:
                    self._queues[SyncPriority.DEFERRED].get_nowait()
                    queue.put_nowait(event)
                    return True
                except:
                    pass
            return False

    async def get(self, timeout: float = 1.0) -> Optional[SyncEvent]:
        """Get highest priority event available."""
        for priority in SyncPriority:
            queue = self._queues[priority]
            if not queue.empty():
                try:
                    return queue.get_nowait()
                except asyncio.QueueEmpty:
                    continue

        # Wait on critical queue if nothing available
        try:
            return await asyncio.wait_for(
                self._queues[SyncPriority.CRITICAL].get(),
                timeout=timeout,
            )
        except asyncio.TimeoutError:
            return None

    def pending_count(self) -> int:
        """Get total pending events."""
        return sum(q.qsize() for q in self._queues.values())

    def clear(self) -> int:
        """Clear all queues and return count of cleared events."""
        count = 0
        for queue in self._queues.values():
            while not queue.empty():
                try:
                    queue.get_nowait()
                    count += 1
                except asyncio.QueueEmpty:
                    break
        return count


# =============================================================================
# Vector Clock for Distributed Ordering
# =============================================================================

class VectorClock:
    """Vector clock for causal ordering across repos."""

    def __init__(self):
        self._clock: Dict[str, int] = {repo.value: 0 for repo in RepoType}
        self._lock = asyncio.Lock()

    async def tick(self, repo: RepoType) -> Dict[str, int]:
        """Increment clock for a repo and return new state."""
        async with self._lock:
            self._clock[repo.value] += 1
            return self._clock.copy()

    async def merge(self, other_clock: Dict[str, int]) -> Dict[str, int]:
        """Merge another clock (take max of each component)."""
        async with self._lock:
            for repo, count in other_clock.items():
                if repo in self._clock:
                    self._clock[repo] = max(self._clock[repo], count)
            return self._clock.copy()

    async def get(self) -> Dict[str, int]:
        """Get current clock state."""
        async with self._lock:
            return self._clock.copy()

    def happened_before(
        self,
        clock_a: Dict[str, int],
        clock_b: Dict[str, int],
    ) -> bool:
        """Check if clock_a happened before clock_b."""
        at_least_one_less = False
        for repo in RepoType:
            key = repo.value
            a_val = clock_a.get(key, 0)
            b_val = clock_b.get(key, 0)
            if a_val > b_val:
                return False
            if a_val < b_val:
                at_least_one_less = True
        return at_least_one_less


# =============================================================================
# Conflict Resolver
# =============================================================================

class ConflictResolver:
    """Resolves conflicts when multiple repos modify related code."""

    def __init__(self, vector_clock: VectorClock):
        self._vector_clock = vector_clock
        self._pending_conflicts: List[Tuple[SyncEvent, SyncEvent]] = []

    async def check_conflict(
        self,
        event: SyncEvent,
        existing_events: List[SyncEvent],
    ) -> Optional[Tuple[SyncEvent, SyncEvent]]:
        """Check if new event conflicts with existing events."""
        for existing in existing_events:
            if self._events_conflict(event, existing):
                return (event, existing)
        return None

    def _events_conflict(self, a: SyncEvent, b: SyncEvent) -> bool:
        """Check if two events conflict."""
        # Same file in different repos within short timeframe
        if a.file_path == b.file_path:
            return False  # Same file is not a conflict

        # Check for related files (e.g., same module name across repos)
        a_name = Path(a.file_path).stem
        b_name = Path(b.file_path).stem

        if a_name == b_name and a.source_repo != b.source_repo:
            # Same named file modified in different repos within 60 seconds
            if abs(a.timestamp - b.timestamp) < 60:
                return True

        return False

    async def resolve_conflict(
        self,
        conflict: Tuple[SyncEvent, SyncEvent],
    ) -> SyncEvent:
        """Resolve a conflict by choosing the winner."""
        event_a, event_b = conflict

        # Strategy: Prefer higher priority, then newer timestamp
        if event_a.priority.value < event_b.priority.value:
            return event_a
        elif event_b.priority.value < event_a.priority.value:
            return event_b
        else:
            # Same priority, use timestamp
            return event_a if event_a.timestamp > event_b.timestamp else event_b


# =============================================================================
# Cross-Repo Reference Tracker
# =============================================================================

class CrossRepoReferenceTracker:
    """Tracks references between repositories."""

    def __init__(self):
        self._references: List[CrossRepoReference] = []
        self._by_source: Dict[Tuple[RepoType, str], List[CrossRepoReference]] = {}
        self._by_target: Dict[Tuple[RepoType, str], List[CrossRepoReference]] = {}
        self._lock = asyncio.Lock()

    async def add_reference(self, ref: CrossRepoReference) -> None:
        """Add a cross-repo reference."""
        async with self._lock:
            self._references.append(ref)

            source_key = (ref.source_repo, ref.source_file)
            if source_key not in self._by_source:
                self._by_source[source_key] = []
            self._by_source[source_key].append(ref)

            target_key = (ref.target_repo, ref.target_file)
            if target_key not in self._by_target:
                self._by_target[target_key] = []
            self._by_target[target_key].append(ref)

    async def get_outgoing(
        self,
        repo: RepoType,
        file_path: str,
    ) -> List[CrossRepoReference]:
        """Get references from a file to other repos."""
        async with self._lock:
            return list(self._by_source.get((repo, file_path), []))

    async def get_incoming(
        self,
        repo: RepoType,
        file_path: str,
    ) -> List[CrossRepoReference]:
        """Get references to a file from other repos."""
        async with self._lock:
            return list(self._by_target.get((repo, file_path), []))

    async def get_affected_files(
        self,
        repo: RepoType,
        file_path: str,
    ) -> List[Tuple[RepoType, str]]:
        """Get all files that might be affected by changes to a file."""
        affected = []

        # Files that reference this file
        incoming = await self.get_incoming(repo, file_path)
        for ref in incoming:
            affected.append((ref.source_repo, ref.source_file))

        # Files this file references
        outgoing = await self.get_outgoing(repo, file_path)
        for ref in outgoing:
            affected.append((ref.target_repo, ref.target_file))

        return list(set(affected))


# =============================================================================
# Distributed Suggestion Cache
# =============================================================================

class DistributedSuggestionCache:
    """
    Cache suggestions that can be shared across repos.

    Uses ARM64 SIMD acceleration for faster hash computations when available.
    """

    def __init__(self, max_size: int = 5000):
        self._cache: Dict[str, SuggestionCacheEntry] = {}
        self._by_context: Dict[str, List[str]] = {}  # context_hash -> suggestion_ids
        self._max_size = max_size
        self._lock = asyncio.Lock()

        # ARM64 SIMD acceleration
        self._accelerator = _get_accelerator()
        self._use_fast_hash = self._accelerator is not None

    def _hash_context(self, context: str) -> str:
        """Hash context for quick lookup (ARM64 accelerated when available)."""
        # Use ARM64 accelerated hash if available (20-30x faster)
        if self._use_fast_hash and self._accelerator:
            try:
                fast_hash = self._accelerator.fast_hash(context)
                return f"{fast_hash:08x}"
            except Exception:
                pass

        # Fallback to MD5
        return hashlib.md5(context.encode()).hexdigest()[:16]

    async def add(
        self,
        trigger_context: str,
        suggestion_text: str,
        source_repo: RepoType,
        applicable_repos: Optional[Set[RepoType]] = None,
    ) -> str:
        """Add a suggestion to the cache."""
        if applicable_repos is None:
            applicable_repos = set(RepoType)  # Apply to all by default

        # Generate suggestion ID (ARM64 accelerated when available)
        combined = f"{trigger_context}:{suggestion_text}"
        if self._use_fast_hash and self._accelerator:
            try:
                suggestion_id = f"{self._accelerator.fast_hash(combined):08x}"
            except Exception:
                suggestion_id = hashlib.md5(combined.encode()).hexdigest()[:16]
        else:
            suggestion_id = hashlib.md5(combined.encode()).hexdigest()[:16]

        async with self._lock:
            # Evict if at capacity
            if len(self._cache) >= self._max_size:
                await self._evict_lru()

            entry = SuggestionCacheEntry(
                suggestion_id=suggestion_id,
                trigger_context=trigger_context,
                suggestion_text=suggestion_text,
                source_repo=source_repo,
                applicable_repos=applicable_repos,
            )
            self._cache[suggestion_id] = entry

            context_hash = self._hash_context(trigger_context)
            if context_hash not in self._by_context:
                self._by_context[context_hash] = []
            self._by_context[context_hash].append(suggestion_id)

        return suggestion_id

    async def get(
        self,
        context: str,
        target_repo: RepoType,
    ) -> List[SuggestionCacheEntry]:
        """Get cached suggestions for a context and repo."""
        context_hash = self._hash_context(context)

        async with self._lock:
            suggestion_ids = self._by_context.get(context_hash, [])
            results = []

            for sid in suggestion_ids:
                entry = self._cache.get(sid)
                if entry and target_repo in entry.applicable_repos:
                    entry.use_count += 1
                    results.append(entry)

            # Sort by acceptance rate
            results.sort(key=lambda e: e.acceptance_rate, reverse=True)
            return results

    async def record_feedback(
        self,
        suggestion_id: str,
        accepted: bool,
    ) -> None:
        """Record whether a suggestion was accepted."""
        async with self._lock:
            entry = self._cache.get(suggestion_id)
            if entry:
                # Exponential moving average for acceptance rate
                alpha = 0.2
                new_value = 1.0 if accepted else 0.0
                entry.acceptance_rate = (
                    alpha * new_value + (1 - alpha) * entry.acceptance_rate
                )

    async def _evict_lru(self) -> None:
        """Evict least recently used entries."""
        if not self._cache:
            return

        # Sort by use count and age
        sorted_entries = sorted(
            self._cache.items(),
            key=lambda x: (x[1].use_count, x[1].created_at),
        )

        # Remove bottom 10%
        to_remove = max(1, len(sorted_entries) // 10)
        for suggestion_id, _ in sorted_entries[:to_remove]:
            del self._cache[suggestion_id]


# =============================================================================
# Trinity IDE Synchronizer (Main Class)
# =============================================================================

class TrinityIDESynchronizer:
    """
    Main synchronization orchestrator for cross-repo IDE integration.

    This class coordinates:
    - Real-time file change propagation
    - Cross-repo context sharing
    - Distributed suggestion caching
    - Event mesh for multi-repo awareness
    """

    def __init__(self):
        self._repos: Dict[RepoType, RepoContext] = {}
        self._event_queue = PrioritySyncQueue()
        self._vector_clock = VectorClock()
        self._conflict_resolver = ConflictResolver(self._vector_clock)
        self._reference_tracker = CrossRepoReferenceTracker()
        self._suggestion_cache = DistributedSuggestionCache()

        # Callbacks
        self._event_handlers: Dict[SyncEventType, List[Callable]] = {
            event_type: [] for event_type in SyncEventType
        }

        # Sync state
        self._running = False
        self._sync_task: Optional[asyncio.Task] = None
        self._heartbeat_task: Optional[asyncio.Task] = None

        # Metrics
        self._events_processed = 0
        self._events_dropped = 0
        self._conflicts_resolved = 0
        self._last_sync_duration = 0.0
        self._start_time: Optional[float] = None

        # Initialize sync directory (use dynamic config)
        sync_dir = _get_sync_dir()
        sync_dir.mkdir(parents=True, exist_ok=True)

    async def initialize(self) -> bool:
        """Initialize the synchronizer with dynamic configuration."""
        logger.info("[TrinitySynchronizer] Initializing cross-repo sync...")

        # Get repos from dynamic config
        trinity_repos = _get_trinity_repos()

        # Initialize repo contexts
        for repo_type, path in trinity_repos.items():
            repo_enum = RepoType(repo_type)
            self._repos[repo_enum] = RepoContext(
                repo_type=repo_enum,
                root_path=path,
                healthy=path.exists(),
            )

            if path.exists():
                logger.info(f"[TrinitySynchronizer] Found {repo_type} at {path}")
            else:
                logger.warning(f"[TrinitySynchronizer] {repo_type} not found at {path}")

        # Load persisted state
        await self._load_state()

        self._start_time = time.time()
        return True

    async def start(self) -> None:
        """Start the sync workers."""
        if self._running:
            return

        self._running = True

        # Start sync worker
        self._sync_task = asyncio.create_task(self._sync_worker())

        # Start heartbeat worker
        self._heartbeat_task = asyncio.create_task(self._heartbeat_worker())

        logger.info("[TrinitySynchronizer] Started sync workers")

    async def stop(self) -> None:
        """Stop the sync workers."""
        self._running = False

        # Cancel tasks
        if self._sync_task:
            self._sync_task.cancel()
            try:
                await self._sync_task
            except asyncio.CancelledError:
                pass

        if self._heartbeat_task:
            self._heartbeat_task.cancel()
            try:
                await self._heartbeat_task
            except asyncio.CancelledError:
                pass

        # Persist state
        await self._save_state()

        logger.info("[TrinitySynchronizer] Stopped sync workers")

    # -------------------------------------------------------------------------
    # Event Publishing
    # -------------------------------------------------------------------------

    async def publish_event(self, event: SyncEvent) -> bool:
        """Publish a sync event."""
        # Update vector clock
        await self._vector_clock.tick(event.source_repo)

        # Add to queue
        success = await self._event_queue.put(event)

        if not success:
            self._events_dropped += 1
            logger.warning(f"[TrinitySynchronizer] Dropped event: {event.event_id}")

        return success

    async def publish_file_change(
        self,
        repo: RepoType,
        file_path: str,
        change_type: str,
        content: Optional[str] = None,
        priority: SyncPriority = SyncPriority.NORMAL,
    ) -> bool:
        """Convenience method to publish a file change event."""
        event_type_map = {
            "created": SyncEventType.FILE_CREATED,
            "modified": SyncEventType.FILE_MODIFIED,
            "deleted": SyncEventType.FILE_DELETED,
            "renamed": SyncEventType.FILE_RENAMED,
        }

        event_type = event_type_map.get(change_type, SyncEventType.FILE_MODIFIED)

        event = SyncEvent(
            event_type=event_type,
            source_repo=repo,
            file_path=file_path,
            priority=priority,
            payload={"content_hash": hashlib.md5(content.encode()).hexdigest() if content else None},
        )

        return await self.publish_event(event)

    # -------------------------------------------------------------------------
    # Event Subscription
    # -------------------------------------------------------------------------

    def subscribe(
        self,
        event_type: SyncEventType,
        handler: Callable[[SyncEvent], Any],
    ) -> Callable[[], None]:
        """Subscribe to an event type. Returns unsubscribe function."""
        self._event_handlers[event_type].append(handler)

        def unsubscribe():
            try:
                self._event_handlers[event_type].remove(handler)
            except ValueError:
                pass

        return unsubscribe

    # -------------------------------------------------------------------------
    # Context Access
    # -------------------------------------------------------------------------

    async def get_repo_context(self, repo: RepoType) -> Optional[RepoContext]:
        """Get context for a specific repo."""
        return self._repos.get(repo)

    async def get_all_contexts(self) -> Dict[RepoType, RepoContext]:
        """Get contexts for all repos."""
        return self._repos.copy()

    async def get_cross_repo_references(
        self,
        repo: RepoType,
        file_path: str,
    ) -> Dict[str, List[CrossRepoReference]]:
        """Get cross-repo references for a file."""
        incoming = await self._reference_tracker.get_incoming(repo, file_path)
        outgoing = await self._reference_tracker.get_outgoing(repo, file_path)

        return {
            "incoming": incoming,
            "outgoing": outgoing,
        }

    async def get_affected_files(
        self,
        repo: RepoType,
        file_path: str,
    ) -> List[Tuple[RepoType, str]]:
        """Get files that might be affected by changes."""
        return await self._reference_tracker.get_affected_files(repo, file_path)

    # -------------------------------------------------------------------------
    # Suggestion Cache Access
    # -------------------------------------------------------------------------

    async def cache_suggestion(
        self,
        trigger_context: str,
        suggestion_text: str,
        source_repo: RepoType,
        applicable_repos: Optional[Set[RepoType]] = None,
    ) -> str:
        """Cache a suggestion for cross-repo sharing."""
        return await self._suggestion_cache.add(
            trigger_context=trigger_context,
            suggestion_text=suggestion_text,
            source_repo=source_repo,
            applicable_repos=applicable_repos,
        )

    async def get_cached_suggestions(
        self,
        context: str,
        target_repo: RepoType,
    ) -> List[SuggestionCacheEntry]:
        """Get cached suggestions for a context."""
        return await self._suggestion_cache.get(context, target_repo)

    async def record_suggestion_feedback(
        self,
        suggestion_id: str,
        accepted: bool,
    ) -> None:
        """Record feedback on a suggestion."""
        await self._suggestion_cache.record_feedback(suggestion_id, accepted)

    # -------------------------------------------------------------------------
    # Status and Metrics
    # -------------------------------------------------------------------------

    def get_status(self) -> Dict[str, Any]:
        """Get synchronizer status."""
        return {
            "running": self._running,
            "uptime_seconds": time.time() - self._start_time if self._start_time else 0,
            "repos": {
                repo.value: ctx.to_dict()
                for repo, ctx in self._repos.items()
            },
            "metrics": {
                "events_processed": self._events_processed,
                "events_dropped": self._events_dropped,
                "events_pending": self._event_queue.pending_count(),
                "conflicts_resolved": self._conflicts_resolved,
                "last_sync_duration_ms": self._last_sync_duration * 1000,
            },
            "vector_clock": asyncio.create_task(self._vector_clock.get()) if self._running else {},
        }

    # -------------------------------------------------------------------------
    # Internal Workers
    # -------------------------------------------------------------------------

    async def _sync_worker(self) -> None:
        """Main sync worker loop with dynamic interval."""
        while self._running:
            try:
                sync_start = time.time()

                # Get dynamic sync interval
                sync_interval = _get_sync_interval()

                # Process events
                event = await self._event_queue.get(timeout=sync_interval)

                if event:
                    await self._process_event(event)

                self._last_sync_duration = time.time() - sync_start

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"[TrinitySynchronizer] Sync error: {e}")
                await asyncio.sleep(1.0)

    async def _process_event(self, event: SyncEvent) -> None:
        """Process a single sync event."""
        try:
            # Update repo context
            repo_ctx = self._repos.get(event.source_repo)
            if repo_ctx:
                repo_ctx.recent_changes.append(event)
                repo_ctx.recent_changes = repo_ctx.recent_changes[-100:]  # Keep last 100
                repo_ctx.last_sync = time.time()

            # Call registered handlers
            handlers = self._event_handlers.get(event.event_type, [])
            for handler in handlers:
                try:
                    result = handler(event)
                    if asyncio.iscoroutine(result):
                        await result
                except Exception as e:
                    logger.error(f"[TrinitySynchronizer] Handler error: {e}")

            # Propagate to affected files
            if event.event_type in (SyncEventType.FILE_MODIFIED, SyncEventType.FILE_CREATED):
                affected = await self._reference_tracker.get_affected_files(
                    event.source_repo,
                    event.file_path,
                )

                for target_repo, target_file in affected:
                    propagated_event = SyncEvent(
                        event_type=SyncEventType.CONTEXT_UPDATED,
                        source_repo=target_repo,
                        file_path=target_file,
                        priority=SyncPriority.LOW,
                        payload={
                            "trigger_repo": event.source_repo.value,
                            "trigger_file": event.file_path,
                        },
                    )
                    await self._event_queue.put(propagated_event)

            self._events_processed += 1

        except Exception as e:
            logger.error(f"[TrinitySynchronizer] Event processing error: {e}")

    async def _heartbeat_worker(self) -> None:
        """Heartbeat worker to maintain repo health status with dynamic intervals."""
        while self._running:
            try:
                # Get dynamic sync directory and interval
                sync_dir = _get_sync_dir()
                heartbeat_interval = _get_heartbeat_interval()

                for repo_type, ctx in self._repos.items():
                    # Check if repo is accessible
                    ctx.healthy = ctx.root_path.exists()

                    # Update heartbeat file
                    heartbeat_file = sync_dir / f"{repo_type.value}_heartbeat.json"
                    heartbeat_data = {
                        "repo": repo_type.value,
                        "timestamp": time.time(),
                        "healthy": ctx.healthy,
                        "active_files": len(ctx.active_files),
                    }

                    try:
                        with open(heartbeat_file, "w") as f:
                            json.dump(heartbeat_data, f)
                    except Exception as write_err:
                        logger.debug(f"[TrinitySynchronizer] Could not write heartbeat: {write_err}")

                await asyncio.sleep(heartbeat_interval)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"[TrinitySynchronizer] Heartbeat error: {e}")
                await asyncio.sleep(5.0)

    # -------------------------------------------------------------------------
    # State Persistence
    # -------------------------------------------------------------------------

    async def _load_state(self) -> None:
        """Load persisted sync state."""
        sync_dir = _get_sync_dir()
        state_file = sync_dir / "sync_state.json"

        if not state_file.exists():
            return

        try:
            with open(state_file) as f:
                state = json.load(f)

            # Restore vector clock
            if "vector_clock" in state:
                await self._vector_clock.merge(state["vector_clock"])

            logger.info("[TrinitySynchronizer] Loaded persisted state")

        except Exception as e:
            logger.warning(f"[TrinitySynchronizer] Could not load state: {e}")

    async def _save_state(self) -> None:
        """Save sync state to disk."""
        sync_dir = _get_sync_dir()
        state_file = sync_dir / "sync_state.json"

        try:
            state = {
                "vector_clock": await self._vector_clock.get(),
                "timestamp": time.time(),
                "events_processed": self._events_processed,
            }

            with open(state_file, "w") as f:
                json.dump(state, f, indent=2)

            logger.info("[TrinitySynchronizer] Saved state")

        except Exception as e:
            logger.error(f"[TrinitySynchronizer] Could not save state: {e}")


# =============================================================================
# Module-Level Singleton
# =============================================================================

_synchronizer: Optional[TrinityIDESynchronizer] = None
_synchronizer_lock = asyncio.Lock()


async def get_trinity_synchronizer() -> TrinityIDESynchronizer:
    """Get or create the global Trinity synchronizer instance."""
    global _synchronizer

    async with _synchronizer_lock:
        if _synchronizer is None:
            _synchronizer = TrinityIDESynchronizer()
            await _synchronizer.initialize()

        return _synchronizer


async def initialize_trinity_sync() -> TrinityIDESynchronizer:
    """Initialize and start the Trinity synchronizer."""
    sync = await get_trinity_synchronizer()
    await sync.start()
    return sync


async def shutdown_trinity_sync() -> None:
    """Shutdown the Trinity synchronizer."""
    global _synchronizer

    if _synchronizer:
        await _synchronizer.stop()
        _synchronizer = None


# =============================================================================
# Convenience Functions
# =============================================================================

def detect_repo_type(file_path: str) -> Optional[RepoType]:
    """Detect which repo a file belongs to based on path."""
    path = Path(file_path).resolve()
    trinity_repos = _get_trinity_repos()

    for repo_type, repo_path in trinity_repos.items():
        try:
            path.relative_to(repo_path)
            return RepoType(repo_type)
        except ValueError:
            continue

    return None


def get_relative_path(file_path: str, repo: RepoType) -> str:
    """Get the path relative to a repo root."""
    path = Path(file_path).resolve()
    trinity_repos = _get_trinity_repos()
    repo_path = trinity_repos.get(repo.value)

    if not repo_path:
        return file_path

    try:
        return str(path.relative_to(repo_path))
    except ValueError:
        return file_path
