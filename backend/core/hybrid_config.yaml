# Ironcliw Hybrid Architecture Configuration
# Zero-hardcoding, fully dynamic system
# Integrated with UAE, SAI, CAI, and learning_database

hybrid:
  mode: auto  # auto, local-only, cloud-only, hybrid

  # Intelligent Systems Integration
  intelligence:
    uae:
      enabled: true
      local_context: true  # Capture context locally
      cloud_processing: true  # Process in cloud
    sai:
      enabled: true
      self_healing: true
      performance_optimization: true
    cai:
      enabled: true
      intent_prediction: true
      proactive_assistance: true
    learning_database:
      enabled: true
      local_cache: true  # Fast local access
      cloud_sync: true  # Sync to cloud for ML

  # Service Discovery
  discovery:
    enabled: true
    methods:
      - dns  # DNS-SD / mDNS for local services
      - consul  # Consul for cloud services
      - static  # Fallback to static config
    health_check_interval: 30  # seconds
    service_timeout: 5  # seconds

  # Backend Definitions (auto-discovered or static fallback)
  backends:
    local:
      type: local
      priority: 1  # Lower = higher priority for local tasks
      capabilities:
        - vision_capture
        - screen_unlock
        - voice_activation
        - macos_automation
        - display_monitoring
        - wake_word_detection
        - real_time_context
        - immediate_response
      components:
        - VISION
        - VOICE
        - VOICE_UNLOCK
        - WAKE_WORD
        - DISPLAY_MONITOR
      health_endpoint: "http://localhost:8010/health"
      enabled: true

    gcp:
      type: cloud
      priority: 2  # Higher priority for heavy ML tasks
      capabilities:
        - ml_processing
        - nlp_analysis
        - heavy_computation
        - chatbot_inference
        - sentiment_analysis
        - goal_inference
        - uae_processing
        - sai_learning
        - cai_prediction
        - deep_learning
        # Phase 3.1: Local LLM capabilities
        - llm_inference
        - intent_classification
        - query_expansion
        - response_generation
        - conversational_ai
        - code_explanation
        - text_summarization
        - question_answering
      components:
        - CHATBOTS
        - ML_MODELS
        - MEMORY
        - MONITORING
        - LOCAL_LLM  # Phase 3.1: LLaMA 3.1 70B
      discovery:
        method: dns
        hostname: jarvis-backend.gcp.internal
      fallback_url: "http://34.10.137.70:8010"
      health_endpoint: "/health"
      enabled: true

  # Intelligent Routing Rules
  routing:
    strategy: capability_based  # capability_based, round_robin, least_loaded, fastest

    # Task routing rules (matched in order)
    # Enhanced with UAE/SAI/CAI intelligence
    rules:
      # Real-time local-only tasks
      - name: vision_capture
        match:
          capabilities: [vision_capture]
        route_to: local
        fallback: none  # Cannot fallback, requires local
        use_uae: true  # Capture context

      - name: screen_unlock
        match:
          capabilities: [screen_unlock]
        route_to: local
        fallback: none
        use_cai: true  # Context-aware unlock

      - name: voice_activation
        match:
          capabilities: [voice_activation]
        route_to: local
        fallback: none
        use_cai: true  # Predict intent

      - name: wake_word
        match:
          capabilities: [wake_word_detection]
        route_to: local
        fallback: none

      # Cloud-heavy intelligence tasks
      - name: uae_processing
        match:
          keywords: [context, awareness, what am i doing, understand situation]
          capabilities: [uae_processing]
        route_to: gcp
        fallback: local
        use_uae: true
        use_cai: true

      - name: sai_learning
        match:
          keywords: [learn, pattern, remember, adapt, optimize]
          capabilities: [sai_learning]
        route_to: gcp
        fallback: none
        use_sai: true
        use_learning_db: true

      - name: cai_prediction
        match:
          keywords: [predict, suggest, what should, recommend]
          capabilities: [cai_prediction]
        route_to: gcp
        fallback: local
        use_cai: true
        use_learning_db: true

      - name: ml_heavy
        match:
          memory_required: ">8GB"
          capabilities: [ml_processing]
        route_to: gcp
        fallback: local
        use_uae: true

      - name: nlp_analysis
        match:
          keywords: [analyze, summarize, understand, explain]
          capabilities: [nlp_analysis]
        route_to: gcp
        fallback: local
        use_uae: true
        use_cai: true

      - name: chatbot
        match:
          command_type: query
          capabilities: [chatbot_inference]
        route_to: gcp
        fallback: local
        use_uae: true  # Provide context to chatbot
        use_cai: true  # Understand intent
        use_learning_db: true  # Remember conversations

      - name: goal_inference
        match:
          keywords: [goal, intention, trying to, working on]
          capabilities: [goal_inference]
        route_to: gcp
        fallback: local
        use_cai: true
        use_learning_db: true

      # ============== VOICE/TEXT COMMANDS (NEW - Phase 2) ==============
      - name: voice_command_simple
        priority: 80
        match:
          command_type: voice_command
          keywords: [time, date, weather, simple, quick]
        route_to: local
        fallback: gcp
        use_cai: true  # Quick intent detection
        estimated_ram_gb: 0.2
        max_latency_ms: 500

      - name: voice_command_complex
        priority: 100
        match:
          command_type: voice_command
          memory_pressure: ">70"  # If local RAM >70%
        route_to: gcp
        fallback: local
        use_uae: true
        use_cai: true
        use_learning_db: true
        estimated_ram_gb: 3.5
        max_latency_ms: 3000

      - name: voice_command_nlp_heavy
        priority: 110
        match:
          command_type: voice_command
          keywords: [analyze, summarize, explain, understand, write, compose]
        route_to: gcp
        fallback: local
        use_uae: true
        use_cai: true
        use_sai: true
        estimated_ram_gb: 4.5
        max_latency_ms: 5000

      - name: text_command_routing
        priority: 95
        match:
          command_type: text_command
          memory_pressure: ">70"
        route_to: gcp
        fallback: local
        use_cai: true
        use_learning_db: true
        estimated_ram_gb: 2.5
        max_latency_ms: 3000

      - name: vision_analyze_heavy
        priority: 120
        match:
          command_type: vision_analyze
        route_to: gcp  # Always use GCP for heavy vision
        fallback: none
        use_uae: true
        use_sai: true
        estimated_ram_gb: 5.0
        max_latency_ms: 10000

      - name: emergency_memory_pressure
        priority: 200  # Highest priority
        match:
          memory_pressure: ">85"  # Critical memory pressure
        route_to: gcp
        force: true  # Override all other rules
        fallback: none
        use_sai: true  # Learn from emergency situations
        reason: "Critical memory pressure - mandatory GCP routing"
        estimated_ram_gb: "auto"  # Calculate dynamically

      - name: cost_optimization_reclaim
        priority: 50  # Low priority
        match:
          memory_pressure: "<40"  # Lots of free local RAM
          gcp_idle_minutes: ">10"  # GCP been idle
        route_to: local
        fallback: none
        reason: "Cost optimization - reclaim to local"
        trigger_gcp_shutdown: true  # Shutdown GCP if possible

      # ============== LLM INFERENCE ROUTING (Phase 3.1) ==============
      - name: llm_intent_classification
        priority: 90
        match:
          command_type: intent_classification
          keywords: [classify, intent, parse, understand command]
        route_to: gcp  # Use LLaMA 70B on GCP
        fallback: local
        use_llm: true  # Use local LLM inference
        estimated_ram_gb: 24.0
        max_latency_ms: 1000

      - name: llm_query_expansion
        priority: 92
        match:
          command_type: query_expansion
          keywords: [expand, rewrite, rephrase, clarify]
        route_to: gcp
        fallback: local
        use_llm: true
        estimated_ram_gb: 24.0
        max_latency_ms: 1500

      - name: llm_response_generation
        priority: 95
        match:
          command_type: response_generation
          keywords: [generate, respond, answer, explain, write]
        route_to: gcp
        fallback: local
        use_llm: true
        use_uae: true  # Add context
        use_cai: true  # Intent awareness
        estimated_ram_gb: 24.0
        max_latency_ms: 3000

      - name: llm_conversational_ai
        priority: 100
        match:
          command_type: conversational_ai
          keywords: [chat, conversation, talk, discuss]
        route_to: gcp
        fallback: local
        use_llm: true
        use_uae: true
        use_cai: true
        use_learning_db: true  # Remember conversations
        estimated_ram_gb: 24.0
        max_latency_ms: 3000

      - name: llm_code_explanation
        priority: 105
        match:
          command_type: code_explanation
          keywords: [code, function, explain code, how does this work]
        route_to: gcp
        fallback: none  # Only GCP has LLaMA
        use_llm: true
        estimated_ram_gb: 24.0
        max_latency_ms: 5000

      - name: llm_text_summarization
        priority: 98
        match:
          command_type: text_summarization
          keywords: [summarize, summary, tldr, key points]
        route_to: gcp
        fallback: local
        use_llm: true
        estimated_ram_gb: 24.0
        max_latency_ms: 4000

      - name: default
        match:
          all: true
        route_to: auto  # Let system decide
        use_uae: true  # Always capture context
        estimated_ram_gb: 1.0

  # RAM-Aware Routing Configuration (NEW - Phase 2)
  ram_awareness:
    enabled: true
    local_thresholds:
      warning: 70  # % - Start preferring GCP
      critical: 85  # % - Force GCP routing
      recovery: 60  # % - Can shift back to local
      optimization: 40  # % - Actively reclaim from GCP
    gcp_thresholds:
      warning: 75  # % - Start load balancing
      critical: 90  # % - Reject new workloads
      healthy: 60  # % - Can accept new work
    monitoring_interval: 5  # seconds - Check RAM every 5s
    decision_window: 30  # seconds - Consider last 30s of data
    page_outs_threshold: 5000  # macOS - swapping indicator

  # Component RAM Estimates (Dynamic, learned over time)
  component_ram_estimates:
    vision_capture: 0.15  # GB
    vision_full: 3.5  # GB (Claude Vision API)
    voice_command_basic: 0.2  # GB
    voice_nlp_full: 4.0  # GB (Transformers)
    uae_light: 0.15  # GB
    uae_full: 0.8  # GB
    sai_light: 0.1  # GB
    sai_full: 0.6  # GB
    cai_light: 0.1  # GB
    cai_full: 0.5  # GB
    chatbot: 2.5  # GB
    goal_inference: 1.5  # GB
    ml_models: 5.0  # GB
    learning_db_cache: 0.05  # GB
    llama_70b_4bit: 24.0  # GB (LLaMA 3.1 70B 4-bit quantized)
    llm_inference: 24.0  # GB (Alias for llama_70b_4bit)

  # ============== LOCAL LLM CONFIGURATION (Phase 3.1) ==============
  # LLaMA 3.1 70B deployment on GCP 32GB Spot VM
  local_llm:
    enabled: true  # Enable local LLM inference
    backend: "gcp"  # Only deploy on GCP (requires 32GB RAM)

    # Model Configuration
    model:
      name: "meta-llama/Meta-Llama-3.1-70B-Instruct"
      type: "llama"
      version: "3.1"
      parameters: "70B"
      quantization: "4bit"  # 4-bit quantization (70B → 24GB)

    # Quantization Config (BitsAndBytes)
    quantization_config:
      load_in_4bit: true
      bnb_4bit_compute_dtype: "float16"  # float16, bfloat16
      bnb_4bit_quant_type: "nf4"  # nf4, fp4
      bnb_4bit_use_double_quant: true

    # Generation Config
    generation:
      max_new_tokens: 512
      temperature: 0.7
      top_p: 0.9
      top_k: 50
      do_sample: true
      repetition_penalty: 1.1
      num_beams: 1  # 1 for faster generation, >1 for better quality

    # Resource Management
    resources:
      ram_required_gb: 24
      ram_buffer_gb: 2  # Keep 2GB buffer (24 + 2 = 26GB < 32GB)
      device_map: "auto"  # Auto-distribute across GPUs
      max_memory: null  # Let transformers decide
      offload_folder: "/tmp/llm_offload"  # For CPU offloading if needed

    # Performance Optimization
    optimization:
      use_cache: true  # KV cache for faster generation
      use_flash_attention: true  # Flash attention if available
      torch_compile: false  # PyTorch 2.0 compilation (experimental)
      gradient_checkpointing: false  # Not needed for inference

    # Caching & Batching
    inference:
      cache_enabled: true
      cache_ttl: 3600  # seconds (1 hour)
      max_batch_size: 4  # Process up to 4 requests in parallel
      timeout: 30  # seconds
      retry_attempts: 2

    # Model Loading
    loading:
      lazy_load: true  # Load on first request
      preload: false  # Don't load on startup (save RAM)
      download_if_missing: true
      cache_dir: "/tmp/huggingface_cache"  # Model cache directory

    # Health Checks
    health:
      enabled: true
      check_interval: 60  # seconds
      warmup_prompt: "Hello, how are you?"
      max_warmup_time: 10  # seconds

    # Use Cases (routing)
    use_cases:
      - intent_classification
      - query_expansion
      - response_generation
      - conversational_ai
      - code_explanation
      - text_summarization
      - question_answering

  # Circuit Breaker Configuration
  circuit_breaker:
    enabled: true
    failure_threshold: 5  # failures before opening circuit
    success_threshold: 2  # successes to close circuit
    timeout: 60  # seconds circuit stays open
    half_open_timeout: 30  # seconds to retry in half-open state

  # Retry Configuration
  retry:
    enabled: true
    max_attempts: 3
    initial_delay: 1  # seconds
    max_delay: 30  # seconds
    exponential_base: 2
    jitter: true

  # Connection Pooling
  connection_pool:
    max_connections: 100
    max_keepalive_connections: 20
    keepalive_expiry: 30  # seconds
    timeout: 10  # seconds

  # Caching
  cache:
    enabled: true
    backend: redis  # redis, memory, disk
    ttl: 300  # seconds (5 minutes)
    max_size: 1000  # max cached items
    redis:
      host: localhost
      port: 6379
      db: 0

  # Load Balancing
  load_balancing:
    enabled: true
    strategy: least_loaded  # round_robin, least_loaded, weighted
    health_check: true

  # Monitoring
  monitoring:
    enabled: true
    metrics:
      - request_count
      - response_time
      - error_rate
      - cache_hit_rate
      - backend_health
    export:
      prometheus: true
      log_file: "logs/hybrid_metrics.log"

  # WebSocket Configuration
  websocket:
    enabled: true
    ping_interval: 30  # seconds
    ping_timeout: 10  # seconds
    max_message_size: 10485760  # 10MB
    compression: true

  # Fallback Behavior
  fallback:
    on_cloud_failure: local  # local, error, queue
    on_local_failure: cloud  # cloud, error, queue
    queue_enabled: true
    queue_max_size: 1000
    queue_timeout: 300  # seconds

  # ============== MODEL LIFECYCLE MANAGEMENT (Phase 3.1+) ==============
  # Adaptive 3-tier model state management: RAM → Disk → Cloud Storage
  model_lifecycle:
    enabled: true

    # State Transition Policies
    unload_after_idle_seconds: 1800      # 30 min - Unload from RAM to disk if idle
    archive_after_idle_seconds: 86400    # 24 hrs - Archive to cloud storage if rarely used
    min_keep_loaded_models: 1            # Always keep at least N models loaded
    max_loaded_models: 3                 # Max models in RAM simultaneously

    # Background Optimization
    optimizer_interval: 600              # seconds - Run optimizer every 10 min
    ram_monitor_interval: 30             # seconds - Check RAM every 30s

    # Load Queue Configuration
    max_queue_size: 50                   # Max pending load requests
    load_timeout: 120                    # seconds - Max time to wait for model load
    priority_boost_for_urgent: 20        # Priority boost for urgent requests

    # RAM Pressure Response
    aggressive_unload_threshold: 85      # % - Aggressive unloading above this
    panic_unload_threshold: 95           # % - Emergency unloading (keep only critical)

  # Model Selector Configuration
  model_selector:
    enabled: true

    # Scoring Weights (must sum to 1.0)
    weights:
      quality: 0.35                      # Model quality/accuracy
      latency: 0.25                      # Inference + load time
      cost: 0.25                         # $ per query
      ram_efficiency: 0.15               # RAM impact

    # User Preferences
    preferences:
      prefer_local_models: true          # Prefer $0 cost local models
      max_wait_seconds: 30.0             # Max acceptable latency
      quality_threshold: 0.7             # Minimum quality score
      max_cost_per_query: 0.10           # Max $ per query

    # Intent Classification (simple keyword-based for now)
    # TODO: Integrate with CAI for advanced classification
    intent_keywords:
      vision_analysis:
        - screen
        - see
        - look
        - show
        - "what's on"
        - display
      code_explanation:
        - code
        - function
        - explain
        - debug
        - fix
      conversational_ai:
        - chat
        - talk
        - discuss
        - "tell me about"
      semantic_search:
        - find
        - search
        - when did
        - what did i
