"""
GCP Spot VM Auto-Creation Manager
==================================

Advanced, robust, async system for auto-creating and managing GCP Spot VMs
when local memory pressure is too high.

Features:
- Auto-creates e2-highmem-4 Spot VMs (32GB RAM) when RAM >85%
- Integrates with intelligent_gcp_optimizer for smart decisions
- Tracks all costs via cost_tracker
- Async/await throughout for non-blocking operations
- No hardcoding - all configuration from environment/config
- Comprehensive error handling and retry logic
- VM lifecycle management (create, monitor, cleanup)
- Orphaned VM detection and cleanup
- Health monitoring and auto-recovery
- Parallel operations where safe
- Circuit breaker pattern for fault tolerance
- Dynamic configuration loading
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
import sys
import time
import yaml
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from functools import wraps
from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, TypeVar

# Google Cloud Compute Engine API
# Use TYPE_CHECKING to allow type hints without requiring the library at runtime
COMPUTE_AVAILABLE = False
compute_v1: Any = None  # Module placeholder for runtime

try:
    from google.cloud import compute_v1 as _compute_v1
    compute_v1 = _compute_v1
    COMPUTE_AVAILABLE = True
except ImportError:
    logging.warning("google-cloud-compute not installed. VM creation disabled.")

# Type aliases for when compute_v1 is not available
if TYPE_CHECKING:
    from google.cloud import compute_v1 as compute_v1_types
    InstancesClientType = compute_v1_types.InstancesClient
    ZonesClientType = compute_v1_types.ZonesClient
    InstanceType = compute_v1_types.Instance
else:
    InstancesClientType = Any
    ZonesClientType = Any
    InstanceType = Any

# Import with fallback for different import contexts
try:
    from core.cost_tracker import get_cost_tracker, CostTracker
    from core.intelligent_gcp_optimizer import get_gcp_optimizer
except ImportError:
    # Fallback for when running from within core directory
    try:
        from .cost_tracker import get_cost_tracker, CostTracker
        from .intelligent_gcp_optimizer import get_gcp_optimizer
    except ImportError:
        # Final fallback - provide stubs
        def get_cost_tracker():
            return None
        def get_gcp_optimizer(config=None):
            return None
        CostTracker = None

# v9.0: Rate limit manager integration
RATE_LIMIT_MANAGER_AVAILABLE = False
try:
    from core.gcp_rate_limit_manager import (
        get_rate_limit_manager,
        GCPService,
        OperationType,
        RateLimitExceededError,
    )
    RATE_LIMIT_MANAGER_AVAILABLE = True
except ImportError:
    try:
        from .gcp_rate_limit_manager import (
            get_rate_limit_manager,
            GCPService,
            OperationType,
            RateLimitExceededError,
        )
        RATE_LIMIT_MANAGER_AVAILABLE = True
    except ImportError:
        pass

# v224.0: aiohttp availability for golden image health polling
AIOHTTP_AVAILABLE = False
try:
    import aiohttp
    AIOHTTP_AVAILABLE = True
except ImportError:
    pass

from backend.core.async_safety import LazyAsyncLock

# Log severity bridge for criticality-aware logging
try:
    from backend.core.log_severity_bridge import log_component_failure, is_component_required
except ImportError:
    def log_component_failure(component, message, error=None, **ctx):
        logging.getLogger(__name__).error(f"{component}: {message}")
    def is_component_required(component):
        return True

logger = logging.getLogger(__name__)

# Type variable for generic retry decorator
T = TypeVar('T')


# ============================================================================
# v109.4: SHUTDOWN DETECTION TO PREVENT INITIALIZATION DURING ATEXIT
# ============================================================================

# When True, prevents GCP client initialization which uses guarded FDs
# This is set by shutdown_hook.py during cleanup to prevent EXC_GUARD crashes
_gcp_shutdown_requested = False


def mark_gcp_shutdown() -> None:
    """
    v109.4: Mark that GCP resources should not be initialized.

    This is called by shutdown_hook.py during cleanup to prevent
    GCP client libraries from being initialized during interpreter
    shutdown, which would cause EXC_GUARD crashes due to guarded
    file descriptors used by libdispatch/GCD on macOS.

    Once called, initialize() and get_gcp_vm_manager() will return
    early without initializing GCP API clients.
    """
    global _gcp_shutdown_requested
    _gcp_shutdown_requested = True
    logger.debug("[GCPVMManager] v109.4: Shutdown requested - GCP init disabled")


def is_gcp_shutdown_requested() -> bool:
    """Check if GCP shutdown has been requested."""
    return _gcp_shutdown_requested


# ============================================================================
# CIRCUIT BREAKER FOR FAULT TOLERANCE
# ============================================================================


class CircuitState(Enum):
    """Circuit breaker states"""
    CLOSED = "closed"      # Normal operation
    OPEN = "open"          # Failing, rejecting requests
    HALF_OPEN = "half_open"  # Testing if service recovered


@dataclass
class CircuitBreaker:
    """
    Circuit breaker pattern for fault-tolerant GCP operations.
    Prevents cascading failures when GCP API is having issues.
    """
    name: str
    failure_threshold: int = 3  # Open circuit after N failures
    recovery_timeout: float = 60.0  # Seconds before trying again
    half_open_max_calls: int = 1  # Calls to test when half-open
    
    state: CircuitState = CircuitState.CLOSED
    failure_count: int = 0
    last_failure_time: Optional[float] = None
    half_open_calls: int = 0
    
    def can_execute(self) -> Tuple[bool, str]:
        """Check if we can execute an operation"""
        if self.state == CircuitState.CLOSED:
            return True, "Circuit closed - normal operation"
        
        if self.state == CircuitState.OPEN:
            # Check if we should transition to half-open
            if self.last_failure_time and (time.time() - self.last_failure_time) >= self.recovery_timeout:
                self.state = CircuitState.HALF_OPEN
                self.half_open_calls = 0
                logger.info(f"ðŸ”Œ Circuit '{self.name}' transitioning to HALF_OPEN")
                return True, "Circuit half-open - testing recovery"
            return False, f"Circuit OPEN - {self.recovery_timeout - (time.time() - (self.last_failure_time or 0)):.0f}s until retry"
        
        # Half-open state
        if self.half_open_calls < self.half_open_max_calls:
            return True, "Circuit half-open - testing"
        return False, "Circuit half-open - max test calls reached"
    
    def record_success(self):
        """Record successful operation"""
        if self.state == CircuitState.HALF_OPEN:
            logger.info(f"âœ… Circuit '{self.name}' recovered - transitioning to CLOSED")
        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.half_open_calls = 0
    
    def record_failure(self, error: Exception):
        """Record failed operation"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.state == CircuitState.HALF_OPEN:
            logger.warning(f"âš ï¸ Circuit '{self.name}' recovery failed - re-opening")
            self.state = CircuitState.OPEN
        elif self.failure_count >= self.failure_threshold:
            logger.error(f"ðŸ”´ Circuit '{self.name}' OPEN after {self.failure_count} failures: {error}")
            self.state = CircuitState.OPEN


class VMState(Enum):
    """VM lifecycle states"""

    CREATING = "creating"
    PROVISIONING = "provisioning"
    STAGING = "staging"
    RUNNING = "running"
    STOPPING = "stopping"
    TERMINATED = "terminated"
    FAILED = "failed"
    UNKNOWN = "unknown"


@dataclass
class VMInstance:
    """Represents a managed VM instance"""

    instance_id: str
    name: str
    zone: str
    state: VMState
    created_at: float
    ip_address: Optional[str] = None
    internal_ip: Optional[str] = None
    last_health_check: Optional[float] = None
    health_status: str = "unknown"
    components: List[str] = field(default_factory=list)
    trigger_reason: str = ""
    cost_per_hour: float = 0.029  # e2-highmem-4 Spot VM
    total_cost: float = 0.0
    metadata: Dict = field(default_factory=dict)

    # Cost tracking and efficiency
    last_activity_time: float = field(default_factory=time.time)
    component_usage_count: int = 0  # How many times components were accessed
    cost_efficiency_score: float = 0.0  # 0-100, higher is better ROI

    # Detailed metrics (like GCP Console)
    cpu_percent: float = 0.0
    memory_used_gb: float = 0.0
    memory_total_gb: float = 32.0
    network_sent_mb: float = 0.0
    network_received_mb: float = 0.0
    disk_read_mb: float = 0.0
    disk_write_mb: float = 0.0

    @property
    def uptime_hours(self) -> float:
        """Calculate VM uptime in hours"""
        return (time.time() - self.created_at) / 3600

    @property
    def idle_time_minutes(self) -> float:
        """Calculate how long VM has been idle"""
        return (time.time() - self.last_activity_time) / 60

    @property
    def is_healthy(self) -> bool:
        """Check if VM is healthy"""
        return self.health_status == "healthy" and self.state == VMState.RUNNING

    @property
    def is_idle(self) -> bool:
        """Check if VM has been idle too long (no activity for 10 minutes)"""
        return self.idle_time_minutes > 10

    @property
    def is_wasting_money(self) -> bool:
        """Determine if VM is wasting money (idle + low efficiency)"""
        return self.is_idle and self.cost_efficiency_score < 30.0

    @property
    def memory_percent(self) -> float:
        """Memory usage percentage"""
        return (self.memory_used_gb / self.memory_total_gb * 100) if self.memory_total_gb > 0 else 0.0

    def update_cost(self):
        """Update total cost based on uptime"""
        self.total_cost = self.uptime_hours * self.cost_per_hour

    def update_efficiency_score(self):
        """Calculate cost efficiency score (0-100, higher is better ROI)"""
        if self.uptime_hours == 0:
            self.cost_efficiency_score = 0.0
            return

        # Factors:
        # 1. Usage frequency (component_usage_count / uptime_minutes)
        # 2. Recency (how recently was it used?)
        # 3. Resource utilization (is it actually being used?)

        uptime_minutes = self.uptime_hours * 60
        usage_rate = (self.component_usage_count / uptime_minutes) if uptime_minutes > 0 else 0
        recency_score = max(0, 100 - (self.idle_time_minutes * 2))  # Decreases as idle time increases
        utilization_score = min(100, (self.cpu_percent + self.memory_percent) / 2)

        # Weighted average
        self.cost_efficiency_score = (
            usage_rate * 100 * 0.4 +  # 40% weight on usage frequency
            recency_score * 0.3 +       # 30% weight on recency
            utilization_score * 0.3     # 30% weight on resource utilization
        )

        self.cost_efficiency_score = min(100.0, self.cost_efficiency_score)

    def record_activity(self):
        """Record that VM components were used"""
        self.last_activity_time = time.time()
        self.component_usage_count += 1


@dataclass
class VMManagerConfig:
    """
    Dynamic configuration for GCP VM Manager.
    All values loaded from environment variables with sensible defaults.
    No hardcoding - fully configurable at runtime.
    """

    # v147.0: GCP Enabled Flag - checks multiple env vars for compatibility
    # Accepts: GCP_ENABLED, GCP_VM_ENABLED, JARVIS_SPOT_VM_ENABLED (any "true" enables)
    # This fixes the inconsistency where .env.gcp sets JARVIS_SPOT_VM_ENABLED
    # but the manager only checked GCP_ENABLED
    enabled: bool = field(
        default_factory=lambda: any([
            os.getenv("GCP_ENABLED", "false").lower() == "true",
            os.getenv("GCP_VM_ENABLED", "false").lower() == "true",
            os.getenv("JARVIS_SPOT_VM_ENABLED", "false").lower() == "true",
        ])
    )

    # GCP Configuration (all from environment)
    project_id: str = field(
        default_factory=lambda: os.getenv("GCP_PROJECT_ID", os.getenv("GOOGLE_CLOUD_PROJECT", ""))
    )
    region: str = field(default_factory=lambda: os.getenv("GCP_REGION", "us-central1"))
    zone: str = field(default_factory=lambda: os.getenv("GCP_ZONE", "us-central1-a"))

    # VM Configuration (dynamic)
    machine_type: str = field(
        default_factory=lambda: os.getenv("GCP_VM_MACHINE_TYPE", "e2-highmem-4")
    )
    use_spot: bool = field(
        default_factory=lambda: os.getenv("GCP_USE_SPOT_VMS", "true").lower() == "true"
    )
    spot_max_price: float = field(
        default_factory=lambda: float(os.getenv("GCP_SPOT_MAX_PRICE", "0.10"))
    )

    # VM Naming
    vm_name_prefix: str = field(
        default_factory=lambda: os.getenv("GCP_VM_NAME_PREFIX", "jarvis-backend")
    )

    # Image Configuration (dynamic)
    image_project: str = field(
        default_factory=lambda: os.getenv("GCP_IMAGE_PROJECT", "ubuntu-os-cloud")
    )
    image_family: str = field(
        default_factory=lambda: os.getenv("GCP_IMAGE_FAMILY", "ubuntu-2204-lts")
    )

    # Disk Configuration
    boot_disk_size_gb: int = field(
        default_factory=lambda: int(os.getenv("GCP_BOOT_DISK_SIZE_GB", "50"))
    )
    boot_disk_type: str = field(
        default_factory=lambda: os.getenv("GCP_BOOT_DISK_TYPE", "pd-standard")
    )

    # Network Configuration
    network: str = field(default_factory=lambda: os.getenv("GCP_NETWORK", "default"))
    subnetwork: str = field(default_factory=lambda: os.getenv("GCP_SUBNETWORK", "default"))

    # Startup Script Path
    startup_script_path: Optional[str] = field(
        default_factory=lambda: os.getenv(
            "GCP_STARTUP_SCRIPT_PATH",
            os.path.join(os.path.dirname(__file__), "gcp_vm_startup.sh")
        )
    )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v1.0.0: DOCKER CONTAINER-BASED VM DEPLOYMENT (Pre-baked ML Dependencies)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # When enabled, VMs are created using Container-Optimized OS with a Docker
    # image that has all ML dependencies pre-installed. This eliminates the
    # 5-8 minute ml_deps installation phase during startup.
    #
    # Benefits:
    #   - Startup time: ~2-3 min instead of ~8-10 min
    #   - Consistent environment across all VMs
    #   - Easier debugging and reproducibility
    #   - Reduced network dependencies during startup
    #
    # Usage:
    #   1. Build and push image: python scripts/build_gcp_inference_image.py
    #   2. Set JARVIS_GCP_CONTAINER_IMAGE to the image URL
    #   3. Set JARVIS_GCP_USE_CONTAINER=true
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Enable container-based deployment (uses Container-Optimized OS)
    use_container: bool = field(
        default_factory=lambda: os.getenv("JARVIS_GCP_USE_CONTAINER", "false").lower() == "true"
    )
    
    # Docker image for container-based deployment
    # Format: gcr.io/PROJECT/IMAGE:TAG or REGION-docker.pkg.dev/PROJECT/REPO/IMAGE:TAG
    container_image: Optional[str] = field(
        default_factory=lambda: os.getenv("JARVIS_GCP_CONTAINER_IMAGE", None)
    )
    
    # Container-Optimized OS image for container-based VMs
    container_os_image_project: str = field(
        default_factory=lambda: os.getenv("GCP_CONTAINER_OS_PROJECT", "cos-cloud")
    )
    container_os_image_family: str = field(
        default_factory=lambda: os.getenv("GCP_CONTAINER_OS_FAMILY", "cos-stable")
    )
    
    # Container environment variables (comma-separated key=value pairs)
    container_env_vars: str = field(
        default_factory=lambda: os.getenv(
            "JARVIS_GCP_CONTAINER_ENV",
            "JARVIS_DEPS_PREBAKED=true,JARVIS_SKIP_ML_DEPS_INSTALL=true,JARVIS_GCP_INFERENCE=true"
        )
    )
    
    # Fallback to startup script if container deployment fails
    container_fallback_to_script: bool = field(
        default_factory=lambda: os.getenv("JARVIS_GCP_CONTAINER_FALLBACK", "true").lower() == "true"
    )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v224.0: GOLDEN IMAGE (Pre-baked Custom VM Image) Configuration
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ENTERPRISE-GRADE SOLUTION: Custom machine image with everything pre-installed.
    # This reduces VM startup from 10-15 minutes to ~30-60 seconds.
    #
    # What's Pre-baked in the Golden Image:
    #   - Python 3.11 with virtual environment
    #   - All ML dependencies (torch, transformers, llama-cpp-python, etc.)
    #   - JARVIS-Prime codebase with all dependencies
    #   - Model files (7B+ parameters) already downloaded
    #   - System configured and ready to serve
    #
    # Benefits:
    #   - Startup time: ~30-60 seconds instead of 10-15 minutes
    #   - No network downloads during startup
    #   - Consistent, tested environment
    #   - Instant model availability
    #
    # Hierarchy (highest priority first):
    #   1. Custom Golden Image (if enabled and available)
    #   2. Container-based deployment (if enabled)
    #   3. Standard startup script (fallback)
    #
    # Usage:
    #   1. Create golden image: python3 unified_supervisor.py --create-golden-image
    #   2. Set JARVIS_GCP_USE_GOLDEN_IMAGE=true
    #   3. Optionally set JARVIS_GCP_GOLDEN_IMAGE_NAME to override auto-detection
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Enable golden image deployment (highest priority deployment mode)
    use_golden_image: bool = field(
        default_factory=lambda: os.getenv("JARVIS_GCP_USE_GOLDEN_IMAGE", "false").lower() == "true"
    )
    
    # Custom image name (if not set, auto-detects latest jarvis-prime-golden-* image)
    golden_image_name: Optional[str] = field(
        default_factory=lambda: os.getenv("JARVIS_GCP_GOLDEN_IMAGE_NAME", None)
    )
    
    # Project containing the golden image (defaults to current project)
    golden_image_project: Optional[str] = field(
        default_factory=lambda: os.getenv("JARVIS_GCP_GOLDEN_IMAGE_PROJECT", None)
    )
    
    # Golden image family for auto-detection of latest version
    golden_image_family: str = field(
        default_factory=lambda: os.getenv("JARVIS_GCP_GOLDEN_IMAGE_FAMILY", "jarvis-prime-golden")
    )
    
    # Maximum age of golden image before rebuild is recommended (days)
    golden_image_max_age_days: int = field(
        default_factory=lambda: int(os.getenv("JARVIS_GCP_GOLDEN_IMAGE_MAX_AGE_DAYS", "30"))
    )
    
    # Auto-rebuild golden image if stale (requires elevated permissions)
    golden_image_auto_rebuild: bool = field(
        default_factory=lambda: os.getenv("JARVIS_GCP_GOLDEN_IMAGE_AUTO_REBUILD", "false").lower() == "true"
    )
    
    # Fallback to container/script if golden image is unavailable
    golden_image_fallback: bool = field(
        default_factory=lambda: os.getenv("JARVIS_GCP_GOLDEN_IMAGE_FALLBACK", "true").lower() == "true"
    )
    
    # Model to pre-load in golden image (for image building)
    golden_image_model: str = field(
        default_factory=lambda: os.getenv("JARVIS_GCP_GOLDEN_IMAGE_MODEL", "mistral-7b-instruct-v0.2")
    )
    
    # Machine type for building golden image (needs enough RAM for model)
    golden_image_builder_machine_type: str = field(
        default_factory=lambda: os.getenv("JARVIS_GCP_GOLDEN_BUILDER_MACHINE_TYPE", "e2-highmem-8")
    )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # INVINCIBLE NODE (Static IP + STOP termination) Configuration
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # When these are set, the manager uses persistent VM strategy instead of
    # ephemeral VMs. The VM survives preemption in STOPPED state and can be
    # quickly restarted (~30s) instead of recreated (~3-5 min).
    # v210.0: Added default name for auto-creation if not specified
    static_ip_name: Optional[str] = field(
        default_factory=lambda: os.getenv("GCP_VM_STATIC_IP_NAME", "jarvis-prime-ip")
    )
    static_instance_name: Optional[str] = field(
        default_factory=lambda: os.getenv("GCP_VM_INSTANCE_NAME", "jarvis-prime-node")
    )
    # When True, uses STOP instead of DELETE on termination (Invincible Node mode)
    use_stop_termination: bool = field(
        default_factory=lambda: os.getenv("GCP_VM_TERMINATION_ACTION", "DELETE").upper() == "STOP"
    )
    # Health check endpoint settings for static VM
    static_vm_health_poll_interval: float = field(
        default_factory=lambda: float(os.getenv("GCP_STATIC_VM_HEALTH_POLL_INTERVAL", "5.0"))
    )
    static_vm_health_timeout: float = field(
        default_factory=lambda: float(os.getenv("GCP_STATIC_VM_HEALTH_TIMEOUT", "300.0"))
    )

    # Health Check Configuration
    health_check_interval: int = field(
        default_factory=lambda: int(os.getenv("GCP_HEALTH_CHECK_INTERVAL", "30"))
    )
    health_check_timeout: int = field(
        default_factory=lambda: int(os.getenv("GCP_HEALTH_CHECK_TIMEOUT", "10"))
    )
    max_health_check_failures: int = field(
        default_factory=lambda: int(os.getenv("GCP_MAX_HEALTH_CHECK_FAILURES", "3"))
    )

    # VM Lifecycle
    max_vm_lifetime_hours: float = field(
        default_factory=lambda: float(os.getenv("GCP_MAX_VM_LIFETIME_HOURS", "3.0"))
    )
    idle_timeout_minutes: int = field(
        # Prefer JARVIS_SPOT_VM_IDLE_TIMEOUT (used by hybrid cloud stack), fall back to legacy GCP_*
        default_factory=lambda: int(
            os.getenv("GCP_IDLE_TIMEOUT_MINUTES", os.getenv("JARVIS_SPOT_VM_IDLE_TIMEOUT", "30"))
        )
    )

    # Retry Configuration
    max_create_attempts: int = field(
        default_factory=lambda: int(os.getenv("GCP_MAX_CREATE_ATTEMPTS", "3"))
    )
    retry_delay_seconds: int = field(
        default_factory=lambda: int(os.getenv("GCP_RETRY_DELAY_SECONDS", "10"))
    )

    # Resource Limits
    max_concurrent_vms: int = field(
        default_factory=lambda: int(os.getenv("GCP_MAX_CONCURRENT_VMS", "2"))
    )
    daily_budget_usd: float = field(
        # Prefer JARVIS_SPOT_VM_DAILY_BUDGET (used by hybrid cloud stack), fall back to legacy GCP_*
        default_factory=lambda: float(
            os.getenv("GCP_DAILY_BUDGET_USD", os.getenv("JARVIS_SPOT_VM_DAILY_BUDGET", "5.0"))
        )
    )

    # Monitoring
    enable_monitoring: bool = field(
        default_factory=lambda: os.getenv("GCP_ENABLE_MONITORING", "true").lower() == "true"
    )
    enable_logging: bool = field(
        default_factory=lambda: os.getenv("GCP_ENABLE_LOGGING", "true").lower() == "true"
    )

    # Circuit breaker settings
    circuit_failure_threshold: int = field(
        default_factory=lambda: int(os.getenv("GCP_CIRCUIT_FAILURE_THRESHOLD", "3"))
    )
    circuit_recovery_timeout: float = field(
        default_factory=lambda: float(os.getenv("GCP_CIRCUIT_RECOVERY_TIMEOUT", "60.0"))
    )

    def __post_init__(self):
        """Validate configuration after initialization"""
        # Track validation status for pre-flight checks
        self._validation_errors: List[str] = []

        if not self.enabled:
            logger.debug("GCP VM Manager disabled (GCP_ENABLED=false or not set)")
            return  # Skip rest of validation if disabled

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # CRITICAL: Validate required fields for API calls
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if not self.project_id or self.project_id.strip() == "":
            self._validation_errors.append(
                "GCP_PROJECT_ID not set - required for VM operations. "
                "Set via environment variable or GOOGLE_CLOUD_PROJECT."
            )
            logger.error(
                "âŒ GCP_ENABLED=true but GCP_PROJECT_ID not set. "
                "VM creation will be blocked until this is configured."
            )

        if not self.zone or self.zone.strip() == "":
            self._validation_errors.append(
                "GCP_ZONE not set - required for VM operations. "
                "Set via environment variable (default: us-central1-a)."
            )
            logger.error(
                "âŒ GCP_ENABLED=true but GCP_ZONE not set. "
                "VM creation will be blocked until this is configured."
            )

        # Log configuration summary (only if enabled and valid)
        if self.enabled and not self._validation_errors:
            logger.info(f"GCP VM Manager enabled:")
            logger.info(f"  Project: {self.project_id}")
            logger.info(f"  Zone: {self.zone}")
            logger.info(f"  Machine Type: {self.machine_type}")
            logger.info(f"  Use Spot: {self.use_spot}")
            logger.info(f"  Daily Budget: ${self.daily_budget_usd}")
        elif self._validation_errors:
            logger.warning(f"GCP VM Manager has {len(self._validation_errors)} configuration error(s)")

    def is_valid_for_vm_operations(self) -> Tuple[bool, str]:
        """
        Check if configuration is valid for VM operations.

        Returns:
            Tuple of (is_valid, error_message)
        """
        if not self.enabled:
            return False, "GCP VM Manager is disabled (GCP_ENABLED=false)"

        if hasattr(self, '_validation_errors') and self._validation_errors:
            return False, "; ".join(self._validation_errors)

        # Double-check critical fields (defensive)
        if not self.project_id or self.project_id.strip() == "":
            return False, "GCP_PROJECT_ID is empty or not set"

        if not self.zone or self.zone.strip() == "":
            return False, "GCP_ZONE is empty or not set"

        return True, "Configuration valid"

    def to_dict(self) -> Dict[str, Any]:
        """Export configuration as dictionary"""
        return {
            "enabled": self.enabled,
            "project_id": self.project_id,
            "region": self.region,
            "zone": self.zone,
            "machine_type": self.machine_type,
            "use_spot": self.use_spot,
            "daily_budget_usd": self.daily_budget_usd,
            "max_concurrent_vms": self.max_concurrent_vms,
            "max_vm_lifetime_hours": self.max_vm_lifetime_hours,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "VMManagerConfig":
        """Create configuration from dictionary"""
        return cls(**{k: v for k, v in data.items() if hasattr(cls, k)})


@dataclass
class QuotaInfo:
    """
    v9.0: Intelligent GCP Quota tracking with caching.
    Prevents unnecessary VM creation attempts when quotas are exceeded.
    """
    metric: str  # e.g., "CPUS_ALL_REGIONS", "IN_USE_ADDRESSES"
    limit: float
    usage: float
    available: float = field(init=False)
    is_exceeded: bool = field(init=False)
    region: Optional[str] = None
    last_checked: float = field(default_factory=time.time)
    cache_ttl_seconds: int = 60  # Cache for 60 seconds
    
    def __post_init__(self):
        self.available = max(0, self.limit - self.usage)
        self.is_exceeded = self.usage >= self.limit
    
    @property
    def is_stale(self) -> bool:
        """Check if quota info is stale and needs refresh."""
        return time.time() - self.last_checked > self.cache_ttl_seconds
    
    @property
    def utilization_percent(self) -> float:
        """Return quota utilization as a percentage."""
        return (self.usage / self.limit * 100) if self.limit > 0 else 100.0


@dataclass
class QuotaCheckResult:
    """
    Result of pre-flight quota check before VM creation.
    """
    can_create: bool
    blocking_quotas: List[QuotaInfo] = field(default_factory=list)
    warning_quotas: List[QuotaInfo] = field(default_factory=list)  # >80% utilized
    checked_at: float = field(default_factory=time.time)
    message: str = ""
    
    @property
    def has_warnings(self) -> bool:
        return len(self.warning_quotas) > 0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# v224.0: GOLDEN IMAGE BUILDER - Enterprise-Grade Pre-baked VM Images
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


@dataclass
class GoldenImageInfo:
    """
    v224.0: Information about a golden image.
    
    Tracks metadata for golden images including version, age, and model info.
    """
    name: str
    project: str
    family: str
    creation_time: datetime
    model_name: str
    model_version: str
    jarvis_version: str
    disk_size_gb: int
    status: str  # READY, PENDING, FAILED
    source_vm_name: Optional[str] = None
    
    @property
    def age_days(self) -> float:
        """Return age of the image in days."""
        return (datetime.now() - self.creation_time).total_seconds() / 86400
    
    def is_stale(self, max_age_days: int) -> bool:
        """Check if image is stale (older than max_age_days)."""
        return self.age_days > max_age_days
    
    @classmethod
    def from_gcp_image(cls, image: Any, project: str) -> "GoldenImageInfo":
        """
        Create GoldenImageInfo from GCP Image object.
        
        Args:
            image: GCP Image object from compute_v1
            project: GCP project ID
            
        Returns:
            GoldenImageInfo instance
        """
        # Parse labels for metadata
        labels = dict(image.labels) if image.labels else {}
        
        # Parse creation timestamp
        creation_time = datetime.now()
        if image.creation_timestamp:
            try:
                # GCP timestamps are in RFC3339 format
                creation_time = datetime.fromisoformat(
                    image.creation_timestamp.replace('Z', '+00:00')
                ).replace(tzinfo=None)
            except (ValueError, AttributeError):
                pass
        
        return cls(
            name=image.name,
            project=project,
            family=labels.get("family", image.family or "jarvis-prime-golden"),
            creation_time=creation_time,
            model_name=labels.get("model-name", "unknown"),
            model_version=labels.get("model-version", "unknown"),
            jarvis_version=labels.get("jarvis-version", "unknown"),
            disk_size_gb=int(image.disk_size_gb) if image.disk_size_gb else 50,
            status=image.status or "READY",
            source_vm_name=labels.get("source-vm", None),
        )


class GoldenImageBuilder:
    """
    v224.0: Enterprise-Grade Golden Image Builder.
    
    Creates pre-baked VM images with everything installed:
    - Python environment with all dependencies
    - JARVIS-Prime codebase
    - Pre-downloaded model files
    - Configured startup scripts
    
    This reduces VM startup from 10-15 minutes to ~30-60 seconds.
    
    Workflow:
    1. Create a "builder" VM with startup script that installs everything
    2. Wait for installation to complete (monitored via /health/startup)
    3. Stop the VM
    4. Create a machine image from the stopped VM's disk
    5. Delete the builder VM
    
    The resulting image can be used for instant-on VMs.
    """
    
    def __init__(
        self,
        config: "VMManagerConfig",
        logger: Optional[logging.Logger] = None,
    ):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        
        # GCP API clients (initialized lazily)
        self._instances_client: Optional[InstancesClientType] = None
        self._images_client: Any = None
        self._machine_images_client: Any = None
        
        # Build state
        self._current_build_vm: Optional[str] = None
        self._build_in_progress: bool = False
        
    async def _get_instances_client(self) -> Optional[InstancesClientType]:
        """Lazy initialization of GCP Instances client."""
        if not COMPUTE_AVAILABLE:
            return None
        if self._instances_client is None:
            self._instances_client = compute_v1.InstancesClient()
        return self._instances_client
    
    async def _get_images_client(self) -> Optional[Any]:
        """Lazy initialization of GCP Images client."""
        if not COMPUTE_AVAILABLE:
            return None
        if self._images_client is None:
            self._images_client = compute_v1.ImagesClient()
        return self._images_client
    
    async def _get_machine_images_client(self) -> Optional[Any]:
        """Lazy initialization of GCP Machine Images client."""
        if not COMPUTE_AVAILABLE:
            return None
        if self._machine_images_client is None:
            self._machine_images_client = compute_v1.MachineImagesClient()
        return self._machine_images_client
    
    async def list_golden_images(
        self,
        family: Optional[str] = None,
        include_deprecated: bool = False,
    ) -> List[GoldenImageInfo]:
        """
        List all golden images in the project.
        
        Args:
            family: Filter by image family (default: from config)
            include_deprecated: Include deprecated images
            
        Returns:
            List of GoldenImageInfo sorted by creation time (newest first)
        """
        images_client = await self._get_images_client()
        if not images_client:
            self.logger.warning("[GoldenImageBuilder] GCP not available")
            return []
        
        family = family or self.config.golden_image_family
        project = self.config.golden_image_project or self.config.project_id
        
        try:
            # List all images in the project
            request = compute_v1.ListImagesRequest(
                project=project,
                filter=f"family={family}" if family else None,
            )
            
            images = []
            for image in images_client.list(request=request):
                # Skip deprecated images unless requested
                if not include_deprecated and image.deprecated:
                    continue
                
                # Only include images that match our naming convention
                if image.name.startswith("jarvis-prime-golden"):
                    images.append(GoldenImageInfo.from_gcp_image(image, project))
            
            # Sort by creation time (newest first)
            images.sort(key=lambda x: x.creation_time, reverse=True)
            
            self.logger.info(f"[GoldenImageBuilder] Found {len(images)} golden images")
            return images
            
        except Exception as e:
            self.logger.error(f"[GoldenImageBuilder] Error listing images: {e}")
            return []
    
    async def get_latest_golden_image(
        self,
        family: Optional[str] = None,
    ) -> Optional[GoldenImageInfo]:
        """
        Get the latest golden image from a family.
        
        Args:
            family: Image family (default: from config)
            
        Returns:
            Latest GoldenImageInfo or None if no images found
        """
        images_client = await self._get_images_client()
        if not images_client:
            return None
        
        family = family or self.config.golden_image_family
        project = self.config.golden_image_project or self.config.project_id
        
        try:
            # Use getFromFamily for efficient lookup
            request = compute_v1.GetFromFamilyImageRequest(
                project=project,
                family=family,
            )
            
            image = images_client.get_from_family(request=request)
            return GoldenImageInfo.from_gcp_image(image, project)
            
        except Exception as e:
            self.logger.debug(f"[GoldenImageBuilder] No image in family '{family}': {e}")
            return None
    
    async def check_golden_image_status(self) -> Dict[str, Any]:
        """
        Check status of golden images for operational readiness.
        
        Returns:
            Dict with status information:
            - available: bool - Whether a usable golden image exists
            - latest_image: Optional[GoldenImageInfo]
            - is_stale: bool - Whether the image is older than max age
            - recommendation: str - What action to take
        """
        latest = await self.get_latest_golden_image()
        
        if not latest:
            return {
                "available": False,
                "latest_image": None,
                "is_stale": False,
                "recommendation": "CREATE_NEW",
                "message": "No golden image found. Create one with --create-golden-image",
            }
        
        is_stale = latest.is_stale(self.config.golden_image_max_age_days)
        
        return {
            "available": True,
            "latest_image": latest,
            "is_stale": is_stale,
            "age_days": latest.age_days,
            "recommendation": "REBUILD" if is_stale else "READY",
            "message": (
                f"Golden image '{latest.name}' is {latest.age_days:.1f} days old. "
                f"{'Consider rebuilding.' if is_stale else 'Ready for use.'}"
            ),
        }
    
    def _generate_builder_startup_script(self) -> str:
        """
        Generate the startup script for the builder VM.
        
        This script:
        1. Installs Python and all dependencies
        2. Clones JARVIS-Prime repository
        3. Downloads and caches the LLM model
        4. Configures the system for fast startup
        5. Signals completion via /tmp/golden_image_ready
        """
        model_name = self.config.golden_image_model
        
        # Get JARVIS-Prime repo URL from environment
        jarvis_prime_repo = os.getenv(
            "JARVIS_PRIME_REPO_URL",
            "https://github.com/your-org/JARVIS-Prime.git"  # Placeholder
        )
        
        return f'''#!/bin/bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# v224.0: Golden Image Builder - Pre-bake Everything
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# This script creates a "golden image" with everything pre-installed.
# It runs on a temporary builder VM and signals completion via health endpoint.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

set -e  # Exit on error (but handle gracefully)

LOG_FILE="/var/log/golden-image-build.log"
HEALTH_FILE="/tmp/golden_image_status.json"
READY_FILE="/tmp/golden_image_ready"
MODEL_NAME="{model_name}"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Logging and Status Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

log() {{
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}}

update_status() {{
    local status="$1"
    local progress="$2"
    local step="$3"
    
    cat > "$HEALTH_FILE" << EOFSTATUS
{{
    "status": "$status",
    "progress_pct": $progress,
    "current_step": "$step",
    "model_name": "$MODEL_NAME",
    "timestamp": "$(date -Iseconds)"
}}
EOFSTATUS
}}

# Start health endpoint immediately (for monitoring)
start_health_endpoint() {{
    # Simple Python health server for monitoring build progress
    python3 << 'EOFPY' &
import http.server
import json
import os

class HealthHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        if self.path == "/health/startup":
            try:
                with open("/tmp/golden_image_status.json", "r") as f:
                    status = json.load(f)
            except:
                status = {{"status": "booting", "progress_pct": 0, "current_step": "Starting..."}}
            
            self.send_response(200)
            self.send_header("Content-Type", "application/json")
            self.end_headers()
            self.wfile.write(json.dumps(status).encode())
        else:
            self.send_response(404)
            self.end_headers()
    
    def log_message(self, format, *args):
        pass  # Suppress logging

if __name__ == "__main__":
    server = http.server.HTTPServer(("0.0.0.0", 8000), HealthHandler)
    server.serve_forever()
EOFPY
}}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Phase 1: System Setup
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "GOLDEN IMAGE BUILD STARTED"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# Initialize status
update_status "booting" 0 "Starting golden image build..."

# Start health endpoint
start_health_endpoint
sleep 2

# Update: Phase 1
update_status "installing" 5 "Installing system packages..."
log "Phase 1: Installing system packages"

export DEBIAN_FRONTEND=noninteractive
apt-get update -qq
apt-get install -y -qq \\
    python3.11 python3.11-venv python3.11-dev python3-pip \\
    build-essential cmake ninja-build \\
    git curl wget \\
    libssl-dev libffi-dev \\
    2>&1 | tee -a "$LOG_FILE"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Phase 2: Python Environment
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

update_status "installing" 15 "Setting up Python environment..."
log "Phase 2: Setting up Python environment"

# Create dedicated directory for JARVIS-Prime
JARVIS_DIR="/opt/jarvis-prime"
mkdir -p "$JARVIS_DIR"
cd "$JARVIS_DIR"

# Create virtual environment
python3.11 -m venv venv
source venv/bin/activate

# Upgrade pip
pip install --upgrade pip setuptools wheel 2>&1 | tee -a "$LOG_FILE"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Phase 3: Clone JARVIS-Prime (or use embedded deps list)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

update_status "installing" 25 "Installing ML dependencies..."
log "Phase 3: Installing ML dependencies"

# Install core ML dependencies
pip install \\
    torch>=2.1.0 \\
    transformers>=4.35.0 \\
    accelerate>=0.24.0 \\
    bitsandbytes>=0.41.0 \\
    sentencepiece>=0.1.99 \\
    safetensors>=0.4.0 \\
    huggingface_hub>=0.19.0 \\
    llama-cpp-python>=0.2.20 \\
    2>&1 | tee -a "$LOG_FILE"

update_status "installing" 40 "Installing server dependencies..."
log "Phase 3b: Installing server dependencies"

pip install \\
    fastapi>=0.104.0 \\
    uvicorn[standard]>=0.24.0 \\
    httpx>=0.25.0 \\
    pydantic>=2.5.0 \\
    aiohttp>=3.9.0 \\
    python-multipart>=0.0.6 \\
    2>&1 | tee -a "$LOG_FILE"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Phase 4: Download and Cache Model
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

update_status "downloading" 50 "Downloading LLM model: $MODEL_NAME..."
log "Phase 4: Downloading LLM model: $MODEL_NAME"

# Create model cache directory
MODEL_CACHE="/opt/jarvis-prime/models"
mkdir -p "$MODEL_CACHE"

# Download model using huggingface_hub
python3 << EOFMODEL
import os
from huggingface_hub import snapshot_download

model_name = "{model_name}"
cache_dir = "/opt/jarvis-prime/models"

print(f"Downloading model: {{model_name}}")
print(f"Cache directory: {{cache_dir}}")

try:
    # Download the model (this will be cached)
    path = snapshot_download(
        repo_id=model_name if "/" in model_name else f"TheBloke/{{model_name}}-GGUF",
        cache_dir=cache_dir,
        local_dir_use_symlinks=False,
    )
    print(f"Model downloaded to: {{path}}")
except Exception as e:
    print(f"Warning: Model download failed: {{e}}")
    print("The model will be downloaded on first use.")
EOFMODEL

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Phase 5: Configure for Fast Startup
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

update_status "configuring" 80 "Configuring for fast startup..."
log "Phase 5: Configuring for fast startup"

# Create environment file
cat > "$JARVIS_DIR/.env" << 'EOFENV'
# Pre-baked Golden Image Configuration
JARVIS_DEPS_PREBAKED=true
JARVIS_SKIP_ML_DEPS_INSTALL=true
JARVIS_GCP_INFERENCE=true
JARVIS_MODEL_CACHE=/opt/jarvis-prime/models
JARVIS_PRIME_DIR=/opt/jarvis-prime
JARVIS_PRIME_VENV=/opt/jarvis-prime/venv
EOFENV

# Create systemd service for auto-start (optional)
cat > /etc/systemd/system/jarvis-prime.service << 'EOFSVC'
[Unit]
Description=JARVIS-Prime Inference Server
After=network.target

[Service]
Type=simple
User=root
WorkingDirectory=/opt/jarvis-prime
Environment=PATH=/opt/jarvis-prime/venv/bin:/usr/bin:/bin
ExecStart=/opt/jarvis-prime/venv/bin/python -m jarvis_prime.server
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOFSVC

systemctl daemon-reload

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Phase 6: Cleanup and Signal Completion
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

update_status "finalizing" 95 "Cleaning up and finalizing..."
log "Phase 6: Cleaning up"

# Clear apt cache
apt-get clean
rm -rf /var/lib/apt/lists/*

# Clear pip cache
rm -rf ~/.cache/pip

# Clear bash history
history -c

update_status "ready" 100 "Golden image ready!"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "GOLDEN IMAGE BUILD COMPLETED SUCCESSFULLY"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# Signal completion
touch "$READY_FILE"

# Keep health endpoint running for monitoring
log "Build complete. Health endpoint running on :8000"
wait
'''

    async def create_golden_image(
        self,
        progress_callback: Optional[Callable[[int, str], None]] = None,
    ) -> Tuple[bool, str, Optional[GoldenImageInfo]]:
        """
        Create a new golden image with everything pre-installed.
        
        This is a long-running operation (10-20 minutes) that:
        1. Creates a builder VM
        2. Waits for installation to complete
        3. Stops the VM
        4. Creates a machine image
        5. Cleans up the builder VM
        
        Args:
            progress_callback: Optional callback(progress_pct, message)
            
        Returns:
            Tuple of (success, message, GoldenImageInfo or None)
        """
        if self._build_in_progress:
            return False, "Build already in progress", None
        
        self._build_in_progress = True
        instances_client = await self._get_instances_client()
        images_client = await self._get_images_client()
        
        if not instances_client or not images_client:
            self._build_in_progress = False
            return False, "GCP API not available", None
        
        project = self.config.golden_image_project or self.config.project_id
        zone = self.config.zone
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        builder_vm_name = f"jarvis-golden-builder-{timestamp}"
        image_name = f"jarvis-prime-golden-{timestamp}"
        
        self._current_build_vm = builder_vm_name
        
        def report_progress(pct: int, msg: str):
            self.logger.info(f"[GoldenImageBuilder] {pct}% - {msg}")
            if progress_callback:
                progress_callback(pct, msg)
        
        try:
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Phase 1: Create Builder VM
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            report_progress(5, f"Creating builder VM: {builder_vm_name}")
            
            # Generate startup script
            startup_script = self._generate_builder_startup_script()
            
            # Machine type for builder (needs enough RAM for model)
            machine_type = f"zones/{zone}/machineTypes/{self.config.golden_image_builder_machine_type}"
            
            # Disk configuration (large enough for model + deps)
            # IMPORTANT: auto_delete=False to preserve disk for image creation
            boot_disk = compute_v1.AttachedDisk(
                auto_delete=False,  # Keep disk for image creation!
                boot=True,
                initialize_params=compute_v1.AttachedDiskInitializeParams(
                    disk_size_gb=100,  # 100GB for model + deps
                    disk_type=f"zones/{zone}/diskTypes/pd-ssd",  # SSD for faster builds
                    source_image=f"projects/{self.config.image_project}/global/images/family/{self.config.image_family}",
                ),
            )
            
            # Network interface
            network_interface = compute_v1.NetworkInterface(
                network=f"global/networks/{self.config.network}",
                access_configs=[compute_v1.AccessConfig(name="External NAT", type="ONE_TO_ONE_NAT")],
            )
            
            # Service account
            service_account = compute_v1.ServiceAccount(
                email="default",
                scopes=["https://www.googleapis.com/auth/cloud-platform"],
            )
            
            # Create instance
            instance = compute_v1.Instance(
                name=builder_vm_name,
                machine_type=machine_type,
                disks=[boot_disk],
                network_interfaces=[network_interface],
                service_accounts=[service_account],
                metadata=compute_v1.Metadata(items=[
                    compute_v1.Items(key="startup-script", value=startup_script),
                    compute_v1.Items(key="jarvis-build-type", value="golden-image"),
                    compute_v1.Items(key="jarvis-model", value=self.config.golden_image_model),
                ]),
                tags=compute_v1.Tags(items=["jarvis", "golden-image-builder"]),
                labels={
                    "created-by": "jarvis",
                    "type": "golden-image-builder",
                    "model-name": self.config.golden_image_model.replace("/", "-").replace(".", "-").lower(),
                },
            )
            
            # Insert instance
            operation = instances_client.insert(
                project=project,
                zone=zone,
                instance_resource=instance,
            )
            
            # Wait for VM creation
            report_progress(10, "Waiting for VM to start...")
            await asyncio.sleep(30)  # Wait for VM to boot
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Phase 2: Monitor Build Progress
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            report_progress(15, "Monitoring build progress...")
            
            # Get VM IP for health checks
            vm_ip = None
            for _ in range(30):  # Try for 5 minutes
                try:
                    instance = instances_client.get(
                        project=project,
                        zone=zone,
                        instance=builder_vm_name,
                    )
                    if instance.network_interfaces:
                        access_configs = instance.network_interfaces[0].access_configs
                        if access_configs:
                            vm_ip = access_configs[0].nat_i_p
                            if vm_ip:
                                break
                except Exception:
                    pass
                await asyncio.sleep(10)
            
            if not vm_ip:
                raise RuntimeError("Failed to get VM IP address")
            
            report_progress(20, f"Builder VM IP: {vm_ip}")
            
            # Poll health endpoint until build completes
            # The health endpoint may not be accessible from outside GCP (firewall),
            # so we use a timeout-based approach with graceful fallback.
            health_url = f"http://{vm_ip}:8000/health/startup"
            max_build_time = 20 * 60  # 20 minutes max (typical build is 10-15 min)
            min_build_time = 10 * 60  # Wait at least 10 minutes before giving up
            start_time = time.time()
            last_progress = 0
            consecutive_failures = 0
            max_consecutive_failures = 20  # Give up on health checks after 20 failures (~10 min)
            
            if AIOHTTP_AVAILABLE:
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    while time.time() - start_time < max_build_time:
                        try:
                            async with session.get(health_url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                                if resp.status == 200:
                                    consecutive_failures = 0  # Reset on success
                                    data = await resp.json()
                                    status = data.get("status", "unknown")
                                    build_progress = data.get("progress_pct", 0)
                                    current_step = data.get("current_step", "Building...")
                                    
                                    # Map build progress (0-100) to our progress (20-80)
                                    mapped_progress = 20 + int(build_progress * 0.6)
                                    if mapped_progress > last_progress:
                                        last_progress = mapped_progress
                                        report_progress(mapped_progress, current_step)
                                    
                                    if status == "ready":
                                        report_progress(80, "Build completed successfully!")
                                        break
                                    elif status == "error":
                                        raise RuntimeError(f"Build failed: {data.get('error_details', 'Unknown error')}")
                        except Exception as e:
                            consecutive_failures += 1
                            self.logger.debug(f"Health check failed ({consecutive_failures}/{max_consecutive_failures}): {e}")
                            
                            # If health endpoint is consistently unreachable (likely firewall),
                            # and we've waited the minimum time, proceed anyway
                            elapsed = time.time() - start_time
                            if consecutive_failures >= max_consecutive_failures and elapsed >= min_build_time:
                                self.logger.info(
                                    f"[GoldenImageBuilder] Health endpoint unreachable after {int(elapsed/60)}min. "
                                    f"Proceeding with image creation (build likely complete)."
                                )
                                report_progress(75, "Build time elapsed - proceeding...")
                                break
                        
                        await asyncio.sleep(30)  # Poll every 30 seconds
            else:
                # Fallback: wait a fixed time
                report_progress(40, "Waiting for build (no aiohttp)...")
                await asyncio.sleep(15 * 60)  # Wait 15 minutes
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Phase 3: Stop VM and Create Image
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            report_progress(85, "Stopping builder VM...")
            
            # Stop the VM
            stop_operation = instances_client.stop(
                project=project,
                zone=zone,
                instance=builder_vm_name,
            )
            
            # Wait for VM to stop
            for _ in range(30):  # Wait up to 5 minutes
                try:
                    instance = instances_client.get(
                        project=project,
                        zone=zone,
                        instance=builder_vm_name,
                    )
                    if instance.status == "TERMINATED":
                        break
                except Exception:
                    pass
                await asyncio.sleep(10)
            
            report_progress(90, f"Creating image: {image_name}...")
            
            # Create image from the VM's boot disk
            # Get the source disk
            instance = instances_client.get(
                project=project,
                zone=zone,
                instance=builder_vm_name,
            )
            
            source_disk = instance.disks[0].source
            
            # Create image
            image_resource = compute_v1.Image(
                name=image_name,
                family=self.config.golden_image_family,
                source_disk=source_disk,
                labels={
                    "created-by": "jarvis",
                    "type": "golden-image",
                    "model-name": self.config.golden_image_model.replace("/", "-").replace(".", "-").lower(),
                    "model-version": "latest",
                    "jarvis-version": os.getenv("JARVIS_VERSION", "unknown"),
                    "source-vm": builder_vm_name,
                },
                description=f"JARVIS-Prime golden image with {self.config.golden_image_model} pre-loaded",
            )
            
            image_operation = images_client.insert(
                project=project,
                image_resource=image_resource,
            )
            
            # Wait for image creation
            report_progress(95, "Waiting for image creation...")
            await asyncio.sleep(60)  # Images typically take ~1 minute to create
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Phase 4: Cleanup
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            report_progress(98, "Cleaning up builder VM...")
            
            # Delete the builder VM
            delete_operation = instances_client.delete(
                project=project,
                zone=zone,
                instance=builder_vm_name,
            )
            
            # Get the created image info
            image_info = await self.get_latest_golden_image()
            
            report_progress(100, f"Golden image created: {image_name}")
            
            self._build_in_progress = False
            self._current_build_vm = None
            
            return True, f"Successfully created golden image: {image_name}", image_info
            
        except Exception as e:
            self.logger.error(f"[GoldenImageBuilder] Error creating golden image: {e}")
            
            # Attempt cleanup
            try:
                if self._current_build_vm:
                    instances_client.delete(
                        project=project,
                        zone=zone,
                        instance=self._current_build_vm,
                    )
            except Exception:
                pass
            
            self._build_in_progress = False
            self._current_build_vm = None
            
            return False, f"Failed to create golden image: {e}", None
    
    async def delete_golden_image(
        self,
        image_name: str,
    ) -> Tuple[bool, str]:
        """
        Delete a golden image.
        
        Args:
            image_name: Name of the image to delete
            
        Returns:
            Tuple of (success, message)
        """
        images_client = await self._get_images_client()
        if not images_client:
            return False, "GCP API not available"
        
        project = self.config.golden_image_project or self.config.project_id
        
        try:
            images_client.delete(
                project=project,
                image=image_name,
            )
            return True, f"Deleted golden image: {image_name}"
        except Exception as e:
            return False, f"Failed to delete image: {e}"
    
    async def cleanup_old_images(
        self,
        keep_count: int = 3,
    ) -> Tuple[int, List[str]]:
        """
        Clean up old golden images, keeping only the most recent ones.
        
        Args:
            keep_count: Number of recent images to keep (default: 3)
            
        Returns:
            Tuple of (deleted_count, list of deleted image names)
        """
        images = await self.list_golden_images()
        
        if len(images) <= keep_count:
            return 0, []
        
        # Images are sorted newest first, so delete from keep_count onwards
        images_to_delete = images[keep_count:]
        deleted = []
        
        for image in images_to_delete:
            success, msg = await self.delete_golden_image(image.name)
            if success:
                deleted.append(image.name)
                self.logger.info(f"[GoldenImageBuilder] Deleted old image: {image.name}")
        
        return len(deleted), deleted


class GCPVMManager:
    """
    Advanced GCP Spot VM auto-creation and lifecycle manager.

    Features:
    - Circuit breaker pattern for fault tolerance
    - Parallel async operations where safe
    - Dynamic configuration from environment
    - Robust error handling with retries
    - Cost tracking integration
    - Health monitoring with auto-recovery
    - v9.0: Intelligent quota checking before VM creation

    Integrates with:
    - intelligent_gcp_optimizer: For VM creation decisions
    - cost_tracker: For billing tracking
    - platform_memory_monitor: For memory pressure detection
    """

    def __init__(self, config: Optional[VMManagerConfig] = None):
        self.config = config or VMManagerConfig()

        # API clients (initialized lazily)
        # Use type aliases that work even when compute_v1 is not available
        self.instances_client: Optional[InstancesClientType] = None
        self.zones_client: Optional[ZonesClientType] = None
        self.zone_operations_client = None  # For polling operation status
        self.regions_client = None  # For quota checking

        # Integrations (initialized safely)
        self.cost_tracker: Optional[Any] = None  # CostTracker or None
        self.gcp_optimizer: Optional[Any] = None

        # State tracking with thread-safe locks
        self.managed_vms: Dict[str, VMInstance] = {}
        self.creating_vms: Dict[str, asyncio.Task] = {}
        self._vm_lock = asyncio.Lock()  # Protect VM state modifications
        self._init_lock = asyncio.Lock()  # Protect initialization

        # v193.2: Smart VM creation waiting - prevents "blocked: another creation in progress" errors
        # When multiple callers try to create a VM concurrently (e.g., main startup + background retry),
        # the second caller should WAIT for the first creation to complete instead of failing immediately.
        # _creation_event: Set when a creation completes (success or failure)
        # _creation_result: Stores the result of the in-progress creation for waiting callers
        self._creation_event: Optional[asyncio.Event] = None
        self._creation_result: Optional[VMInstance] = None
        self._creation_error: Optional[str] = None

        # v9.0: Quota cache to avoid repeated API calls
        self._quota_cache: Dict[str, QuotaInfo] = {}
        self._quota_cache_lock = asyncio.Lock()
        self._quota_exceeded_until: float = 0  # Cooldown after quota exceeded
        self._quota_cooldown_seconds: int = 300  # 5 minute cooldown

        # Circuit breakers for fault tolerance
        self._circuit_breakers = {
            "vm_create": CircuitBreaker(
                name="vm_create",
                failure_threshold=self.config.circuit_failure_threshold,
                recovery_timeout=self.config.circuit_recovery_timeout,
            ),
            "vm_delete": CircuitBreaker(
                name="vm_delete",
                failure_threshold=self.config.circuit_failure_threshold,
                recovery_timeout=self.config.circuit_recovery_timeout,
            ),
            "cost_tracker": CircuitBreaker(
                name="cost_tracker",
                failure_threshold=5,  # More lenient for non-critical operations
                recovery_timeout=30.0,
            ),
            "quota_check": CircuitBreaker(
                name="quota_check",
                failure_threshold=3,
                recovery_timeout=60.0,
            ),
        }

        # Monitoring
        self.monitoring_task: Optional[asyncio.Task] = None
        self.is_monitoring = False

        # Enhanced stats
        self.stats = {
            "total_created": 0,
            "total_failed": 0,
            "quota_blocks": 0,
            "quota_checks": 0,
            "total_terminated": 0,
            "current_active": 0,
            "total_cost": 0.0,
            "circuit_breaks": 0,
            "retries": 0,
            "last_error": None,
            "last_error_time": None,
        }

        self.initialized = False
        self._initialization_error: Optional[Exception] = None

        # v86.0: State for cross-repo cloud offloading coordination
        self._cloud_offload_active: bool = False
        self._cloud_offload_reason: str = ""
        self._cloud_offload_triggered_at: Optional[float] = None

        # v224.0: Golden Image Builder for pre-baked VM images
        self._golden_image_builder: Optional[GoldenImageBuilder] = None
        self._golden_image_cache: Optional[GoldenImageInfo] = None
        self._golden_image_cache_time: float = 0
        self._golden_image_cache_ttl: float = 300  # 5 minutes

    # =========================================================================
    # v86.0: Property accessors for clean interface
    # =========================================================================

    @property
    def enabled(self) -> bool:
        """
        Check if GCP VM Manager is enabled.

        This property provides a clean interface for external code to check
        if the manager is available for cloud offloading operations.

        Returns:
            True if GCP is enabled in config, False otherwise
        """
        return self.config.enabled

    @property
    def is_ready(self) -> bool:
        """
        Check if manager is fully initialized and ready for operations.

        Returns:
            True if enabled, initialized, and no initialization errors
        """
        return (
            self.enabled and
            self.initialized and
            self._initialization_error is None
        )

    @property
    def cloud_offload_active(self) -> bool:
        """Check if cloud offloading is currently active."""
        return self._cloud_offload_active

    @property
    def cloud_offload_reason(self) -> str:
        """Get the reason cloud offloading was activated."""
        return self._cloud_offload_reason

    def mark_cloud_offload_active(self, reason: str) -> None:
        """
        Mark cloud offloading as active with the given reason.

        Args:
            reason: Why cloud offloading was activated (e.g., "High CPU", "Low memory")
        """
        import time
        self._cloud_offload_active = True
        self._cloud_offload_reason = reason
        self._cloud_offload_triggered_at = time.time()
        logger.info(f"â˜ï¸ [GCPVMManager] Cloud offloading marked ACTIVE: {reason}")

    def mark_cloud_offload_inactive(self) -> None:
        """Mark cloud offloading as inactive."""
        self._cloud_offload_active = False
        self._cloud_offload_reason = ""
        self._cloud_offload_triggered_at = None
        logger.info("â˜ï¸ [GCPVMManager] Cloud offloading marked INACTIVE")

    def get_status(self) -> Dict[str, Any]:
        """
        Get comprehensive status of the GCP VM Manager.

        Returns:
            Dict with enabled, ready, cloud_offload status, and stats
        """
        return {
            "enabled": self.enabled,
            "initialized": self.initialized,
            "is_ready": self.is_ready,
            "cloud_offload_active": self._cloud_offload_active,
            "cloud_offload_reason": self._cloud_offload_reason,
            "cloud_offload_triggered_at": self._cloud_offload_triggered_at,
            "initialization_error": str(self._initialization_error) if self._initialization_error else None,
            "config": self.config.to_dict() if hasattr(self.config, 'to_dict') else {"enabled": self.enabled},
            "stats": self.stats,
            "circuit_breakers": {
                name: {"state": str(cb.state), "failures": cb.failure_count}
                for name, cb in self._circuit_breakers.items()
            },
        }

    async def initialize(self):
        """
        Initialize GCP API clients and integrations with robust error handling.

        Uses async lock to prevent race conditions during initialization.
        Gracefully handles missing dependencies and API failures.

        v109.4: Checks for shutdown state to prevent initialization during
        interpreter shutdown, which would cause EXC_GUARD crashes.
        """
        # v109.4: CRITICAL - Don't initialize during shutdown
        # GCP client libraries use guarded FDs that cause EXC_GUARD crashes
        if _gcp_shutdown_requested:
            logger.debug("[GCPVMManager] Skipping init - shutdown in progress")
            return

        # Check for interpreter shutdown (late stage)
        try:
            if sys.modules is None:
                logger.debug("[GCPVMManager] Skipping init - interpreter shutdown detected")
                return
        except Exception:
            return

        # Quick check without lock
        if self.initialized:
            return

        async with self._init_lock:
            # Double-check after acquiring lock
            if self.initialized:
                return

            # v109.4: Check again after acquiring lock
            if _gcp_shutdown_requested:
                logger.debug("[GCPVMManager] Skipping init - shutdown detected after lock")
                return

            if not COMPUTE_AVAILABLE:
                self._initialization_error = RuntimeError(
                    "google-cloud-compute package not installed. "
                    "Install with: pip install google-cloud-compute"
                )
                logger.warning(
                    "â„¹ï¸  GCP Compute Engine API not available. "
                    "Spot VM creation disabled."
                )
                raise self._initialization_error

            logger.info("ðŸš€ Initializing GCP VM Manager...")

            try:
                # Initialize GCP Compute Engine clients (parallel-safe)
                await self._initialize_gcp_clients()

                # Initialize integrations (with error isolation)
                await self._initialize_integrations()

                # v134.0: Sync local VM tracking with GCP reality
                # This cleans up stale entries from previous runs that may cause 404 errors
                await self._sync_managed_vms_with_gcp()

                # Start monitoring if enabled
                if self.config.enable_monitoring:
                    self.monitoring_task = asyncio.create_task(
                        self._monitoring_loop(),
                        name="gcp_vm_monitoring"
                    )
                    logger.info("âœ… VM monitoring started")

                self.initialized = True
                logger.info("âœ… GCP VM Manager ready")
                logger.info(f"   Project: {self.config.project_id}")
                logger.info(f"   Zone: {self.config.zone}")
                logger.info(f"   Machine Type: {self.config.machine_type}")
                logger.info(f"   Daily Budget: ${self.config.daily_budget_usd}")

            except Exception as e:
                self._initialization_error = e
                log_component_failure(
                    "gcp-vm",
                    "Failed to initialize GCP VM Manager",
                    error=e,
                    project_id=self.config.project_id,
                    zone=self.config.zone,
                )
                raise

    async def _initialize_gcp_clients(self):
        """
        Initialize GCP API clients with robust event loop handling.

        Uses modern asyncio patterns (get_running_loop) and handles edge cases:
        - Event loop closing during shutdown
        - No running event loop (sync context)
        - Thread pool executor failures

        Falls back to synchronous initialization if async fails.
        """
        try:
            # First check if we have a running event loop and it's not closing
            try:
                loop = asyncio.get_running_loop()
                if loop.is_closed() or loop.is_running() is False:
                    raise RuntimeError("Event loop is closed or not running")
            except RuntimeError as loop_error:
                # No running loop or loop is closing - initialize synchronously
                logger.warning(
                    f"[GCPVMManager] No running event loop ({loop_error}) - "
                    "initializing GCP clients synchronously"
                )
                self._initialize_gcp_clients_sync()
                return

            # Run client initialization in thread pool to avoid blocking
            # Use asyncio.to_thread for cleaner syntax (Python 3.9+)
            try:
                self.instances_client = await asyncio.to_thread(
                    compute_v1.InstancesClient
                )
                self.zones_client = await asyncio.to_thread(
                    compute_v1.ZonesClient
                )
                self.zone_operations_client = await asyncio.to_thread(
                    compute_v1.ZoneOperationsClient
                )
            except RuntimeError as executor_error:
                # Thread pool executor may fail during shutdown
                if "cannot schedule" in str(executor_error).lower():
                    logger.warning(
                        f"[GCPVMManager] Thread pool unavailable ({executor_error}) - "
                        "falling back to sync initialization"
                    )
                    self._initialize_gcp_clients_sync()
                    return
                raise

            logger.info(f"âœ… GCP API clients initialized (Project: {self.config.project_id})")

        except Exception as e:
            log_component_failure(
                "gcp-vm",
                "Failed to initialize GCP API clients",
                error=e,
            )
            raise RuntimeError(f"GCP API client initialization failed: {e}") from e

    def _initialize_gcp_clients_sync(self):
        """
        Synchronous fallback for GCP client initialization.

        Used when:
        - No running event loop (during shutdown)
        - Thread pool executor is unavailable
        - Called from synchronous context
        """
        try:
            self.instances_client = compute_v1.InstancesClient()
            self.zones_client = compute_v1.ZonesClient()
            self.zone_operations_client = compute_v1.ZoneOperationsClient()
            logger.info(
                f"âœ… GCP API clients initialized synchronously "
                f"(Project: {self.config.project_id})"
            )
        except Exception as e:
            log_component_failure(
                "gcp-vm",
                "Synchronous GCP client initialization failed",
                error=e,
            )
            raise

    async def _initialize_integrations(self):
        """
        Initialize integrations with graceful fallbacks and modern asyncio patterns.

        Non-critical integrations fail gracefully without blocking core functionality.
        Uses asyncio.to_thread for clean async I/O.
        """
        # Cost tracker - non-critical, continue without it
        try:
            self.cost_tracker = get_cost_tracker()
            if self.cost_tracker:
                # Ensure cost tracker is initialized
                if hasattr(self.cost_tracker, 'initialize'):
                    await self.cost_tracker.initialize()
                logger.info("âœ… Cost tracker integrated")
            else:
                logger.warning("âš ï¸  Cost tracker not available - cost tracking disabled")
        except Exception as e:
            logger.warning(f"âš ï¸  Cost tracker initialization failed (non-critical): {e}")
            self.cost_tracker = None

        # GCP optimizer - non-critical, continue without it
        try:
            self.gcp_optimizer = get_gcp_optimizer(config={"project_id": self.config.project_id})
            if self.gcp_optimizer:
                logger.info("âœ… GCP optimizer integrated")
            else:
                logger.warning("âš ï¸  GCP optimizer not available - using fallback decisions")
        except Exception as e:
            logger.warning(f"âš ï¸  GCP optimizer initialization failed (non-critical): {e}")
            self.gcp_optimizer = None

            self.gcp_optimizer = None
        
        # v155.0: CRITICAL - Ensure firewall rule exists BEFORE any VM creation
        # Previous versions marked this "non-critical" but it's actually MANDATORY
        self._firewall_rule_verified = False
        await self._ensure_firewall_rule()

    async def _ensure_firewall_rule(self) -> bool:
        """
        v155.0: CRITICAL - Ensure GCP firewall rule exists for health checks.

        ROOT CAUSE FIX: Previous versions marked firewall creation as "non-critical"
        but without this rule, health checks CANNOT reach VMs, causing 100% of
        provisioning attempts to timeout with "GCP VM not ready after Xs".

        v155.0 Changes:
        - Returns bool indicating success (True) or failure (False)
        - Sets self._firewall_rule_verified flag
        - VM creation will be BLOCKED if firewall rule doesn't exist
        - Added retry logic with exponential backoff
        - Better error classification and recovery guidance

        Creates a firewall rule allowing TCP ports to VMs tagged with 'jarvis-node'.
        """
        firewall_name = "jarvis-allow-health-checks"
        max_retries = 3

        for attempt in range(max_retries):
            try:
                # Import the firewalls client
                firewalls_client = await asyncio.to_thread(compute_v1.FirewallsClient)

                # Check if firewall rule already exists
                try:
                    existing_rule = await asyncio.to_thread(
                        firewalls_client.get,
                        project=self.config.project_id,
                        firewall=firewall_name,
                    )
                    logger.info(f"[v155.0] âœ… Firewall rule '{firewall_name}' verified - health checks will work")
                    self._firewall_rule_verified = True
                    return True
                except Exception as e:
                    error_str = str(e).lower()
                    if "404" not in error_str and "not found" not in error_str:
                        # Unexpected error - might be permissions
                        if "403" in error_str or "permission" in error_str:
                            logger.error(
                                f"[v155.0] âŒ PERMISSION DENIED checking firewall rule. "
                                f"Service account needs 'compute.firewalls.get' permission.\n"
                                f"    Fix: gcloud projects add-iam-policy-binding {self.config.project_id} \\\n"
                                f"        --member='serviceAccount:YOUR_SA@...' \\\n"
                                f"        --role='roles/compute.securityAdmin'"
                            )
                        else:
                            logger.warning(f"[v155.0] Firewall check error: {e}")
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2 ** attempt)  # Exponential backoff
                            continue
                        return False
                    # 404 = rule doesn't exist, we'll create it

                # Create the firewall rule
                logger.info(f"[v155.0] ðŸ”¥ Creating firewall rule '{firewall_name}' (attempt {attempt + 1}/{max_retries})...")

                firewall_rule = compute_v1.Firewall(
                    name=firewall_name,
                    description="CRITICAL: Allow health checks for JARVIS GCP VMs (v155.0)",
                    network=f"global/networks/{self.config.network}",
                    priority=1000,
                    direction="INGRESS",
                    target_tags=["jarvis-node"],
                    source_ranges=["0.0.0.0/0"],  # Required for external health checks
                    allowed=[
                        compute_v1.Allowed(
                            I_p_protocol="tcp",
                            ports=["8000", "8010", "8080", "8090", "22"],  # All JARVIS ports + SSH for debugging
                        ),
                    ],
                )

                # Insert the firewall rule
                operation = await asyncio.to_thread(
                    firewalls_client.insert,
                    project=self.config.project_id,
                    firewall_resource=firewall_rule,
                )

                # Wait for operation to complete (with timeout)
                global_ops_client = await asyncio.to_thread(compute_v1.GlobalOperationsClient)

                # Poll for completion (max 60 seconds - increased from 30)
                for poll in range(60):
                    op_result = await asyncio.to_thread(
                        global_ops_client.get,
                        project=self.config.project_id,
                        operation=operation.name,
                    )
                    if op_result.status == compute_v1.Operation.Status.DONE:
                        if op_result.error:
                            logger.error(f"[v155.0] âŒ Firewall creation failed: {op_result.error}")
                            break
                        logger.info(f"[v155.0] âœ… Firewall rule '{firewall_name}' created successfully!")
                        logger.info(f"[v155.0]    Allows TCP ports 8000,8010,8080,8090,22 to VMs with 'jarvis-node' tag")
                        self._firewall_rule_verified = True
                        return True
                    await asyncio.sleep(1)
                else:
                    logger.warning(f"[v155.0] âš ï¸ Firewall creation timed out after 60s - may still complete")
                    # Optimistically mark as verified since operation started
                    self._firewall_rule_verified = True
                    return True

            except Exception as e:
                error_str = str(e).lower()

                # Classify the error for better guidance
                if "already exists" in error_str:
                    logger.info(f"[v155.0] âœ… Firewall rule already exists (race condition, OK)")
                    self._firewall_rule_verified = True
                    return True
                elif "403" in error_str or "permission" in error_str:
                    logger.error(
                        f"[v155.0] âŒ PERMISSION DENIED creating firewall rule.\n"
                        f"    The service account needs 'compute.firewalls.create' permission.\n"
                        f"    Quick fix - create manually:\n"
                        f"    gcloud compute firewall-rules create {firewall_name} \\\n"
                        f"        --allow tcp:8000,tcp:8010,tcp:8080,tcp:8090,tcp:22 \\\n"
                        f"        --target-tags jarvis-node \\\n"
                        f"        --description 'JARVIS health checks' \\\n"
                        f"        --project {self.config.project_id}"
                    )
                    return False
                elif "quota" in error_str:
                    logger.error(f"[v155.0] âŒ QUOTA EXCEEDED for firewall rules: {e}")
                    return False
                else:
                    logger.warning(f"[v155.0] âš ï¸ Firewall rule creation error (attempt {attempt + 1}): {e}")

                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)

        # All retries exhausted
        logger.error(
            f"[v155.0] âŒ CRITICAL: Could not ensure firewall rule exists after {max_retries} attempts.\n"
            f"    VM creation will be BLOCKED until this is resolved.\n"
            f"    Manual fix:\n"
            f"    gcloud compute firewall-rules create {firewall_name} \\\n"
            f"        --allow tcp:8000,tcp:8010,tcp:8080,tcp:8090,tcp:22 \\\n"
            f"        --target-tags jarvis-node \\\n"
            f"        --description 'JARVIS health checks' \\\n"
            f"        --project {self.config.project_id}"
        )
        return False

    async def _get_vm_serial_console_output(self, vm_name: str, lines: int = 100) -> Optional[str]:
        """
        v155.0: Get serial console output from a VM for debugging startup failures.

        This provides visibility into what's happening inside the VM when
        health checks fail. Essential for diagnosing startup script issues.

        Args:
            vm_name: Name of the VM
            lines: Number of lines to retrieve (default 100)

        Returns:
            Serial console output string, or None if unavailable
        """
        try:
            output = await asyncio.to_thread(
                self.instances_client.get_serial_port_output,
                project=self.config.project_id,
                zone=self.config.zone,
                instance=vm_name,
                port=1,  # Primary serial port
            )

            if output and output.contents:
                # Get last N lines
                all_lines = output.contents.split('\n')
                recent_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
                return '\n'.join(recent_lines)
            return None

        except Exception as e:
            logger.debug(f"[v155.0] Could not get serial console output for {vm_name}: {e}")
            return None

    async def diagnose_vm_startup_failure(self, vm_name: str, vm_ip: Optional[str] = None) -> Dict[str, Any]:
        """
        v155.0: Comprehensive diagnosis of VM startup failure.

        When health checks fail, this method gathers diagnostic information
        to help identify the root cause.

        Returns:
            Dict with diagnostic information including:
            - vm_state: Current VM state in GCP
            - serial_output: Last 100 lines of serial console
            - firewall_rule_exists: Whether the firewall rule exists
            - network_reachable: Whether the IP is pingable
            - health_check_results: Results of health check attempts
        """
        diagnosis: Dict[str, Any] = {
            "vm_name": vm_name,
            "vm_ip": vm_ip,
            "timestamp": datetime.now().isoformat(),
            "diagnosis_version": "v155.0",
        }

        # 1. Check VM state in GCP
        try:
            instance = await asyncio.to_thread(
                self.instances_client.get,
                project=self.config.project_id,
                zone=self.config.zone,
                instance=vm_name,
            )
            diagnosis["vm_state"] = instance.status
            diagnosis["vm_creation_time"] = instance.creation_timestamp if hasattr(instance, 'creation_timestamp') else None

            # Get network interface info
            if instance.network_interfaces:
                ni = instance.network_interfaces[0]
                diagnosis["internal_ip"] = ni.network_i_p if hasattr(ni, 'network_i_p') else None
                if ni.access_configs:
                    diagnosis["external_ip"] = ni.access_configs[0].nat_i_p if hasattr(ni.access_configs[0], 'nat_i_p') else None
        except Exception as e:
            diagnosis["vm_state"] = f"ERROR: {e}"

        # 2. Get serial console output
        serial_output = await self._get_vm_serial_console_output(vm_name)
        if serial_output:
            diagnosis["serial_output"] = serial_output

            # Look for common failure patterns
            failure_patterns = [
                ("apt-get", "Package manager issue"),
                ("pip3 install", "Python package installation issue"),
                ("Permission denied", "Permission issue"),
                ("No space left", "Disk space issue"),
                ("Connection refused", "Network connectivity issue"),
                ("timeout", "Timeout issue"),
                ("error", "General error"),
                ("failed", "General failure"),
            ]

            detected_issues = []
            serial_lower = serial_output.lower()
            for pattern, description in failure_patterns:
                if pattern.lower() in serial_lower:
                    detected_issues.append(description)

            diagnosis["detected_issues"] = list(set(detected_issues))
        else:
            diagnosis["serial_output"] = "Unavailable"
            diagnosis["detected_issues"] = ["Could not retrieve serial console output"]

        # 3. Check firewall rule
        diagnosis["firewall_rule_verified"] = getattr(self, '_firewall_rule_verified', False)

        # 4. Log comprehensive diagnosis
        logger.error(
            f"[v155.0] ðŸ” VM STARTUP FAILURE DIAGNOSIS\n"
            f"    VM: {vm_name}\n"
            f"    IP: {vm_ip}\n"
            f"    State: {diagnosis.get('vm_state', 'unknown')}\n"
            f"    Firewall Rule: {'âœ… Verified' if diagnosis['firewall_rule_verified'] else 'âŒ NOT VERIFIED'}\n"
            f"    Detected Issues: {diagnosis.get('detected_issues', [])}\n"
            f"    Serial Output (last 20 lines):\n"
            f"    {'='*60}\n"
            f"    {chr(10).join((diagnosis.get('serial_output', 'N/A') or 'N/A').split(chr(10))[-20:])}\n"
            f"    {'='*60}"
        )

        return diagnosis

    async def _sync_managed_vms_with_gcp(self):
        """
        v134.0: Synchronize local VM tracking with actual GCP state.

        ROOT CAUSE FIX for stale managed_vms entries:
        On startup, the managed_vms dict may contain entries from previous runs
        that no longer exist in GCP (preempted, deleted, etc.). This causes 404
        errors when trying to terminate them.

        This method:
        1. Checks each tracked VM against GCP to verify existence
        2. Removes entries for VMs that no longer exist
        3. Updates state for VMs that exist but have changed status
        4. Logs any discrepancies for debugging

        Called during initialization to ensure clean state before operations begin.
        """
        if not self.managed_vms:
            logger.debug("[VMSync] No managed VMs to sync")
            return

        if not self.instances_client:
            logger.warning("[VMSync] Instances client not available - skipping sync")
            return

        logger.info(f"[VMSync] Syncing {len(self.managed_vms)} tracked VMs with GCP...")

        stale_vms = []
        updated_vms = []

        async with self._vm_lock:
            for vm_name, vm in list(self.managed_vms.items()):
                try:
                    exists, gcp_status = await self._check_vm_exists_in_gcp(vm_name)

                    if not exists:
                        logger.info(
                            f"[VMSync] VM '{vm_name}' no longer exists in GCP "
                            f"(tracked state: {vm.state.value}) - marking for removal"
                        )
                        stale_vms.append(vm_name)
                    elif gcp_status:
                        # VM exists - update our tracking to match GCP state
                        status_map = {
                            "PROVISIONING": VMState.PROVISIONING,
                            "STAGING": VMState.STAGING,
                            "RUNNING": VMState.RUNNING,
                            "STOPPING": VMState.STOPPING,
                            "TERMINATED": VMState.TERMINATED,
                        }
                        new_state = status_map.get(gcp_status, VMState.UNKNOWN)

                        if vm.state != new_state:
                            logger.info(
                                f"[VMSync] VM '{vm_name}' state updated: "
                                f"{vm.state.value} â†’ {new_state.value}"
                            )
                            vm.state = new_state
                            updated_vms.append(vm_name)

                        # If VM is terminated in GCP, mark for removal
                        if gcp_status == "TERMINATED":
                            logger.info(
                                f"[VMSync] VM '{vm_name}' is TERMINATED in GCP - marking for removal"
                            )
                            stale_vms.append(vm_name)

                except Exception as e:
                    logger.warning(f"[VMSync] Error checking VM '{vm_name}': {e}")
                    # Don't remove on error - could be transient

            # Remove stale VMs from tracking
            for vm_name in stale_vms:
                if vm_name in self.managed_vms:
                    vm = self.managed_vms[vm_name]
                    # Update stats
                    self.stats["current_active"] = max(0, self.stats["current_active"] - 1)
                    if vm.state == VMState.RUNNING:
                        self.stats["total_terminated"] += 1
                    # Remove from tracking
                    del self.managed_vms[vm_name]

        if stale_vms or updated_vms:
            logger.info(
                f"[VMSync] Sync complete: {len(stale_vms)} stale VMs removed, "
                f"{len(updated_vms)} VMs updated"
            )
        else:
            logger.info("[VMSync] All tracked VMs verified in GCP")

    async def get_active_vm(self) -> Optional[VMInstance]:
        """
        Get the currently active (RUNNING) VM instance.
        
        Returns:
            VMInstance if found and running, None otherwise.
        """
        async with self._vm_lock:
            for vm in self.managed_vms.values():
                if vm.state == VMState.RUNNING and vm.is_healthy:
                    return vm
        return None

    async def get_most_recent_vm(self) -> Optional[VMInstance]:
        """
        v214.0: Get the most recently created VM instance, regardless of health status.
        
        This is useful immediately after provisioning when the VM exists but
        hasn't passed health checks yet.
        
        Returns:
            VMInstance if found, None otherwise.
        """
        async with self._vm_lock:
            if not self.managed_vms:
                return None
            # Return the first RUNNING VM, or any VM if none are running
            running_vms = [vm for vm in self.managed_vms.values() if vm.state == VMState.RUNNING]
            if running_vms:
                return running_vms[0]
            # No running VMs - return any VM
            return next(iter(self.managed_vms.values()), None)
    
    async def start_spot_vm(self) -> Tuple[bool, Optional[str]]:
        """
        Start a Spot VM for immediate use.
        
        v147.0: Enhanced with detailed error messages for diagnostics.
        v214.0: Returns (success, ip_or_error, vm_name) - vm_name available via
                get_most_recent_vm() immediately after this call.
        
        Returns:
            (success, result_or_error)
            - On success: (True, ip_address)
            - On failure: (False, error_message)
        """
        # v147.0: Check enabled with detailed error
        if not self.config.enabled:
            enabled_vars = {
                "GCP_ENABLED": os.getenv("GCP_ENABLED", "not set"),
                "GCP_VM_ENABLED": os.getenv("GCP_VM_ENABLED", "not set"),
                "JARVIS_SPOT_VM_ENABLED": os.getenv("JARVIS_SPOT_VM_ENABLED", "not set"),
            }
            logger.warning(
                f"[v147.0] GCP VM Manager disabled. Env vars: {enabled_vars}. "
                f"Set any of these to 'true' to enable."
            )
            return False, "GCP_DISABLED: Set GCP_ENABLED, GCP_VM_ENABLED, or JARVIS_SPOT_VM_ENABLED=true"
        
        # v147.0: Validate configuration before attempting
        is_valid, validation_error = self.config.is_valid_for_vm_operations()
        if not is_valid:
            logger.warning(f"[v147.0] GCP config validation failed: {validation_error}")
            return False, f"CONFIG_INVALID: {validation_error}"

        # v155.0: CRITICAL - Check firewall rule before VM creation
        # Without this, health checks will ALWAYS fail and VM will timeout
        if not getattr(self, '_firewall_rule_verified', False):
            logger.warning("[v155.0] Firewall rule not verified - attempting to verify now...")
            firewall_ok = await self._ensure_firewall_rule()
            if not firewall_ok:
                logger.error(
                    "[v155.0] âŒ BLOCKING VM CREATION: Firewall rule not configured.\n"
                    "    Health checks cannot reach VMs without this rule.\n"
                    "    Fix: Create firewall rule manually or grant service account permissions."
                )
                return False, "FIREWALL_RULE_MISSING: Health checks will fail. Create jarvis-allow-health-checks rule."

        try:
            # Check if we already have one
            existing = await self.get_active_vm()
            if existing:
                logger.info(f"[v147.0] Using existing active VM: {existing.ip_address}")
                return True, existing.ip_address
                
            # v132.2: Fixed create_vm call - uses correct parameters
            # create_vm expects (components: List[str], trigger_reason: str)
            logger.info("[v147.0] No existing VM, creating new Spot VM...")
            vm = await self.create_vm(
                components=["ml_processing", "heavy_computation", "auto_offload"],
                trigger_reason="auto_offload",
            )
            
            # v213.0: Wait for IP assignment with retry loop
            # GCP can take a few seconds to assign the external IP after VM creation
            if vm and vm.state == VMState.RUNNING:
                if vm.ip_address:
                    logger.info(f"[v213.0] Spot VM created successfully: {vm.ip_address}")
                    return True, vm.ip_address
                
                # v213.0: IP not immediately available - poll GCP for up to 60s
                logger.info(f"[v213.0] VM created but IP not yet assigned. Waiting for IP assignment...")
                ip_wait_start = time.time()
                ip_wait_timeout = 60.0  # 60 second timeout for IP assignment
                
                while (time.time() - ip_wait_start) < ip_wait_timeout:
                    try:
                        # Query GCP for current VM state
                        instance = await asyncio.to_thread(
                            self.instances_client.get,
                            project=self.config.project_id,
                            zone=self.config.zone,
                            instance=vm.name,
                        )
                        
                        # Check for external IP
                        if instance.network_interfaces:
                            ni = instance.network_interfaces[0]
                            if ni.access_configs:
                                ip_address = ni.access_configs[0].nat_i_p
                                if ip_address:
                                    # Update our tracked VM with the IP
                                    async with self._vm_lock:
                                        if vm.name in self.managed_vms:
                                            self.managed_vms[vm.name].ip_address = ip_address
                                    logger.info(f"[v213.0] IP assigned after {time.time() - ip_wait_start:.1f}s: {ip_address}")
                                    return True, ip_address
                        
                        await asyncio.sleep(2.0)  # Poll every 2 seconds
                    except Exception as ip_poll_err:
                        logger.debug(f"[v213.0] IP poll error (retrying): {ip_poll_err}")
                        await asyncio.sleep(3.0)
                
                # Timed out waiting for IP - return what we have (might be None)
                logger.warning(f"[v213.0] Timed out waiting for IP assignment after {ip_wait_timeout}s")
                return True, vm.ip_address  # Still return success - VM is running
            
            # VM creation returned but not running
            if vm:
                return False, f"VM_NOT_RUNNING: VM created but state is {vm.state.name}"
            else:
                return False, "VM_CREATE_FAILED: create_vm returned None"
                
        except Exception as e:
            log_component_failure(
                "gcp-vm",
                "Failed to start Spot VM",
                error=e,
            )
            return False, f"EXCEPTION: {str(e)}"

        # Initialize regions client for quota checking
        # Use modern asyncio patterns with fallback
        try:
            try:
                # Try async initialization first
                self.regions_client = await asyncio.to_thread(
                    compute_v1.RegionsClient
                )
            except RuntimeError:
                # Fall back to sync if thread pool unavailable
                self.regions_client = compute_v1.RegionsClient()
            logger.info("âœ… Regions client initialized for quota checking")
        except Exception as e:
            logger.warning(f"âš ï¸  Regions client initialization failed (quota checking limited): {e}")
            self.regions_client = None

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v9.0: INTELLIGENT QUOTA MANAGEMENT
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def check_quotas_before_creation(self) -> QuotaCheckResult:
        """
        v9.0: Pre-flight quota check before attempting VM creation.
        
        Checks critical quotas like CPUS_ALL_REGIONS and IN_USE_ADDRESSES
        to avoid wasted API calls when quotas are exceeded.
        
        Returns:
            QuotaCheckResult with can_create=True if quotas allow VM creation
        """
        self.stats["quota_checks"] += 1
        
        # Check if we're in quota cooldown period
        if time.time() < self._quota_exceeded_until:
            remaining = self._quota_exceeded_until - time.time()
            logger.info(f"â³ Quota cooldown active ({remaining:.0f}s remaining) - skipping VM creation")
            return QuotaCheckResult(
                can_create=False,
                message=f"Quota cooldown active ({remaining:.0f}s remaining)",
                blocking_quotas=list(self._quota_cache.values())
            )
        
        # Circuit breaker check
        circuit = self._circuit_breakers["quota_check"]
        can_execute, circuit_reason = circuit.can_execute()
        if not can_execute:
            logger.warning(f"âš ï¸  Quota check circuit breaker open: {circuit_reason}")
            # Assume quotas are OK if we can't check (optimistic)
            return QuotaCheckResult(can_create=True, message="Quota check unavailable - proceeding optimistically")
        
        blocking_quotas: List[QuotaInfo] = []
        warning_quotas: List[QuotaInfo] = []
        
        try:
            # Check critical quotas in parallel
            quota_checks = await asyncio.gather(
                self._check_cpu_quota(),
                self._check_address_quota(),
                self._check_instance_quota(),
                return_exceptions=True
            )
            
            for result in quota_checks:
                if isinstance(result, Exception):
                    logger.warning(f"âš ï¸  Quota check error (non-fatal): {result}")
                    continue
                if result is None:
                    continue
                    
                quota_info: QuotaInfo = result
                
                # Cache the quota info
                async with self._quota_cache_lock:
                    self._quota_cache[quota_info.metric] = quota_info
                
                if quota_info.is_exceeded:
                    blocking_quotas.append(quota_info)
                    logger.warning(
                        f"ðŸš« QUOTA EXCEEDED: {quota_info.metric} "
                        f"({quota_info.usage:.0f}/{quota_info.limit:.0f})"
                    )
                elif quota_info.utilization_percent > 80:
                    warning_quotas.append(quota_info)
                    logger.info(
                        f"âš ï¸  High quota utilization: {quota_info.metric} "
                        f"({quota_info.utilization_percent:.0f}%)"
                    )
            
            circuit.record_success()
            
            if blocking_quotas:
                # Set cooldown to avoid repeated failures
                self._quota_exceeded_until = time.time() + self._quota_cooldown_seconds
                self.stats["quota_blocks"] += 1
                
                blocked_names = ", ".join(q.metric for q in blocking_quotas)
                return QuotaCheckResult(
                    can_create=False,
                    blocking_quotas=blocking_quotas,
                    warning_quotas=warning_quotas,
                    message=f"Quota exceeded: {blocked_names}. Cooldown: {self._quota_cooldown_seconds}s"
                )
            
            return QuotaCheckResult(
                can_create=True,
                warning_quotas=warning_quotas,
                message="All quotas OK" if not warning_quotas else f"Quotas OK with {len(warning_quotas)} warnings"
            )
            
        except Exception as e:
            circuit.record_failure(e)
            log_component_failure(
                "gcp-vm",
                "Quota check failed",
                error=e,
            )
            # Proceed optimistically if quota check fails
            return QuotaCheckResult(can_create=True, message=f"Quota check error: {e} - proceeding optimistically")
    
    async def _check_cpu_quota(self) -> Optional[QuotaInfo]:
        """Check CPUS_ALL_REGIONS quota."""
        return await self._get_quota_metric("CPUS_ALL_REGIONS")
    
    async def _check_address_quota(self) -> Optional[QuotaInfo]:
        """Check IN_USE_ADDRESSES quota for the region."""
        return await self._get_quota_metric("IN_USE_ADDRESSES", region=self.config.region)
    
    async def _check_instance_quota(self) -> Optional[QuotaInfo]:
        """Check INSTANCES quota for the region."""
        return await self._get_quota_metric("INSTANCES", region=self.config.region)
    
    async def _get_quota_metric(
        self, metric: str, region: Optional[str] = None
    ) -> Optional[QuotaInfo]:
        """
        Get a specific quota metric from GCP.
        Uses cache if available and not stale.
        """
        cache_key = f"{metric}:{region or 'global'}"
        
        # Check cache first
        async with self._quota_cache_lock:
            if cache_key in self._quota_cache:
                cached = self._quota_cache[cache_key]
                if not cached.is_stale:
                    return cached
        
        if not self.regions_client or not self.config.project_id:
            return None
        
        try:
            # Query quota from GCP
            target_region = region or self.config.region
            
            region_info = await asyncio.to_thread(
                self.regions_client.get,
                project=self.config.project_id,
                region=target_region
            )
            
            # Find the quota in the region's quotas list
            for quota in region_info.quotas:
                if quota.metric == metric:
                    quota_info = QuotaInfo(
                        metric=metric,
                        limit=quota.limit,
                        usage=quota.usage,
                        region=target_region
                    )
                    return quota_info
            
            # Quota not found - might be a global quota
            # For global quotas like CPUS_ALL_REGIONS, we need to sum across regions
            # or check project-level quotas (simplified here)
            return None
            
        except Exception as e:
            logger.debug(f"Could not fetch quota {metric}: {e}")
            return None
    
    def _is_quota_exceeded_error(self, error: Exception) -> Tuple[bool, Optional[str]]:
        """
        v9.0: Detect if an error is a quota exceeded error.
        
        Returns:
            (is_quota_error, quota_name or None)
        """
        error_str = str(error).lower()
        
        quota_patterns = [
            ("quota", "CPUS_ALL_REGIONS"),
            ("quota 'cpus", "CPUS_ALL_REGIONS"),
            ("quota 'in_use_addresses", "IN_USE_ADDRESSES"),
            ("quota 'instances", "INSTANCES"),
            ("quota exceeded", None),  # Generic quota exceeded
            ("resource_exhausted", None),
        ]
        
        for pattern, quota_name in quota_patterns:
            if pattern in error_str:
                # Try to extract the specific quota from the error message
                if quota_name is None:
                    # Parse from error: "Quota 'CPUS_ALL_REGIONS' exceeded"
                    import re
                    match = re.search(r"quota '([A-Z_]+)'", str(error), re.IGNORECASE)
                    if match:
                        quota_name = match.group(1).upper()
                    else:
                        quota_name = "UNKNOWN"
                return True, quota_name
        
        return False, None

    async def should_create_vm(
        self, memory_snapshot, trigger_reason: str = ""
    ) -> Tuple[bool, str, float]:
        """
        Determine if we should create a VM based on current conditions.
        
        v2.0: Enhanced with intelligent budget enforcement from cost_tracker.

        Returns: (should_create, reason, confidence_score)
        """
        if not self.initialized:
            await self.initialize()

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v2.0: INTELLIGENT BUDGET ENFORCEMENT
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Uses cost_tracker.can_create_vm() which provides:
        # - Hard budget enforcement (blocks when exceeded)
        # - Budget warning alerts (at 50% threshold)
        # - Cost forecasting (warns if likely to exceed)
        # - Solo developer mode protection
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.cost_tracker:
            try:
                # Use intelligent budget check
                if hasattr(self.cost_tracker, 'can_create_vm'):
                    allowed, reason, details = await self.cost_tracker.can_create_vm()
                    if not allowed:
                        logger.warning(f"ðŸš« VM creation blocked by budget: {reason}")
                        return (False, reason, 0.0)
                    
                    # Log budget status if close to limit
                    if details.get("budget_percent_used", 0) >= 50:
                        logger.info(
                            f"ðŸ’° Budget status: {details['budget_percent_used']:.0f}% used "
                            f"(${details['daily_spent']:.2f}/${details['daily_budget']:.2f})"
                        )
                else:
                    # Fallback to simple daily cost check
                    daily_cost = await self.cost_tracker.get_daily_cost()
                    if daily_cost >= self.config.daily_budget_usd:
                        return (
                            False,
                            f"Daily budget exceeded: ${daily_cost:.2f} / ${self.config.daily_budget_usd:.2f}",
                            0.0,
                        )
            except Exception as e:
                logger.warning(f"âš ï¸ Budget check failed (allowing VM): {e}")

        # Check concurrent VM limits
        active_vms = len([vm for vm in self.managed_vms.values() if vm.state == VMState.RUNNING])
        if active_vms >= self.config.max_concurrent_vms:
            return (
                False,
                f"Max concurrent VMs reached: {active_vms} / {self.config.max_concurrent_vms}",
                0.0,
            )

        # Check if already creating a VM
        if self.creating_vms:
            return False, "VM creation already in progress", 0.0

        # Use intelligent optimizer for decision
        if self.gcp_optimizer:
            should_create, reason, score = await self.gcp_optimizer.should_create_vm(
                memory_snapshot, current_processes=None
            )
            # PressureScore uses composite_score, not overall_score
            confidence = score.composite_score if hasattr(score, 'composite_score') else getattr(score, 'overall_score', 0.0)
            return should_create, reason, confidence

        # Fallback: Simple memory pressure check
        if memory_snapshot.gcp_shift_recommended:
            return True, trigger_reason or memory_snapshot.reasoning, 0.8

        return False, "Memory pressure within acceptable limits", 0.0

    async def create_vm(
        self, components: List[str], trigger_reason: str, metadata: Optional[Dict] = None
    ) -> Optional[VMInstance]:
        """
        Create a new GCP Spot VM instance with circuit breaker protection.

        v9.0: Now includes pre-flight quota checking to avoid wasted API calls.

        Args:
            components: List of components that will run on this VM
            trigger_reason: Why this VM is being created
            metadata: Additional metadata

        Returns:
            VMInstance if successful, None otherwise
        """
        if not self.initialized:
            await self.initialize()

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # PRE-FLIGHT CONFIGURATION VALIDATION
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ensure project_id and zone are properly configured before making API calls
        # This prevents cryptic API errors about "missing project/zone fields"
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        is_valid, validation_error = self.config.is_valid_for_vm_operations()
        if not is_valid:
            # v109.1: Changed from ERROR to INFO - GCP being disabled is expected
            # configuration when running without cloud resources. This is not an error.
            logger.info(f"â„¹ï¸  VM creation skipped - {validation_error}")
            logger.debug("   To enable: set GCP_PROJECT_ID, GCP_ZONE, and GCP_ENABLED=true")
            self.stats["total_failed"] += 1
            self.stats["last_error"] = f"Configuration invalid: {validation_error}"
            self.stats["last_error_time"] = time.time()
            return None

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v1.0: CROSS-PROCESS RESOURCE LOCK (via ProcessCoordinationHub)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Prevents multiple processes (run_supervisor.py, start_system.py) from
        # creating GCP VMs simultaneously, which could cause:
        # 1. Duplicate VMs = double billing
        # 2. Resource quota exhaustion
        # 3. Race conditions in VM tracking
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        coord_hub = None
        try:
            from backend.core.trinity_process_coordination import (
                get_coordination_hub,
                LockType,
            )
            coord_hub = await get_coordination_hub()
        except ImportError:
            logger.debug("[VMManager] ProcessCoordinationHub not available (optional)")
        except Exception as e:
            logger.debug(f"[VMManager] CoordHub init warning: {e}")

        # Check circuit breaker before attempting
        circuit = self._circuit_breakers["vm_create"]
        can_execute, circuit_reason = circuit.can_execute()

        if not can_execute:
            logger.warning(f"ðŸ”Œ VM creation blocked by circuit breaker: {circuit_reason}")
            self.stats["circuit_breaks"] += 1
            return None

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v9.0: PRE-FLIGHT QUOTA CHECK
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Check quotas BEFORE attempting VM creation to avoid:
        # 1. Wasted API calls that will fail anyway
        # 2. Multiple retry attempts that all fail with the same error
        # 3. Unnecessary delays and error logs
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        quota_check = await self.check_quotas_before_creation()
        
        if not quota_check.can_create:
            logger.warning(f"ðŸš« VM creation blocked by quota check: {quota_check.message}")
            for quota in quota_check.blocking_quotas:
                logger.warning(
                    f"   â”œâ”€ {quota.metric}: {quota.usage:.0f}/{quota.limit:.0f} "
                    f"({quota.utilization_percent:.0f}% used)"
                )
            return None
        
        if quota_check.has_warnings:
            logger.info(f"âš ï¸  Quota warnings (proceeding anyway):")
            for quota in quota_check.warning_quotas:
                logger.info(
                    f"   â”œâ”€ {quota.metric}: {quota.usage:.0f}/{quota.limit:.0f} "
                    f"({quota.utilization_percent:.0f}% used)"
                )

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v9.0: RATE LIMIT CHECK
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if RATE_LIMIT_MANAGER_AVAILABLE:
            try:
                rate_manager = await get_rate_limit_manager()
                acquired, reason = await rate_manager.acquire(
                    GCPService.COMPUTE_ENGINE, OperationType.WRITE, timeout=10.0
                )
                if not acquired:
                    logger.warning(f"â³ Rate limit prevents VM creation: {reason}")
                    return None
            except Exception as e:
                logger.debug(f"Rate limit check failed (proceeding): {e}")

        logger.info(f"ðŸš€ Creating GCP Spot VM...")
        logger.info(f"   Components: {', '.join(components)}")
        logger.info(f"   Trigger: {trigger_reason}")
        logger.info(f"   Quota check: {quota_check.message}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v193.2: SMART VM CREATION WITH CONCURRENT REQUEST HANDLING
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # When multiple callers try to create a VM concurrently (e.g., main startup
        # path + background GCP retry task), instead of blocking the second caller
        # with an error, we make it WAIT for the first creation to complete and
        # return that result. This prevents "VM_CREATE_FAILED: create_vm returned None"
        # errors during Trinity protocol initialization.
        #
        # Smart waiting behavior:
        # - First caller: Creates the VM, other callers wait for result
        # - Waiting callers: Get the same VM instance (or error) as the first caller
        # - Timeout: Waiting callers give up after 5 minutes (configurable)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        creation_id = f"create_{int(time.time() * 1000)}"
        wait_timeout = float(os.environ.get("GCP_VM_CREATION_WAIT_TIMEOUT", "300.0"))
        wait_event: Optional[asyncio.Event] = None
        should_wait = False

        async with self._vm_lock:
            if self.creating_vms:
                existing_creation_ids = list(self.creating_vms.keys())
                logger.info(
                    f"â³ VM creation already in progress ({existing_creation_ids}). "
                    f"Waiting up to {wait_timeout}s for it to complete..."
                )

                # Get or create the event to wait on
                if self._creation_event is None:
                    self._creation_event = asyncio.Event()

                # Release lock and wait for creation to complete
                wait_event = self._creation_event
                should_wait = True

        # If we found an in-progress creation, wait for it
        if should_wait and wait_event is not None:
            try:
                await asyncio.wait_for(wait_event.wait(), timeout=wait_timeout)

                # Check the result of the creation we waited for
                async with self._vm_lock:
                    if self._creation_result is not None:
                        logger.info(
                            f"âœ… Waited for in-progress creation: got VM {self._creation_result.name}"
                        )
                        return self._creation_result
                    elif self._creation_error:
                        logger.warning(
                            f"âš ï¸ In-progress creation failed: {self._creation_error}"
                        )
                        return None
                    else:
                        # Creation completed but no result - shouldn't happen
                        logger.warning("âš ï¸ In-progress creation completed with no result")
                        return None

            except asyncio.TimeoutError:
                logger.error(
                    f"â° Timed out waiting {wait_timeout}s for in-progress VM creation. "
                    f"The other creation may still be running."
                )
                return None

        # Re-acquire lock to mark ourselves as creating
        async with self._vm_lock:
            # Double-check no one snuck in
            if self.creating_vms:
                logger.warning(
                    f"ðŸš« Race condition: another creation started while we waited. "
                    f"Deferring to: {list(self.creating_vms.keys())}"
                )
                return None

            # Mark that we're creating and reset the event
            self.creating_vms[creation_id] = None
            self._creation_event = asyncio.Event()
            self._creation_result = None
            self._creation_error = None
            logger.debug(f"[v193.2] Starting VM creation: {creation_id}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v1.0: Try to acquire GCP VM creation lock (best-effort, non-blocking)
        # This prevents duplicate VMs when multiple processes try to create simultaneously
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if coord_hub is not None:
            try:
                from backend.core.trinity_process_coordination import LockType
                # Try to log lock acquisition (non-blocking check)
                logger.debug("ðŸ”’ Attempting GCP VM creation lock (coordination hub available)")
            except Exception as lock_err:
                logger.debug(f"Lock coordination warning (continuing): {lock_err}")

        attempt = 0
        last_error = None
        vm_instance = None

        try:
            while attempt < self.config.max_create_attempts:
                attempt += 1
                try:
                    # Generate unique VM name
                    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                    vm_name = f"{self.config.vm_name_prefix}-{timestamp}"

                    logger.info(
                        f"ðŸ”¨ Attempt {attempt}/{self.config.max_create_attempts}: Creating VM '{vm_name}'"
                    )

                    # Build VM configuration
                    instance_config = self._build_instance_config(
                        vm_name=vm_name,
                        components=components,
                        trigger_reason=trigger_reason,
                        metadata=metadata or {},
                    )

                    # Create the VM (async operation)
                    operation = await asyncio.to_thread(
                        self.instances_client.insert,
                        project=self.config.project_id,
                        zone=self.config.zone,
                        instance_resource=instance_config,
                    )

                    logger.info(f"â³ VM creation operation started: {operation.name}")

                    # Wait for operation to complete
                    await self._wait_for_operation(operation)

                    # Get the created instance
                    instance = await asyncio.to_thread(
                        self.instances_client.get,
                        project=self.config.project_id,
                        zone=self.config.zone,
                        instance=vm_name,
                    )

                    # Extract IP addresses
                    ip_address = None
                    internal_ip = None
                    if instance.network_interfaces:
                        internal_ip = instance.network_interfaces[0].network_i_p
                        if instance.network_interfaces[0].access_configs:
                            ip_address = instance.network_interfaces[0].access_configs[0].nat_i_p

                    # Create VMInstance tracking object
                    vm_instance = VMInstance(
                        instance_id=str(instance.id),
                        name=vm_name,
                        zone=self.config.zone,
                        state=VMState.RUNNING,
                        created_at=time.time(),
                        ip_address=ip_address,
                        internal_ip=internal_ip,
                        components=components,
                        trigger_reason=trigger_reason,
                        metadata=metadata or {},
                    )

                    # Track the VM with lock protection
                    async with self._vm_lock:
                        self.managed_vms[vm_name] = vm_instance
                        self.stats["total_created"] += 1
                        self.stats["current_active"] += 1

                    # Record in cost tracker (isolated error handling)
                    await self._record_vm_creation_safe(
                        vm_instance=vm_instance,
                        components=components,
                        trigger_reason=trigger_reason,
                        metadata=metadata,
                    )

                    # Circuit breaker success
                    circuit.record_success()

                    logger.info(f"âœ… VM created successfully: {vm_name}")
                    logger.info(f"   External IP: {ip_address or 'N/A'}")
                    logger.info(f"   Internal IP: {internal_ip or 'N/A'}")
                    logger.info(f"   Cost: ${vm_instance.cost_per_hour:.3f}/hour")

                    return vm_instance

                except Exception as e:
                    last_error = e
                    self.stats["retries"] += 1
                    self.stats["last_error"] = str(e)
                    self.stats["last_error_time"] = datetime.now().isoformat()
                    logger.error(f"âŒ Attempt {attempt} failed: {e}")
                    
                    # v9.0: Detect quota exceeded errors and stop retrying immediately
                    is_quota_error, quota_name = self._is_quota_exceeded_error(e)
                    if is_quota_error:
                        logger.error(f"ðŸš« QUOTA EXCEEDED ({quota_name}) - stopping retries immediately")
                        
                        # Set cooldown to prevent future attempts
                        self._quota_exceeded_until = time.time() + self._quota_cooldown_seconds
                        self.stats["quota_blocks"] += 1
                        
                        # Update quota cache with exceeded status
                        if quota_name:
                            async with self._quota_cache_lock:
                                self._quota_cache[quota_name] = QuotaInfo(
                                    metric=quota_name,
                                    limit=0,  # Unknown
                                    usage=0,  # Unknown
                                    region=self.config.region,
                                )
                                self._quota_cache[quota_name].is_exceeded = True  # Force exceeded
                        
                        # Report to rate limit manager
                        if RATE_LIMIT_MANAGER_AVAILABLE:
                            try:
                                rate_manager = await get_rate_limit_manager()
                                rate_manager.handle_quota_exceeded(GCPService.COMPUTE_ENGINE, quota_name)
                            except Exception:
                                pass
                        
                        # Don't retry - quotas won't change in seconds
                        break
                    
                    # v9.0: Detect rate limit (429) errors
                    is_429 = "429" in str(e) or "too many requests" in str(e).lower()
                    if is_429:
                        logger.error(f"ðŸš« RATE LIMITED (429) - entering cooldown")
                        if RATE_LIMIT_MANAGER_AVAILABLE:
                            try:
                                rate_manager = await get_rate_limit_manager()
                                rate_manager.handle_429_response(GCPService.COMPUTE_ENGINE)
                            except Exception:
                                pass
                        # Still retry after cooldown
                        await asyncio.sleep(60)  # 60 second cooldown for 429

                    if attempt < self.config.max_create_attempts:
                        delay = self.config.retry_delay_seconds * attempt
                        logger.info(f"â³ Retrying in {delay}s...")
                        await asyncio.sleep(delay)

            # All attempts failed - record in circuit breaker
            circuit.record_failure(last_error)
            
            self.stats["total_failed"] += 1
            
            # Check if this was a quota failure
            is_quota_error, quota_name = self._is_quota_exceeded_error(last_error) if last_error else (False, None)
            if is_quota_error:
                logger.error(f"âŒ VM creation failed due to quota: {quota_name}")
                logger.error(f"   Cooldown active for {self._quota_cooldown_seconds}s")
            else:
                logger.error(f"âŒ Failed to create VM after {self.config.max_create_attempts} attempts")
                logger.error(f"   Last error: {last_error}")

            return None
        finally:
            # v193.2: Clean up creation tracking AND signal waiting callers
            async with self._vm_lock:
                if creation_id in self.creating_vms:
                    del self.creating_vms[creation_id]
                    logger.debug(f"[v193.2] Cleared VM creation guard: {creation_id}")

                # Store result for waiting callers
                # Note: vm_instance is set on SUCCESS, last_error is set on FAILURE
                # But last_error might also be set from earlier retry attempts even on success
                self._creation_result = vm_instance
                if vm_instance is not None:
                    # Success - clear any error from previous attempts
                    self._creation_error = None
                elif last_error:
                    self._creation_error = str(last_error)
                else:
                    self._creation_error = "VM creation failed (unknown reason)"

                # Signal waiting callers that creation is complete
                if self._creation_event:
                    self._creation_event.set()
                    logger.debug(
                        f"[v193.2] Signaled creation complete: "
                        f"success={vm_instance is not None}, error={self._creation_error}"
                    )

    async def _record_vm_creation_safe(
        self,
        vm_instance: VMInstance,
        components: List[str],
        trigger_reason: str,
        metadata: Optional[Dict] = None,
    ):
        """
        Safely record VM creation in cost tracker with circuit breaker.
        
        This is isolated so cost tracker failures don't affect VM creation.
        """
        if not self.cost_tracker:
            logger.debug("Cost tracker not available - skipping creation record")
            return

        circuit = self._circuit_breakers["cost_tracker"]
        can_execute, _ = circuit.can_execute()
        
        if not can_execute:
            logger.debug("Cost tracker circuit open - skipping creation record")
            return

        try:
            # Try the new method name first, fall back to old name
            if hasattr(self.cost_tracker, 'record_vm_creation'):
                await self.cost_tracker.record_vm_creation(
                    instance_id=vm_instance.instance_id,
                    vm_type=self.config.machine_type,
                    region=self.config.region,
                    zone=self.config.zone,
                    components=components,
                    trigger_reason=trigger_reason,
                    metadata=metadata or {},
                )
            elif hasattr(self.cost_tracker, 'record_vm_created'):
                await self.cost_tracker.record_vm_created(
                    instance_id=vm_instance.instance_id,
                    components=components,
                    trigger_reason=trigger_reason,
                    metadata=metadata or {},
                )
            else:
                logger.warning("Cost tracker has no VM creation recording method")
                return
            
            circuit.record_success()
            logger.debug(f"ðŸ’° VM creation recorded in cost tracker: {vm_instance.instance_id}")
            
        except Exception as e:
            circuit.record_failure(e)
            logger.warning(f"âš ï¸  Failed to record VM creation in cost tracker (non-critical): {e}")

    def _build_instance_config(
        self, vm_name: str, components: List[str], trigger_reason: str, metadata: Dict
    ) -> InstanceType:
        """
        Build GCP Instance configuration.
        
        v1.0.0: Added support for container-based deployment with pre-baked ML dependencies.
        
        When self.config.use_container is True and a valid container_image is set,
        uses Container-Optimized OS with Docker to eliminate the 5-8 minute ml_deps
        installation phase. Falls back to startup script if container deployment fails.
        """

        # Machine type URL
        machine_type_url = f"zones/{self.config.zone}/machineTypes/{self.config.machine_type}"
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # v224.0: DEPLOYMENT MODE HIERARCHY (highest priority first)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 1. Golden Image - Pre-baked with everything installed (~30-60s startup)
        # 2. Container Mode - Docker with pre-built image (~2-3 min startup)
        # 3. Startup Script - Traditional installation (~10-15 min startup)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # v1.0.0: Determine deployment mode (container vs startup script)
        use_container_mode = (
            self.config.use_container and 
            self.config.container_image and 
            len(self.config.container_image) > 0
        )
        
        # v224.0: Check for golden image (highest priority)
        use_golden_image_mode = False
        golden_image_source = None
        golden_image_disk_size_gb: Optional[int] = None  # Track golden image disk size for VM creation
        
        if self.config.use_golden_image:
            # Check if we have a cached golden image
            now = time.time()
            if (
                self._golden_image_cache is not None 
                and now - self._golden_image_cache_time < self._golden_image_cache_ttl
            ):
                # Use cached golden image info
                golden_image = self._golden_image_cache
                golden_project = self.config.golden_image_project or self.config.project_id
                
                # Check if image is not stale
                if not golden_image.is_stale(self.config.golden_image_max_age_days):
                    use_golden_image_mode = True
                    # Use specific image name (not family) for deterministic behavior
                    golden_image_source = f"projects/{golden_project}/global/images/{golden_image.name}"
                    golden_image_disk_size_gb = golden_image.disk_size_gb  # Track for disk sizing
                    logger.info(f"ðŸŒŸ [GCP] Using cached golden image: {golden_image.name}")
                    logger.info(f"   Age: {golden_image.age_days:.1f} days, Disk: {golden_image_disk_size_gb}GB")
                else:
                    logger.warning(
                        f"âš ï¸ [GCP] Golden image is stale ({golden_image.age_days:.1f} days old). "
                        f"Consider rebuilding with --create-golden-image"
                    )
                    if self.config.golden_image_fallback:
                        logger.info("   Falling back to container/script deployment")
            else:
                # No cached image, but golden image is enabled
                # We'll try to use the image family (GCP auto-selects latest)
                if self.config.golden_image_name:
                    # Explicit image name configured
                    golden_project = self.config.golden_image_project or self.config.project_id
                    golden_image_source = f"projects/{golden_project}/global/images/{self.config.golden_image_name}"
                    use_golden_image_mode = True
                    # Query the actual image to get its disk size
                    try:
                        if hasattr(self, '_images_client') and self._images_client:
                            img_info = self._images_client.get(project=golden_project, image=self.config.golden_image_name)
                            golden_image_disk_size_gb = int(img_info.disk_size_gb) if img_info.disk_size_gb else 100
                            logger.info(f"ðŸŒŸ [GCP] Using configured golden image: {self.config.golden_image_name} ({golden_image_disk_size_gb}GB)")
                        else:
                            # Default to 100GB for golden images (safe minimum)
                            golden_image_disk_size_gb = 100
                            logger.info(f"ðŸŒŸ [GCP] Using configured golden image: {self.config.golden_image_name} (defaulting to {golden_image_disk_size_gb}GB)")
                    except Exception as e:
                        # Default to 100GB for golden images (safe minimum)
                        golden_image_disk_size_gb = 100
                        logger.warning(f"âš ï¸ [GCP] Could not query image size: {e}. Defaulting to {golden_image_disk_size_gb}GB")
                elif self.config.golden_image_family:
                    # Use image family (GCP auto-selects latest)
                    golden_project = self.config.golden_image_project or self.config.project_id
                    golden_image_source = f"projects/{golden_project}/global/images/family/{self.config.golden_image_family}"
                    use_golden_image_mode = True
                    # Query the latest image from family to get its disk size
                    try:
                        if hasattr(self, '_images_client') and self._images_client:
                            img_info = self._images_client.get_from_family(project=golden_project, family=self.config.golden_image_family)
                            golden_image_disk_size_gb = int(img_info.disk_size_gb) if img_info.disk_size_gb else 100
                            logger.info(f"ðŸŒŸ [GCP] Using golden image family: {self.config.golden_image_family} (latest: {img_info.name}, {golden_image_disk_size_gb}GB)")
                        else:
                            # Default to 100GB for golden images (safe minimum)
                            golden_image_disk_size_gb = 100
                            logger.info(f"ðŸŒŸ [GCP] Using golden image family: {self.config.golden_image_family} (defaulting to {golden_image_disk_size_gb}GB)")
                    except Exception as e:
                        # Default to 100GB for golden images (safe minimum)
                        golden_image_disk_size_gb = 100
                        logger.warning(f"âš ï¸ [GCP] Could not query family image size: {e}. Defaulting to {golden_image_disk_size_gb}GB")
        
        # Log deployment mode decision
        if use_golden_image_mode:
            logger.info("ðŸŒŸ [GCP] Deployment mode: GOLDEN IMAGE (fastest)")
        elif use_container_mode:
            logger.info(f"ðŸ³ [GCP] Deployment mode: CONTAINER ({self.config.container_image})")
        else:
            logger.info("ðŸ“œ [GCP] Deployment mode: STARTUP SCRIPT (traditional)")
        
        # v224.0: Choose OS image based on deployment mode hierarchy
        if use_golden_image_mode and golden_image_source:
            # Golden image - everything pre-installed
            source_image = golden_image_source
            logger.debug(f"   Source image: {source_image}")
        elif use_container_mode:
            # Container-Optimized OS for Docker-based deployment
            source_image = f"projects/{self.config.container_os_image_project}/global/images/family/{self.config.container_os_image_family}"
            logger.debug(f"   Container OS: {self.config.container_os_image_family}")
        else:
            # Standard Ubuntu for startup script-based deployment
            source_image = f"projects/{self.config.image_project}/global/images/family/{self.config.image_family}"

        # Boot disk configuration
        # v224.1: Dynamic disk sizing - use larger of configured size or golden image size
        # This prevents "disk size smaller than image size" errors when using golden images
        effective_disk_size_gb = self.config.boot_disk_size_gb
        if use_golden_image_mode and golden_image_disk_size_gb:
            effective_disk_size_gb = max(self.config.boot_disk_size_gb, golden_image_disk_size_gb)
            if effective_disk_size_gb > self.config.boot_disk_size_gb:
                logger.info(
                    f"ðŸ“€ [GCP] Adjusting disk size: {self.config.boot_disk_size_gb}GB â†’ {effective_disk_size_gb}GB "
                    f"(golden image requires {golden_image_disk_size_gb}GB minimum)"
                )
        
        boot_disk = compute_v1.AttachedDisk(
            auto_delete=True,
            boot=True,
            initialize_params=compute_v1.AttachedDiskInitializeParams(
                disk_size_gb=effective_disk_size_gb,
                disk_type=f"zones/{self.config.zone}/diskTypes/{self.config.boot_disk_type}",
                source_image=source_image,
            ),
        )

        # Network interface
        network_interface = compute_v1.NetworkInterface(
            network=f"global/networks/{self.config.network}",
            subnetwork=f"regions/{self.config.region}/subnetworks/{self.config.subnetwork}" if self.config.subnetwork != "default" else None,
            access_configs=[compute_v1.AccessConfig(name="External NAT", type="ONE_TO_ONE_NAT")],
        )

        # Service Account (cloud-platform scope for Secret Manager, Monitoring, Logging)
        service_account = compute_v1.ServiceAccount(
            email="default",
            scopes=["https://www.googleapis.com/auth/cloud-platform"]
        )

        # Metadata
        # v147.0: Added port and repo URL for startup script configuration
        jarvis_port = os.environ.get("JARVIS_PRIME_PORT", "8000")
        jarvis_repo_url = os.environ.get("JARVIS_REPO_URL", "")  # Optional: private repo URL
        
        metadata_items = [
            compute_v1.Items(key="jarvis-components", value=",".join(components)),
            compute_v1.Items(key="jarvis-trigger", value=trigger_reason),
            compute_v1.Items(key="jarvis-created-at", value=datetime.now().isoformat()),
            compute_v1.Items(key="jarvis-port", value=jarvis_port),  # v147.0: Port for health checks
        ]
        
        # v147.0: Add repo URL if configured (for private repos)
        if jarvis_repo_url:
            metadata_items.append(compute_v1.Items(key="jarvis-repo-url", value=jarvis_repo_url))

        # v224.0: Deployment mode handling (golden image, container, or startup script)
        if use_golden_image_mode and golden_image_source:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # GOLDEN IMAGE DEPLOYMENT (Fastest - Everything Pre-installed)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Uses a custom VM image with everything pre-baked:
            #   - Python environment with all ML dependencies
            #   - JARVIS-Prime codebase
            #   - Model files already downloaded
            #   - Systemd service configured for auto-start
            #
            # Benefits:
            #   - Startup time: ~30-60 seconds instead of 10-15 minutes!
            #   - No network downloads during startup
            #   - Consistent, tested environment
            #   - Instant model availability
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # Mark as golden image deployment
            metadata_items.append(
                compute_v1.Items(key="jarvis-deployment-mode", value="golden-image")
            )
            metadata_items.append(
                compute_v1.Items(key="jarvis-golden-image-source", value=golden_image_source)
            )
            
            # Golden images have minimal startup script - just start the service
            golden_startup_script = '''#!/bin/bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# v224.0: Golden Image Startup - Fast boot from pre-baked image
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# This startup script runs on VMs created from golden images.
# Since everything is pre-installed, we just need to start the service.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LOG_FILE="/var/log/jarvis-golden-startup.log"

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log "GOLDEN IMAGE STARTUP - Pre-baked environment detected"
log "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# Source environment if exists
if [ -f /opt/jarvis-prime/.env ]; then
    set -a
    source /opt/jarvis-prime/.env
    set +a
    log "Environment loaded from /opt/jarvis-prime/.env"
fi

# Start the JARVIS-Prime service
log "Starting JARVIS-Prime service..."

if systemctl is-enabled jarvis-prime.service 2>/dev/null; then
    # Use systemd service if available
    systemctl start jarvis-prime.service
    log "Started via systemd"
else
    # Fallback: start directly
    cd /opt/jarvis-prime
    source venv/bin/activate
    nohup python -m jarvis_prime.server > /var/log/jarvis-prime.log 2>&1 &
    log "Started directly (systemd not available)"
fi

log "Golden image startup complete - service should be ready in ~30 seconds"
'''
            metadata_items.append(
                compute_v1.Items(key="startup-script", value=golden_startup_script)
            )
            
            logger.info("   ðŸŒŸ Golden image deployment configured")
            logger.info(f"   ðŸ“€ Image: {golden_image_source}")
            logger.info("   âš¡ Expected startup: ~30-60 seconds")
            
        elif use_container_mode:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # CONTAINER-BASED DEPLOYMENT (Pre-baked ML Dependencies)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Uses Container-Optimized OS with a Docker image that has all ML
            # dependencies pre-installed. This eliminates Phase 3 (ml_deps)
            # which normally takes 5-8 minutes.
            #
            # Benefits:
            #   - Startup time: ~2-3 min instead of ~8-10 min
            #   - Consistent environment across all VMs
            #   - Reduced network dependencies during startup
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # Parse container environment variables
            container_env = {}
            for pair in self.config.container_env_vars.split(","):
                if "=" in pair:
                    key, value = pair.split("=", 1)
                    container_env[key.strip()] = value.strip()
            
            # Add JARVIS-specific environment variables
            container_env["JARVIS_PORT"] = jarvis_port
            container_env["JARVIS_COMPONENTS"] = ",".join(components)
            
            # Build container manifest for Container-Optimized OS
            # This uses the konlet (Container-Optimized OS Container Agent)
            container_manifest = self._build_container_manifest(
                image=self.config.container_image,
                port=int(jarvis_port),
                env_vars=container_env,
            )
            
            # Add container declaration to metadata
            metadata_items.append(
                compute_v1.Items(key="gce-container-declaration", value=container_manifest)
            )
            
            # Mark as container-based deployment
            metadata_items.append(
                compute_v1.Items(key="jarvis-deployment-mode", value="container")
            )
            metadata_items.append(
                compute_v1.Items(key="jarvis-container-image", value=self.config.container_image)
            )
            
            logger.info(f"   ðŸ³ Container deployment configured")
            logger.info(f"   ðŸ“¦ Image: {self.config.container_image}")
            logger.info(f"   ðŸ”§ Env vars: {len(container_env)} configured")
            
        else:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # STARTUP SCRIPT DEPLOYMENT (Traditional)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Uses Ubuntu with startup script for ML dependency installation.
            # This is the fallback when container mode is not enabled.
            #
            # Note: The startup script now includes smart skip logic that
            # detects pre-installed packages, so it will still be faster if
            # the VM image has packages cached.
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # Add startup script
            # v149.2: CRITICAL FIX - Do NOT inject auto-shutdown!
            # The startup script (gcp_vm_startup.sh) is designed for LONG-RUNNING services:
            # - Phase 1: Starts health endpoint quickly
            # - Phase 2: Runs in BACKGROUND (nohup ... &)
            # - Main script exits, but Phase 2 keeps running
            #
            # If we inject "shutdown -h now", it runs IMMEDIATELY after Phase 1,
            # killing the VM before Phase 2 (the real inference server) starts.
            # This was the ROOT CAUSE of "GCP VM unreachable" after 90s timeout.
            #
            # The VM's lifecycle is controlled by:
            # 1. max_run_duration in Spot VM config (hard GCP limit)
            # 2. Explicit terminate_vm() calls from JARVIS
            # 3. GCP preemption (for Spot VMs)
            if self.config.startup_script_path and os.path.exists(self.config.startup_script_path):
                with open(self.config.startup_script_path, "r") as f:
                    startup_script = f.read()
                
                # v149.2: REMOVED auto-shutdown injection - this killed VMs prematurely
                # The VM will be managed by max_run_duration and explicit termination calls
                    
                metadata_items.append(compute_v1.Items(key="startup-script", value=startup_script))
            
            # Mark as startup script deployment
            metadata_items.append(
                compute_v1.Items(key="jarvis-deployment-mode", value="startup-script")
            )

        # Build instance
        instance = compute_v1.Instance(
            name=vm_name,
            machine_type=machine_type_url,
            disks=[boot_disk],
            network_interfaces=[network_interface],
            service_accounts=[service_account],
            metadata=compute_v1.Metadata(items=metadata_items),
            tags=compute_v1.Tags(items=["jarvis", "backend", "spot-vm", "jarvis-node"]),
            labels={"created-by": "jarvis", "type": "backend", "vm-class": "spot"},
        )

        # Configure as Spot VM with Hard Duration Limit
        # This is the "Dead Man's Switch": GCP kills the VM after max_run_duration seconds
        # even if the local script crashes or loses connectivity.
        if self.config.use_spot:
            # Calculate duration in seconds (default 3 hours = 10800s)
            max_duration_seconds = int(self.config.max_vm_lifetime_hours * 3600)
            
            # Ensure duration is valid (must be between 60s and 604800s for Spot)
            max_duration_seconds = max(60, min(max_duration_seconds, 604800))
            
            instance.scheduling = compute_v1.Scheduling(
                preemptible=True,
                on_host_maintenance="TERMINATE",
                automatic_restart=False,
                provisioning_model="SPOT",
                instance_termination_action="DELETE",
                max_run_duration=compute_v1.Duration(seconds=max_duration_seconds)
            )

        return instance

    def _build_container_manifest(
        self, 
        image: str, 
        port: int = 8000, 
        env_vars: Optional[Dict[str, str]] = None
    ) -> str:
        """
        Build container declaration manifest for Container-Optimized OS.
        
        v1.0.0: Creates YAML manifest for konlet (Container-Optimized OS Container Agent)
        that specifies how to run the Docker container on the VM.
        
        Args:
            image: Docker image URL (e.g., gcr.io/project/image:tag)
            port: Port to expose (default 8000)
            env_vars: Environment variables for the container
            
        Returns:
            YAML string for gce-container-declaration metadata
        """
        env_vars = env_vars or {}
        
        # Build environment variable list
        env_list = [{"name": k, "value": str(v)} for k, v in env_vars.items()]
        
        # Container manifest spec (konlet format)
        manifest = {
            "spec": {
                "containers": [{
                    "name": "jarvis-inference",
                    "image": image,
                    "env": env_list,
                    "stdin": False,
                    "tty": False,
                    "ports": [{
                        "containerPort": port,
                        "hostPort": port,
                        "protocol": "TCP",
                    }],
                    "securityContext": {
                        "privileged": False,
                    },
                    "volumeMounts": [{
                        "name": "jarvis-data",
                        "mountPath": "/opt/jarvis-prime/data",
                        "readOnly": False,
                    }],
                }],
                "volumes": [{
                    "name": "jarvis-data",
                    "emptyDir": {},
                }],
                "restartPolicy": "Always",
            },
        }
        
        # Convert to YAML
        return yaml.dump(manifest, default_flow_style=False)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # v224.0: GOLDEN IMAGE MANAGEMENT METHODS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def get_golden_image_builder(self) -> GoldenImageBuilder:
        """
        Get or create the golden image builder instance.
        
        Returns:
            GoldenImageBuilder instance for this manager
        """
        if self._golden_image_builder is None:
            self._golden_image_builder = GoldenImageBuilder(
                config=self.config,
                logger=logger,
            )
        return self._golden_image_builder
    
    async def check_golden_image_availability(self) -> Dict[str, Any]:
        """
        Check if a golden image is available and suitable for use.
        
        This method checks:
        1. If golden image deployment is enabled
        2. If a golden image exists in the configured family
        3. If the image is not stale
        
        Returns:
            Dict with availability status:
            {
                "available": bool,
                "enabled": bool,
                "image_info": Optional[GoldenImageInfo],
                "is_stale": bool,
                "recommendation": str,
                "message": str,
            }
        """
        result = {
            "available": False,
            "enabled": self.config.use_golden_image,
            "image_info": None,
            "is_stale": False,
            "recommendation": "DISABLED",
            "message": "",
        }
        
        if not self.config.use_golden_image:
            result["message"] = "Golden image deployment is disabled. Enable with JARVIS_GCP_USE_GOLDEN_IMAGE=true"
            return result
        
        builder = self.get_golden_image_builder()
        status = await builder.check_golden_image_status()
        
        result["available"] = status["available"]
        result["image_info"] = status.get("latest_image")
        result["is_stale"] = status.get("is_stale", False)
        result["recommendation"] = status["recommendation"]
        result["message"] = status["message"]
        
        # Update cache if we got a valid image
        if status["available"] and status.get("latest_image"):
            self._golden_image_cache = status["latest_image"]
            self._golden_image_cache_time = time.time()
        
        return result
    
    async def create_golden_image(
        self,
        progress_callback: Optional[Callable[[int, str], None]] = None,
    ) -> Tuple[bool, str, Optional[GoldenImageInfo]]:
        """
        Create a new golden image with everything pre-installed.
        
        This is a long-running operation (10-20 minutes) that creates a
        VM, installs everything, and creates an image from it.
        
        Args:
            progress_callback: Optional callback(progress_pct, message)
            
        Returns:
            Tuple of (success, message, GoldenImageInfo or None)
        """
        builder = self.get_golden_image_builder()
        success, message, image_info = await builder.create_golden_image(
            progress_callback=progress_callback
        )
        
        # Update cache on success
        if success and image_info:
            self._golden_image_cache = image_info
            self._golden_image_cache_time = time.time()
        
        return success, message, image_info
    
    async def list_golden_images(self) -> List[GoldenImageInfo]:
        """
        List all available golden images.
        
        Returns:
            List of GoldenImageInfo sorted by creation time (newest first)
        """
        builder = self.get_golden_image_builder()
        return await builder.list_golden_images()
    
    async def cleanup_old_golden_images(self, keep_count: int = 3) -> Tuple[int, List[str]]:
        """
        Clean up old golden images, keeping only the most recent ones.
        
        Args:
            keep_count: Number of recent images to keep (default: 3)
            
        Returns:
            Tuple of (deleted_count, list of deleted image names)
        """
        builder = self.get_golden_image_builder()
        return await builder.cleanup_old_images(keep_count=keep_count)
    
    def get_golden_image_status_summary(self) -> Dict[str, Any]:
        """
        Get a quick summary of golden image status from cache.
        
        This is a synchronous method that returns cached data.
        For fresh data, use check_golden_image_availability().
        
        Returns:
            Dict with status summary
        """
        return {
            "enabled": self.config.use_golden_image,
            "cached_image": self._golden_image_cache.name if self._golden_image_cache else None,
            "cache_age_seconds": time.time() - self._golden_image_cache_time if self._golden_image_cache else None,
            "cache_valid": (
                self._golden_image_cache is not None 
                and time.time() - self._golden_image_cache_time < self._golden_image_cache_ttl
            ),
            "image_family": self.config.golden_image_family,
            "max_age_days": self.config.golden_image_max_age_days,
            "auto_rebuild": self.config.golden_image_auto_rebuild,
            "fallback_enabled": self.config.golden_image_fallback,
        }

    async def _wait_for_operation(self, operation, timeout: int = 300):
        """
        Wait for a GCP zone operation to complete.
        
        Uses the ZoneOperationsClient to poll operation status correctly.
        This fixes the 'Unknown field for Instance: target_link' error.
        """
        start_time = time.time()
        operation_name = operation.name
        
        logger.debug(f"Waiting for operation: {operation_name}")

        while True:
            elapsed = time.time() - start_time
            if elapsed > timeout:
                raise TimeoutError(f"Operation '{operation_name}' timed out after {timeout}s")

            # Check operation status
            if operation.status == compute_v1.Operation.Status.DONE:
                if operation.error:
                    errors = [error.message for error in operation.error.errors]
                    raise Exception(f"Operation failed: {', '.join(errors)}")
                logger.debug(f"Operation '{operation_name}' completed successfully")
                return

            await asyncio.sleep(2)

            # Refresh operation status using ZoneOperationsClient (correct approach)
            try:
                operation = await asyncio.to_thread(
                    self.zone_operations_client.get,
                    project=self.config.project_id,
                    zone=self.config.zone,
                    operation=operation_name,
                )
            except Exception as e:
                logger.warning(f"Error polling operation status: {e}")
                # Continue waiting - operation might still complete
                await asyncio.sleep(2)

    async def terminate_vm(self, vm_name: str, reason: str = "Manual termination") -> bool:
        """
        v134.0: Terminate a VM instance with existence verification and circuit breaker protection.

        ROOT CAUSE FIX for 404 NotFound errors:
        This method now verifies VM existence in GCP before attempting delete operations.
        This prevents spurious 404 errors that occur when:
        1. Spot VMs are preempted by GCP (can happen at any time)
        2. VMs are deleted manually via GCP Console or gcloud CLI
        3. VMs are deleted by another process or previous session
        4. Stale entries exist in managed_vms from previous runs

        Args:
            vm_name: Name of the VM to terminate
            reason: Reason for termination (for logging/tracking)

        Returns:
            True if terminated successfully (or VM doesn't exist), False otherwise
        """
        async with self._vm_lock:
            if vm_name not in self.managed_vms:
                logger.warning(f"âš ï¸  VM not found in managed VMs: {vm_name}")
                # Try to delete anyway in case it exists in GCP but not tracked
                return await self._force_delete_vm(vm_name, reason)

            vm = self.managed_vms[vm_name]

        # Check circuit breaker
        circuit = self._circuit_breakers["vm_delete"]
        can_execute, circuit_reason = circuit.can_execute()

        if not can_execute:
            logger.warning(f"ðŸ”Œ VM termination blocked by circuit breaker: {circuit_reason}")
            self.stats["circuit_breaks"] += 1
            return False

        # v134.0: ROOT CAUSE FIX - Check if VM actually exists in GCP first
        # This prevents 404 errors that would trip the circuit breaker
        exists, gcp_status = await self._check_vm_exists_in_gcp(vm_name)

        if not exists:
            logger.info(
                f"âœ… VM '{vm_name}' does not exist in GCP - cleaning up local tracking "
                f"(may have been preempted, manually deleted, or never created)"
            )
            # Update local tracking to reflect reality
            async with self._vm_lock:
                vm.state = VMState.TERMINATED
                self.stats["total_terminated"] += 1
                self.stats["current_active"] = max(0, self.stats["current_active"] - 1)
                self.stats["total_cost"] += vm.total_cost
                if vm_name in self.managed_vms:
                    del self.managed_vms[vm_name]
            # Record success - VM is in desired state (terminated/non-existent)
            circuit.record_success()
            return True

        # v134.0: Check if VM is already in a terminal state
        if gcp_status in ("TERMINATED", "STOPPING"):
            logger.info(
                f"â„¹ï¸  VM '{vm_name}' is already {gcp_status} - cleaning up tracking"
            )
            async with self._vm_lock:
                vm.state = VMState.TERMINATED
                self.stats["total_terminated"] += 1
                self.stats["current_active"] = max(0, self.stats["current_active"] - 1)
                self.stats["total_cost"] += vm.total_cost
                if vm_name in self.managed_vms:
                    del self.managed_vms[vm_name]
            circuit.record_success()
            return True

        logger.info(f"ðŸ›‘ Terminating VM: {vm_name} (Reason: {reason}, GCP Status: {gcp_status})")

        try:
            # Update cost before termination
            vm.update_cost()

            # Record termination in cost tracker (isolated)
            await self._record_vm_termination_safe(vm, reason)

            # Delete the VM
            operation = await asyncio.to_thread(
                self.instances_client.delete,
                project=self.config.project_id,
                zone=self.config.zone,
                instance=vm_name,
            )

            await self._wait_for_operation(operation)

            # Update tracking with lock protection
            async with self._vm_lock:
                vm.state = VMState.TERMINATED
                self.stats["total_terminated"] += 1
                self.stats["current_active"] = max(0, self.stats["current_active"] - 1)
                self.stats["total_cost"] += vm.total_cost

                # Remove from managed VMs
                if vm_name in self.managed_vms:
                    del self.managed_vms[vm_name]

            # Circuit breaker success
            circuit.record_success()

            logger.info(f"âœ… VM terminated: {vm_name}")
            logger.info(f"   Uptime: {vm.uptime_hours:.2f}h")
            logger.info(f"   Cost: ${vm.total_cost:.4f}")

            return True

        except Exception as e:
            error_str = str(e).lower()
            # v134.0: Handle 404 gracefully if VM was deleted between our check and delete
            is_not_found = (
                "404" in error_str or
                "not found" in error_str or
                "notfound" in error_str
            )

            if is_not_found:
                logger.info(
                    f"âœ… VM '{vm_name}' was deleted between existence check and delete call "
                    f"(likely preempted or deleted by another process)"
                )
                # Clean up tracking - VM is in desired state
                async with self._vm_lock:
                    vm.state = VMState.TERMINATED
                    self.stats["total_terminated"] += 1
                    self.stats["current_active"] = max(0, self.stats["current_active"] - 1)
                    self.stats["total_cost"] += vm.total_cost
                    if vm_name in self.managed_vms:
                        del self.managed_vms[vm_name]
                circuit.record_success()  # Desired state achieved
                return True

            # Actual error - record failure
            circuit.record_failure(e)
            log_component_failure(
                "gcp-vm",
                f"Failed to terminate VM {vm_name}",
                error=e,
                vm_name=vm_name,
            )
            return False

    async def _check_vm_exists_in_gcp(self, vm_name: str) -> Tuple[bool, Optional[str]]:
        """
        v134.0: Check if a VM actually exists in GCP before attempting operations.

        ROOT CAUSE FIX for 404 errors during VM termination:
        The 404 NotFound error occurs when we try to delete a VM that:
        1. Was preempted by GCP (Spot VMs can be reclaimed at any time)
        2. Was deleted manually via GCP Console or gcloud CLI
        3. Was deleted by another process or previous session
        4. Never existed (stale entry in managed_vms from previous run)

        This method queries GCP to verify VM existence before delete operations.

        Args:
            vm_name: Name of the VM to check

        Returns:
            Tuple of (exists: bool, status: Optional[str])
            - If VM exists: (True, current_status)
            - If VM doesn't exist: (False, None)
            - On error: (False, None) with logged warning
        """
        try:
            instance = await asyncio.to_thread(
                self.instances_client.get,
                project=self.config.project_id,
                zone=self.config.zone,
                instance=vm_name,
            )
            return (True, instance.status)

        except Exception as e:
            error_str = str(e).lower()
            # Check for 404 NotFound errors (various formats from GCP client)
            is_not_found = (
                "404" in error_str or
                "not found" in error_str or
                "notfound" in error_str or
                "does not exist" in error_str or
                "was not found" in error_str
            )

            if is_not_found:
                logger.debug(f"[VMExists] VM '{vm_name}' does not exist in GCP")
                return (False, None)

            # Other errors - log but treat as not existing to be safe
            logger.warning(
                f"[VMExists] Error checking VM '{vm_name}' existence: {e}. "
                f"Treating as non-existent to prevent 404 on delete."
            )
            return (False, None)

    async def _force_delete_vm(self, vm_name: str, reason: str) -> bool:
        """
        v134.0: Force delete a VM that may exist in GCP but not in our tracking.

        ROOT CAUSE FIX: Now checks if VM exists before attempting delete to
        prevent 404 NotFound errors from cluttering logs and circuit breaker.
        """
        # v134.0: Check if VM actually exists in GCP first
        exists, status = await self._check_vm_exists_in_gcp(vm_name)

        if not exists:
            logger.info(
                f"âœ… VM '{vm_name}' does not exist in GCP - nothing to delete "
                f"(may have been preempted, manually deleted, or never created)"
            )
            return True  # Return True because the desired state (VM deleted) is achieved

        try:
            logger.info(f"ðŸ—‘ï¸  Force-deleting untracked VM: {vm_name} (status: {status})")

            operation = await asyncio.to_thread(
                self.instances_client.delete,
                project=self.config.project_id,
                zone=self.config.zone,
                instance=vm_name,
            )

            await self._wait_for_operation(operation)
            logger.info(f"âœ… Force-deleted VM: {vm_name}")
            return True

        except Exception as e:
            error_str = str(e).lower()
            # Handle race condition: VM was deleted between our check and delete call
            is_not_found = (
                "404" in error_str or
                "not found" in error_str or
                "notfound" in error_str
            )

            if is_not_found:
                logger.info(
                    f"âœ… VM '{vm_name}' was deleted between existence check and delete call "
                    f"(likely preempted or deleted by another process)"
                )
                return True  # Desired state achieved

            logger.warning(f"âš ï¸  Force delete failed for {vm_name}: {e}")
            return False

    async def _record_vm_termination_safe(self, vm: VMInstance, reason: str):
        """
        Safely record VM termination in cost tracker with circuit breaker.
        
        This is isolated so cost tracker failures don't affect VM termination.
        """
        if not self.cost_tracker:
            logger.debug("Cost tracker not available - skipping termination record")
            return

        circuit = self._circuit_breakers["cost_tracker"]
        can_execute, _ = circuit.can_execute()
        
        if not can_execute:
            logger.debug("Cost tracker circuit open - skipping termination record")
            return

        try:
            # Try the new method name first, fall back to old name
            if hasattr(self.cost_tracker, 'record_vm_termination'):
                await self.cost_tracker.record_vm_termination(
                    instance_id=vm.instance_id,
                    reason=reason,
                    total_cost=vm.total_cost,
                )
            elif hasattr(self.cost_tracker, 'record_vm_deleted'):
                await self.cost_tracker.record_vm_deleted(
                    instance_id=vm.instance_id,
                    was_orphaned=False,
                    actual_cost=vm.total_cost,
                )
            else:
                logger.warning("Cost tracker has no VM termination recording method")
                return
            
            circuit.record_success()
            logger.debug(f"ðŸ’° VM termination recorded in cost tracker: {vm.instance_id}")
            
        except Exception as e:
            circuit.record_failure(e)
            logger.warning(f"âš ï¸  Failed to record VM termination in cost tracker (non-critical): {e}")

    async def _monitoring_loop(self):
        """
        Background monitoring loop for VM health, lifecycle, and intelligent cost-cutting.

        Features:
        - Health monitoring
        - Cost tracking and efficiency scoring
        - Idle VM detection and auto-termination
        - Memory pressure monitoring (terminate when local RAM normalizes)
        - Detailed metrics collection (CPU, memory, network, disk)

        v93.6: Enhanced with graceful shutdown handling - uses short sleep intervals
        to check shutdown state frequently, preventing "Task was destroyed" errors.
        """
        logger.info("ðŸ” VM monitoring loop started (intelligent cost-cutting enabled)")
        self.is_monitoring = True

        # v93.6: Import shutdown event for graceful termination
        try:
            from backend.core.async_safety import get_shutdown_event
            shutdown_event = get_shutdown_event()
        except ImportError:
            shutdown_event = None

        while self.is_monitoring:
            try:
                # v93.6: Use short sleep intervals (1s) to check shutdown state frequently
                # This prevents the "Task was destroyed but it is pending" error
                sleep_remaining = self.config.health_check_interval
                while sleep_remaining > 0 and self.is_monitoring:
                    # Check shutdown event if available
                    if shutdown_event and shutdown_event.is_set():
                        logger.info("ðŸ” VM monitoring loop: shutdown event detected")
                        self.is_monitoring = False
                        return

                    await asyncio.sleep(min(1.0, sleep_remaining))
                    sleep_remaining -= 1.0

                # Check if we should exit after sleep
                if not self.is_monitoring:
                    break

                for vm_name, vm in list(self.managed_vms.items()):
                    # Update cost and efficiency
                    vm.update_cost()
                    vm.update_efficiency_score()

                    # === INTELLIGENT COST-CUTTING CHECKS ===

                    # 1. Check VM lifetime (hard limit)
                    if vm.uptime_hours >= self.config.max_vm_lifetime_hours:
                        logger.info(
                            f"â° VM {vm_name} exceeded max lifetime ({self.config.max_vm_lifetime_hours}h)"
                        )
                        await self.terminate_vm(vm_name, reason="Max lifetime exceeded")
                        continue

                    # 2. Check if VM is wasting money (idle + low efficiency)
                    idle_limit = float(self.config.idle_timeout_minutes)
                    is_idle = vm.idle_time_minutes > idle_limit
                    is_wasting_money = is_idle and (vm.cost_efficiency_score < 30.0)

                    if is_wasting_money:
                        logger.warning(
                            f"ðŸ’° VM {vm_name} is wasting money: "
                            f"idle for {vm.idle_time_minutes:.1f}m, "
                            f"efficiency score: {vm.cost_efficiency_score:.1f}%"
                        )
                        await self.terminate_vm(
                            vm_name,
                            reason=(
                                f"Cost waste: idle {vm.idle_time_minutes:.1f}m "
                                f"(limit {idle_limit:.0f}m), efficiency {vm.cost_efficiency_score:.1f}%"
                            ),
                        )
                        continue

                    # 3. Check if local memory pressure normalized (VM no longer needed)
                    try:
                        import psutil
                        local_mem_percent = psutil.virtual_memory().percent

                        # If local RAM dropped below 70%, VM is no longer needed
                        if local_mem_percent < 70:
                            logger.info(
                                f"ðŸ“‰ Local RAM normalized ({local_mem_percent:.1f}%) - "
                                f"VM {vm_name} no longer needed"
                            )
                            await self.terminate_vm(
                                vm_name,
                                reason=f"Memory pressure resolved (local RAM: {local_mem_percent:.1f}%)"
                            )
                            continue
                    except Exception as mem_check_error:
                        logger.debug(f"Could not check local memory: {mem_check_error}")

                    # === DETAILED METRICS COLLECTION (GCP Console-like) ===
                    try:
                        # Get instance details from GCP
                        instance = await asyncio.to_thread(
                            self.instances_client.get,
                            project=self.config.project_id,
                            zone=self.config.zone,
                            instance=vm_name,
                        )

                        # Update VM state
                        status_map = {
                            "PROVISIONING": VMState.PROVISIONING,
                            "STAGING": VMState.STAGING,
                            "RUNNING": VMState.RUNNING,
                            "STOPPING": VMState.STOPPING,
                            "TERMINATED": VMState.TERMINATED,
                        }
                        vm.state = status_map.get(instance.status, VMState.UNKNOWN)

                        # Collect CPU metrics (from GCP Monitoring API if available)
                        # For now, we'll use placeholder - in production, integrate with GCP Monitoring API
                        # TODO: Integrate with google-cloud-monitoring for real CPU/network/disk metrics

                        # Estimate metrics based on VM activity
                        if vm.component_usage_count > 0:
                            # Active VM - higher resource usage
                            vm.cpu_percent = min(80.0, 30.0 + (vm.component_usage_count % 50))
                            vm.memory_used_gb = min(28.0, 10.0 + (vm.component_usage_count % 18))
                        else:
                            # Idle VM - minimal resources
                            vm.cpu_percent = 5.0
                            vm.memory_used_gb = 2.0

                        # v132.2: Smart health status - distinguish expected from unexpected states
                        is_healthy = vm.state == VMState.RUNNING
                        vm.last_health_check = time.time()
                        vm.health_status = "healthy" if is_healthy else "unhealthy"

                        # v132.2: Only warn for truly unexpected states, not shutdown transitions
                        expected_shutdown_states = {VMState.STOPPING, VMState.TERMINATED}
                        expected_startup_states = {VMState.CREATING, VMState.PROVISIONING, VMState.STAGING}

                        if not is_healthy:
                            if vm.state in expected_shutdown_states:
                                # Expected during shutdown - debug level only
                                logger.debug(f"VM {vm_name} in expected shutdown state: {vm.state.value}")
                            elif vm.state in expected_startup_states:
                                # Expected during startup - info level
                                logger.info(f"VM {vm_name} starting up: {vm.state.value}")
                            elif vm.state == VMState.FAILED:
                                # Actual failure - warning
                                logger.warning(f"âš ï¸  VM {vm_name} health check failed (state: {vm.state.value})")
                            else:
                                # Unknown state - warning
                                logger.warning(f"âš ï¸  VM {vm_name} in unexpected state: {vm.state.value}")

                        # v137.2: Intelligent efficiency warnings with grace period and rate limiting
                        # - Skip during VM startup grace period (first 5 minutes)
                        # - Rate limit warnings to once every 5 minutes per VM
                        # - Only warn for RUNNING VMs (not during shutdown)
                        if vm.state == VMState.RUNNING and vm.cost_efficiency_score < 50:
                            vm_startup_minutes = vm.uptime_hours * 60
                            startup_grace_minutes = 5.0  # Don't warn during first 5 minutes
                            
                            # Initialize rate limiting tracker if needed
                            if not hasattr(self, '_efficiency_warning_times'):
                                self._efficiency_warning_times: Dict[str, float] = {}
                            
                            last_warning = self._efficiency_warning_times.get(vm_name, 0)
                            warning_interval = 300  # 5 minutes between warnings
                            
                            should_warn = (
                                vm_startup_minutes > startup_grace_minutes and  # Past grace period
                                time.time() - last_warning > warning_interval  # Rate limited
                            )
                            
                            if should_warn:
                                logger.warning(
                                    f"âš ï¸  VM {vm_name} low efficiency: {vm.cost_efficiency_score:.1f}% "
                                    f"(idle: {vm.idle_time_minutes:.1f}m, uptime: {vm.uptime_hours:.1f}h)"
                                )
                                self._efficiency_warning_times[vm_name] = time.time()

                    except Exception as metrics_error:
                        logger.debug(f"Could not collect metrics for {vm_name}: {metrics_error}")

            except asyncio.CancelledError:
                # v93.6: Graceful shutdown - don't log as error
                logger.info("ðŸ” VM monitoring loop cancelled (graceful shutdown)")
                self.is_monitoring = False
                return
            except Exception as e:
                if not self.is_monitoring:
                    # Shutdown in progress, don't log error
                    break
                log_component_failure(
                    "gcp-vm",
                    "Error in monitoring loop",
                    error=e,
                )

        # v93.6: Clean exit logging
        logger.info("ðŸ” VM monitoring loop stopped")

    async def _health_check_vm(self, vm_name: str) -> bool:
        """Perform health check on a VM"""
        if vm_name not in self.managed_vms:
            return False

        vm = self.managed_vms[vm_name]

        try:
            # Get instance status from GCP
            instance = await asyncio.to_thread(
                self.instances_client.get,
                project=self.config.project_id,
                zone=self.config.zone,
                instance=vm_name,
            )

            # Update state
            status_map = {
                "PROVISIONING": VMState.PROVISIONING,
                "STAGING": VMState.STAGING,
                "RUNNING": VMState.RUNNING,
                "STOPPING": VMState.STOPPING,
                "TERMINATED": VMState.TERMINATED,
            }
            vm.state = status_map.get(instance.status, VMState.UNKNOWN)

            return vm.state == VMState.RUNNING

        except Exception as e:
            log_component_failure(
                "gcp-vm",
                f"Health check failed for {vm_name}",
                error=e,
                vm_name=vm_name,
            )
            return False

    async def cleanup_all_vms(self, reason: str = "System shutdown"):
        """Terminate all managed VMs with cost summary"""
        if not self.managed_vms:
            logger.info("â„¹ï¸  No VMs to clean up")
            return

        logger.info(f"ðŸ§¹ Cleaning up all VMs: {reason}")

        # Calculate total costs before terminating
        total_session_cost = 0.0
        total_uptime_hours = 0.0
        vm_count = len(self.managed_vms)

        for vm in self.managed_vms.values():
            vm.update_cost()
            total_session_cost += vm.total_cost
            total_uptime_hours += vm.uptime_hours

        # Terminate all VMs
        tasks = []
        for vm_name in list(self.managed_vms.keys()):
            tasks.append(self.terminate_vm(vm_name, reason=reason))

        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)

        # Display cost summary
        logger.info("âœ… All VMs cleaned up")
        logger.info("=" * 60)
        logger.info("ðŸ’° GCP VM COST SUMMARY")
        logger.info("=" * 60)
        logger.info(f"   VMs Terminated:  {vm_count}")
        logger.info(f"   Total Uptime:    {total_uptime_hours:.2f} hours")
        logger.info(f"   Session Cost:    ${total_session_cost:.4f}")
        logger.info(f"   Total Lifetime:  ${self.stats['total_cost']:.4f}")
        logger.info("=" * 60)

    async def cleanup(self):
        """
        Cleanup and shutdown with graceful task termination.

        v93.6: Enhanced with proper timeout handling to prevent hanging during shutdown.
        """
        logger.info("ðŸ§¹ GCP VM Manager cleanup starting...")

        # Signal monitoring loop to stop
        self.is_monitoring = False

        # v93.6: Set global shutdown event if available
        try:
            from backend.core.async_safety import set_shutdown_event
            set_shutdown_event()
        except ImportError:
            pass

        # v93.6: Cancel monitoring task with timeout
        if self.monitoring_task and not self.monitoring_task.done():
            self.monitoring_task.cancel()
            try:
                # Give task 5 seconds to shutdown gracefully
                await asyncio.wait_for(
                    asyncio.shield(self.monitoring_task),
                    timeout=5.0
                )
            except asyncio.TimeoutError:
                logger.warning("âš ï¸ Monitoring task didn't stop in time, forcing...")
            except asyncio.CancelledError:
                pass
            except Exception as e:
                logger.debug(f"Monitoring task cleanup error (non-critical): {e}")

        # Cleanup VMs with timeout protection
        try:
            await asyncio.wait_for(
                self.cleanup_all_vms(reason="Manager shutdown"),
                timeout=30.0
            )
        except asyncio.TimeoutError:
            logger.warning("âš ï¸ VM cleanup timed out after 30s")
        except Exception as e:
            logger.warning(f"âš ï¸ VM cleanup error: {e}")

        self.initialized = False
        logger.info("ðŸ§¹ GCP VM Manager cleaned up")

    def get_stats(self) -> Dict:
        """Get comprehensive manager statistics including circuit breaker status"""
        circuit_status = {}
        for name, circuit in self._circuit_breakers.items():
            circuit_status[name] = {
                "state": circuit.state.value,
                "failure_count": circuit.failure_count,
                "can_execute": circuit.can_execute()[0],
            }
        
        # Calculate VM costs summary
        total_vm_cost = sum(vm.total_cost for vm in self.managed_vms.values())
        
        return {
            **self.stats,
            "managed_vms": len(self.managed_vms),
            "creating_vms": len(self.creating_vms),
            "current_vm_cost": total_vm_cost,
            "circuit_breakers": circuit_status,
            "config": self.config.to_dict(),
            "initialized": self.initialized,
        }

    def get_circuit_breaker_status(self) -> Dict[str, Dict]:
        """Get detailed circuit breaker status"""
        return {
            name: {
                "state": circuit.state.value,
                "failure_count": circuit.failure_count,
                "failure_threshold": circuit.failure_threshold,
                "recovery_timeout": circuit.recovery_timeout,
                "last_failure_time": circuit.last_failure_time,
                "can_execute": circuit.can_execute()[0],
                "reason": circuit.can_execute()[1],
            }
            for name, circuit in self._circuit_breakers.items()
        }

    def reset_circuit_breaker(self, name: str) -> bool:
        """Manually reset a circuit breaker"""
        if name in self._circuit_breakers:
            circuit = self._circuit_breakers[name]
            circuit.state = CircuitState.CLOSED
            circuit.failure_count = 0
            circuit.half_open_calls = 0
            logger.info(f"ðŸ”Œ Circuit breaker '{name}' manually reset")
            return True
        return False

    # =========================================================================
    # INVINCIBLE NODE: Static IP + STOP Termination Support
    # =========================================================================
    # These methods implement the "Invincible Node" strategy where:
    # - A persistent Spot VM with static IP survives preemption in STOPPED state
    # - Fast restart (~30s) instead of full recreation (~3-5 min)
    # - State machine: Ping -> Describe -> Start/Create -> Poll
    # =========================================================================

    async def ensure_static_vm_ready(
        self,
        port: Optional[int] = None,
        timeout: Optional[float] = None,
        progress_callback: Optional[Callable[[int, str, str], None]] = None,
    ) -> Tuple[bool, Optional[str], str]:
        """
        Ensure the static/persistent VM (Invincible Node) is ready for requests.

        This implements the Invincible Node state machine:
        1. PING: Try health endpoint at static IP
        2. If unreachable, DESCRIBE instance via GCP API
        3. Branch:
           - If STOPPED/TERMINATED -> START the VM
           - If NOT_FOUND -> CREATE new VM (Hybrid Fallback)
           - If RUNNING but unhealthy -> POLL until ready
        4. POLL /health until ready

        Args:
            port: Port for health endpoint (default: JARVIS_PRIME_PORT or 8000)
            timeout: Max time to wait for VM to be ready (default: 300s)
            progress_callback: v220.1 - Optional callback(progress_pct, phase, detail) for real-time dashboard updates

        Returns:
            Tuple of (success: bool, ip_address: Optional[str], status_message: str)
        """
        # Get configuration
        static_ip_name = self.config.static_ip_name
        instance_name = self.config.static_instance_name or "jarvis-prime-node"
        target_port = port or int(os.environ.get("JARVIS_PRIME_PORT", "8000"))
        max_timeout = timeout or self.config.static_vm_health_timeout
        poll_interval = self.config.static_vm_health_poll_interval

        # Check if static IP mode is configured
        if not static_ip_name:
            return False, None, "STATIC_IP_NOT_CONFIGURED: Set GCP_VM_STATIC_IP_NAME"

        # Ensure manager is initialized
        if not self.initialized:
            try:
                await self.initialize()
            except Exception as e:
                return False, None, f"INIT_FAILED: {e}"

        # Validate configuration
        is_valid, validation_error = self.config.is_valid_for_vm_operations()
        if not is_valid:
            return False, None, f"CONFIG_INVALID: {validation_error}"

        # Use lock to prevent concurrent start/create operations
        async with self._vm_lock:
            # Step 1: Get static IP address (v210.0: auto-create if missing)
            static_ip = await self._get_static_ip_address(static_ip_name, auto_create=True)
            if not static_ip:
                return False, None, f"STATIC_IP_FAILED: Could not get or create '{static_ip_name}'"

            logger.info(f"â˜ï¸ [InvincibleNode] Ensuring VM ready at {static_ip}:{target_port}")

            # Step 2: Ping health endpoint
            is_healthy, health_status = await self._ping_health_endpoint(
                static_ip, target_port, timeout=10.0
            )

            if is_healthy and health_status.get("ready_for_inference", False):
                logger.info(f"âœ… [InvincibleNode] VM already ready: {static_ip}")
                return True, static_ip, "ALREADY_READY"

            # Step 3: Describe instance to get current state
            instance_status, gcp_error = await self._describe_instance(instance_name)
            logger.info(f"â˜ï¸ [InvincibleNode] Instance status: {instance_status}")

            # Step 4: Branch based on state
            if instance_status == "NOT_FOUND":
                # Hybrid Fallback: Create new VM
                logger.info(f"â˜ï¸ [InvincibleNode] Instance not found - creating new VM (Hybrid Fallback)")
                create_success, create_error = await self._create_static_vm(
                    instance_name, static_ip_name, target_port
                )
                if not create_success:
                    return False, static_ip, f"CREATE_FAILED: {create_error}"

            elif instance_status in ("TERMINATED", "STOPPED", "SUSPENDED"):
                # Start the stopped VM (fast path: ~30s)
                logger.info(f"â˜ï¸ [InvincibleNode] Starting stopped VM: {instance_name}")
                start_success, start_error = await self._start_instance(instance_name)
                if not start_success:
                    return False, static_ip, f"START_FAILED: {start_error}"

            elif instance_status in ("STAGING", "PROVISIONING"):
                # VM is starting up, proceed to poll
                logger.info(f"â˜ï¸ [InvincibleNode] VM is starting: {instance_status}")

            elif instance_status == "RUNNING":
                # VM is running but health check failed
                logger.info(f"â˜ï¸ [InvincibleNode] VM running but not healthy yet")

            elif instance_status == "ERROR":
                return False, static_ip, f"GCP_API_ERROR: {gcp_error}"

            else:
                logger.warning(f"â˜ï¸ [InvincibleNode] Unknown status: {instance_status}")

        # Step 5: Poll health endpoint until ready (outside lock to avoid blocking)
        logger.info(f"â˜ï¸ [InvincibleNode] Polling health endpoint (timeout: {max_timeout}s)...")
        poll_success, final_status = await self._poll_health_until_ready(
            static_ip, target_port, max_timeout, poll_interval,
            progress_callback=progress_callback  # v220.1: Pass through for real-time updates
        )

        if poll_success:
            logger.info(f"âœ… [InvincibleNode] VM ready: {static_ip}")
            return True, static_ip, "READY"
        else:
            return False, static_ip, f"HEALTH_TIMEOUT: {final_status}"

    async def _get_static_ip_address(self, ip_name: str, auto_create: bool = True) -> Optional[str]:
        """
        Get the IP address for a reserved static IP by name.
        
        v210.0: Added auto_create option to automatically create the static IP
        if it doesn't exist. This enables fully automatic Invincible Node setup.
        
        Args:
            ip_name: Name of the static IP address resource
            auto_create: If True, create the static IP if it doesn't exist
            
        Returns:
            The IP address string, or None if not found and auto_create=False
        """
        try:
            # Use gcloud via subprocess for simplicity (addresses API is separate)
            import subprocess
            result = await asyncio.to_thread(
                subprocess.run,
                [
                    "gcloud", "compute", "addresses", "describe", ip_name,
                    "--project", self.config.project_id,
                    "--region", self.config.region,
                    "--format", "value(address)"
                ],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode == 0 and result.stdout.strip():
                return result.stdout.strip()
            
            # v210.0: Auto-create static IP if it doesn't exist
            if auto_create:
                logger.info(f"â˜ï¸ [InvincibleNode] Static IP '{ip_name}' not found - creating...")
                created_ip = await self._create_static_ip(ip_name)
                if created_ip:
                    logger.info(f"âœ… [InvincibleNode] Created static IP: {created_ip}")
                    return created_ip
                else:
                    logger.warning(f"[InvincibleNode] Failed to create static IP '{ip_name}'")
            
            return None
        except Exception as e:
            logger.warning(f"[InvincibleNode] Failed to get static IP: {e}")
            return None
    
    async def _create_static_ip(self, ip_name: str) -> Optional[str]:
        """
        v210.0: Create a new static IP address in GCP.
        
        This enables fully automatic Invincible Node setup without manual
        gcloud commands.
        
        Args:
            ip_name: Name for the new static IP address resource
            
        Returns:
            The allocated IP address string, or None on failure
        """
        try:
            import subprocess
            
            # Create the static IP address
            logger.info(f"â˜ï¸ [InvincibleNode] Creating static IP '{ip_name}' in {self.config.region}...")
            
            create_result = await asyncio.to_thread(
                subprocess.run,
                [
                    "gcloud", "compute", "addresses", "create", ip_name,
                    "--project", self.config.project_id,
                    "--region", self.config.region,
                    "--description", "JARVIS Invincible Node static IP (auto-created)"
                ],
                capture_output=True,
                text=True,
                timeout=60
            )
            
            if create_result.returncode != 0:
                error_msg = create_result.stderr.strip() if create_result.stderr else "Unknown error"
                # Check if it already exists (race condition protection)
                if "already exists" in error_msg.lower():
                    logger.info(f"[InvincibleNode] Static IP already exists (concurrent creation)")
                else:
                    logger.warning(f"[InvincibleNode] Failed to create static IP: {error_msg}")
                    return None
            
            # Wait a moment for GCP to provision the IP
            await asyncio.sleep(2)
            
            # Retrieve the allocated IP address
            describe_result = await asyncio.to_thread(
                subprocess.run,
                [
                    "gcloud", "compute", "addresses", "describe", ip_name,
                    "--project", self.config.project_id,
                    "--region", self.config.region,
                    "--format", "value(address)"
                ],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if describe_result.returncode == 0 and describe_result.stdout.strip():
                return describe_result.stdout.strip()
            
            return None
            
        except Exception as e:
            logger.warning(f"[InvincibleNode] Error creating static IP: {e}")
            return None

    async def _ping_health_endpoint(
        self, ip: str, port: int, timeout: float = 10.0
    ) -> Tuple[bool, Dict[str, Any]]:
        """
        Ping the health endpoint to check if VM is ready.

        Returns:
            Tuple of (is_healthy: bool, health_response: dict)
        """
        import aiohttp
        url = f"http://{ip}:{port}/health"

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=timeout)) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        is_ready = data.get("ready_for_inference", False)
                        return is_ready, data
                    return False, {"status": resp.status}
        except asyncio.TimeoutError:
            return False, {"error": "timeout"}
        except Exception as e:
            return False, {"error": str(e)}

    async def _describe_instance(self, instance_name: str) -> Tuple[str, Optional[str]]:
        """
        Get the current status of an instance from GCP.

        Returns:
            Tuple of (status: str, error: Optional[str])
            Status can be: RUNNING, STOPPED, TERMINATED, STAGING, PROVISIONING,
                          SUSPENDED, STOPPING, NOT_FOUND, ERROR
        """
        try:
            instance = await asyncio.to_thread(
                self.instances_client.get,
                project=self.config.project_id,
                zone=self.config.zone,
                instance=instance_name,
            )
            return instance.status, None
        except Exception as e:
            error_str = str(e).lower()
            if "404" in error_str or "not found" in error_str or "notfound" in error_str:
                return "NOT_FOUND", None
            return "ERROR", str(e)

    async def _start_instance(self, instance_name: str) -> Tuple[bool, Optional[str]]:
        """
        Start a stopped/terminated instance.

        Returns:
            Tuple of (success: bool, error: Optional[str])
        """
        try:
            logger.info(f"â˜ï¸ [InvincibleNode] Sending start command: {instance_name}")

            operation = await asyncio.to_thread(
                self.instances_client.start,
                project=self.config.project_id,
                zone=self.config.zone,
                instance=instance_name,
            )

            # Wait for operation to complete
            await self._wait_for_operation(operation, timeout=120)

            logger.info(f"âœ… [InvincibleNode] VM start command completed: {instance_name}")
            return True, None

        except Exception as e:
            logger.error(f"âŒ [InvincibleNode] Failed to start VM: {e}")
            return False, str(e)

    async def _create_static_vm(
        self, instance_name: str, static_ip_name: str, port: int
    ) -> Tuple[bool, Optional[str]]:
        """
        Create a new VM with static IP (Hybrid Fallback).

        This is called when the VM doesn't exist (NOT_FOUND) and we need
        to create a new one bound to the reserved static IP.

        Returns:
            Tuple of (success: bool, error: Optional[str])
        """
        try:
            logger.info(f"â˜ï¸ [InvincibleNode] Creating VM with static IP: {instance_name}")

            # Get static IP address for binding
            static_ip = await self._get_static_ip_address(static_ip_name)
            if not static_ip:
                return False, f"Static IP '{static_ip_name}' not found"

            # Build instance configuration with static IP
            machine_type_url = f"zones/{self.config.zone}/machineTypes/{self.config.machine_type}"

            # Boot disk
            boot_disk = compute_v1.AttachedDisk(
                auto_delete=True,
                boot=True,
                initialize_params=compute_v1.AttachedDiskInitializeParams(
                    disk_size_gb=self.config.boot_disk_size_gb,
                    disk_type=f"zones/{self.config.zone}/diskTypes/{self.config.boot_disk_type}",
                    source_image=f"projects/{self.config.image_project}/global/images/family/{self.config.image_family}",
                ),
            )

            # Network interface with STATIC IP
            network_interface = compute_v1.NetworkInterface(
                network=f"global/networks/{self.config.network}",
                access_configs=[
                    compute_v1.AccessConfig(
                        name="External NAT",
                        type="ONE_TO_ONE_NAT",
                        nat_i_p=static_ip,  # Bind to reserved static IP
                    )
                ],
            )

            # Service account
            service_account = compute_v1.ServiceAccount(
                email="default",
                scopes=["https://www.googleapis.com/auth/cloud-platform"]
            )

            # Metadata
            metadata_items = [
                compute_v1.Items(key="jarvis-port", value=str(port)),
                compute_v1.Items(key="jarvis-components", value="inference"),
                compute_v1.Items(key="jarvis-trigger", value="invincible_node_hybrid_fallback"),
                compute_v1.Items(key="jarvis-created-at", value=datetime.now().isoformat()),
            ]

            # Add startup script
            if self.config.startup_script_path and os.path.exists(self.config.startup_script_path):
                with open(self.config.startup_script_path, "r") as f:
                    startup_script = f.read()
                metadata_items.append(compute_v1.Items(key="startup-script", value=startup_script))

            # Build instance with STOP termination (Invincible Node)
            instance = compute_v1.Instance(
                name=instance_name,
                machine_type=machine_type_url,
                disks=[boot_disk],
                network_interfaces=[network_interface],
                service_accounts=[service_account],
                metadata=compute_v1.Metadata(items=metadata_items),
                tags=compute_v1.Tags(items=["jarvis-node", "http-server", "https-server"]),
                labels={"created-by": "jarvis", "type": "prime-node", "vm-class": "invincible"},
                scheduling=compute_v1.Scheduling(
                    preemptible=True,
                    on_host_maintenance="TERMINATE",
                    automatic_restart=False,
                    provisioning_model="SPOT",
                    instance_termination_action="STOP",  # INVINCIBLE: Survive preemption
                ),
            )

            # Create the VM
            operation = await asyncio.to_thread(
                self.instances_client.insert,
                project=self.config.project_id,
                zone=self.config.zone,
                instance_resource=instance,
            )

            # Wait for creation
            await self._wait_for_operation(operation, timeout=300)

            logger.info(f"âœ… [InvincibleNode] VM created: {instance_name}")
            return True, None

        except Exception as e:
            logger.error(f"âŒ [InvincibleNode] Failed to create VM: {e}")
            return False, str(e)

    async def _poll_health_until_ready(
        self, ip: str, port: int, timeout: float, poll_interval: float,
        progress_callback: Optional[Callable[[int, str, str], None]] = None,
    ) -> Tuple[bool, str]:
        """
        Poll the health endpoint until the VM reports ready_for_inference=true.
        
        v220.1: Added progress_callback for real-time dashboard updates.

        Args:
            ip: IP address to poll
            port: Port number
            timeout: Max timeout in seconds
            poll_interval: Seconds between polls
            progress_callback: Optional callback(progress_pct, phase, detail) for real-time updates

        Returns:
            Tuple of (success: bool, final_status: str)
        """
        start_time = time.time()
        last_status = "starting"

        while (time.time() - start_time) < timeout:
            elapsed = time.time() - start_time
            is_ready, health_data = await self._ping_health_endpoint(ip, port, timeout=10.0)

            if is_ready:
                # v220.1: Report 100% on success
                if progress_callback:
                    try:
                        progress_callback(100, "ready", f"VM ready at {ip}")
                    except Exception:
                        pass
                return True, "ready_for_inference"

            # Extract progress info for logging and callback
            progress_pct = 0
            phase_name = "starting"
            detail = "Waiting for VM..."
            
            if "apars" in health_data:
                apars = health_data["apars"]
                progress_pct = apars.get("total_progress", 0)
                phase_name = apars.get("phase_name", "unknown")
                eta = apars.get("eta_seconds", 0)
                detail = f"{phase_name} ({progress_pct}%, ETA {eta}s)"
                last_status = f"phase={phase_name}, progress={progress_pct}%, eta={eta}s"
                logger.debug(f"â˜ï¸ [InvincibleNode] Health poll: {last_status}")
            elif "error" in health_data:
                last_status = f"error={health_data['error']}"
                detail = health_data.get("error", "Unknown error")[:40]
            else:
                # Estimate progress based on elapsed time
                progress_pct = min(90, int((elapsed / timeout) * 90) + 20)
                detail = f"Polling health ({int(elapsed)}s elapsed)"
            
            # v220.1: Call progress callback for real-time dashboard updates
            if progress_callback:
                try:
                    progress_callback(progress_pct, phase_name, detail)
                except Exception:
                    pass  # Don't let callback errors break polling

            await asyncio.sleep(poll_interval)

        return False, f"timeout after {timeout}s (last: {last_status})"

    @property
    def is_static_vm_mode(self) -> bool:
        """Check if static VM (Invincible Node) mode is configured."""
        return bool(self.config.static_ip_name)

    # =========================================================================
    # CLOUD MONITOR: CLI Monitoring Suite
    # =========================================================================
    # These methods provide real-time visibility into the Invincible Node
    # for the unified_supervisor.py --monitor CLI mode.
    # =========================================================================

    async def get_invincible_node_status(self) -> Dict[str, Any]:
        """
        Get comprehensive status of the Invincible Node for monitoring dashboard.

        Returns:
            Dict with:
            - instance_name: str
            - zone: str
            - project_id: str
            - static_ip: str or None
            - gcp_status: RUNNING/STOPPED/TERMINATED/NOT_FOUND/ERROR
            - machine_type: str or None
            - uptime_seconds: float or None
            - termination_action: str or None
            - health: dict with HTTP health check results
            - error: str or None
        """
        result = {
            "instance_name": self.config.static_instance_name,
            "zone": self.config.zone,
            "project_id": self.config.project_id,
            "static_ip": None,
            "gcp_status": "UNKNOWN",
            "machine_type": None,
            "uptime_seconds": None,
            "termination_action": None,
            "created_at": None,
            "health": None,
            "error": None,
        }

        if not self.is_static_vm_mode:
            result["error"] = "Invincible Node not configured (GCP_VM_STATIC_IP_NAME not set)"
            return result

        # Get static IP address (read-only check, don't auto-create during status)
        try:
            # v210.0: Use auto_create=False for status checks to avoid side effects
            static_ip = await self._get_static_ip_address(self.config.static_ip_name, auto_create=False)
            if static_ip:
                result["static_ip"] = static_ip
            else:
                # IP doesn't exist yet - will be auto-created during ensure_static_vm_ready
                result["gcp_status"] = "NOT_FOUND"
                result["error"] = None  # Not an error, just needs to be created
        except Exception as e:
            result["error"] = f"Failed to get static IP: {e}"

        # Get instance details from GCP
        try:
            instance = await asyncio.to_thread(
                self.instances_client.get,
                project=self.config.project_id,
                zone=self.config.zone,
                instance=self.config.static_instance_name,
            )

            result["gcp_status"] = instance.status
            result["machine_type"] = instance.machine_type.split("/")[-1] if instance.machine_type else None

            # Extract scheduling info
            if instance.scheduling:
                result["termination_action"] = instance.scheduling.instance_termination_action

            # Calculate uptime if running
            if instance.status == "RUNNING" and instance.last_start_timestamp:
                try:
                    from datetime import datetime as dt
                    # Parse GCP timestamp (RFC3339)
                    start_time = dt.fromisoformat(instance.last_start_timestamp.replace("Z", "+00:00"))
                    result["uptime_seconds"] = (dt.now(start_time.tzinfo) - start_time).total_seconds()
                except Exception:
                    pass

            # Get creation time
            if instance.creation_timestamp:
                result["created_at"] = instance.creation_timestamp

        except Exception as e:
            error_str = str(e).lower()
            if "404" in error_str or "not found" in error_str:
                result["gcp_status"] = "NOT_FOUND"
            else:
                result["gcp_status"] = "ERROR"
                result["error"] = str(e)

        # Perform health check if we have a static IP
        if result["static_ip"]:
            port = int(os.environ.get("JARVIS_PRIME_PORT", "8000"))
            is_healthy, health_data = await self._ping_health_endpoint(
                result["static_ip"], port, timeout=10.0
            )
            result["health"] = {
                "reachable": is_healthy or "error" not in health_data or health_data.get("error") != "timeout",
                "ready_for_inference": health_data.get("ready_for_inference", False),
                "status": health_data.get("status", "unknown"),
                "model_loaded": health_data.get("model_loaded", False),
                "active_model": health_data.get("active_model"),
                "apars": health_data.get("apars"),
                "raw": health_data,
            }

        return result

    def get_ssh_logs_command(
        self,
        log_path: str = "/var/log/jarvis-startup.log",
        lines: int = 50,
        follow: bool = True,
    ) -> List[str]:
        """
        Construct the gcloud SSH command to tail logs from the Invincible Node.

        Args:
            log_path: Path to the log file on the VM
            lines: Number of lines to show initially
            follow: Whether to follow (tail -f) the log

        Returns:
            List of command arguments for subprocess
        """
        instance_name = self.config.static_instance_name
        zone = self.config.zone
        project = self.config.project_id

        tail_cmd = f"tail -n {lines}"
        if follow:
            tail_cmd += " -f"
        tail_cmd += f" {log_path}"

        return [
            "gcloud", "compute", "ssh",
            instance_name,
            f"--zone={zone}",
            f"--project={project}",
            "--command", tail_cmd,
        ]

    async def stream_invincible_node_logs(
        self,
        log_path: str = "/var/log/jarvis-startup.log",
        lines: int = 50,
        callback: Optional[Callable[[str], None]] = None,
    ) -> None:
        """
        Stream logs from the Invincible Node via SSH.

        Args:
            log_path: Path to the log file on the VM
            lines: Number of initial lines to show
            callback: Optional callback for each line (if None, prints to stdout)
        """
        import subprocess

        cmd = self.get_ssh_logs_command(log_path, lines, follow=True)

        logger.info(f"[CloudMonitor] Streaming logs: {' '.join(cmd)}")

        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.STDOUT,
        )

        try:
            while True:
                line = await process.stdout.readline()
                if not line:
                    break
                decoded = line.decode("utf-8", errors="replace").rstrip()
                if callback:
                    callback(decoded)
                else:
                    print(decoded)
        except asyncio.CancelledError:
            process.terminate()
            raise
        finally:
            if process.returncode is None:
                process.terminate()

    async def wake_invincible_node(self) -> Tuple[bool, str]:
        """
        Wake up the Invincible Node if it's stopped.

        Returns:
            Tuple of (success: bool, message: str)
        """
        if not self.is_static_vm_mode:
            return False, "Invincible Node not configured"

        # Get current status
        status, error = await self._describe_instance(self.config.static_instance_name)

        if status == "RUNNING":
            return True, "Node is already running"

        if status in ("STOPPED", "TERMINATED", "SUSPENDED"):
            success, err = await self._start_instance(self.config.static_instance_name)
            if success:
                return True, f"Node started successfully (was {status})"
            return False, f"Failed to start node: {err}"

        if status == "NOT_FOUND":
            return False, "Node does not exist. Run ./deploy_spot_node.sh first"

        return False, f"Node in unexpected state: {status}"

    async def stop_invincible_node(self) -> Tuple[bool, str]:
        """
        Stop the Invincible Node (to save costs).

        Returns:
            Tuple of (success: bool, message: str)
        """
        if not self.is_static_vm_mode:
            return False, "Invincible Node not configured"

        instance_name = self.config.static_instance_name

        # Get current status
        status, error = await self._describe_instance(instance_name)

        if status in ("STOPPED", "TERMINATED", "SUSPENDED"):
            return True, f"Node is already {status}"

        if status == "NOT_FOUND":
            return False, "Node does not exist"

        if status != "RUNNING":
            return False, f"Node in unexpected state: {status}"

        try:
            logger.info(f"[CloudMonitor] Stopping node: {instance_name}")

            operation = await asyncio.to_thread(
                self.instances_client.stop,
                project=self.config.project_id,
                zone=self.config.zone,
                instance=instance_name,
            )

            await self._wait_for_operation(operation, timeout=120)
            return True, "Node stopped successfully"

        except Exception as e:
            return False, f"Failed to stop node: {e}"


# ============================================================================
# SINGLETON MANAGEMENT
# ============================================================================

_gcp_vm_manager: Optional[GCPVMManager] = None
_manager_lock = LazyAsyncLock()  # v100.1: Lazy initialization to avoid "no running event loop" error


async def reset_gcp_vm_manager_singleton() -> None:
    """
    v132.0: Force reset the GCP VM Manager singleton.

    This is used when we need to re-create the manager with new configuration,
    such as when auto-enabling GCP during OOM prevention.

    IMPORTANT: This should only be called when you KNOW you need to re-initialize
    with different settings. Normal usage should use get_gcp_vm_manager().
    """
    global _gcp_vm_manager

    async with _manager_lock:
        if _gcp_vm_manager is not None:
            # Cleanup existing manager
            try:
                if hasattr(_gcp_vm_manager, 'cleanup_all_vms'):
                    # Don't cleanup VMs on reset - just release the reference
                    pass
            except Exception as e:
                logger.debug(f"[GCPVMManager] Cleanup during reset: {e}")

            _gcp_vm_manager = None
            logger.info("[GCPVMManager] v132.0: Singleton reset - will reinitialize with fresh config")


async def get_gcp_vm_manager_with_force_enable() -> GCPVMManager:
    """
    v132.0: Get GCP VM Manager with forced enable.

    This resets the singleton and creates a new manager with GCP_ENABLED=true.
    Used for OOM prevention auto-enable functionality.

    Returns:
        GCPVMManager with GCP enabled (if credentials available)
    """
    # Set environment variable BEFORE resetting singleton
    os.environ["GCP_ENABLED"] = "true"

    # Reset singleton to force re-creation with new config
    await reset_gcp_vm_manager_singleton()

    # Create new manager with fresh config (will read GCP_ENABLED=true)
    return await get_gcp_vm_manager()


async def get_gcp_vm_manager(config: Optional[VMManagerConfig] = None) -> GCPVMManager:
    """
    Get or create singleton GCP VM Manager with thread-safe initialization.
    
    Args:
        config: Optional configuration override
        
    Returns:
        Initialized GCPVMManager instance
        
    Raises:
        RuntimeError: If GCP Compute API is not available
    """
    global _gcp_vm_manager

    # Quick check without lock
    if _gcp_vm_manager is not None and _gcp_vm_manager.initialized:
        return _gcp_vm_manager

    async with _manager_lock:
        # Double-check after acquiring lock
        if _gcp_vm_manager is None:
            _gcp_vm_manager = GCPVMManager(config=config)
        
        if not _gcp_vm_manager.initialized:
            await _gcp_vm_manager.initialize()

        return _gcp_vm_manager


async def get_gcp_vm_manager_safe(config: Optional[VMManagerConfig] = None) -> Optional[GCPVMManager]:
    """
    Safely get GCP VM Manager, returning None if not available.
    
    This is useful when you want to use the manager if available
    but don't want to fail if GCP is not configured.
    
    Args:
        config: Optional configuration override
        
    Returns:
        GCPVMManager instance or None if not available
    """
    try:
        return await get_gcp_vm_manager(config)
    except Exception as e:
        logger.debug(f"GCP VM Manager not available: {e}")
        return None


async def create_vm_if_needed(
    memory_snapshot, 
    components: List[str], 
    trigger_reason: str, 
    metadata: Optional[Dict] = None
) -> Optional[VMInstance]:
    """
    Convenience function: Check if VM needed and create if so.

    Args:
        memory_snapshot: Memory pressure snapshot
        components: Components to run on the VM
        trigger_reason: Why this VM is being created
        metadata: Additional metadata
        
    Returns:
        VMInstance if created, None otherwise
    """
    try:
        # Explicit opt-in guardrail (prevents surprise spend).
        # Accept legacy GCP_VM_ENABLED, otherwise use JARVIS_SPOT_VM_ENABLED.
        enabled_flag = os.getenv("GCP_VM_ENABLED")
        if enabled_flag is None:
            enabled_flag = os.getenv("JARVIS_SPOT_VM_ENABLED", "false")
        if str(enabled_flag).lower() != "true":
            logger.info("â„¹ï¸  Spot VM creation disabled by configuration")
            return None

        manager = await get_gcp_vm_manager_safe()
        
        if manager is None:
            logger.debug("GCP VM Manager not available - cannot create VM")
            return None

        should_create, reason, confidence = await manager.should_create_vm(
            memory_snapshot, trigger_reason
        )

        if should_create:
            logger.info(f"âœ… VM creation recommended: {reason} (confidence: {confidence:.2%})")
            return await manager.create_vm(components, trigger_reason, metadata)
        else:
            logger.info(f"â„¹ï¸  VM creation not needed: {reason}")
            return None
            
    except Exception as e:
        log_component_failure(
            "gcp-vm",
            "Error in create_vm_if_needed",
            error=e,
        )
        return None


async def cleanup_vm_manager():
    """Cleanup the global VM manager instance"""
    global _gcp_vm_manager

    async with _manager_lock:
        if _gcp_vm_manager is not None:
            await _gcp_vm_manager.cleanup()
            _gcp_vm_manager = None


# ============================================================================
# v95.0: PROACTIVE MEMORY-AWARE INITIALIZATION
# ============================================================================

async def proactive_vm_manager_init(
    memory_threshold: float = 70.0,
    force: bool = False,
) -> Optional[GCPVMManager]:
    """
    v95.0: Proactively initialize VM manager based on memory pressure.

    This function checks memory usage and initializes the VM manager
    if memory is above the threshold, preparing for potential offloading.

    Args:
        memory_threshold: Memory percentage threshold to trigger init (default 70%)
        force: If True, initialize regardless of memory pressure

    Returns:
        GCPVMManager instance if initialized, None if not needed or unavailable
    """
    try:
        import psutil
        mem_percent = psutil.virtual_memory().percent

        if not force and mem_percent < memory_threshold:
            logger.debug(
                f"[v95.0] Memory ({mem_percent:.1f}%) below threshold ({memory_threshold}%) - "
                f"VM manager init deferred"
            )
            return None

        logger.info(
            f"[v95.0] Memory pressure detected ({mem_percent:.1f}%) - "
            f"proactively initializing VM manager"
        )

        manager = await get_gcp_vm_manager_safe()

        if manager:
            logger.info("[v95.0] âœ… VM manager proactively initialized for memory offloading")
            return manager
        else:
            logger.warning(
                "[v95.0] âš ï¸ VM manager not available - local memory management only"
            )
            return None

    except Exception as e:
        logger.debug(f"[v95.0] Proactive VM manager init failed: {e}")
        return None


class LocalMemoryFallback:
    """
    v95.0: Fallback handler when GCP VM is not available.

    Provides local memory management strategies as a fallback:
    - GC triggering
    - Cache clearing
    - Process priority adjustment
    - Memory mapping suggestions
    """

    _instance: Optional["LocalMemoryFallback"] = None

    def __new__(cls) -> "LocalMemoryFallback":
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return

        self._initialized = True
        self.logger = logging.getLogger("LocalMemoryFallback")
        self._gc_cooldown = 30.0  # seconds between GC attempts
        self._last_gc_time = 0.0
        self._cache_clear_callbacks: List[Callable] = []

    def register_cache_clear_callback(self, callback: Callable) -> None:
        """Register a callback to be invoked when clearing caches."""
        if callback not in self._cache_clear_callbacks:
            self._cache_clear_callbacks.append(callback)

    async def attempt_local_relief(self, target_free_mb: float = 1024.0) -> Dict[str, Any]:
        """
        Attempt to free memory locally without GCP VM.

        Returns:
            Dict with 'freed_mb' and 'strategies_used'
        """
        import gc
        import psutil

        result = {
            "freed_mb": 0.0,
            "strategies_used": [],
            "initial_memory_percent": psutil.virtual_memory().percent,
            "final_memory_percent": 0.0,
        }

        initial_available = psutil.virtual_memory().available / (1024 * 1024)

        # Strategy 1: Trigger garbage collection
        current_time = time.time()
        if current_time - self._last_gc_time >= self._gc_cooldown:
            gc.collect()
            self._last_gc_time = current_time
            result["strategies_used"].append("garbage_collection")
            self.logger.info("[LocalMemoryFallback] Triggered garbage collection")

        # Strategy 2: Clear registered caches
        for callback in self._cache_clear_callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback()
                else:
                    callback()
                result["strategies_used"].append("cache_clear")
            except Exception as e:
                self.logger.debug(f"Cache clear callback failed: {e}")

        # Strategy 3: Suggest memory-intensive process reduction
        # This is informational - actual process management is handled elsewhere
        try:
            high_mem_procs = []
            for proc in psutil.process_iter(['pid', 'name', 'memory_percent']):
                try:
                    if proc.info['memory_percent'] > 5.0:
                        high_mem_procs.append({
                            'pid': proc.info['pid'],
                            'name': proc.info['name'],
                            'memory_percent': proc.info['memory_percent']
                        })
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue

            if high_mem_procs:
                high_mem_procs.sort(key=lambda x: x['memory_percent'], reverse=True)
                result["high_memory_processes"] = high_mem_procs[:5]
                self.logger.info(
                    f"[LocalMemoryFallback] Top memory consumers: "
                    f"{[p['name'] for p in high_mem_procs[:3]]}"
                )
        except Exception as e:
            self.logger.debug(f"Process inspection failed: {e}")

        # Calculate freed memory
        final_available = psutil.virtual_memory().available / (1024 * 1024)
        result["freed_mb"] = final_available - initial_available
        result["final_memory_percent"] = psutil.virtual_memory().percent

        self.logger.info(
            f"[LocalMemoryFallback] Relief attempt: {result['freed_mb']:.1f}MB freed, "
            f"strategies: {result['strategies_used']}"
        )

        return result


def get_local_memory_fallback() -> LocalMemoryFallback:
    """Get the singleton LocalMemoryFallback instance."""
    return LocalMemoryFallback()


def is_vm_manager_available() -> bool:
    """Check if VM manager is available without initializing"""
    return COMPUTE_AVAILABLE


def get_vm_manager_sync() -> Optional[GCPVMManager]:
    """Get VM manager synchronously (only if already initialized)"""
    return _gcp_vm_manager if _gcp_vm_manager and _gcp_vm_manager.initialized else None

