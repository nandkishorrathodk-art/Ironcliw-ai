"""
Native Self-Improvement Integration v2.0
========================================

This module transforms Ouroboros from an external CLI into a native capability
of the Ironcliw Body. Like how your hand is part of your body - you don't "exit"
yourself to use your hand, you simply think and your hand moves.

v2.0 Enhancements:
    - Integrates with Trinity Integration Layer v2.0
    - Distributed locking for concurrent improvements
    - Code review before applying changes (Coding Council)
    - Automatic rollback on failure
    - Learning cache to avoid repeated failures
    - Circular dependency detection
    - Manual review queue for complete failures

Architecture:
    ┌─────────────────────────────────────────────────────────────────────────┐
    │                    Ironcliw BODY (The Living System)                       │
    ├─────────────────────────────────────────────────────────────────────────┤
    │                                                                          │
    │  ┌─────────────────────────────────────────────────────────────────┐    │
    │  │                    CONSCIOUS MIND (Ironcliw Prime)                 │    │
    │  │  "I realize there's a bug in main.py"                           │    │
    │  └─────────────────────┬───────────────────────────────────────────┘    │
    │                        │                                                 │
    │                        ▼ (Intent)                                        │
    │  ┌─────────────────────────────────────────────────────────────────┐    │
    │  │               NATIVE SELF-IMPROVEMENT (Motor Function)           │    │
    │  │                                                                  │    │
    │  │  execute_self_improvement(                                       │    │
    │  │      target="main.py",                                           │    │
    │  │      goal="Fix the race condition bug"                           │    │
    │  │  )                                                               │    │
    │  │                                                                  │    │
    │  │  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐         │    │
    │  │  │ Analyze  │─▶│ Generate │─▶│ Validate │─▶│  Apply   │         │    │
    │  │  └──────────┘  └──────────┘  └──────────┘  └──────────┘         │    │
    │  │       │              │             │             │              │    │
    │  │       ▼              ▼             ▼             ▼              │    │
    │  │    ┌──────────────────────────────────────────────────┐         │    │
    │  │    │     PROGRESS BROADCASTER (To Menu Bar & UI)       │         │    │
    │  │    │  "Analyzing... Generating... Testing... Done!"    │         │    │
    │  │    └──────────────────────────────────────────────────┘         │    │
    │  └─────────────────────────────────────────────────────────────────┘    │
    │                                                                          │
    │                        ▼ (Feedback)                                      │
    │  ┌─────────────────────────────────────────────────────────────────┐    │
    │  │                    NERVOUS SYSTEM (Reactor Core)                 │    │
    │  │  "I've learned from this improvement - storing experience"       │    │
    │  └─────────────────────────────────────────────────────────────────┘    │
    │                                                                          │
    └─────────────────────────────────────────────────────────────────────────┘

Security Features:
    - Path traversal protection with canonical path validation
    - Command injection prevention with parameterized execution
    - Thread-safe metrics with atomic operations
    - Circuit breaker with proper state machine
    - Rate limiting with burst protection
    - Sandbox environment isolation

Author: Trinity System
Version: 2.0.0
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
import os
import random
import re
import shlex
import shutil
import stat
import sys
import tempfile
import time
import uuid
import weakref
from datetime import datetime, timedelta
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from enum import Enum, auto
from functools import wraps
from pathlib import Path
from threading import Lock as ThreadLock
from typing import (
    Any,
    Awaitable,
    Callable,
    Dict,
    List,
    Optional,
    Set,
    Tuple,
    TypeVar,
    Union,
)
from collections import deque, defaultdict

try:
    import aiofiles
except ImportError:
    aiofiles = None

# v109.3: Safe file descriptor management to prevent EXC_GUARD crashes
try:
    from backend.core.safe_fd import safe_close, safe_open, async_safe_sync_file
except ImportError:
    # Fallback if module not available
    safe_close = lambda fd, **kwargs: os.close(fd) if fd >= 0 else None  # noqa: E731
    safe_open = os.open
    async_safe_sync_file = None

try:
    import aiohttp
except ImportError:
    aiohttp = None

try:
    import aiosqlite
except ImportError:
    aiosqlite = None

logger = logging.getLogger("Ouroboros.NativeIntegration")

T = TypeVar("T")


# =============================================================================
# CONFIGURATION - Dynamic, No Hardcoding
# =============================================================================

class NativeConfig:
    """
    Dynamic configuration with validation.
    All values from environment with sensible defaults and range validation.
    """

    # Project paths - dynamically resolved
    PROJECT_ROOT = Path(os.getenv(
        "Ironcliw_PROJECT_ROOT",
        Path(__file__).parent.parent.parent.parent
    ))

    # Allowed paths for self-improvement (security boundary)
    ALLOWED_PATHS: List[Path] = [
        PROJECT_ROOT,
        Path(os.getenv("Ironcliw_PRIME_ROOT", Path.home() / "Documents/repos/Ironcliw-Prime")),
        Path(os.getenv("REACTOR_CORE_ROOT", Path.home() / "Documents/repos/reactor-core")),
    ]

    # Rate limiting (validated)
    @staticmethod
    def get_rate_limit() -> float:
        value = float(os.getenv("OUROBOROS_RATE_LIMIT", "1.0"))
        return max(0.01, min(100.0, value))  # Clamp to valid range

    @staticmethod
    def get_burst_limit() -> int:
        value = int(os.getenv("OUROBOROS_BURST_LIMIT", "5"))
        return max(1, min(50, value))

    # Timeouts (validated)
    @staticmethod
    def get_llm_timeout() -> float:
        value = float(os.getenv("OUROBOROS_LLM_TIMEOUT", "120.0"))
        return max(10.0, min(600.0, value))

    @staticmethod
    def get_test_timeout() -> float:
        value = float(os.getenv("OUROBOROS_TEST_TIMEOUT", "300.0"))
        return max(10.0, min(1800.0, value))

    @staticmethod
    def get_lock_timeout() -> float:
        value = float(os.getenv("OUROBOROS_LOCK_TIMEOUT", "30.0"))
        return max(1.0, min(300.0, value))

    # Resource limits (validated)
    @staticmethod
    def get_max_file_size() -> int:
        """Maximum file size in bytes (default 1MB)."""
        value = int(os.getenv("OUROBOROS_MAX_FILE_SIZE", str(1024 * 1024)))
        return max(1024, min(100 * 1024 * 1024, value))

    @staticmethod
    def get_max_output_size() -> int:
        """Maximum output size in bytes (default 100KB)."""
        value = int(os.getenv("OUROBOROS_MAX_OUTPUT_SIZE", str(100 * 1024)))
        return max(1024, min(10 * 1024 * 1024, value))

    # Retry configuration (validated)
    @staticmethod
    def get_max_retries() -> int:
        value = int(os.getenv("OUROBOROS_MAX_RETRIES", "5"))
        return max(1, min(20, value))

    @staticmethod
    def get_retry_backoff_base() -> float:
        value = float(os.getenv("OUROBOROS_RETRY_BACKOFF", "2.0"))
        return max(1.1, min(5.0, value))

    # Circuit breaker (validated)
    @staticmethod
    def get_circuit_threshold() -> int:
        value = int(os.getenv("OUROBOROS_CIRCUIT_THRESHOLD", "5"))
        return max(1, min(100, value))

    @staticmethod
    def get_circuit_timeout() -> float:
        value = float(os.getenv("OUROBOROS_CIRCUIT_TIMEOUT", "300.0"))
        return max(10.0, min(3600.0, value))

    @staticmethod
    def get_circuit_recovery_threshold() -> int:
        """Consecutive successes needed to close circuit from HALF_OPEN."""
        value = int(os.getenv("OUROBOROS_CIRCUIT_RECOVERY", "3"))
        return max(1, min(10, value))

    # Sandbox configuration
    @staticmethod
    def get_sandbox_dir() -> Path:
        default = Path(tempfile.gettempdir()) / "ouroboros" / "sandbox"
        return Path(os.getenv("OUROBOROS_SANDBOX_DIR", str(default)))

    @staticmethod
    def is_sandbox_enabled() -> bool:
        return os.getenv("OUROBOROS_SANDBOX_ENABLED", "true").lower() in ("true", "1", "yes")

    # Safe environment variables to pass to sandbox
    SAFE_ENV_VARS = {
        "PATH", "HOME", "USER", "SHELL", "LANG", "LC_ALL", "LC_CTYPE",
        "PYTHONPATH", "VIRTUAL_ENV", "CONDA_PREFIX",
        "TERM", "COLORTERM", "TMPDIR", "TMP", "TEMP",
    }

    # Dangerous environment variable prefixes to exclude
    DANGEROUS_ENV_PREFIXES = {
        "AWS_", "GCP_", "GOOGLE_", "AZURE_", "GITHUB_", "GITLAB_",
        "API_KEY", "SECRET", "PASSWORD", "TOKEN", "CREDENTIAL",
        "ANTHROPIC_", "OPENAI_", "Ironcliw_PRIME_API_KEY",
    }


# =============================================================================
# SECURITY UTILITIES
# =============================================================================

class SecurityValidator:
    """
    Security validation utilities.
    Prevents path traversal, command injection, and other attacks.
    """

    @staticmethod
    def validate_path(
        path: Union[str, Path],
        allowed_roots: Optional[List[Path]] = None,
    ) -> Path:
        """
        Validate and resolve a path, ensuring it's within allowed boundaries.

        Args:
            path: Path to validate
            allowed_roots: List of allowed root directories

        Returns:
            Canonical resolved path

        Raises:
            SecurityError: If path is outside allowed boundaries
        """
        if allowed_roots is None:
            allowed_roots = NativeConfig.ALLOWED_PATHS

        # Convert to Path and resolve to canonical form
        resolved = Path(path).resolve()

        # Check against allowed roots
        for root in allowed_roots:
            try:
                resolved_root = root.resolve()
                resolved.relative_to(resolved_root)
                return resolved  # Path is within this root
            except ValueError:
                continue  # Not within this root, try next

        raise SecurityError(
            f"Path '{path}' is outside allowed boundaries. "
            f"Allowed roots: {[str(r) for r in allowed_roots]}"
        )

    @staticmethod
    def validate_file_for_improvement(path: Path) -> None:
        """
        Validate a file is safe for self-improvement.

        Raises:
            SecurityError: If file is not safe
        """
        # Must exist
        if not path.exists():
            raise SecurityError(f"File does not exist: {path}")

        # Must be a file, not directory or symlink
        if path.is_symlink():
            raise SecurityError(f"Symbolic links not allowed: {path}")

        if not path.is_file():
            raise SecurityError(f"Path is not a file: {path}")

        # Check file size
        size = path.stat().st_size
        max_size = NativeConfig.get_max_file_size()
        if size > max_size:
            raise SecurityError(
                f"File too large: {size} bytes (max: {max_size})"
            )

        # Must be a Python file for code improvement
        if path.suffix not in (".py", ".pyi"):
            logger.warning(f"Non-Python file: {path.suffix}")

    @staticmethod
    def sanitize_for_shell(value: str) -> str:
        """
        Sanitize a value for safe shell usage.
        Uses shlex.quote for proper escaping.
        """
        return shlex.quote(value)

    @staticmethod
    def build_safe_command(
        executable: str,
        args: List[str],
        env: Optional[Dict[str, str]] = None,
    ) -> Tuple[List[str], Dict[str, str]]:
        """
        Build a safe command without shell interpolation.

        Returns:
            Tuple of (command_list, safe_env)
        """
        # Validate executable exists
        exe_path = shutil.which(executable)
        if not exe_path:
            raise SecurityError(f"Executable not found: {executable}")

        # Build command list (no shell interpolation)
        cmd = [exe_path] + list(args)

        # Build safe environment
        safe_env = SecurityValidator.get_safe_environment()
        if env:
            # Only allow safe additional env vars
            for key, value in env.items():
                if SecurityValidator._is_safe_env_var(key):
                    safe_env[key] = value

        return cmd, safe_env

    @staticmethod
    def get_safe_environment() -> Dict[str, str]:
        """Get a sanitized environment dictionary."""
        safe_env = {}
        for key in NativeConfig.SAFE_ENV_VARS:
            if key in os.environ:
                safe_env[key] = os.environ[key]
        return safe_env

    @staticmethod
    def _is_safe_env_var(key: str) -> bool:
        """Check if an environment variable is safe to pass through."""
        upper_key = key.upper()
        for prefix in NativeConfig.DANGEROUS_ENV_PREFIXES:
            if upper_key.startswith(prefix):
                return False
        return True


class SecurityError(Exception):
    """Security validation failure."""
    pass


# =============================================================================
# THREAD-SAFE METRICS
# =============================================================================

class AtomicCounter:
    """Thread-safe atomic counter."""

    def __init__(self, initial: int = 0):
        self._value = initial
        self._lock = ThreadLock()

    def increment(self, amount: int = 1) -> int:
        with self._lock:
            self._value += amount
            return self._value

    def decrement(self, amount: int = 1) -> int:
        with self._lock:
            self._value -= amount
            return self._value

    @property
    def value(self) -> int:
        with self._lock:
            return self._value

    def reset(self, value: int = 0) -> None:
        with self._lock:
            self._value = value


class ThreadSafeMetrics:
    """Thread-safe metrics collection with atomic operations."""

    def __init__(self):
        self._lock = ThreadLock()
        self._counters: Dict[str, AtomicCounter] = {}
        self._gauges: Dict[str, float] = {}
        self._histograms: Dict[str, List[float]] = {}
        self._histogram_max_size = 1000

    def increment(self, name: str, amount: int = 1) -> int:
        """Increment a counter atomically."""
        if name not in self._counters:
            with self._lock:
                if name not in self._counters:
                    self._counters[name] = AtomicCounter()
        return self._counters[name].increment(amount)

    def set_gauge(self, name: str, value: float) -> None:
        """Set a gauge value."""
        with self._lock:
            self._gauges[name] = value

    def record_histogram(self, name: str, value: float) -> None:
        """Record a value in a histogram."""
        with self._lock:
            if name not in self._histograms:
                self._histograms[name] = []
            hist = self._histograms[name]
            hist.append(value)
            # Keep bounded size
            if len(hist) > self._histogram_max_size:
                self._histograms[name] = hist[-self._histogram_max_size:]

    def get_counter(self, name: str) -> int:
        """Get counter value."""
        counter = self._counters.get(name)
        return counter.value if counter else 0

    def get_gauge(self, name: str) -> Optional[float]:
        """Get gauge value."""
        with self._lock:
            return self._gauges.get(name)

    def get_histogram_stats(self, name: str) -> Dict[str, float]:
        """Get histogram statistics."""
        with self._lock:
            hist = self._histograms.get(name, [])
            if not hist:
                return {}
            sorted_hist = sorted(hist)
            n = len(sorted_hist)
            return {
                "count": n,
                "min": sorted_hist[0],
                "max": sorted_hist[-1],
                "mean": sum(sorted_hist) / n,
                "p50": sorted_hist[n // 2],
                "p95": sorted_hist[int(n * 0.95)] if n >= 20 else sorted_hist[-1],
                "p99": sorted_hist[int(n * 0.99)] if n >= 100 else sorted_hist[-1],
            }

    def snapshot(self) -> Dict[str, Any]:
        """Get a snapshot of all metrics."""
        with self._lock:
            return {
                "counters": {k: v.value for k, v in self._counters.items()},
                "gauges": dict(self._gauges),
                "histograms": {
                    k: self.get_histogram_stats(k) for k in self._histograms
                },
            }


# =============================================================================
# IMPROVED CIRCUIT BREAKER
# =============================================================================

class CircuitState(Enum):
    """Circuit breaker states."""
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"


class ImprovedCircuitBreaker:
    """
    Improved circuit breaker with proper state machine.

    Features:
    - Thread-safe state transitions
    - Configurable recovery threshold (N consecutive successes)
    - Per-request limiting in HALF_OPEN state
    - Metrics tracking
    """

    def __init__(
        self,
        name: str,
        failure_threshold: Optional[int] = None,
        recovery_timeout: Optional[float] = None,
        recovery_threshold: Optional[int] = None,
    ):
        self.name = name
        self.failure_threshold = failure_threshold or NativeConfig.get_circuit_threshold()
        self.recovery_timeout = recovery_timeout or NativeConfig.get_circuit_timeout()
        self.recovery_threshold = recovery_threshold or NativeConfig.get_circuit_recovery_threshold()

        self._lock = asyncio.Lock()
        self._state = CircuitState.CLOSED
        self._failures = 0
        self._successes = 0
        self._half_open_requests = 0
        self._last_failure_time = 0.0
        self._last_success_time = 0.0

        self._metrics = ThreadSafeMetrics()

    @property
    def state(self) -> CircuitState:
        return self._state

    async def can_execute(self) -> bool:
        """Check if request can be executed."""
        async with self._lock:
            if self._state == CircuitState.CLOSED:
                return True

            if self._state == CircuitState.OPEN:
                # Check if recovery timeout elapsed
                if time.time() - self._last_failure_time >= self.recovery_timeout:
                    self._transition_to(CircuitState.HALF_OPEN)
                    self._half_open_requests = 0
                    return True
                return False

            # HALF_OPEN: Allow limited requests
            if self._half_open_requests < self.recovery_threshold:
                self._half_open_requests += 1
                return True
            return False

    async def record_success(self) -> None:
        """Record a successful execution."""
        async with self._lock:
            self._successes += 1
            self._last_success_time = time.time()
            self._metrics.increment("successes")

            if self._state == CircuitState.HALF_OPEN:
                # Need N consecutive successes to close
                if self._successes >= self.recovery_threshold:
                    self._transition_to(CircuitState.CLOSED)
                    self._failures = 0
                    self._successes = 0

    async def record_failure(self, error: Optional[str] = None) -> None:
        """Record a failed execution."""
        async with self._lock:
            self._failures += 1
            self._successes = 0  # Reset consecutive successes
            self._last_failure_time = time.time()
            self._metrics.increment("failures")

            if error:
                logger.warning(f"Circuit {self.name} failure: {error}")

            if self._state == CircuitState.HALF_OPEN:
                # Any failure in HALF_OPEN goes back to OPEN
                self._transition_to(CircuitState.OPEN)
            elif self._state == CircuitState.CLOSED:
                if self._failures >= self.failure_threshold:
                    self._transition_to(CircuitState.OPEN)

    def _transition_to(self, new_state: CircuitState) -> None:
        """Transition to a new state (must hold lock)."""
        old_state = self._state
        self._state = new_state
        logger.info(f"Circuit {self.name}: {old_state.value} -> {new_state.value}")
        self._metrics.increment(f"transitions_to_{new_state.value}")

    def get_status(self) -> Dict[str, Any]:
        """Get circuit breaker status."""
        return {
            "name": self.name,
            "state": self._state.value,
            "failures": self._failures,
            "successes": self._successes,
            "last_failure": self._last_failure_time,
            "last_success": self._last_success_time,
            "metrics": self._metrics.snapshot(),
        }


# =============================================================================
# PROGRESS BROADCASTER
# =============================================================================

class ImprovementPhase(Enum):
    """Phases of self-improvement."""
    INITIALIZING = "initializing"
    ANALYZING = "analyzing"
    GENERATING = "generating"
    VALIDATING = "validating"
    TESTING = "testing"
    APPLYING = "applying"
    COMMITTING = "committing"
    LEARNING = "learning"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class ImprovementProgress:
    """Progress information for self-improvement."""
    task_id: str
    phase: ImprovementPhase
    target_file: str
    goal: str
    progress_percent: float = 0.0
    message: str = ""
    iteration: int = 0
    max_iterations: int = 0
    provider: str = ""
    error: Optional[str] = None
    started_at: float = field(default_factory=time.time)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "task_id": self.task_id,
            "phase": self.phase.value,
            "target_file": self.target_file,
            "goal": self.goal[:100] + "..." if len(self.goal) > 100 else self.goal,
            "progress_percent": self.progress_percent,
            "message": self.message,
            "iteration": self.iteration,
            "max_iterations": self.max_iterations,
            "provider": self.provider,
            "error": self.error,
            "elapsed_seconds": time.time() - self.started_at,
        }


class ProgressBroadcaster:
    """
    Broadcasts improvement progress to the UI.

    Integrates with:
    - Menu bar status indicator
    - WebSocket connections
    - Event bus
    - Logging
    """

    def __init__(self):
        self._listeners: List[Callable[[ImprovementProgress], Awaitable[None]]] = []
        self._progress_history: Dict[str, ImprovementProgress] = {}
        self._lock = asyncio.Lock()
        self._max_history = 100

    def add_listener(
        self,
        callback: Callable[[ImprovementProgress], Awaitable[None]],
    ) -> Callable[[], None]:
        """
        Add a progress listener.

        Returns:
            A function to remove the listener
        """
        self._listeners.append(callback)

        def remove():
            if callback in self._listeners:
                self._listeners.remove(callback)

        return remove

    async def broadcast(self, progress: ImprovementProgress) -> None:
        """Broadcast progress to all listeners."""
        async with self._lock:
            self._progress_history[progress.task_id] = progress

            # Trim history
            if len(self._progress_history) > self._max_history:
                oldest_keys = sorted(
                    self._progress_history.keys(),
                    key=lambda k: self._progress_history[k].started_at
                )[:len(self._progress_history) - self._max_history]
                for key in oldest_keys:
                    del self._progress_history[key]

        # Log progress
        phase_icons = {
            ImprovementPhase.INITIALIZING: "🔧",
            ImprovementPhase.ANALYZING: "🔍",
            ImprovementPhase.GENERATING: "⚡",
            ImprovementPhase.VALIDATING: "✅",
            ImprovementPhase.TESTING: "🧪",
            ImprovementPhase.APPLYING: "📝",
            ImprovementPhase.COMMITTING: "💾",
            ImprovementPhase.LEARNING: "🧠",
            ImprovementPhase.COMPLETED: "✨",
            ImprovementPhase.FAILED: "❌",
        }
        icon = phase_icons.get(progress.phase, "⏳")
        logger.info(
            f"{icon} [{progress.task_id[:8]}] {progress.phase.value}: "
            f"{progress.message} ({progress.progress_percent:.0f}%)"
        )

        # Notify listeners
        for listener in self._listeners:
            try:
                await listener(progress)
            except Exception as e:
                logger.error(f"Progress listener error: {e}")

    async def get_active_improvements(self) -> List[ImprovementProgress]:
        """Get all active (non-completed) improvements."""
        async with self._lock:
            return [
                p for p in self._progress_history.values()
                if p.phase not in (ImprovementPhase.COMPLETED, ImprovementPhase.FAILED)
            ]

    async def get_progress(self, task_id: str) -> Optional[ImprovementProgress]:
        """Get progress for a specific task."""
        async with self._lock:
            return self._progress_history.get(task_id)


# =============================================================================
# NATIVE SELF-IMPROVEMENT ENGINE
# =============================================================================

@dataclass
class ImprovementRequest:
    """Request for self-improvement."""
    target_file: Path
    goal: str
    test_command: Optional[str] = None
    context: Optional[str] = None
    max_iterations: int = 5
    dry_run: bool = False
    auto_commit: bool = False


@dataclass
class ImprovementResult:
    """Result of self-improvement."""
    success: bool
    task_id: str
    target_file: str
    goal: str
    iterations: int
    total_time: float
    provider_used: str = ""
    changes_applied: bool = False
    error: Optional[str] = None
    diff: Optional[str] = None


class NativeSelfImprovement:
    """
    Native self-improvement engine integrated into Ironcliw.

    This is the "motor function" - when Ironcliw thinks "fix this bug",
    this class handles the actual execution transparently.

    Features:
    - Deep integration with Ironcliw event loop
    - Progress broadcasting to UI
    - Thread-safe metrics
    - Secure sandbox execution
    - Circuit breaker protection
    - Cross-repo awareness
    """

    def __init__(self):
        self.logger = logging.getLogger("Ouroboros.NativeSelfImprovement")

        # Core components
        self._progress_broadcaster = ProgressBroadcaster()
        self._metrics = ThreadSafeMetrics()
        self._circuit_breakers: Dict[str, ImprovedCircuitBreaker] = {}

        # State
        self._running = False
        self._active_tasks: Dict[str, asyncio.Task] = {}
        self._lock = asyncio.Lock()

        # Rate limiting
        self._rate_limiter: Optional[asyncio.Semaphore] = None
        self._last_request_time = 0.0

        # Cached integration reference - Trinity is the preferred integration layer
        self._trinity_integration = None
        self._legacy_integration = None  # Fallback for older integration.py
        self._brain_orchestrator = None

    async def initialize(self) -> None:
        """Initialize the native self-improvement engine."""
        self.logger.info("Initializing Native Self-Improvement Engine...")

        # Initialize rate limiter
        self._rate_limiter = asyncio.Semaphore(NativeConfig.get_burst_limit())

        # Initialize circuit breakers for providers
        provider_names = ["jarvis-prime", "ollama", "anthropic"]
        for name in provider_names:
            self._circuit_breakers[name] = ImprovedCircuitBreaker(name)

        # Try Trinity Integration first (preferred - production-grade)
        try:
            from backend.core.ouroboros.trinity_integration import (
                get_trinity_integration,
                initialize_trinity_integration,
            )
            self._trinity_integration = get_trinity_integration()
            await initialize_trinity_integration()
            self.logger.info("✅ Connected to Trinity Integration (production-grade)")
        except ImportError as e:
            self.logger.warning(f"Trinity Integration not available: {e}")
        except Exception as e:
            self.logger.warning(f"Trinity Integration init failed: {e}")

        # Connect to brain orchestrator if available (fallback)
        if not self._trinity_integration:
            try:
                from backend.core.ouroboros.brain_orchestrator import get_brain_orchestrator
                self._brain_orchestrator = get_brain_orchestrator()
                self.logger.info("✅ Connected to Brain Orchestrator (fallback)")
            except ImportError:
                self.logger.warning("Brain orchestrator not available")

        # Connect to legacy integration layer if available (final fallback)
        if not self._trinity_integration and not self._brain_orchestrator:
            try:
                from backend.core.ouroboros.integration import get_ouroboros_integration
                self._legacy_integration = get_ouroboros_integration()
                self.logger.info("✅ Connected to Legacy Integration (fallback)")
            except ImportError:
                self.logger.warning("Legacy integration layer not available")

        self._running = True
        self.logger.info("Native Self-Improvement Engine initialized")

    async def shutdown(self) -> None:
        """Shutdown the engine."""
        self.logger.info("Shutting down Native Self-Improvement Engine...")
        self._running = False

        # Cancel active tasks
        for task_id, task in list(self._active_tasks.items()):
            if not task.done():
                task.cancel()
                try:
                    await asyncio.wait_for(task, timeout=5.0)
                except (asyncio.CancelledError, asyncio.TimeoutError):
                    pass

        self._active_tasks.clear()
        self.logger.info("Native Self-Improvement Engine shutdown complete")

    @property
    def progress_broadcaster(self) -> ProgressBroadcaster:
        """Get the progress broadcaster for UI integration."""
        return self._progress_broadcaster

    async def execute_self_improvement(
        self,
        target: Union[str, Path],
        goal: str,
        test_command: Optional[str] = None,
        context: Optional[str] = None,
        max_iterations: int = 5,
        dry_run: bool = False,
        auto_commit: bool = False,
    ) -> ImprovementResult:
        """
        Execute self-improvement on a target file.

        This is the main entry point - called naturally from Ironcliw:

            await execute_self_improvement(
                target="main.py",
                goal="Fix the race condition bug"
            )

        Args:
            target: Path to file to improve
            goal: Natural language description of the improvement
            test_command: Optional test command to validate
            context: Additional context about the improvement
            max_iterations: Maximum improvement attempts
            dry_run: If True, show changes without applying
            auto_commit: If True, commit changes to git

        Returns:
            ImprovementResult with success status and details
        """
        task_id = f"imp_{uuid.uuid4().hex[:12]}"
        start_time = time.time()

        # Create request
        try:
            target_path = SecurityValidator.validate_path(target)
            SecurityValidator.validate_file_for_improvement(target_path)
        except SecurityError as e:
            return ImprovementResult(
                success=False,
                task_id=task_id,
                target_file=str(target),
                goal=goal,
                iterations=0,
                total_time=time.time() - start_time,
                error=str(e),
            )

        request = ImprovementRequest(
            target_file=target_path,
            goal=goal,
            test_command=test_command,
            context=context,
            max_iterations=max_iterations,
            dry_run=dry_run,
            auto_commit=auto_commit,
        )

        # Broadcast initial progress
        progress = ImprovementProgress(
            task_id=task_id,
            phase=ImprovementPhase.INITIALIZING,
            target_file=str(target_path),
            goal=goal,
            progress_percent=0,
            message="Starting self-improvement...",
            max_iterations=max_iterations,
        )
        await self._progress_broadcaster.broadcast(progress)

        # Execute improvement
        try:
            result = await self._execute_improvement_loop(task_id, request, progress)

            # Record metrics
            self._metrics.increment("improvements_attempted")
            if result.success:
                self._metrics.increment("improvements_succeeded")
            else:
                self._metrics.increment("improvements_failed")
            self._metrics.record_histogram("improvement_duration", result.total_time)

            return result

        except asyncio.CancelledError:
            progress.phase = ImprovementPhase.FAILED
            progress.error = "Cancelled"
            await self._progress_broadcaster.broadcast(progress)
            raise
        except Exception as e:
            self.logger.error(f"Improvement failed: {e}", exc_info=True)
            progress.phase = ImprovementPhase.FAILED
            progress.error = str(e)
            await self._progress_broadcaster.broadcast(progress)

            return ImprovementResult(
                success=False,
                task_id=task_id,
                target_file=str(target_path),
                goal=goal,
                iterations=0,
                total_time=time.time() - start_time,
                error=str(e),
            )

    async def _execute_improvement_loop(
        self,
        task_id: str,
        request: ImprovementRequest,
        progress: ImprovementProgress,
    ) -> ImprovementResult:
        """Execute the improvement loop with Trinity v2.0 integration."""
        start_time = time.time()
        target_str = str(request.target_file)

        # =====================================================================
        # PRE-FLIGHT CHECKS (Trinity v2.0)
        # =====================================================================

        # Check 1: Circular dependency detection
        if self._trinity_integration:
            is_circular, reason = await self._trinity_integration.coordinator.check_circular_dependency(
                target_str
            )
            if is_circular:
                self.logger.warning(f"Circular dependency detected: {reason}")
                return ImprovementResult(
                    success=False,
                    task_id=task_id,
                    target_file=target_str,
                    goal=request.goal,
                    iterations=0,
                    total_time=time.time() - start_time,
                    error=f"Circular dependency: {reason}",
                )

        # Check 2: Learning cache - should we skip?
        if self._trinity_integration:
            should_skip, skip_reason = await self._trinity_integration.check_should_skip(
                target_str, request.goal
            )
            if should_skip:
                self.logger.info(f"Skipping improvement (learning cache): {skip_reason}")
                return ImprovementResult(
                    success=False,
                    task_id=task_id,
                    target_file=target_str,
                    goal=request.goal,
                    iterations=0,
                    total_time=time.time() - start_time,
                    error=f"Skipped: {skip_reason}",
                )

        # =====================================================================
        # ACQUIRE DISTRIBUTED LOCK (Trinity v2.0)
        # =====================================================================

        lock_acquired = True  # Default to True if no lock manager
        if self._trinity_integration:
            try:
                async with self._trinity_integration.acquire_lock(target_str) as acquired:
                    lock_acquired = acquired
                    if not acquired:
                        self.logger.warning(f"Could not acquire lock for: {target_str}")
                        return ImprovementResult(
                            success=False,
                            task_id=task_id,
                            target_file=target_str,
                            goal=request.goal,
                            iterations=0,
                            total_time=time.time() - start_time,
                            error="Could not acquire distributed lock - file may be locked by another process",
                        )

                    # Execute within lock context
                    return await self._execute_locked_improvement(
                        task_id, request, progress, start_time
                    )
            except Exception as e:
                self.logger.warning(f"Lock acquisition failed, proceeding without lock: {e}")
                # Fall through to execute without lock

        # Execute without lock (fallback or no Trinity integration)
        return await self._execute_locked_improvement(
            task_id, request, progress, start_time
        )

    async def _execute_locked_improvement(
        self,
        task_id: str,
        request: ImprovementRequest,
        progress: ImprovementProgress,
        start_time: float,
    ) -> ImprovementResult:
        """Execute improvement with lock already held."""
        target_str = str(request.target_file)

        # Phase 1: Analyze
        progress.phase = ImprovementPhase.ANALYZING
        progress.progress_percent = 10
        progress.message = f"Analyzing {request.target_file.name}..."
        await self._progress_broadcaster.broadcast(progress)

        original_content = await self._read_file_safe(request.target_file)

        # Phase 2: Generate improvements
        last_error = None
        improved_content = None
        provider_used = ""

        for iteration in range(1, request.max_iterations + 1):
            progress.phase = ImprovementPhase.GENERATING
            progress.iteration = iteration
            progress.progress_percent = 20 + (iteration / request.max_iterations * 40)
            progress.message = f"Generating improvement (attempt {iteration}/{request.max_iterations})..."
            await self._progress_broadcaster.broadcast(progress)

            try:
                improved_content, provider_used = await self._generate_improvement(
                    original_content,
                    request.goal,
                    last_error,
                    request.context,
                )
                progress.provider = provider_used

                if not improved_content:
                    last_error = "No improvement generated"
                    continue

                # Phase 3: Validate syntax
                progress.phase = ImprovementPhase.VALIDATING
                progress.progress_percent = 60
                progress.message = "Validating syntax..."
                await self._progress_broadcaster.broadcast(progress)

                valid, syntax_error = self._validate_syntax(improved_content)
                if not valid:
                    last_error = f"Syntax error: {syntax_error}"
                    continue

                # Phase 4: Test (if test command provided)
                if request.test_command:
                    progress.phase = ImprovementPhase.TESTING
                    progress.progress_percent = 70
                    progress.message = "Running tests..."
                    await self._progress_broadcaster.broadcast(progress)

                    test_passed, test_output = await self._run_tests_safe(
                        request.target_file,
                        improved_content,
                        request.test_command,
                    )

                    if not test_passed:
                        last_error = f"Tests failed: {test_output[:500]}"
                        continue

                # =========================================================
                # CODE REVIEW (Trinity v2.0 - Coding Council Integration)
                # =========================================================
                if self._trinity_integration:
                    try:
                        review = await self._trinity_integration.review_code(
                            original_code=original_content,
                            improved_code=improved_content,
                            goal=request.goal,
                            file_path=str(request.target_file),
                        )

                        # Import ReviewResult from trinity_integration
                        from backend.core.ouroboros.trinity_integration import ReviewResult

                        if review.result == ReviewResult.REJECTED:
                            last_error = f"Code review rejected: {review.feedback}"
                            self.logger.warning(f"Code review rejected: {review.feedback}")
                            if review.security_issues:
                                self.logger.warning(f"Security issues: {review.security_issues}")
                            continue

                        if review.result == ReviewResult.NEEDS_REVISION:
                            last_error = f"Code needs revision: {review.feedback}"
                            self.logger.info(f"Code needs revision: {review.suggestions}")
                            continue

                        # APPROVED - proceed with apply
                        self.logger.info(f"Code review passed (risk: {review.risk_score:.2f})")

                    except Exception as e:
                        self.logger.warning(f"Code review failed, proceeding: {e}")

                # Success! Apply changes
                if not request.dry_run:
                    progress.phase = ImprovementPhase.APPLYING
                    progress.progress_percent = 85
                    progress.message = "Applying changes..."
                    await self._progress_broadcaster.broadcast(progress)

                    # =========================================================
                    # APPLY WITH ROLLBACK (Trinity v2.0)
                    # =========================================================
                    if self._trinity_integration:
                        try:
                            async with self._trinity_integration.with_rollback(
                                request.target_file, task_id
                            ) as snapshot_ok:
                                if snapshot_ok:
                                    await self._write_file_safe(request.target_file, improved_content)
                                else:
                                    # No snapshot (file didn't exist?), write anyway
                                    await self._write_file_safe(request.target_file, improved_content)
                        except Exception as e:
                            # Rollback happened automatically
                            last_error = f"Apply failed (rolled back): {e}"
                            self.logger.error(f"Apply failed, rolled back: {e}")
                            continue
                    else:
                        await self._write_file_safe(request.target_file, improved_content)

                    # Auto-commit if requested
                    if request.auto_commit:
                        progress.phase = ImprovementPhase.COMMITTING
                        progress.message = "Committing changes..."
                        await self._progress_broadcaster.broadcast(progress)
                        await self._git_commit_safe(request.target_file, request.goal)

                # Phase 5: Learn
                progress.phase = ImprovementPhase.LEARNING
                progress.progress_percent = 95
                progress.message = "Recording experience..."
                await self._progress_broadcaster.broadcast(progress)

                await self._publish_experience(
                    original_content,
                    improved_content,
                    request.goal,
                    success=True,
                    iterations=iteration,
                    provider_used=provider_used,
                    duration_seconds=time.time() - start_time,
                )

                # =========================================================
                # RECORD SUCCESS IN LEARNING CACHE (Trinity v2.0)
                # =========================================================
                if self._trinity_integration:
                    await self._trinity_integration.record_improvement_attempt(
                        target=target_str,
                        goal=request.goal,
                        success=True,
                        error=None,
                    )

                # Complete!
                progress.phase = ImprovementPhase.COMPLETED
                progress.progress_percent = 100
                progress.message = "Improvement complete!"
                await self._progress_broadcaster.broadcast(progress)

                return ImprovementResult(
                    success=True,
                    task_id=task_id,
                    target_file=str(request.target_file),
                    goal=request.goal,
                    iterations=iteration,
                    total_time=time.time() - start_time,
                    provider_used=provider_used,
                    changes_applied=not request.dry_run,
                    diff=self._generate_diff(original_content, improved_content),
                )

            except Exception as e:
                last_error = str(e)
                self.logger.warning(f"Iteration {iteration} failed: {e}")
                continue

        # =====================================================================
        # ALL ITERATIONS FAILED - Trinity v2.0 Failure Handling
        # =====================================================================

        # Record failure in learning cache
        if self._trinity_integration:
            await self._trinity_integration.record_improvement_attempt(
                target=target_str,
                goal=request.goal,
                success=False,
                error=last_error,
            )

        # Queue for manual review if all automated attempts failed
        if self._trinity_integration:
            try:
                review_id = await self._trinity_integration.queue_for_manual_review(
                    target=target_str,
                    goal=request.goal,
                    original_code=original_content,
                    failure_reason=last_error or "Unknown failure after all iterations",
                    attempts=request.max_iterations,
                )
                self.logger.warning(
                    f"Improvement queued for manual review: {review_id}\n"
                    f"  Target: {target_str}\n"
                    f"  Goal: {request.goal[:100]}...\n"
                    f"  Reason: {last_error}"
                )
            except Exception as e:
                self.logger.error(f"Failed to queue for manual review: {e}")

        progress.phase = ImprovementPhase.FAILED
        progress.error = last_error
        await self._progress_broadcaster.broadcast(progress)

        return ImprovementResult(
            success=False,
            task_id=task_id,
            target_file=str(request.target_file),
            goal=request.goal,
            iterations=request.max_iterations,
            total_time=time.time() - start_time,
            provider_used=provider_used,
            error=last_error,
        )

    async def _generate_improvement(
        self,
        original_code: str,
        goal: str,
        error_log: Optional[str],
        context: Optional[str],
    ) -> Tuple[Optional[str], str]:
        """Generate improved code using available providers.

        Provider hierarchy (graceful degradation):
        1. Trinity Integration (preferred) - UnifiedModelServing + Neural Mesh
        2. Brain Orchestrator - Direct Ollama/Provider access
        3. Legacy Integration - Original integration.py
        """
        # Build prompt
        prompt = self._build_improvement_prompt(original_code, goal, error_log, context)
        system_prompt = (
            "You are an expert Python developer. You improve code based on goals "
            "and fix errors. Return ONLY valid Python code in ```python blocks."
        )

        # Strategy 1: Trinity Integration (preferred - production-grade)
        if self._trinity_integration:
            try:
                content, provider = await self._trinity_integration.generate_improvement(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    temperature=0.3,
                    max_tokens=4096,
                )
                if content:
                    extracted = self._extract_code(content)
                    if extracted:
                        return extracted, f"trinity:{provider}"
            except Exception as e:
                self.logger.warning(f"Trinity Integration failed: {e}")

        # Strategy 2: Brain Orchestrator (fallback)
        if self._brain_orchestrator:
            provider = self._brain_orchestrator.get_best_provider()
            if provider and provider.is_healthy:
                circuit = self._circuit_breakers.get(provider.name)
                if circuit and await circuit.can_execute():
                    try:
                        result = await self._call_provider(
                            provider.endpoint,
                            prompt,
                        )
                        await circuit.record_success()
                        extracted = self._extract_code(result)
                        if extracted:
                            return extracted, f"brain:{provider.name}"
                    except Exception as e:
                        await circuit.record_failure(str(e))

        # Strategy 3: Legacy Integration (final fallback)
        if self._legacy_integration:
            try:
                result = await self._legacy_integration.generate_improvement(
                    original_code=original_code,
                    goal=goal,
                    error_log=error_log,
                    context=context,
                )
                if result:
                    return result, "legacy"
            except Exception as e:
                self.logger.warning(f"Legacy integration failed: {e}")

        return None, ""

    def _build_improvement_prompt(
        self,
        original_code: str,
        goal: str,
        error_log: Optional[str],
        context: Optional[str],
    ) -> str:
        """Build the improvement prompt."""
        parts = [
            "You are an expert Python developer. Improve the following code.",
            "",
            "## Goal",
            goal,
            "",
            "## Original Code",
            "```python",
            original_code,
            "```",
            "",
        ]

        if error_log:
            parts.extend([
                "## Previous Error (fix this)",
                "```",
                error_log[:2000],
                "```",
                "",
            ])

        if context:
            parts.extend([
                "## Additional Context",
                context[:3000],
                "",
            ])

        parts.extend([
            "## Instructions",
            "1. Return ONLY the improved Python code",
            "2. Wrap the code in ```python ... ``` markers",
            "3. Preserve all existing functionality",
            "4. Add comments only where necessary",
            "5. Follow PEP 8 style guidelines",
        ])

        return "\n".join(parts)

    async def _call_provider(
        self,
        endpoint: str,
        prompt: str,
    ) -> str:
        """Call an LLM provider."""
        if not aiohttp:
            raise RuntimeError("aiohttp not available")

        async with aiohttp.ClientSession() as session:
            payload = {
                "model": "default",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,
                "max_tokens": 4096,
            }

            timeout = aiohttp.ClientTimeout(total=NativeConfig.get_llm_timeout())
            url = f"{endpoint.rstrip('/')}/v1/chat/completions"

            async with session.post(url, json=payload, timeout=timeout) as resp:
                if resp.status != 200:
                    text = await resp.text()
                    raise RuntimeError(f"Provider error ({resp.status}): {text[:500]}")

                data = await resp.json()
                choices = data.get("choices", [])
                if not choices:
                    raise RuntimeError("No choices in response")

                message = choices[0].get("message", {})
                content = message.get("content", "")
                if not content:
                    raise RuntimeError("Empty content in response")

                return content

    def _extract_code(self, response: str) -> str:
        """Extract code from LLM response."""
        # Prefer python-tagged blocks, then any code block, then raw response
        patterns = [
            r"```python\s*([\s\S]*?)```",
            r"```\s*([\s\S]*?)```",
        ]

        for pattern in patterns:
            matches = list(re.finditer(pattern, response))
            if matches:
                # Return the LAST match (model often puts final code at end)
                return matches[-1].group(1).strip()

        return response.strip()

    def _validate_syntax(self, code: str) -> Tuple[bool, Optional[str]]:
        """Validate Python syntax."""
        import ast
        try:
            ast.parse(code)
            return True, None
        except SyntaxError as e:
            return False, f"Line {e.lineno}: {e.msg}"
        except Exception as e:
            return False, str(e)

    async def _read_file_safe(self, path: Path) -> str:
        """Safely read a file with size limits."""
        # Validate path again
        path = SecurityValidator.validate_path(path)

        # Check size
        size = path.stat().st_size
        if size > NativeConfig.get_max_file_size():
            raise SecurityError(f"File too large: {size} bytes")

        # Read with explicit encoding
        if aiofiles:
            async with aiofiles.open(path, "r", encoding="utf-8") as f:
                return await f.read()
        else:
            return await asyncio.to_thread(path.read_text, "utf-8")

    async def _write_file_safe(self, path: Path, content: str) -> None:
        """Safely write a file."""
        # Validate path
        path = SecurityValidator.validate_path(path)

        if aiofiles:
            async with aiofiles.open(path, "w", encoding="utf-8") as f:
                await f.write(content)
        else:
            await asyncio.to_thread(path.write_text, content, "utf-8")

    async def _run_tests_safe(
        self,
        target_file: Path,
        modified_content: str,
        test_command: str,
    ) -> Tuple[bool, str]:
        """Run tests safely in sandbox."""
        if not NativeConfig.is_sandbox_enabled():
            self.logger.warning("Sandbox disabled - skipping test validation")
            return True, "Sandbox disabled"

        # Create sandbox directory
        sandbox_dir = NativeConfig.get_sandbox_dir() / f"sandbox_{uuid.uuid4().hex[:8]}"
        sandbox_dir.mkdir(parents=True, exist_ok=True)

        try:
            # Copy file to sandbox
            sandbox_file = sandbox_dir / target_file.name
            sandbox_file.write_text(modified_content, encoding="utf-8")

            # Build safe command
            cmd, env = SecurityValidator.build_safe_command(
                "python",
                ["-m", "pytest", str(sandbox_file), "-v", "--tb=short"],
                {"PYTHONPATH": str(sandbox_dir)},
            )

            # Execute with timeout
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=sandbox_dir,
                env=env,
            )

            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(),
                    timeout=NativeConfig.get_test_timeout(),
                )

                output = stdout.decode()[:NativeConfig.get_max_output_size()]
                output += stderr.decode()[:NativeConfig.get_max_output_size()]

                return process.returncode == 0, output

            except asyncio.TimeoutError:
                # Graceful termination
                process.terminate()
                try:
                    await asyncio.wait_for(process.wait(), timeout=5.0)
                except asyncio.TimeoutError:
                    process.kill()
                return False, "Test timeout"

        finally:
            # Cleanup sandbox
            try:
                shutil.rmtree(sandbox_dir)
            except Exception:
                pass

    async def _git_commit_safe(self, file_path: Path, goal: str) -> None:
        """Safely commit changes to git."""
        # Build safe commit message
        message = f"[Ouroboros] {goal[:100]}"

        cmd, env = SecurityValidator.build_safe_command(
            "git",
            ["commit", "-m", message, "--", str(file_path)],
        )

        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=file_path.parent,
            env=env,
        )

        await process.communicate()

    async def _publish_experience(
        self,
        original_code: str,
        improved_code: str,
        goal: str,
        success: bool,
        iterations: int,
        provider_used: str = "",
        duration_seconds: float = 0.0,
    ) -> None:
        """Publish improvement experience to Reactor Core.

        Uses multi-channel publishing for reliability:
        1. Trinity Integration (preferred) - CrossRepoExperienceForwarder + Neural Mesh
        2. Legacy Integration (fallback) - Original integration.py
        """
        # Strategy 1: Trinity Integration (preferred - multi-channel)
        if self._trinity_integration:
            try:
                await self._trinity_integration.publish_experience(
                    original_code=original_code,
                    improved_code=improved_code,
                    goal=goal,
                    success=success,
                    iterations=iterations,
                    provider_used=provider_used,
                    duration_seconds=duration_seconds,
                )
                return  # Success - Trinity handles fallbacks internally
            except Exception as e:
                self.logger.warning(f"Trinity experience publish failed: {e}")

        # Strategy 2: Legacy Integration (fallback)
        if self._legacy_integration:
            try:
                await self._legacy_integration.publish_experience(
                    original_code=original_code,
                    improved_code=improved_code,
                    goal=goal,
                    success=success,
                    iterations=iterations,
                )
            except Exception as e:
                self.logger.warning(f"Legacy experience publish failed: {e}")

    def _generate_diff(self, original: str, modified: str) -> str:
        """Generate a simple diff."""
        import difflib
        diff = difflib.unified_diff(
            original.splitlines(keepends=True),
            modified.splitlines(keepends=True),
            fromfile="original",
            tofile="modified",
        )
        return "".join(diff)

    def get_status(self) -> Dict[str, Any]:
        """Get engine status."""
        status = {
            "running": self._running,
            "active_tasks": len(self._active_tasks),
            "metrics": self._metrics.snapshot(),
            "circuit_breakers": {
                name: cb.get_status()
                for name, cb in self._circuit_breakers.items()
            },
            "integration": {
                "trinity_available": self._trinity_integration is not None,
                "brain_orchestrator_available": self._brain_orchestrator is not None,
                "legacy_integration_available": self._legacy_integration is not None,
            },
        }

        # Add Trinity Integration status if available
        if self._trinity_integration:
            try:
                status["trinity"] = self._trinity_integration.get_status()
            except Exception:
                status["trinity"] = {"error": "Unable to retrieve status"}

        return status


# =============================================================================
# GLOBAL INSTANCE & CONVENIENCE FUNCTIONS
# =============================================================================

_native_engine: Optional[NativeSelfImprovement] = None


def get_native_self_improvement() -> NativeSelfImprovement:
    """Get the global native self-improvement engine."""
    global _native_engine
    if _native_engine is None:
        _native_engine = NativeSelfImprovement()
    return _native_engine


async def initialize_native_self_improvement() -> None:
    """Initialize the native self-improvement engine."""
    engine = get_native_self_improvement()
    await engine.initialize()


async def shutdown_native_self_improvement() -> None:
    """Shutdown the native self-improvement engine."""
    global _native_engine
    if _native_engine:
        await _native_engine.shutdown()
        _native_engine = None


def is_native_self_improvement_running() -> bool:
    """
    Check if the native self-improvement engine exists and is running
    WITHOUT creating a new instance.

    This is critical for probes/health checks that need to verify state
    without side effects.

    Returns:
        True if engine exists and is running, False otherwise
    """
    return _native_engine is not None and getattr(_native_engine, '_running', False)


def get_native_self_improvement_if_exists() -> Optional[NativeSelfImprovement]:
    """
    Get the native self-improvement instance if it exists, without creating a new one.

    Returns:
        The existing engine instance or None if not initialized
    """
    return _native_engine


async def execute_self_improvement(
    target: Union[str, Path],
    goal: str,
    test_command: Optional[str] = None,
    context: Optional[str] = None,
    max_iterations: int = 5,
    dry_run: bool = False,
    auto_commit: bool = False,
) -> ImprovementResult:
    """
    Execute self-improvement on a target file.

    This is the main convenience function - the "motor function" of Ironcliw.

    Example:
        result = await execute_self_improvement(
            target="backend/core/utils.py",
            goal="Fix the race condition in the cache update"
        )

        if result.success:
            print(f"Fixed in {result.iterations} iteration(s)!")
    """
    engine = get_native_self_improvement()
    if not engine._running:
        await engine.initialize()

    return await engine.execute_self_improvement(
        target=target,
        goal=goal,
        test_command=test_command,
        context=context,
        max_iterations=max_iterations,
        dry_run=dry_run,
        auto_commit=auto_commit,
    )


# =============================================================================
# CLAUDE CODE-LIKE BEHAVIORS v1.0
# =============================================================================
# These additions provide Claude Code-like features:
# 1. Diff Preview - Show changes before applying with user approval
# 2. Multi-File Orchestration - Atomic editing of multiple related files
# 3. Session Memory - Track what changed in current session
# 4. Streaming Changes - Real-time diff updates to UI
# 5. Iterative Refinement - User-directed change refinement loop
# 6. Connection Pooling - Efficient API connection reuse
# =============================================================================


@dataclass
class DiffChunk:
    """A chunk of a diff for streaming."""
    file_path: str
    line_start: int
    line_end: int
    old_content: str
    new_content: str
    change_type: str  # 'add', 'remove', 'modify'
    context_before: List[str] = field(default_factory=list)
    context_after: List[str] = field(default_factory=list)


@dataclass
class DiffPreview:
    """Complete diff preview for user approval with git integration."""
    id: str
    files: Dict[str, List[DiffChunk]]
    total_additions: int = 0
    total_deletions: int = 0
    total_modifications: int = 0
    risk_score: float = 0.0
    affected_entities: List[str] = field(default_factory=list)
    generated_at: float = field(default_factory=time.time)
    expires_at: float = 0.0
    # v3.1: Git integration fields
    git_tracked: bool = False
    git_root: Optional[str] = None
    relative_path: Optional[str] = None
    commit_message: Optional[str] = None
    branch_name: Optional[str] = None

    def __post_init__(self):
        if self.expires_at == 0.0:
            # Preview expires after 5 minutes
            self.expires_at = self.generated_at + 300.0

    @property
    def is_expired(self) -> bool:
        return time.time() > self.expires_at

    @property
    def file_count(self) -> int:
        """Number of files in this preview."""
        return len(self.files)

    @property
    def total_changes(self) -> int:
        """Total number of changes across all files."""
        return self.total_additions + self.total_deletions + self.total_modifications

    def to_unified_diff(self) -> str:
        """Generate unified diff string for display."""
        lines = []
        for file_path, chunks in self.files.items():
            lines.append(f"--- a/{file_path}")
            lines.append(f"+++ b/{file_path}")
            for chunk in chunks:
                lines.append(f"@@ -{chunk.line_start},{len(chunk.old_content.splitlines())} "
                           f"+{chunk.line_start},{len(chunk.new_content.splitlines())} @@")
                for ctx in chunk.context_before:
                    lines.append(f" {ctx}")
                for old_line in chunk.old_content.splitlines():
                    lines.append(f"-{old_line}")
                for new_line in chunk.new_content.splitlines():
                    lines.append(f"+{new_line}")
                for ctx in chunk.context_after:
                    lines.append(f" {ctx}")
        return "\n".join(lines)

    def to_git_diff_format(self) -> str:
        """Generate git-style diff format with headers."""
        lines = []

        for file_path, chunks in self.files.items():
            # Git diff header
            rel_path = self.relative_path if self.relative_path else file_path
            lines.append(f"diff --git a/{rel_path} b/{rel_path}")
            lines.append(f"--- a/{rel_path}")
            lines.append(f"+++ b/{rel_path}")

            for chunk in chunks:
                # Hunk header
                old_count = len(chunk.old_content.splitlines()) if chunk.old_content else 0
                new_count = len(chunk.new_content.splitlines()) if chunk.new_content else 0
                lines.append(f"@@ -{chunk.line_start},{old_count} +{chunk.line_start},{new_count} @@")

                # Context before
                for ctx in chunk.context_before:
                    lines.append(f" {ctx}")

                # Changes
                for old_line in chunk.old_content.splitlines():
                    lines.append(f"-{old_line}")
                for new_line in chunk.new_content.splitlines():
                    lines.append(f"+{new_line}")

                # Context after
                for ctx in chunk.context_after:
                    lines.append(f" {ctx}")

        return "\n".join(lines)

    def to_summary(self) -> Dict[str, Any]:
        """Generate a summary dict for UI display."""
        return {
            "id": self.id,
            "file_count": self.file_count,
            "total_additions": self.total_additions,
            "total_deletions": self.total_deletions,
            "total_modifications": self.total_modifications,
            "total_changes": self.total_changes,
            "risk_score": self.risk_score,
            "risk_level": "high" if self.risk_score > 0.7 else "medium" if self.risk_score > 0.3 else "low",
            "git_tracked": self.git_tracked,
            "commit_message": self.commit_message,
            "is_expired": self.is_expired,
            "files": list(self.files.keys()),
        }


class DiffPreviewEngine:
    """
    Provides diff preview before applying changes - Claude Code-like behavior.

    Flow:
    1. Generate improvement
    2. Create diff preview
    3. Stream diff to user
    4. Wait for user approval/rejection/modification request
    5. Apply or iterate based on feedback
    """

    def __init__(self):
        self._pending_previews: Dict[str, DiffPreview] = {}
        self._approval_callbacks: Dict[str, asyncio.Event] = {}
        self._approval_results: Dict[str, Tuple[bool, Optional[str]]] = {}
        self._lock = asyncio.Lock()
        self._stream_listeners: List[Callable[[DiffChunk], Awaitable[None]]] = []

    def add_stream_listener(
        self, callback: Callable[[DiffChunk], Awaitable[None]]
    ) -> Callable[[], None]:
        """Add a listener for streaming diff chunks."""
        self._stream_listeners.append(callback)
        return lambda: self._stream_listeners.remove(callback) if callback in self._stream_listeners else None

    async def create_preview(
        self,
        original_content: str,
        modified_content: str,
        file_path: str,
        goal: str,
    ) -> DiffPreview:
        """Create a diff preview for user approval."""
        import difflib

        preview_id = f"preview_{uuid.uuid4().hex[:12]}"

        # Parse both versions
        original_lines = original_content.splitlines(keepends=True)
        modified_lines = modified_content.splitlines(keepends=True)

        # Generate unified diff
        differ = difflib.unified_diff(
            original_lines,
            modified_lines,
            fromfile=f"a/{file_path}",
            tofile=f"b/{file_path}",
            lineterm=""
        )
        diff_lines = list(differ)

        # Parse diff into chunks
        chunks = self._parse_diff_chunks(diff_lines, original_lines, modified_lines)

        # Calculate statistics
        additions = sum(1 for c in chunks if c.change_type == 'add')
        deletions = sum(1 for c in chunks if c.change_type == 'remove')
        modifications = sum(1 for c in chunks if c.change_type == 'modify')

        # Calculate risk score based on change magnitude
        total_original_lines = len(original_lines)
        total_changes = additions + deletions + modifications
        risk_score = min(1.0, total_changes / max(total_original_lines, 1) * 2)

        preview = DiffPreview(
            id=preview_id,
            files={file_path: chunks},
            total_additions=additions,
            total_deletions=deletions,
            total_modifications=modifications,
            risk_score=risk_score,
        )

        async with self._lock:
            self._pending_previews[preview_id] = preview
            self._approval_callbacks[preview_id] = asyncio.Event()

        return preview

    def _parse_diff_chunks(
        self,
        diff_lines: List[str],
        original_lines: List[str],
        modified_lines: List[str],
    ) -> List[DiffChunk]:
        """Parse unified diff into structured chunks."""
        chunks = []
        current_chunk = None
        line_num = 0

        for line in diff_lines:
            if line.startswith('@@'):
                # Parse hunk header
                import re
                match = re.match(r'@@ -(\d+),?\d* \+(\d+),?\d* @@', line)
                if match:
                    if current_chunk:
                        chunks.append(current_chunk)
                    line_num = int(match.group(1))
                    current_chunk = DiffChunk(
                        file_path="",
                        line_start=line_num,
                        line_end=line_num,
                        old_content="",
                        new_content="",
                        change_type="modify",
                    )
            elif current_chunk is not None:
                if line.startswith('-') and not line.startswith('---'):
                    current_chunk.old_content += line[1:] + "\n"
                    current_chunk.change_type = "remove" if not current_chunk.new_content else "modify"
                elif line.startswith('+') and not line.startswith('+++'):
                    current_chunk.new_content += line[1:] + "\n"
                    current_chunk.change_type = "add" if not current_chunk.old_content else "modify"
                elif line.startswith(' '):
                    if not current_chunk.old_content and not current_chunk.new_content:
                        current_chunk.context_before.append(line[1:])
                    else:
                        current_chunk.context_after.append(line[1:])
                    line_num += 1

        if current_chunk and (current_chunk.old_content or current_chunk.new_content):
            chunks.append(current_chunk)

        return chunks

    # =========================================================================
    # v3.1: Git Integration for Proper Diff Format
    # =========================================================================

    async def is_git_tracked(self, file_path: str) -> bool:
        """Check if a file is tracked in a git repository."""
        try:
            proc = await asyncio.create_subprocess_exec(
                "git", "ls-files", "--error-unmatch", file_path,
                cwd=os.path.dirname(file_path) or ".",
                stdout=asyncio.subprocess.DEVNULL,
                stderr=asyncio.subprocess.DEVNULL,
            )
            await proc.wait()
            return proc.returncode == 0
        except Exception:
            return False

    async def get_git_root(self, file_path: str) -> Optional[str]:
        """Get the git repository root for a file."""
        try:
            proc = await asyncio.create_subprocess_exec(
                "git", "rev-parse", "--show-toplevel",
                cwd=os.path.dirname(file_path) or ".",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.DEVNULL,
            )
            stdout, _ = await proc.communicate()
            if proc.returncode == 0:
                return stdout.decode().strip()
        except Exception:
            pass
        return None

    async def create_git_preview(
        self,
        original_content: str,
        modified_content: str,
        file_path: str,
        goal: str,
    ) -> DiffPreview:
        """
        Create a diff preview using git for proper formatting.

        Falls back to difflib if file is not git-tracked.
        """
        # Check if file is in a git repo
        git_root = await self.get_git_root(file_path)
        if git_root is None:
            # Fall back to standard diff
            return await self.create_preview(original_content, modified_content, file_path, goal)

        preview_id = f"git_preview_{uuid.uuid4().hex[:12]}"

        # Get relative path for git
        abs_path = os.path.abspath(file_path)
        rel_path = os.path.relpath(abs_path, git_root)

        # Create temp file with modified content for git diff
        import tempfile
        diff_output = ""

        try:
            # Write original to temp file to compare
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as tmp_orig:
                tmp_orig.write(original_content)
                tmp_orig_path = tmp_orig.name

            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as tmp_mod:
                tmp_mod.write(modified_content)
                tmp_mod_path = tmp_mod.name

            # Use git diff with no-index for comparing arbitrary files
            proc = await asyncio.create_subprocess_exec(
                "git", "diff", "--no-index", "--no-color",
                f"--src-prefix=a/", f"--dst-prefix=b/",
                tmp_orig_path, tmp_mod_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.DEVNULL,
            )
            stdout, _ = await proc.communicate()
            diff_output = stdout.decode()

            # Replace temp paths with actual file path in output
            diff_output = diff_output.replace(tmp_orig_path, rel_path)
            diff_output = diff_output.replace(tmp_mod_path, rel_path)

        except Exception as e:
            logger.warning(f"Git diff failed, falling back to difflib: {e}")
            return await self.create_preview(original_content, modified_content, file_path, goal)
        finally:
            # Cleanup temp files
            for tmp in [tmp_orig_path, tmp_mod_path]:
                try:
                    os.unlink(tmp)
                except Exception:
                    pass

        # Parse git diff output into chunks
        chunks = self._parse_git_diff(diff_output, file_path)

        # Calculate statistics
        additions = sum(1 for c in chunks if c.change_type == 'add')
        deletions = sum(1 for c in chunks if c.change_type == 'remove')
        modifications = sum(1 for c in chunks if c.change_type == 'modify')

        # Calculate risk score
        original_lines = len(original_content.splitlines())
        total_changes = additions + deletions + modifications
        risk_score = min(1.0, total_changes / max(original_lines, 1) * 2)

        preview = DiffPreview(
            id=preview_id,
            files={file_path: chunks},
            total_additions=additions,
            total_deletions=deletions,
            total_modifications=modifications,
            risk_score=risk_score,
            git_tracked=True,
            git_root=git_root,
            relative_path=rel_path,
        )

        async with self._lock:
            self._pending_previews[preview_id] = preview
            self._approval_callbacks[preview_id] = asyncio.Event()

        return preview

    def _parse_git_diff(self, diff_output: str, file_path: str) -> List[DiffChunk]:
        """Parse git diff output into DiffChunk objects."""
        chunks = []
        current_chunk = None
        in_hunk = False

        for line in diff_output.splitlines():
            if line.startswith('@@'):
                # New hunk - save previous chunk
                if current_chunk and (current_chunk.old_content or current_chunk.new_content):
                    chunks.append(current_chunk)

                # Parse hunk header: @@ -start,count +start,count @@
                import re
                match = re.match(r'@@ -(\d+)(?:,\d+)? \+(\d+)(?:,\d+)? @@', line)
                if match:
                    old_start = int(match.group(1))
                    new_start = int(match.group(2))
                    current_chunk = DiffChunk(
                        file_path=file_path,
                        line_start=old_start,
                        line_end=old_start,
                        old_content="",
                        new_content="",
                        change_type="modify",
                    )
                    in_hunk = True
            elif in_hunk and current_chunk is not None:
                if line.startswith('-') and not line.startswith('---'):
                    # Removed line
                    current_chunk.old_content += line[1:] + "\n"
                    current_chunk.line_end += 1
                    if not current_chunk.new_content:
                        current_chunk.change_type = "remove"
                elif line.startswith('+') and not line.startswith('+++'):
                    # Added line
                    current_chunk.new_content += line[1:] + "\n"
                    if not current_chunk.old_content:
                        current_chunk.change_type = "add"
                    else:
                        current_chunk.change_type = "modify"
                elif line.startswith(' '):
                    # Context line
                    if not current_chunk.old_content and not current_chunk.new_content:
                        current_chunk.context_before.append(line[1:])
                    else:
                        current_chunk.context_after.append(line[1:])
            elif line.startswith('diff --git'):
                # Start of new file diff - reset state
                if current_chunk and (current_chunk.old_content or current_chunk.new_content):
                    chunks.append(current_chunk)
                current_chunk = None
                in_hunk = False

        # Don't forget the last chunk
        if current_chunk and (current_chunk.old_content or current_chunk.new_content):
            chunks.append(current_chunk)

        return chunks

    async def get_git_status(self, file_path: str) -> Dict[str, Any]:
        """Get detailed git status for a file."""
        result = {
            "tracked": False,
            "staged": False,
            "modified": False,
            "untracked": False,
            "git_root": None,
            "relative_path": None,
        }

        git_root = await self.get_git_root(file_path)
        if git_root is None:
            return result

        result["git_root"] = git_root
        abs_path = os.path.abspath(file_path)
        result["relative_path"] = os.path.relpath(abs_path, git_root)

        try:
            # Get porcelain status
            proc = await asyncio.create_subprocess_exec(
                "git", "status", "--porcelain", result["relative_path"],
                cwd=git_root,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.DEVNULL,
            )
            stdout, _ = await proc.communicate()

            if proc.returncode == 0:
                status_line = stdout.decode().strip()
                if status_line:
                    xy = status_line[:2]
                    index_status = xy[0]
                    worktree_status = xy[1]

                    result["tracked"] = index_status != '?' and worktree_status != '?'
                    result["staged"] = index_status in 'MADRCU'
                    result["modified"] = worktree_status in 'M'
                    result["untracked"] = xy == '??'
                else:
                    # Empty output means clean and tracked
                    result["tracked"] = await self.is_git_tracked(file_path)
        except Exception as e:
            logger.warning(f"Failed to get git status: {e}")

        return result

    async def stage_changes(self, file_path: str) -> bool:
        """Stage changes to git (git add)."""
        git_root = await self.get_git_root(file_path)
        if git_root is None:
            return False

        try:
            abs_path = os.path.abspath(file_path)
            rel_path = os.path.relpath(abs_path, git_root)

            proc = await asyncio.create_subprocess_exec(
                "git", "add", rel_path,
                cwd=git_root,
                stdout=asyncio.subprocess.DEVNULL,
                stderr=asyncio.subprocess.DEVNULL,
            )
            await proc.wait()
            return proc.returncode == 0
        except Exception as e:
            logger.error(f"Failed to stage changes: {e}")
            return False

    async def get_head_version(self, file_path: str) -> Optional[str]:
        """Get the HEAD version of a file from git."""
        git_root = await self.get_git_root(file_path)
        if git_root is None:
            return None

        try:
            abs_path = os.path.abspath(file_path)
            rel_path = os.path.relpath(abs_path, git_root)

            proc = await asyncio.create_subprocess_exec(
                "git", "show", f"HEAD:{rel_path}",
                cwd=git_root,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.DEVNULL,
            )
            stdout, _ = await proc.communicate()

            if proc.returncode == 0:
                return stdout.decode()
        except Exception as e:
            logger.warning(f"Failed to get HEAD version: {e}")

        return None

    async def create_commit_preview(
        self,
        files: Dict[str, Tuple[str, str]],  # path -> (original, modified)
        commit_message: str,
    ) -> DiffPreview:
        """
        Create a preview for a multi-file commit.

        Args:
            files: Dict mapping file paths to (original_content, modified_content) tuples
            commit_message: Proposed commit message

        Returns:
            DiffPreview with all file changes consolidated
        """
        preview_id = f"commit_preview_{uuid.uuid4().hex[:12]}"
        all_chunks: Dict[str, List[DiffChunk]] = {}
        total_additions = 0
        total_deletions = 0
        total_modifications = 0

        for file_path, (original, modified) in files.items():
            # Use git diff for each file
            sub_preview = await self.create_git_preview(original, modified, file_path, commit_message)
            if sub_preview.files:
                all_chunks.update(sub_preview.files)
                total_additions += sub_preview.total_additions
                total_deletions += sub_preview.total_deletions
                total_modifications += sub_preview.total_modifications

        # Calculate overall risk score
        total_changes = total_additions + total_deletions + total_modifications
        risk_score = min(1.0, total_changes / 100)  # Scale by 100 lines

        preview = DiffPreview(
            id=preview_id,
            files=all_chunks,
            total_additions=total_additions,
            total_deletions=total_deletions,
            total_modifications=total_modifications,
            risk_score=risk_score,
            commit_message=commit_message,
        )

        async with self._lock:
            self._pending_previews[preview_id] = preview
            self._approval_callbacks[preview_id] = asyncio.Event()

        return preview

    async def stream_preview(self, preview: DiffPreview) -> None:
        """Stream diff chunks to all listeners."""
        for file_path, chunks in preview.files.items():
            for chunk in chunks:
                chunk.file_path = file_path
                for listener in self._stream_listeners:
                    try:
                        await listener(chunk)
                    except Exception as e:
                        logger.warning(f"Stream listener error: {e}")
                # Small delay for visual effect
                await asyncio.sleep(0.05)

    async def wait_for_approval(
        self,
        preview_id: str,
        timeout: float = 300.0,
    ) -> Tuple[bool, Optional[str]]:
        """
        Wait for user approval of a preview.

        Returns:
            Tuple of (approved, feedback)
            - approved=True, feedback=None: Apply changes
            - approved=False, feedback=None: Reject changes
            - approved=False, feedback="...": Request modifications
        """
        async with self._lock:
            event = self._approval_callbacks.get(preview_id)
            if not event:
                return False, "Preview not found or expired"

        try:
            await asyncio.wait_for(event.wait(), timeout=timeout)
            async with self._lock:
                result = self._approval_results.get(preview_id, (False, "No response"))
                # Cleanup
                self._pending_previews.pop(preview_id, None)
                self._approval_callbacks.pop(preview_id, None)
                self._approval_results.pop(preview_id, None)
                return result
        except asyncio.TimeoutError:
            async with self._lock:
                self._pending_previews.pop(preview_id, None)
                self._approval_callbacks.pop(preview_id, None)
            return False, "Approval timeout"

    async def submit_approval(
        self,
        preview_id: str,
        approved: bool,
        feedback: Optional[str] = None,
    ) -> bool:
        """Submit user's approval decision."""
        async with self._lock:
            event = self._approval_callbacks.get(preview_id)
            if not event:
                return False

            self._approval_results[preview_id] = (approved, feedback)
            event.set()
            return True

    def get_pending_previews(self) -> List[DiffPreview]:
        """Get all pending previews awaiting approval."""
        return [p for p in self._pending_previews.values() if not p.is_expired]


class SessionMemoryManager:
    """
    Tracks changes made within the current session.

    Provides:
    - What files were modified
    - What changes were made
    - Session-scoped rollback capability
    - Cross-reference between changes
    """

    def __init__(self):
        self._session_id = f"session_{uuid.uuid4().hex[:12]}"
        self._started_at = time.time()
        self._changes: List[Dict[str, Any]] = []
        self._file_snapshots: Dict[str, str] = {}  # path -> original content
        self._lock = asyncio.Lock()

    @property
    def session_id(self) -> str:
        return self._session_id

    @property
    def duration_seconds(self) -> float:
        return time.time() - self._started_at

    async def record_change(
        self,
        file_path: str,
        original_content: str,
        new_content: str,
        change_type: str,
        goal: str,
        success: bool,
    ) -> None:
        """Record a change made in this session."""
        async with self._lock:
            # Store original snapshot if first change to this file
            if file_path not in self._file_snapshots:
                self._file_snapshots[file_path] = original_content

            self._changes.append({
                "id": f"change_{uuid.uuid4().hex[:8]}",
                "file_path": file_path,
                "change_type": change_type,
                "goal": goal,
                "success": success,
                "timestamp": time.time(),
                "lines_added": len(new_content.splitlines()) - len(original_content.splitlines()),
                "original_hash": hashlib.md5(original_content.encode()).hexdigest()[:12],
                "new_hash": hashlib.md5(new_content.encode()).hexdigest()[:12],
            })

    async def get_session_summary(self) -> Dict[str, Any]:
        """Get a summary of the current session."""
        async with self._lock:
            files_modified = set(c["file_path"] for c in self._changes)
            successful = sum(1 for c in self._changes if c["success"])
            failed = len(self._changes) - successful

            return {
                "session_id": self._session_id,
                "duration_seconds": self.duration_seconds,
                "total_changes": len(self._changes),
                "files_modified": list(files_modified),
                "successful_changes": successful,
                "failed_changes": failed,
                "changes": self._changes[-10:],  # Last 10 changes
            }

    async def get_original_content(self, file_path: str) -> Optional[str]:
        """Get the original content of a file before any changes."""
        async with self._lock:
            return self._file_snapshots.get(file_path)

    async def rollback_session(self) -> Dict[str, bool]:
        """Rollback all changes in this session to original state."""
        results = {}
        async with self._lock:
            for file_path, original_content in self._file_snapshots.items():
                try:
                    path = Path(file_path)
                    if path.exists():
                        path.write_text(original_content, encoding="utf-8")
                        results[file_path] = True
                except Exception as e:
                    logger.error(f"Rollback failed for {file_path}: {e}")
                    results[file_path] = False
        return results

    def can_reference_previous_change(self, file_path: str) -> bool:
        """Check if we have previous changes for this file in session."""
        return file_path in self._file_snapshots


# =============================================================================
# v3.1: Checkpoint-Based Mid-Edit Error Recovery
# =============================================================================

@dataclass
class Checkpoint:
    """Represents a saved state checkpoint for error recovery."""
    id: str
    name: str
    created_at: float
    file_states: Dict[str, str]  # path -> content at checkpoint time
    metadata: Dict[str, Any] = field(default_factory=dict)
    parent_checkpoint: Optional[str] = None  # For nested checkpoints

    def __post_init__(self):
        if not self.metadata:
            self.metadata = {}


class CheckpointManager:
    """
    v3.1: Robust checkpoint-based error recovery for multi-file editing.

    Provides:
    - Named checkpoints with timestamps
    - Automatic pre-edit checkpointing
    - Nested checkpoints for complex operations
    - Atomic rollback to any checkpoint
    - Checkpoint persistence to disk for crash recovery
    - Cleanup of stale checkpoints

    Usage:
        manager = CheckpointManager()

        # Create checkpoint before risky operation
        cp_id = await manager.create_checkpoint("before_refactor", [file1, file2])

        try:
            # Perform edits...
            await edit_files()
        except Exception:
            # Rollback on error
            await manager.rollback_to_checkpoint(cp_id)
        finally:
            # Clean up checkpoint
            await manager.delete_checkpoint(cp_id)
    """

    DEFAULT_CHECKPOINT_DIR = ".jarvis_checkpoints"
    MAX_CHECKPOINTS = 50
    CHECKPOINT_TTL_HOURS = 24

    def __init__(
        self,
        checkpoint_dir: Optional[Path] = None,
        persist_to_disk: bool = True,
    ):
        """
        Initialize checkpoint manager.

        Args:
            checkpoint_dir: Directory to store checkpoint data. Defaults to .jarvis_checkpoints
            persist_to_disk: Whether to persist checkpoints to disk for crash recovery
        """
        self._checkpoint_dir = checkpoint_dir or Path(self.DEFAULT_CHECKPOINT_DIR)
        self._persist_to_disk = persist_to_disk
        self._checkpoints: Dict[str, Checkpoint] = {}
        self._checkpoint_stack: List[str] = []  # For nested checkpoints
        self._lock = asyncio.Lock()

        if self._persist_to_disk:
            self._checkpoint_dir.mkdir(parents=True, exist_ok=True)
            # Load existing checkpoints from disk
            asyncio.create_task(self._load_persisted_checkpoints())

    async def _load_persisted_checkpoints(self) -> None:
        """Load checkpoints from disk on startup."""
        if not self._persist_to_disk:
            return

        try:
            index_path = self._checkpoint_dir / "checkpoint_index.json"
            if index_path.exists():
                with open(index_path, 'r') as f:
                    index = json.load(f)

                for cp_id, cp_data in index.items():
                    # Load file states
                    file_states = {}
                    for file_path in cp_data.get("files", []):
                        state_file = self._checkpoint_dir / f"{cp_id}_{hashlib.md5(file_path.encode()).hexdigest()[:12]}.state"
                        if state_file.exists():
                            file_states[file_path] = state_file.read_text(encoding="utf-8")

                    self._checkpoints[cp_id] = Checkpoint(
                        id=cp_id,
                        name=cp_data.get("name", ""),
                        created_at=cp_data.get("created_at", 0),
                        file_states=file_states,
                        metadata=cp_data.get("metadata", {}),
                        parent_checkpoint=cp_data.get("parent_checkpoint"),
                    )

                logger.info(f"✅ Loaded {len(self._checkpoints)} checkpoints from disk")
        except Exception as e:
            logger.warning(f"Failed to load persisted checkpoints: {e}")

    async def _persist_checkpoint(self, checkpoint: Checkpoint) -> None:
        """Persist a checkpoint to disk."""
        if not self._persist_to_disk:
            return

        try:
            # Save file states
            for file_path, content in checkpoint.file_states.items():
                state_file = self._checkpoint_dir / f"{checkpoint.id}_{hashlib.md5(file_path.encode()).hexdigest()[:12]}.state"
                state_file.write_text(content, encoding="utf-8")

            # Update index
            index_path = self._checkpoint_dir / "checkpoint_index.json"
            index = {}
            if index_path.exists():
                with open(index_path, 'r') as f:
                    index = json.load(f)

            index[checkpoint.id] = {
                "name": checkpoint.name,
                "created_at": checkpoint.created_at,
                "files": list(checkpoint.file_states.keys()),
                "metadata": checkpoint.metadata,
                "parent_checkpoint": checkpoint.parent_checkpoint,
            }

            with open(index_path, 'w') as f:
                json.dump(index, f, indent=2)

        except Exception as e:
            logger.error(f"Failed to persist checkpoint {checkpoint.id}: {e}")

    async def create_checkpoint(
        self,
        name: str,
        file_paths: List[str],
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Create a checkpoint saving the current state of specified files.

        Args:
            name: Human-readable name for the checkpoint
            file_paths: List of file paths to include in checkpoint
            metadata: Optional metadata to attach to checkpoint

        Returns:
            Checkpoint ID for later reference
        """
        async with self._lock:
            checkpoint_id = f"cp_{uuid.uuid4().hex[:12]}"

            # Read current content of all files
            file_states = {}
            for file_path in file_paths:
                try:
                    path = Path(file_path)
                    if path.exists():
                        file_states[file_path] = path.read_text(encoding="utf-8")
                    else:
                        # File doesn't exist yet - store empty string as marker
                        file_states[file_path] = ""
                except Exception as e:
                    logger.warning(f"Failed to read {file_path} for checkpoint: {e}")

            # Get parent checkpoint if we're in a nested operation
            parent_id = self._checkpoint_stack[-1] if self._checkpoint_stack else None

            checkpoint = Checkpoint(
                id=checkpoint_id,
                name=name,
                created_at=time.time(),
                file_states=file_states,
                metadata=metadata or {},
                parent_checkpoint=parent_id,
            )

            self._checkpoints[checkpoint_id] = checkpoint
            self._checkpoint_stack.append(checkpoint_id)

            # Persist to disk
            await self._persist_checkpoint(checkpoint)

            # Cleanup old checkpoints if needed
            await self._cleanup_old_checkpoints()

            logger.info(f"✅ Created checkpoint '{name}' ({checkpoint_id}) with {len(file_states)} files")
            return checkpoint_id

    async def rollback_to_checkpoint(
        self,
        checkpoint_id: str,
        delete_after_rollback: bool = True,
    ) -> Dict[str, bool]:
        """
        Rollback all files to their state at the given checkpoint.

        Args:
            checkpoint_id: ID of checkpoint to rollback to
            delete_after_rollback: Whether to delete the checkpoint after successful rollback

        Returns:
            Dict mapping file paths to success status
        """
        async with self._lock:
            checkpoint = self._checkpoints.get(checkpoint_id)
            if not checkpoint:
                logger.error(f"Checkpoint {checkpoint_id} not found")
                return {}

            results = {}
            for file_path, original_content in checkpoint.file_states.items():
                try:
                    path = Path(file_path)
                    if original_content == "":
                        # File didn't exist at checkpoint time - delete if it exists now
                        if path.exists():
                            path.unlink()
                            logger.info(f"Deleted {file_path} (didn't exist at checkpoint)")
                        results[file_path] = True
                    else:
                        # Restore original content
                        path.parent.mkdir(parents=True, exist_ok=True)
                        path.write_text(original_content, encoding="utf-8")
                        results[file_path] = True
                        logger.info(f"Restored {file_path} to checkpoint state")
                except Exception as e:
                    logger.error(f"Failed to rollback {file_path}: {e}")
                    results[file_path] = False

            if delete_after_rollback:
                await self._delete_checkpoint_internal(checkpoint_id)

            # Pop from stack if this was the most recent checkpoint
            if self._checkpoint_stack and self._checkpoint_stack[-1] == checkpoint_id:
                self._checkpoint_stack.pop()

            return results

    async def get_checkpoint(self, checkpoint_id: str) -> Optional[Checkpoint]:
        """Get checkpoint by ID."""
        async with self._lock:
            return self._checkpoints.get(checkpoint_id)

    async def list_checkpoints(self) -> List[Dict[str, Any]]:
        """List all checkpoints with summary info."""
        async with self._lock:
            return [
                {
                    "id": cp.id,
                    "name": cp.name,
                    "created_at": cp.created_at,
                    "file_count": len(cp.file_states),
                    "files": list(cp.file_states.keys()),
                    "metadata": cp.metadata,
                    "parent": cp.parent_checkpoint,
                    "age_seconds": time.time() - cp.created_at,
                }
                for cp in sorted(
                    self._checkpoints.values(),
                    key=lambda c: c.created_at,
                    reverse=True
                )
            ]

    async def delete_checkpoint(self, checkpoint_id: str) -> bool:
        """Delete a checkpoint."""
        async with self._lock:
            return await self._delete_checkpoint_internal(checkpoint_id)

    async def _delete_checkpoint_internal(self, checkpoint_id: str) -> bool:
        """Internal method to delete checkpoint without acquiring lock."""
        checkpoint = self._checkpoints.pop(checkpoint_id, None)
        if not checkpoint:
            return False

        # Remove from stack
        if checkpoint_id in self._checkpoint_stack:
            self._checkpoint_stack.remove(checkpoint_id)

        # Delete persisted files
        if self._persist_to_disk:
            try:
                for file_path in checkpoint.file_states:
                    state_file = self._checkpoint_dir / f"{checkpoint_id}_{hashlib.md5(file_path.encode()).hexdigest()[:12]}.state"
                    if state_file.exists():
                        state_file.unlink()

                # Update index
                index_path = self._checkpoint_dir / "checkpoint_index.json"
                if index_path.exists():
                    with open(index_path, 'r') as f:
                        index = json.load(f)
                    index.pop(checkpoint_id, None)
                    with open(index_path, 'w') as f:
                        json.dump(index, f, indent=2)
            except Exception as e:
                logger.warning(f"Failed to delete persisted checkpoint files: {e}")

        return True

    async def _cleanup_old_checkpoints(self) -> int:
        """Remove checkpoints older than TTL or exceeding max count."""
        cleaned = 0
        cutoff = time.time() - (self.CHECKPOINT_TTL_HOURS * 3600)

        # Sort by creation time (oldest first)
        sorted_checkpoints = sorted(
            self._checkpoints.values(),
            key=lambda c: c.created_at
        )

        # Remove old checkpoints
        for cp in sorted_checkpoints:
            if cp.created_at < cutoff:
                await self._delete_checkpoint_internal(cp.id)
                cleaned += 1

        # Remove excess checkpoints
        while len(self._checkpoints) > self.MAX_CHECKPOINTS:
            oldest = min(self._checkpoints.values(), key=lambda c: c.created_at)
            await self._delete_checkpoint_internal(oldest.id)
            cleaned += 1

        if cleaned:
            logger.info(f"Cleaned up {cleaned} old checkpoints")

        return cleaned

    async def get_current_checkpoint_id(self) -> Optional[str]:
        """Get the ID of the current (most recent) checkpoint in the stack."""
        async with self._lock:
            return self._checkpoint_stack[-1] if self._checkpoint_stack else None

    async def pop_checkpoint(self) -> Optional[str]:
        """Pop and return the most recent checkpoint from the stack."""
        async with self._lock:
            if self._checkpoint_stack:
                return self._checkpoint_stack.pop()
            return None

    async def commit_checkpoint(self, checkpoint_id: str) -> bool:
        """
        Commit a checkpoint (mark edits as successful, delete checkpoint).

        Use this when edits complete successfully and you no longer need the
        rollback capability.
        """
        return await self.delete_checkpoint(checkpoint_id)

    @asynccontextmanager
    async def checkpoint_context(
        self,
        name: str,
        file_paths: List[str],
        metadata: Optional[Dict[str, Any]] = None,
        auto_rollback_on_error: bool = True,
    ):
        """
        Context manager for automatic checkpoint management.

        Usage:
            async with checkpoint_manager.checkpoint_context(
                "refactoring_foo",
                ["file1.py", "file2.py"]
            ) as cp_id:
                # Perform edits...
                # If an exception is raised, files are automatically rolled back

        Args:
            name: Checkpoint name
            file_paths: Files to checkpoint
            metadata: Optional metadata
            auto_rollback_on_error: Whether to automatically rollback on exception
        """
        checkpoint_id = await self.create_checkpoint(name, file_paths, metadata)
        try:
            yield checkpoint_id
            # Success - commit the checkpoint
            await self.commit_checkpoint(checkpoint_id)
        except Exception as e:
            if auto_rollback_on_error:
                logger.warning(f"Error during checkpoint '{name}', rolling back: {e}")
                await self.rollback_to_checkpoint(checkpoint_id)
            else:
                # Just delete checkpoint without rollback
                await self.delete_checkpoint(checkpoint_id)
            raise

    async def get_file_diff_from_checkpoint(
        self,
        checkpoint_id: str,
        file_path: str,
    ) -> Optional[str]:
        """
        Get diff between checkpoint state and current file state.

        Returns:
            Unified diff string, or None if file not in checkpoint
        """
        async with self._lock:
            checkpoint = self._checkpoints.get(checkpoint_id)
            if not checkpoint or file_path not in checkpoint.file_states:
                return None

            original = checkpoint.file_states[file_path]

            try:
                current = Path(file_path).read_text(encoding="utf-8")
            except Exception:
                current = ""

            import difflib
            diff = difflib.unified_diff(
                original.splitlines(keepends=True),
                current.splitlines(keepends=True),
                fromfile=f"checkpoint/{file_path}",
                tofile=f"current/{file_path}",
            )
            return "".join(diff)


# Global checkpoint manager instance
_checkpoint_manager: Optional[CheckpointManager] = None


def get_checkpoint_manager() -> CheckpointManager:
    """Get or create the global checkpoint manager."""
    global _checkpoint_manager
    if _checkpoint_manager is None:
        _checkpoint_manager = CheckpointManager()
    return _checkpoint_manager


# =============================================================================
# v3.1: Smart File Chunking for Large Files
# =============================================================================

class ChunkBoundaryType(Enum):
    """Types of logical boundaries for chunking."""
    CLASS = "class"
    FUNCTION = "function"
    METHOD = "method"
    MODULE_SECTION = "module_section"
    IMPORT_BLOCK = "import_block"
    DOCSTRING = "docstring"
    COMMENT_BLOCK = "comment_block"
    ARBITRARY = "arbitrary"  # Fallback when no logical boundary found


@dataclass
class FileChunk:
    """Represents a chunk of a large file."""
    id: str
    file_path: str
    start_line: int
    end_line: int
    content: str
    boundary_type: ChunkBoundaryType
    entity_name: Optional[str] = None  # e.g., class name, function name
    dependencies: List[str] = field(default_factory=list)  # IDs of dependent chunks
    context_before: str = ""  # Lines needed for context
    context_after: str = ""

    @property
    def line_count(self) -> int:
        return self.end_line - self.start_line + 1

    @property
    def token_estimate(self) -> int:
        """Rough estimate of tokens (4 chars per token)."""
        return len(self.content) // 4


@dataclass
class ChunkedFile:
    """A file that has been split into chunks."""
    file_path: str
    total_lines: int
    total_chunks: int
    chunks: List[FileChunk]
    chunk_order: List[str]  # IDs in order
    dependency_graph: Dict[str, List[str]]  # chunk_id -> list of dependent chunk_ids
    created_at: float = field(default_factory=time.time)

    def get_chunk_by_id(self, chunk_id: str) -> Optional[FileChunk]:
        """Get a chunk by ID."""
        for chunk in self.chunks:
            if chunk.id == chunk_id:
                return chunk
        return None

    def get_chunks_for_entity(self, entity_name: str) -> List[FileChunk]:
        """Get all chunks that belong to a named entity."""
        return [c for c in self.chunks if c.entity_name == entity_name]


class SmartFileChunker:
    """
    v3.1: Intelligent file chunking for large files that exceed context limits.

    Features:
    - AST-based parsing for Python files to find logical boundaries
    - Fallback to line-based chunking with overlap for other languages
    - Dependency tracking between chunks (e.g., methods need class context)
    - Configurable chunk sizes with soft/hard limits
    - Context preservation across chunk boundaries
    - Efficient reassembly after processing

    Usage:
        chunker = SmartFileChunker(max_chunk_tokens=4000)
        chunked = await chunker.chunk_file("large_file.py")

        for chunk in chunked.chunks:
            # Process each chunk...
            improved = await improve_chunk(chunk)

        # Reassemble
        final_content = await chunker.reassemble_chunks(chunked, modified_chunks)
    """

    DEFAULT_MAX_CHUNK_TOKENS = 4000  # ~16K characters
    DEFAULT_OVERLAP_LINES = 10  # Context overlap between chunks
    DEFAULT_CONTEXT_LINES = 5  # Lines of context before/after

    def __init__(
        self,
        max_chunk_tokens: int = DEFAULT_MAX_CHUNK_TOKENS,
        overlap_lines: int = DEFAULT_OVERLAP_LINES,
        context_lines: int = DEFAULT_CONTEXT_LINES,
    ):
        """
        Initialize chunker.

        Args:
            max_chunk_tokens: Maximum tokens per chunk (soft limit)
            overlap_lines: Lines of overlap between consecutive chunks
            context_lines: Lines of context to include before/after chunks
        """
        self._max_tokens = max_chunk_tokens
        self._overlap_lines = overlap_lines
        self._context_lines = context_lines

    def should_chunk(self, content: str) -> bool:
        """Check if a file needs to be chunked."""
        estimated_tokens = len(content) // 4
        return estimated_tokens > self._max_tokens

    async def chunk_file(
        self,
        file_path: str,
        content: Optional[str] = None,
    ) -> ChunkedFile:
        """
        Chunk a file into logical segments.

        Args:
            file_path: Path to the file
            content: Optional file content (reads from disk if not provided)

        Returns:
            ChunkedFile with chunks and metadata
        """
        if content is None:
            content = Path(file_path).read_text(encoding="utf-8")

        lines = content.splitlines(keepends=True)
        total_lines = len(lines)

        # Detect file type and use appropriate chunking strategy
        if file_path.endswith('.py'):
            chunks = await self._chunk_python_file(file_path, content, lines)
        else:
            # Fallback to line-based chunking
            chunks = await self._chunk_by_lines(file_path, content, lines)

        # Build dependency graph
        dependency_graph = self._build_dependency_graph(chunks)

        return ChunkedFile(
            file_path=file_path,
            total_lines=total_lines,
            total_chunks=len(chunks),
            chunks=chunks,
            chunk_order=[c.id for c in chunks],
            dependency_graph=dependency_graph,
        )

    async def _chunk_python_file(
        self,
        file_path: str,
        content: str,
        lines: List[str],
    ) -> List[FileChunk]:
        """Chunk a Python file using AST for logical boundaries."""
        chunks = []

        try:
            import ast
            tree = ast.parse(content)
        except SyntaxError:
            # Fallback to line-based if parsing fails
            return await self._chunk_by_lines(file_path, content, lines)

        # Extract top-level entities
        entities = []

        # First, handle imports (usually at the top)
        import_end = 0
        for node in ast.iter_child_nodes(tree):
            if isinstance(node, (ast.Import, ast.ImportFrom)):
                import_end = max(import_end, node.end_lineno or 0)
            else:
                break

        if import_end > 0:
            entities.append({
                "type": ChunkBoundaryType.IMPORT_BLOCK,
                "name": "__imports__",
                "start": 1,
                "end": import_end,
            })

        # Extract classes and functions
        for node in ast.iter_child_nodes(tree):
            if isinstance(node, ast.ClassDef):
                entities.append({
                    "type": ChunkBoundaryType.CLASS,
                    "name": node.name,
                    "start": node.lineno,
                    "end": node.end_lineno or node.lineno,
                })
            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                entities.append({
                    "type": ChunkBoundaryType.FUNCTION,
                    "name": node.name,
                    "start": node.lineno,
                    "end": node.end_lineno or node.lineno,
                })

        # Sort by start line
        entities.sort(key=lambda e: e["start"])

        # Create chunks from entities
        current_line = 1
        chunk_idx = 0

        for entity in entities:
            # Handle gap before this entity
            if entity["start"] > current_line:
                gap_start = current_line
                gap_end = entity["start"] - 1
                gap_content = "".join(lines[gap_start - 1:gap_end])

                if gap_content.strip():  # Only create chunk if non-empty
                    chunks.append(FileChunk(
                        id=f"chunk_{chunk_idx}_{hashlib.md5(gap_content.encode()).hexdigest()[:8]}",
                        file_path=file_path,
                        start_line=gap_start,
                        end_line=gap_end,
                        content=gap_content,
                        boundary_type=ChunkBoundaryType.MODULE_SECTION,
                        entity_name=None,
                    ))
                    chunk_idx += 1

            # Create chunk for this entity
            entity_content = "".join(lines[entity["start"] - 1:entity["end"]])

            # Check if entity is too large and needs sub-chunking
            if len(entity_content) // 4 > self._max_tokens:
                sub_chunks = await self._chunk_large_entity(
                    file_path, entity, lines, chunk_idx
                )
                chunks.extend(sub_chunks)
                chunk_idx += len(sub_chunks)
            else:
                chunks.append(FileChunk(
                    id=f"chunk_{chunk_idx}_{entity['name']}",
                    file_path=file_path,
                    start_line=entity["start"],
                    end_line=entity["end"],
                    content=entity_content,
                    boundary_type=entity["type"],
                    entity_name=entity["name"],
                ))
                chunk_idx += 1

            current_line = entity["end"] + 1

        # Handle remaining content after last entity
        if current_line <= len(lines):
            remaining = "".join(lines[current_line - 1:])
            if remaining.strip():
                chunks.append(FileChunk(
                    id=f"chunk_{chunk_idx}_tail",
                    file_path=file_path,
                    start_line=current_line,
                    end_line=len(lines),
                    content=remaining,
                    boundary_type=ChunkBoundaryType.MODULE_SECTION,
                ))

        # Add context to chunks
        self._add_context_to_chunks(chunks, lines)

        return chunks

    async def _chunk_large_entity(
        self,
        file_path: str,
        entity: Dict[str, Any],
        lines: List[str],
        base_idx: int,
    ) -> List[FileChunk]:
        """Sub-chunk a large entity (class or function) by methods."""
        chunks = []
        entity_lines = lines[entity["start"] - 1:entity["end"]]
        entity_content = "".join(entity_lines)

        if entity["type"] == ChunkBoundaryType.CLASS:
            # Parse class to extract methods
            try:
                import ast
                # Add minimal context for parsing
                tree = ast.parse(entity_content)
                class_node = tree.body[0]

                # Get class header (decorators + class def line + docstring)
                header_end = class_node.lineno
                if class_node.body:
                    first_body = class_node.body[0]
                    if isinstance(first_body, ast.Expr) and isinstance(first_body.value, ast.Constant):
                        # Has docstring
                        header_end = first_body.end_lineno or first_body.lineno

                # Create header chunk
                header_content = "".join(entity_lines[:header_end])
                chunks.append(FileChunk(
                    id=f"chunk_{base_idx}_{entity['name']}_header",
                    file_path=file_path,
                    start_line=entity["start"],
                    end_line=entity["start"] + header_end - 1,
                    content=header_content,
                    boundary_type=ChunkBoundaryType.CLASS,
                    entity_name=f"{entity['name']}.__header__",
                ))

                # Create chunks for each method
                method_idx = 0
                for node in class_node.body:
                    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                        method_start = node.lineno - 1  # Adjust for 0-indexed
                        method_end = (node.end_lineno or node.lineno) - 1
                        method_content = "".join(entity_lines[method_start:method_end + 1])

                        chunks.append(FileChunk(
                            id=f"chunk_{base_idx + method_idx + 1}_{entity['name']}.{node.name}",
                            file_path=file_path,
                            start_line=entity["start"] + method_start,
                            end_line=entity["start"] + method_end,
                            content=method_content,
                            boundary_type=ChunkBoundaryType.METHOD,
                            entity_name=f"{entity['name']}.{node.name}",
                            dependencies=[chunks[0].id],  # Depends on class header
                        ))
                        method_idx += 1

                return chunks
            except Exception:
                pass  # Fall through to line-based

        # Fallback: split by lines
        return await self._chunk_entity_by_lines(file_path, entity, lines, base_idx)

    async def _chunk_entity_by_lines(
        self,
        file_path: str,
        entity: Dict[str, Any],
        lines: List[str],
        base_idx: int,
    ) -> List[FileChunk]:
        """Split a large entity by lines with overlap."""
        chunks = []
        entity_lines = lines[entity["start"] - 1:entity["end"]]
        total_lines = len(entity_lines)

        # Calculate lines per chunk
        max_lines = self._max_tokens // 20  # Rough estimate: 20 chars per line

        current_start = 0
        chunk_idx = 0

        while current_start < total_lines:
            chunk_end = min(current_start + max_lines, total_lines)
            chunk_content = "".join(entity_lines[current_start:chunk_end])

            chunks.append(FileChunk(
                id=f"chunk_{base_idx + chunk_idx}_{entity['name']}_part{chunk_idx}",
                file_path=file_path,
                start_line=entity["start"] + current_start,
                end_line=entity["start"] + chunk_end - 1,
                content=chunk_content,
                boundary_type=ChunkBoundaryType.ARBITRARY,
                entity_name=f"{entity['name']}_part{chunk_idx}",
                dependencies=[chunks[-1].id] if chunks else [],
            ))

            current_start = chunk_end - self._overlap_lines
            chunk_idx += 1

        return chunks

    async def _chunk_by_lines(
        self,
        file_path: str,
        content: str,
        lines: List[str],
    ) -> List[FileChunk]:
        """Fallback line-based chunking with overlap."""
        chunks = []
        total_lines = len(lines)
        max_lines = self._max_tokens // 20  # Rough estimate

        current_start = 0
        chunk_idx = 0

        while current_start < total_lines:
            chunk_end = min(current_start + max_lines, total_lines)
            chunk_content = "".join(lines[current_start:chunk_end])

            chunks.append(FileChunk(
                id=f"chunk_{chunk_idx}",
                file_path=file_path,
                start_line=current_start + 1,
                end_line=chunk_end,
                content=chunk_content,
                boundary_type=ChunkBoundaryType.ARBITRARY,
            ))

            current_start = chunk_end - self._overlap_lines
            chunk_idx += 1

        self._add_context_to_chunks(chunks, lines)
        return chunks

    def _add_context_to_chunks(
        self,
        chunks: List[FileChunk],
        lines: List[str],
    ) -> None:
        """Add context lines before/after each chunk."""
        for i, chunk in enumerate(chunks):
            # Context before
            ctx_start = max(0, chunk.start_line - 1 - self._context_lines)
            ctx_end = chunk.start_line - 1
            if ctx_end > ctx_start:
                chunk.context_before = "".join(lines[ctx_start:ctx_end])

            # Context after
            ctx_start = chunk.end_line
            ctx_end = min(len(lines), chunk.end_line + self._context_lines)
            if ctx_end > ctx_start:
                chunk.context_after = "".join(lines[ctx_start:ctx_end])

    def _build_dependency_graph(
        self,
        chunks: List[FileChunk],
    ) -> Dict[str, List[str]]:
        """Build dependency graph between chunks."""
        graph: Dict[str, List[str]] = {}

        for chunk in chunks:
            graph[chunk.id] = chunk.dependencies.copy()

            # Classes depend on imports
            if chunk.boundary_type == ChunkBoundaryType.CLASS:
                for other in chunks:
                    if other.boundary_type == ChunkBoundaryType.IMPORT_BLOCK:
                        if other.id not in graph[chunk.id]:
                            graph[chunk.id].append(other.id)

            # Functions might depend on other functions (basic heuristic)
            if chunk.boundary_type == ChunkBoundaryType.FUNCTION:
                for other in chunks:
                    if other.boundary_type == ChunkBoundaryType.IMPORT_BLOCK:
                        if other.id not in graph[chunk.id]:
                            graph[chunk.id].append(other.id)

        return graph

    async def reassemble_chunks(
        self,
        chunked_file: ChunkedFile,
        modified_chunks: Dict[str, str],  # chunk_id -> new content
    ) -> str:
        """
        Reassemble a chunked file with modifications.

        Args:
            chunked_file: The original chunked file
            modified_chunks: Dict mapping chunk IDs to their new content

        Returns:
            Reassembled file content
        """
        # Build final content by going through chunks in order
        result_lines: List[str] = []
        last_end_line = 0

        for chunk_id in chunked_file.chunk_order:
            chunk = chunked_file.get_chunk_by_id(chunk_id)
            if not chunk:
                continue

            # Use modified content if available, otherwise original
            content = modified_chunks.get(chunk_id, chunk.content)

            # Handle gaps (shouldn't happen if chunking was correct)
            if chunk.start_line > last_end_line + 1:
                # There's a gap - this shouldn't happen normally
                pass

            # Add the chunk content
            result_lines.append(content)
            last_end_line = chunk.end_line

        return "".join(result_lines)

    async def get_chunk_for_line(
        self,
        chunked_file: ChunkedFile,
        line_number: int,
    ) -> Optional[FileChunk]:
        """Get the chunk containing a specific line number."""
        for chunk in chunked_file.chunks:
            if chunk.start_line <= line_number <= chunk.end_line:
                return chunk
        return None

    async def get_chunks_with_dependencies(
        self,
        chunked_file: ChunkedFile,
        chunk_id: str,
    ) -> List[FileChunk]:
        """Get a chunk and all its dependencies (for providing full context)."""
        result = []
        visited = set()

        def collect_deps(cid: str):
            if cid in visited:
                return
            visited.add(cid)

            chunk = chunked_file.get_chunk_by_id(cid)
            if chunk:
                # First collect dependencies
                for dep_id in chunked_file.dependency_graph.get(cid, []):
                    collect_deps(dep_id)
                result.append(chunk)

        collect_deps(chunk_id)
        return result

    def estimate_chunks_needed(self, content: str) -> int:
        """Estimate how many chunks a file will need."""
        tokens = len(content) // 4
        return max(1, tokens // self._max_tokens + 1)


# Global chunker instance
_smart_chunker: Optional[SmartFileChunker] = None


def get_smart_chunker(
    max_tokens: int = SmartFileChunker.DEFAULT_MAX_CHUNK_TOKENS
) -> SmartFileChunker:
    """Get or create the global smart chunker."""
    global _smart_chunker
    if _smart_chunker is None or _smart_chunker._max_tokens != max_tokens:
        _smart_chunker = SmartFileChunker(max_chunk_tokens=max_tokens)
    return _smart_chunker


# =============================================================================
# v3.1: ADVANCED SELF-PROGRAMMING FEATURES
# =============================================================================

class RefactoringType(Enum):
    """Types of refactoring operations."""
    RENAME_SYMBOL = "rename_symbol"
    EXTRACT_METHOD = "extract_method"
    EXTRACT_CLASS = "extract_class"
    INLINE_VARIABLE = "inline_variable"
    MOVE_FUNCTION = "move_function"
    CHANGE_SIGNATURE = "change_signature"


@dataclass
class SymbolReference:
    """Represents a reference to a symbol in code."""
    file_path: str
    line: int
    column: int
    end_column: int
    symbol_name: str
    context: str  # Line content for verification
    is_definition: bool = False
    is_import: bool = False


@dataclass
class RefactoringPlan:
    """Plan for a refactoring operation."""
    id: str
    refactoring_type: RefactoringType
    target_symbol: str
    new_value: Optional[str]  # New name for rename, etc.
    affected_files: List[str]
    references: List[SymbolReference]
    estimated_changes: int
    risk_score: float
    created_at: float = field(default_factory=time.time)


@dataclass
class DependencyInfo:
    """Information about a Python dependency."""
    name: str
    import_name: str  # e.g., 'cv2' for 'opencv-python'
    version_spec: Optional[str] = None
    is_stdlib: bool = False
    is_local: bool = False
    first_seen_file: Optional[str] = None


class AdvancedRefactoringEngine:
    """
    v3.1: Enterprise-grade refactoring engine with cross-file symbol awareness.

    Provides:
    - Cross-file symbol renaming using LSP references
    - Import statement auto-updates when files move
    - Extract method/class refactorings
    - Inline variable transformations
    - Safe atomic multi-file changes with rollback

    Uses The Watcher's LSP integration for accurate symbol resolution
    (no regex guessing - compiler-level accuracy).
    """

    # Known import name -> package name mappings
    IMPORT_TO_PACKAGE = {
        "cv2": "opencv-python",
        "PIL": "Pillow",
        "sklearn": "scikit-learn",
        "yaml": "PyYAML",
        "bs4": "beautifulsoup4",
        "dotenv": "python-dotenv",
        "jwt": "PyJWT",
        "dateutil": "python-dateutil",
        "magic": "python-magic",
        "gi": "PyGObject",
    }

    def __init__(self, watcher: Optional[Any] = None):
        """
        Initialize refactoring engine.

        Args:
            watcher: The Watcher LSP integration instance
        """
        self._watcher = watcher
        self._lock = asyncio.Lock()
        self._pending_plans: Dict[str, RefactoringPlan] = {}
        self._checkpoint_manager = get_checkpoint_manager()

    async def set_watcher(self, watcher: Any) -> None:
        """Set The Watcher instance for LSP operations."""
        self._watcher = watcher

    async def rename_symbol(
        self,
        file_path: str,
        line: int,
        column: int,
        new_name: str,
        dry_run: bool = True,
    ) -> RefactoringPlan:
        """
        Rename a symbol across all files that reference it.

        This is THE killer feature - rename a function/class and it updates
        every single usage across the entire codebase.

        Args:
            file_path: File containing the symbol definition
            line: Line number of the symbol (1-indexed)
            column: Column number of the symbol (1-indexed)
            new_name: New name for the symbol
            dry_run: If True, only plan the refactoring without executing

        Returns:
            RefactoringPlan with all affected locations
        """
        if not self._watcher:
            raise RuntimeError("The Watcher not available for symbol resolution")

        async with self._lock:
            # Get the symbol at this location
            symbol_info = await self._get_symbol_at_location(file_path, line, column)
            if not symbol_info:
                raise ValueError(f"No symbol found at {file_path}:{line}:{column}")

            old_name = symbol_info["name"]

            # Get all references to this symbol across the workspace
            references = await self._get_all_references(
                file_path, line, column, old_name
            )

            if not references:
                raise ValueError(f"No references found for symbol '{old_name}'")

            # Build the refactoring plan
            affected_files = list(set(ref.file_path for ref in references))
            plan = RefactoringPlan(
                id=f"rename_{uuid.uuid4().hex[:12]}",
                refactoring_type=RefactoringType.RENAME_SYMBOL,
                target_symbol=old_name,
                new_value=new_name,
                affected_files=affected_files,
                references=references,
                estimated_changes=len(references),
                risk_score=self._calculate_risk_score(references),
            )

            self._pending_plans[plan.id] = plan

            if not dry_run:
                await self._execute_rename(plan)

            return plan

    async def _get_symbol_at_location(
        self,
        file_path: str,
        line: int,
        column: int,
    ) -> Optional[Dict[str, Any]]:
        """Get symbol information at a specific location."""
        try:
            # Read the file to get the symbol name
            content = Path(file_path).read_text(encoding="utf-8")
            lines = content.splitlines()

            if line > len(lines):
                return None

            line_content = lines[line - 1]

            # Find word at column
            word_start = column - 1
            word_end = column - 1

            while word_start > 0 and line_content[word_start - 1].isalnum() or line_content[word_start - 1] == '_':
                word_start -= 1

            while word_end < len(line_content) and (line_content[word_end].isalnum() or line_content[word_end] == '_'):
                word_end += 1

            symbol_name = line_content[word_start:word_end]

            if not symbol_name:
                return None

            return {
                "name": symbol_name,
                "line": line,
                "column": column,
                "start_column": word_start + 1,
                "end_column": word_end + 1,
            }

        except Exception as e:
            logger.error(f"Failed to get symbol at location: {e}")
            return None

    async def _get_all_references(
        self,
        file_path: str,
        line: int,
        column: int,
        symbol_name: str,
    ) -> List[SymbolReference]:
        """Get all references to a symbol using The Watcher's LSP."""
        references = []

        try:
            if self._watcher and hasattr(self._watcher, 'get_references'):
                # Use LSP to get accurate references
                lsp_refs = await self._watcher.get_references(
                    Path(file_path), line, column, include_declaration=True
                )

                for ref in lsp_refs:
                    ref_path = self._uri_to_path(ref.uri) if hasattr(ref, 'uri') else str(ref.get('uri', ''))
                    ref_line = ref.range.start.line + 1 if hasattr(ref, 'range') else ref.get('range', {}).get('start', {}).get('line', 0) + 1
                    ref_col = ref.range.start.character + 1 if hasattr(ref, 'range') else ref.get('range', {}).get('start', {}).get('character', 0) + 1
                    ref_end_col = ref.range.end.character + 1 if hasattr(ref, 'range') else ref.get('range', {}).get('end', {}).get('character', 0) + 1

                    # Get context
                    try:
                        content = Path(ref_path).read_text(encoding="utf-8")
                        context = content.splitlines()[ref_line - 1] if ref_line <= len(content.splitlines()) else ""
                    except Exception:
                        context = ""

                    references.append(SymbolReference(
                        file_path=ref_path,
                        line=ref_line,
                        column=ref_col,
                        end_column=ref_end_col,
                        symbol_name=symbol_name,
                        context=context,
                        is_definition=(ref_path == file_path and ref_line == line),
                        is_import="import " in context or "from " in context,
                    ))
            else:
                # Fallback to AST-based reference finding
                references = await self._find_references_with_ast(
                    file_path, symbol_name
                )

        except Exception as e:
            logger.error(f"Failed to get references: {e}")
            # Fallback
            references = await self._find_references_with_ast(file_path, symbol_name)

        return references

    async def _find_references_with_ast(
        self,
        definition_file: str,
        symbol_name: str,
    ) -> List[SymbolReference]:
        """Fallback: Find references using AST parsing."""
        import ast
        references = []

        # Get workspace root
        workspace = Path(definition_file).parent
        while workspace.parent != workspace:
            if (workspace / ".git").exists() or (workspace / "pyproject.toml").exists():
                break
            workspace = workspace.parent

        # Scan all Python files
        for py_file in workspace.rglob("*.py"):
            if "__pycache__" in str(py_file) or ".venv" in str(py_file):
                continue

            try:
                content = py_file.read_text(encoding="utf-8")
                tree = ast.parse(content)

                for node in ast.walk(tree):
                    if isinstance(node, ast.Name) and node.id == symbol_name:
                        lines = content.splitlines()
                        line_content = lines[node.lineno - 1] if node.lineno <= len(lines) else ""

                        references.append(SymbolReference(
                            file_path=str(py_file),
                            line=node.lineno,
                            column=node.col_offset + 1,
                            end_column=node.col_offset + len(symbol_name) + 1,
                            symbol_name=symbol_name,
                            context=line_content,
                            is_definition=str(py_file) == definition_file,
                            is_import="import " in line_content,
                        ))

            except Exception:
                continue

        return references

    def _uri_to_path(self, uri: str) -> str:
        """Convert file:// URI to path."""
        if uri.startswith("file://"):
            return uri[7:]
        return uri

    def _calculate_risk_score(self, references: List[SymbolReference]) -> float:
        """Calculate risk score for a refactoring operation."""
        if not references:
            return 0.0

        # Factors that increase risk
        file_count = len(set(ref.file_path for ref in references))
        import_count = sum(1 for ref in references if ref.is_import)

        # More files = higher risk
        file_risk = min(file_count / 10, 1.0) * 0.4

        # More imports = higher risk (breaking imports is bad)
        import_risk = min(import_count / 5, 1.0) * 0.4

        # Large number of references = higher risk
        ref_risk = min(len(references) / 50, 1.0) * 0.2

        return file_risk + import_risk + ref_risk

    async def _execute_rename(self, plan: RefactoringPlan) -> Dict[str, bool]:
        """Execute a rename refactoring plan."""
        results = {}

        # Create checkpoint before making changes
        async with self._checkpoint_manager.checkpoint_context(
            f"rename_{plan.target_symbol}_to_{plan.new_value}",
            plan.affected_files,
        ) as checkpoint_id:

            # Group references by file
            refs_by_file: Dict[str, List[SymbolReference]] = {}
            for ref in plan.references:
                if ref.file_path not in refs_by_file:
                    refs_by_file[ref.file_path] = []
                refs_by_file[ref.file_path].append(ref)

            # Process each file
            for file_path, refs in refs_by_file.items():
                try:
                    content = Path(file_path).read_text(encoding="utf-8")
                    lines = content.splitlines(keepends=True)

                    # Sort refs by line (descending) to avoid offset issues
                    refs.sort(key=lambda r: (r.line, r.column), reverse=True)

                    for ref in refs:
                        line_idx = ref.line - 1
                        if line_idx < len(lines):
                            line = lines[line_idx]
                            # Replace the symbol
                            before = line[:ref.column - 1]
                            after = line[ref.end_column - 1:]
                            lines[line_idx] = before + plan.new_value + after

                    # Write updated content
                    new_content = "".join(lines)
                    Path(file_path).write_text(new_content, encoding="utf-8")
                    results[file_path] = True
                    logger.info(f"Updated {len(refs)} references in {file_path}")

                except Exception as e:
                    logger.error(f"Failed to update {file_path}: {e}")
                    results[file_path] = False
                    raise  # Trigger rollback

        return results

    async def update_imports_for_move(
        self,
        old_path: str,
        new_path: str,
        workspace_root: Optional[str] = None,
    ) -> Dict[str, bool]:
        """
        Update all import statements when a file is moved/renamed.

        This automatically fixes broken imports after file operations.

        Args:
            old_path: Original file path
            new_path: New file path
            workspace_root: Root of the workspace to scan

        Returns:
            Dict mapping file paths to update success status
        """
        results = {}

        if not workspace_root:
            workspace_root = self._find_workspace_root(old_path)

        # Calculate old and new module paths
        old_module = self._path_to_module(old_path, workspace_root)
        new_module = self._path_to_module(new_path, workspace_root)

        if not old_module or not new_module:
            logger.warning("Could not determine module paths for import update")
            return results

        # Find all files that import from the old module
        workspace = Path(workspace_root)
        affected_files = []

        for py_file in workspace.rglob("*.py"):
            if "__pycache__" in str(py_file) or ".venv" in str(py_file):
                continue

            try:
                content = py_file.read_text(encoding="utf-8")
                if old_module in content:
                    affected_files.append(str(py_file))
            except Exception:
                continue

        if not affected_files:
            logger.info(f"No files import from {old_module}")
            return results

        # Create checkpoint
        async with self._checkpoint_manager.checkpoint_context(
            f"import_update_{old_module}",
            affected_files,
        ):
            for file_path in affected_files:
                try:
                    content = Path(file_path).read_text(encoding="utf-8")

                    # Update import statements
                    new_content = self._update_import_statements(
                        content, old_module, new_module
                    )

                    if new_content != content:
                        Path(file_path).write_text(new_content, encoding="utf-8")
                        results[file_path] = True
                        logger.info(f"Updated imports in {file_path}")
                    else:
                        results[file_path] = True  # No changes needed

                except Exception as e:
                    logger.error(f"Failed to update imports in {file_path}: {e}")
                    results[file_path] = False
                    raise

        return results

    def _find_workspace_root(self, file_path: str) -> str:
        """Find the workspace root directory."""
        path = Path(file_path).parent
        while path.parent != path:
            if (path / ".git").exists() or (path / "pyproject.toml").exists():
                return str(path)
            path = path.parent
        return str(Path(file_path).parent)

    def _path_to_module(self, file_path: str, workspace_root: str) -> Optional[str]:
        """Convert a file path to a Python module path."""
        try:
            rel_path = Path(file_path).relative_to(workspace_root)
            # Remove .py extension and convert to module path
            module_path = str(rel_path).replace("/", ".").replace("\\", ".")
            if module_path.endswith(".py"):
                module_path = module_path[:-3]
            return module_path
        except ValueError:
            return None

    def _update_import_statements(
        self,
        content: str,
        old_module: str,
        new_module: str,
    ) -> str:
        """Update import statements in file content."""
        import re

        # Handle different import patterns
        patterns = [
            # from old_module import ...
            (rf'from\s+{re.escape(old_module)}\s+import',
             f'from {new_module} import'),
            # import old_module
            (rf'import\s+{re.escape(old_module)}(\s|$|,)',
             f'import {new_module}\\1'),
            # from old_module.submodule import ...
            (rf'from\s+{re.escape(old_module)}\.(\S+)\s+import',
             f'from {new_module}.\\1 import'),
        ]

        for pattern, replacement in patterns:
            content = re.sub(pattern, replacement, content)

        return content

    async def extract_method(
        self,
        file_path: str,
        start_line: int,
        end_line: int,
        new_method_name: str,
    ) -> Optional[str]:
        """
        Extract lines into a new method.

        Args:
            file_path: Path to the file
            start_line: Start line of code to extract (1-indexed)
            end_line: End line of code to extract (1-indexed)
            new_method_name: Name for the new method

        Returns:
            Modified file content, or None on failure
        """
        try:
            content = Path(file_path).read_text(encoding="utf-8")
            lines = content.splitlines(keepends=True)

            # Get the lines to extract
            extracted = lines[start_line - 1:end_line]
            extracted_content = "".join(extracted)

            # Analyze the extracted code
            import ast
            try:
                tree = ast.parse(extracted_content)
            except SyntaxError:
                # Wrap in function body and try again
                indented = "\n".join("    " + line.rstrip() for line in extracted_content.splitlines())
                wrapped = f"def _temp():\n{indented}\n"
                tree = ast.parse(wrapped).body[0]

            # Find used variables (potential parameters)
            used_names = set()
            assigned_names = set()

            for node in ast.walk(tree):
                if isinstance(node, ast.Name):
                    if isinstance(node.ctx, ast.Load):
                        used_names.add(node.id)
                    elif isinstance(node.ctx, ast.Store):
                        assigned_names.add(node.id)

            # Parameters are used but not assigned within the extracted code
            parameters = list(used_names - assigned_names - {'self', 'cls'})
            parameters.sort()

            # Determine indentation
            first_line = extracted[0] if extracted else ""
            base_indent = len(first_line) - len(first_line.lstrip())
            indent_str = first_line[:base_indent]

            # Build the new method
            param_str = ", ".join(parameters)
            method_lines = [f"{indent_str}def {new_method_name}(self, {param_str}):\n"]

            for line in extracted:
                # Add extra indentation for method body
                method_lines.append("    " + line if line.strip() else line)

            method_content = "".join(method_lines)

            # Build the call to the new method
            call_line = f"{indent_str}self.{new_method_name}({', '.join(parameters)})\n"

            # Replace extracted lines with the call
            new_lines = lines[:start_line - 1] + [call_line] + lines[end_line:]

            # Insert the new method (find a good location)
            # For now, insert right before the extraction point
            new_lines = new_lines[:start_line - 1] + [method_content, "\n"] + new_lines[start_line - 1:]

            return "".join(new_lines)

        except Exception as e:
            logger.error(f"Extract method failed: {e}")
            return None


class DependencyTracker:
    """
    v3.1: Intelligent dependency tracking and requirements.txt management.

    Features:
    - Auto-detect new imports in code
    - Update requirements.txt automatically
    - Distinguish stdlib from third-party
    - Version pinning based on installed packages
    - Import name to package name resolution
    """

    # Python stdlib modules (3.10+)
    STDLIB_MODULES = frozenset({
        "abc", "aifc", "argparse", "array", "ast", "asynchat", "asyncio",
        "asyncore", "atexit", "audioop", "base64", "bdb", "binascii",
        "binhex", "bisect", "builtins", "bz2", "calendar", "cgi", "cgitb",
        "chunk", "cmath", "cmd", "code", "codecs", "codeop", "collections",
        "colorsys", "compileall", "concurrent", "configparser", "contextlib",
        "contextvars", "copy", "copyreg", "cProfile", "crypt", "csv",
        "ctypes", "curses", "dataclasses", "datetime", "dbm", "decimal",
        "difflib", "dis", "distutils", "doctest", "email", "encodings",
        "enum", "errno", "faulthandler", "fcntl", "filecmp", "fileinput",
        "fnmatch", "fractions", "ftplib", "functools", "gc", "getopt",
        "getpass", "gettext", "glob", "graphlib", "grp", "gzip", "hashlib",
        "heapq", "hmac", "html", "http", "idlelib", "imaplib", "imghdr",
        "imp", "importlib", "inspect", "io", "ipaddress", "itertools",
        "json", "keyword", "lib2to3", "linecache", "locale", "logging",
        "lzma", "mailbox", "mailcap", "marshal", "math", "mimetypes",
        "mmap", "modulefinder", "multiprocessing", "netrc", "nis",
        "nntplib", "numbers", "operator", "optparse", "os", "ossaudiodev",
        "pathlib", "pdb", "pickle", "pickletools", "pipes", "pkgutil",
        "platform", "plistlib", "poplib", "posix", "posixpath", "pprint",
        "profile", "pstats", "pty", "pwd", "py_compile", "pyclbr",
        "pydoc", "queue", "quopri", "random", "re", "readline", "reprlib",
        "resource", "rlcompleter", "runpy", "sched", "secrets", "select",
        "selectors", "shelve", "shlex", "shutil", "signal", "site",
        "smtpd", "smtplib", "sndhdr", "socket", "socketserver", "spwd",
        "sqlite3", "ssl", "stat", "statistics", "string", "stringprep",
        "struct", "subprocess", "sunau", "symtable", "sys", "sysconfig",
        "syslog", "tabnanny", "tarfile", "telnetlib", "tempfile", "termios",
        "test", "textwrap", "threading", "time", "timeit", "tkinter",
        "token", "tokenize", "trace", "traceback", "tracemalloc", "tty",
        "turtle", "turtledemo", "types", "typing", "unicodedata", "unittest",
        "urllib", "uu", "uuid", "venv", "warnings", "wave", "weakref",
        "webbrowser", "winreg", "winsound", "wsgiref", "xdrlib", "xml",
        "xmlrpc", "zipapp", "zipfile", "zipimport", "zlib",
    })

    # Import name -> PyPI package name
    IMPORT_TO_PACKAGE = {
        "cv2": "opencv-python",
        "PIL": "Pillow",
        "sklearn": "scikit-learn",
        "yaml": "PyYAML",
        "bs4": "beautifulsoup4",
        "dotenv": "python-dotenv",
        "jwt": "PyJWT",
        "dateutil": "python-dateutil",
        "magic": "python-magic",
        "gi": "PyGObject",
        "OpenSSL": "pyOpenSSL",
        "Crypto": "pycryptodome",
        "serial": "pyserial",
        "usb": "pyusb",
        "dns": "dnspython",
        "wx": "wxPython",
        "lxml": "lxml",
        "numpy": "numpy",
        "pandas": "pandas",
        "scipy": "scipy",
        "matplotlib": "matplotlib",
        "torch": "torch",
        "tensorflow": "tensorflow",
        "keras": "keras",
        "flask": "Flask",
        "django": "Django",
        "fastapi": "fastapi",
        "requests": "requests",
        "aiohttp": "aiohttp",
        "httpx": "httpx",
        "sqlalchemy": "SQLAlchemy",
        "redis": "redis",
        "celery": "celery",
        "pydantic": "pydantic",
        "pytest": "pytest",
        "black": "black",
        "isort": "isort",
        "mypy": "mypy",
        "flake8": "flake8",
        "pylint": "pylint",
        "rich": "rich",
        "typer": "typer",
        "click": "click",
        "toml": "toml",
        "langchain": "langchain",
        "openai": "openai",
        "anthropic": "anthropic",
        "chromadb": "chromadb",
        "langfuse": "langfuse",
        "helicone": "helicone",
        "playwright": "playwright",
        "pyautogui": "pyautogui",
        "speechrecognition": "SpeechRecognition",
        "sounddevice": "sounddevice",
        "soundfile": "soundfile",
        "webrtcvad": "webrtcvad",
    }

    def __init__(self, project_root: Optional[Path] = None):
        """
        Initialize dependency tracker.

        Args:
            project_root: Root directory of the project
        """
        self._project_root = project_root or Path.cwd()
        self._requirements_file = self._project_root / "requirements.txt"
        self._lock = asyncio.Lock()
        self._known_deps: Dict[str, DependencyInfo] = {}

    async def scan_project_imports(self) -> Dict[str, DependencyInfo]:
        """
        Scan entire project for imports and categorize them.

        Returns:
            Dict mapping import names to DependencyInfo
        """
        import ast

        imports: Dict[str, DependencyInfo] = {}

        for py_file in self._project_root.rglob("*.py"):
            if "__pycache__" in str(py_file) or ".venv" in str(py_file):
                continue

            try:
                content = py_file.read_text(encoding="utf-8")
                tree = ast.parse(content)

                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            module_name = alias.name.split(".")[0]
                            if module_name not in imports:
                                imports[module_name] = self._create_dep_info(
                                    module_name, str(py_file)
                                )

                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            module_name = node.module.split(".")[0]
                            if module_name not in imports:
                                imports[module_name] = self._create_dep_info(
                                    module_name, str(py_file)
                                )

            except Exception as e:
                logger.debug(f"Failed to parse {py_file}: {e}")

        self._known_deps = imports
        return imports

    def _create_dep_info(self, import_name: str, first_file: str) -> DependencyInfo:
        """Create DependencyInfo for an import."""
        is_stdlib = import_name in self.STDLIB_MODULES

        # Check if it's a local module
        is_local = False
        local_path = self._project_root / import_name
        local_file = self._project_root / f"{import_name}.py"
        if local_path.exists() or local_file.exists():
            is_local = True

        # Get package name
        package_name = self.IMPORT_TO_PACKAGE.get(import_name, import_name)

        # Try to get installed version
        version_spec = None
        if not is_stdlib and not is_local:
            version_spec = self._get_installed_version(package_name)

        return DependencyInfo(
            name=package_name,
            import_name=import_name,
            version_spec=version_spec,
            is_stdlib=is_stdlib,
            is_local=is_local,
            first_seen_file=first_file,
        )

    def _get_installed_version(self, package_name: str) -> Optional[str]:
        """Get the installed version of a package."""
        try:
            import importlib.metadata
            version = importlib.metadata.version(package_name)
            return f">={version}"
        except Exception:
            return None

    async def update_requirements_txt(
        self,
        add_new: bool = True,
        pin_versions: bool = True,
        remove_unused: bool = False,
    ) -> Dict[str, str]:
        """
        Update requirements.txt based on project imports.

        Args:
            add_new: Add newly detected dependencies
            pin_versions: Pin to currently installed versions
            remove_unused: Remove packages not imported (DANGEROUS)

        Returns:
            Dict of changes made {package: "added"/"updated"/"removed"}
        """
        async with self._lock:
            changes = {}

            # Scan current imports
            if not self._known_deps:
                await self.scan_project_imports()

            # Read existing requirements
            existing_reqs = self._parse_requirements_txt()

            # Determine third-party deps
            third_party = {
                name: info for name, info in self._known_deps.items()
                if not info.is_stdlib and not info.is_local
            }

            # Build new requirements
            new_reqs: Dict[str, str] = {}

            for import_name, info in third_party.items():
                package_name = info.name

                if package_name in existing_reqs:
                    # Keep existing version spec
                    new_reqs[package_name] = existing_reqs[package_name]
                elif add_new:
                    # Add new dependency
                    if pin_versions and info.version_spec:
                        new_reqs[package_name] = info.version_spec
                    else:
                        new_reqs[package_name] = ""
                    changes[package_name] = "added"

            # Keep existing reqs that we didn't detect (could be indirect deps)
            for pkg, ver in existing_reqs.items():
                if pkg not in new_reqs:
                    if remove_unused:
                        changes[pkg] = "removed"
                    else:
                        new_reqs[pkg] = ver

            # Write requirements.txt
            self._write_requirements_txt(new_reqs)

            return changes

    def _parse_requirements_txt(self) -> Dict[str, str]:
        """Parse requirements.txt into dict of package -> version spec."""
        reqs = {}

        if not self._requirements_file.exists():
            return reqs

        try:
            content = self._requirements_file.read_text(encoding="utf-8")
            for line in content.splitlines():
                line = line.strip()
                if not line or line.startswith("#") or line.startswith("-"):
                    continue

                # Parse package==version, package>=version, etc.
                for op in ["==", ">=", "<=", "~=", "!=", ">", "<"]:
                    if op in line:
                        pkg, ver = line.split(op, 1)
                        reqs[pkg.strip()] = op + ver.strip()
                        break
                else:
                    # No version specifier
                    reqs[line] = ""

        except Exception as e:
            logger.error(f"Failed to parse requirements.txt: {e}")

        return reqs

    def _write_requirements_txt(self, reqs: Dict[str, str]) -> None:
        """Write requirements.txt from dict."""
        lines = []
        lines.append("# Auto-generated by Ironcliw Ouroboros DependencyTracker")
        lines.append(f"# Last updated: {datetime.now().isoformat()}")
        lines.append("")

        for pkg in sorted(reqs.keys()):
            ver = reqs[pkg]
            if ver:
                lines.append(f"{pkg}{ver}")
            else:
                lines.append(pkg)

        self._requirements_file.write_text("\n".join(lines) + "\n", encoding="utf-8")

    async def check_for_new_imports(
        self,
        file_path: str,
        content: str,
    ) -> List[DependencyInfo]:
        """
        Check if a file has new imports not in requirements.txt.

        Args:
            file_path: Path to the file
            content: File content

        Returns:
            List of new dependencies found
        """
        import ast

        new_deps = []
        existing = self._parse_requirements_txt()

        try:
            tree = ast.parse(content)

            for node in ast.walk(tree):
                import_name = None

                if isinstance(node, ast.Import):
                    for alias in node.names:
                        import_name = alias.name.split(".")[0]
                elif isinstance(node, ast.ImportFrom) and node.module:
                    import_name = node.module.split(".")[0]

                if import_name:
                    dep_info = self._create_dep_info(import_name, file_path)

                    if not dep_info.is_stdlib and not dep_info.is_local:
                        if dep_info.name not in existing:
                            new_deps.append(dep_info)

        except Exception as e:
            logger.debug(f"Failed to check imports: {e}")

        return new_deps


class ProactiveSuggestionEngine:
    """
    v3.1: Background analyzer that proactively suggests code improvements.

    Features:
    - Async background scanning of modified files
    - Pattern-based issue detection
    - Performance bottleneck identification
    - Security vulnerability hints
    - Code smell detection
    - Prioritized suggestion queue
    """

    class SuggestionPriority(Enum):
        CRITICAL = 1  # Security issues, crash bugs
        HIGH = 2      # Performance issues, bad patterns
        MEDIUM = 3    # Code smells, maintainability
        LOW = 4       # Style issues, minor improvements

    @dataclass
    class Suggestion:
        id: str
        file_path: str
        line: Optional[int]
        priority: 'ProactiveSuggestionEngine.SuggestionPriority'
        category: str  # security, performance, style, pattern
        title: str
        description: str
        suggested_fix: Optional[str] = None
        created_at: float = field(default_factory=time.time)

    # Patterns to detect (category, regex, priority, title, description)
    PATTERNS = [
        # Security
        ("security", r"eval\s*\(", SuggestionPriority.CRITICAL,
         "Dangerous eval() usage",
         "eval() can execute arbitrary code. Consider using ast.literal_eval() or a safer alternative."),

        ("security", r"exec\s*\(", SuggestionPriority.CRITICAL,
         "Dangerous exec() usage",
         "exec() can execute arbitrary code. This is a security risk."),

        ("security", r"subprocess\.(call|run|Popen)\s*\([^)]*shell\s*=\s*True",
         SuggestionPriority.CRITICAL,
         "Shell injection risk",
         "shell=True in subprocess can lead to command injection. Use shell=False with a list of arguments."),

        ("security", r"pickle\.(load|loads)\s*\(", SuggestionPriority.HIGH,
         "Insecure deserialization",
         "pickle can execute arbitrary code during deserialization. Consider using json or a safer format."),

        ("security", r"password\s*=\s*['\"][^'\"]+['\"]", SuggestionPriority.CRITICAL,
         "Hardcoded password detected",
         "Passwords should not be hardcoded. Use environment variables or a secrets manager."),

        # Performance
        ("performance", r"for\s+\w+\s+in\s+range\(len\(", SuggestionPriority.MEDIUM,
         "Inefficient iteration pattern",
         "Use 'for item in iterable' or 'for i, item in enumerate(iterable)' instead."),

        ("performance", r"\+\s*=.*\+.*in.*for", SuggestionPriority.MEDIUM,
         "String concatenation in loop",
         "String concatenation in loops is O(n²). Use ''.join() or a list."),

        ("performance", r"time\.sleep\s*\(\s*\d+\s*\)", SuggestionPriority.LOW,
         "Blocking sleep in async context",
         "time.sleep() blocks the event loop. Use await asyncio.sleep() in async code."),

        # Code patterns
        ("pattern", r"except\s*:", SuggestionPriority.MEDIUM,
         "Bare except clause",
         "Catching all exceptions can hide bugs. Specify the exception types."),

        ("pattern", r"except\s+Exception\s*:", SuggestionPriority.LOW,
         "Overly broad exception handling",
         "Consider catching more specific exceptions."),

        ("pattern", r"TODO|FIXME|XXX|HACK", SuggestionPriority.LOW,
         "TODO/FIXME comment found",
         "There's a TODO or FIXME that should be addressed."),

        ("pattern", r"print\s*\(", SuggestionPriority.LOW,
         "Print statement in production code",
         "Consider using logging instead of print for production code."),

        # Style
        ("style", r"^class\s+\w+\s*:", SuggestionPriority.LOW,
         "Class without docstring",
         "Classes should have docstrings explaining their purpose."),
    ]

    def __init__(self, project_root: Optional[Path] = None):
        """Initialize proactive suggestion engine."""
        self._project_root = project_root or Path.cwd()
        self._suggestions: Dict[str, 'ProactiveSuggestionEngine.Suggestion'] = {}
        self._lock = asyncio.Lock()
        self._scan_task: Optional[asyncio.Task] = None
        self._running = False
        self._file_hashes: Dict[str, str] = {}

    async def start_background_scan(
        self,
        interval_seconds: float = 60.0,
    ) -> None:
        """Start background scanning for issues."""
        self._running = True
        self._scan_task = asyncio.create_task(
            self._background_scanner(interval_seconds)
        )
        logger.info("ProactiveSuggestionEngine background scan started")

    async def stop_background_scan(self) -> None:
        """Stop background scanning."""
        self._running = False
        if self._scan_task:
            self._scan_task.cancel()
            try:
                await self._scan_task
            except asyncio.CancelledError:
                pass
        logger.info("ProactiveSuggestionEngine background scan stopped")

    async def _background_scanner(self, interval: float) -> None:
        """Background task that scans for issues."""
        while self._running:
            try:
                await self._scan_modified_files()
            except Exception as e:
                logger.error(f"Background scan error: {e}")

            await asyncio.sleep(interval)

    async def _scan_modified_files(self) -> None:
        """Scan recently modified files for issues."""
        for py_file in self._project_root.rglob("*.py"):
            if "__pycache__" in str(py_file) or ".venv" in str(py_file):
                continue

            try:
                content = py_file.read_text(encoding="utf-8")
                content_hash = hashlib.md5(content.encode()).hexdigest()

                # Skip if file hasn't changed
                if self._file_hashes.get(str(py_file)) == content_hash:
                    continue

                self._file_hashes[str(py_file)] = content_hash

                # Scan for issues
                await self.analyze_file(str(py_file), content)

            except Exception as e:
                logger.debug(f"Failed to scan {py_file}: {e}")

    async def analyze_file(
        self,
        file_path: str,
        content: Optional[str] = None,
    ) -> List['ProactiveSuggestionEngine.Suggestion']:
        """
        Analyze a file for issues and return suggestions.

        Args:
            file_path: Path to the file
            content: Optional file content (reads from disk if not provided)

        Returns:
            List of suggestions for the file
        """
        import re

        if content is None:
            content = Path(file_path).read_text(encoding="utf-8")

        suggestions = []
        lines = content.splitlines()

        for line_num, line in enumerate(lines, 1):
            for category, pattern, priority, title, description in self.PATTERNS:
                if re.search(pattern, line):
                    suggestion = self.Suggestion(
                        id=f"suggest_{uuid.uuid4().hex[:8]}",
                        file_path=file_path,
                        line=line_num,
                        priority=priority,
                        category=category,
                        title=title,
                        description=description,
                    )
                    suggestions.append(suggestion)

                    async with self._lock:
                        self._suggestions[suggestion.id] = suggestion

        return suggestions

    async def get_suggestions(
        self,
        file_path: Optional[str] = None,
        min_priority: Optional['ProactiveSuggestionEngine.SuggestionPriority'] = None,
        category: Optional[str] = None,
    ) -> List['ProactiveSuggestionEngine.Suggestion']:
        """Get current suggestions, optionally filtered."""
        async with self._lock:
            suggestions = list(self._suggestions.values())

        if file_path:
            suggestions = [s for s in suggestions if s.file_path == file_path]

        if min_priority:
            suggestions = [s for s in suggestions if s.priority.value <= min_priority.value]

        if category:
            suggestions = [s for s in suggestions if s.category == category]

        # Sort by priority
        suggestions.sort(key=lambda s: (s.priority.value, s.created_at))

        return suggestions

    async def dismiss_suggestion(self, suggestion_id: str) -> bool:
        """Dismiss a suggestion."""
        async with self._lock:
            return self._suggestions.pop(suggestion_id, None) is not None

    async def get_summary(self) -> Dict[str, Any]:
        """Get summary of all suggestions."""
        async with self._lock:
            suggestions = list(self._suggestions.values())

        return {
            "total": len(suggestions),
            "by_priority": {
                "critical": len([s for s in suggestions if s.priority == self.SuggestionPriority.CRITICAL]),
                "high": len([s for s in suggestions if s.priority == self.SuggestionPriority.HIGH]),
                "medium": len([s for s in suggestions if s.priority == self.SuggestionPriority.MEDIUM]),
                "low": len([s for s in suggestions if s.priority == self.SuggestionPriority.LOW]),
            },
            "by_category": {
                "security": len([s for s in suggestions if s.category == "security"]),
                "performance": len([s for s in suggestions if s.category == "performance"]),
                "pattern": len([s for s in suggestions if s.category == "pattern"]),
                "style": len([s for s in suggestions if s.category == "style"]),
            },
            "files_affected": len(set(s.file_path for s in suggestions)),
        }


class DocumentationGenerator:
    """
    v3.1: Automatic documentation generation for undocumented code.

    Features:
    - Detect undocumented functions, classes, modules
    - Generate docstrings using LLM or templates
    - Support multiple docstring formats (Google, NumPy, Sphinx)
    - Analyze function signatures for parameter documentation
    - Generate README sections
    """

    class DocstringFormat(Enum):
        GOOGLE = "google"
        NUMPY = "numpy"
        SPHINX = "sphinx"
        SIMPLE = "simple"

    def __init__(
        self,
        format_style: 'DocumentationGenerator.DocstringFormat' = None,
        llm_client: Optional[Any] = None,
    ):
        """
        Initialize documentation generator.

        Args:
            format_style: Docstring format to use
            llm_client: Optional LLM client for AI-generated docs
        """
        self._format = format_style or self.DocstringFormat.GOOGLE
        self._llm_client = llm_client

    async def find_undocumented(
        self,
        file_path: str,
        content: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        Find all undocumented functions, classes, and methods.

        Returns:
            List of dicts with 'type', 'name', 'line', 'signature'
        """
        import ast

        if content is None:
            content = Path(file_path).read_text(encoding="utf-8")

        undocumented = []

        try:
            tree = ast.parse(content)

            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    if not ast.get_docstring(node):
                        undocumented.append({
                            "type": "function",
                            "name": node.name,
                            "line": node.lineno,
                            "signature": self._get_signature(node),
                            "is_method": self._is_method(node, tree),
                        })

                elif isinstance(node, ast.ClassDef):
                    if not ast.get_docstring(node):
                        undocumented.append({
                            "type": "class",
                            "name": node.name,
                            "line": node.lineno,
                            "bases": [self._get_name(b) for b in node.bases],
                        })

        except Exception as e:
            logger.error(f"Failed to parse {file_path}: {e}")

        return undocumented

    def _get_signature(self, node: Any) -> str:
        """Get function signature string."""
        import ast

        params = []
        for arg in node.args.args:
            param = arg.arg
            if arg.annotation:
                param += f": {ast.unparse(arg.annotation)}"
            params.append(param)

        for i, default in enumerate(reversed(node.args.defaults)):
            idx = len(params) - i - 1
            params[idx] += f" = {ast.unparse(default)}"

        if node.args.vararg:
            params.append(f"*{node.args.vararg.arg}")
        if node.args.kwarg:
            params.append(f"**{node.args.kwarg.arg}")

        return_type = ""
        if node.returns:
            return_type = f" -> {ast.unparse(node.returns)}"

        return f"({', '.join(params)}){return_type}"

    def _is_method(self, node: Any, tree: Any) -> bool:
        """Check if a function is a method (inside a class)."""
        import ast

        for parent in ast.walk(tree):
            if isinstance(parent, ast.ClassDef):
                for child in parent.body:
                    if child is node:
                        return True
        return False

    def _get_name(self, node: Any) -> str:
        """Get name from a node."""
        import ast

        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            return f"{self._get_name(node.value)}.{node.attr}"
        return str(node)

    async def generate_docstring(
        self,
        item: Dict[str, Any],
        context: Optional[str] = None,
    ) -> str:
        """
        Generate a docstring for an undocumented item.

        Args:
            item: Item info from find_undocumented()
            context: Optional surrounding code context

        Returns:
            Generated docstring
        """
        if self._llm_client:
            return await self._generate_with_llm(item, context)

        return self._generate_template_docstring(item)

    def _generate_template_docstring(self, item: Dict[str, Any]) -> str:
        """Generate a template docstring without LLM."""
        if item["type"] == "class":
            if self._format == self.DocstringFormat.GOOGLE:
                return f'"""{item["name"]} class.\n\n    TODO: Add class description.\n    """'
            return f'"""{item["name"]} class."""'

        # Function/method
        signature = item.get("signature", "()")

        # Parse parameters from signature
        params = self._parse_params(signature)

        if self._format == self.DocstringFormat.GOOGLE:
            return self._google_docstring(item["name"], params)
        elif self._format == self.DocstringFormat.NUMPY:
            return self._numpy_docstring(item["name"], params)
        elif self._format == self.DocstringFormat.SPHINX:
            return self._sphinx_docstring(item["name"], params)

        return f'"""TODO: Document {item["name"]}."""'

    def _parse_params(self, signature: str) -> List[Dict[str, str]]:
        """Parse parameters from signature string."""
        params = []

        # Remove parentheses and return type
        sig = signature.strip("()")
        if " -> " in sig:
            sig = sig.split(" -> ")[0]

        if not sig or sig == "self":
            return params

        for part in sig.split(","):
            part = part.strip()
            if part in ("self", "cls"):
                continue

            name = part.split(":")[0].split("=")[0].strip()
            type_hint = ""
            default = ""

            if ":" in part:
                type_hint = part.split(":")[1].split("=")[0].strip()
            if "=" in part:
                default = part.split("=")[1].strip()

            params.append({
                "name": name,
                "type": type_hint,
                "default": default,
            })

        return params

    def _google_docstring(self, name: str, params: List[Dict]) -> str:
        """Generate Google-style docstring."""
        lines = [f'"""TODO: Describe {name}.', ""]

        if params:
            lines.append("    Args:")
            for p in params:
                type_str = f" ({p['type']})" if p['type'] else ""
                default_str = f" Defaults to {p['default']}." if p['default'] else ""
                lines.append(f"        {p['name']}{type_str}: TODO: Describe.{default_str}")
            lines.append("")

        lines.append("    Returns:")
        lines.append("        TODO: Describe return value.")
        lines.append('    """')

        return "\n".join(lines)

    def _numpy_docstring(self, name: str, params: List[Dict]) -> str:
        """Generate NumPy-style docstring."""
        lines = [f'"""TODO: Describe {name}.', ""]

        if params:
            lines.append("    Parameters")
            lines.append("    ----------")
            for p in params:
                type_str = f" : {p['type']}" if p['type'] else ""
                lines.append(f"    {p['name']}{type_str}")
                lines.append("        TODO: Describe.")
            lines.append("")

        lines.append("    Returns")
        lines.append("    -------")
        lines.append("    TODO")
        lines.append("        TODO: Describe return value.")
        lines.append('    """')

        return "\n".join(lines)

    def _sphinx_docstring(self, name: str, params: List[Dict]) -> str:
        """Generate Sphinx-style docstring."""
        lines = [f'"""TODO: Describe {name}.', ""]

        for p in params:
            type_str = f" {p['type']}" if p['type'] else ""
            lines.append(f"    :param{type_str} {p['name']}: TODO: Describe.")

        lines.append("    :returns: TODO: Describe return value.")
        lines.append('    """')

        return "\n".join(lines)

    async def _generate_with_llm(
        self,
        item: Dict[str, Any],
        context: Optional[str],
    ) -> str:
        """Generate docstring using LLM."""
        # This would integrate with the Trinity LLM system
        # For now, fall back to template
        return self._generate_template_docstring(item)

    async def document_file(
        self,
        file_path: str,
        dry_run: bool = True,
    ) -> Dict[str, Any]:
        """
        Add docstrings to all undocumented items in a file.

        Args:
            file_path: Path to the file
            dry_run: If True, return changes without applying

        Returns:
            Dict with 'changes' list and 'new_content'
        """
        content = Path(file_path).read_text(encoding="utf-8")
        undocumented = await self.find_undocumented(file_path, content)

        if not undocumented:
            return {"changes": [], "new_content": content}

        changes = []
        lines = content.splitlines(keepends=True)

        # Process in reverse order to maintain line numbers
        for item in sorted(undocumented, key=lambda x: x["line"], reverse=True):
            docstring = await self.generate_docstring(item, content)

            # Find the line to insert after (the def/class line)
            line_idx = item["line"] - 1

            # Find the indentation
            def_line = lines[line_idx]
            indent = len(def_line) - len(def_line.lstrip())
            doc_indent = " " * (indent + 4)

            # Format docstring with proper indentation
            doc_lines = docstring.splitlines()
            formatted_doc = doc_lines[0] + "\n"
            for dl in doc_lines[1:]:
                formatted_doc += doc_indent + dl + "\n"

            # Insert after the def line (account for multi-line defs)
            insert_idx = line_idx + 1
            while insert_idx < len(lines) and not lines[insert_idx - 1].rstrip().endswith(":"):
                insert_idx += 1

            lines.insert(insert_idx, doc_indent + formatted_doc)

            changes.append({
                "type": item["type"],
                "name": item["name"],
                "line": item["line"],
                "docstring": docstring,
            })

        new_content = "".join(lines)

        if not dry_run:
            Path(file_path).write_text(new_content, encoding="utf-8")

        return {"changes": changes, "new_content": new_content}


class VoiceCommandRouter:
    """
    v3.1: Routes voice commands to appropriate self-programming handlers.

    Provides natural language understanding for:
    - File creation ("create a hello world program")
    - Code improvement ("optimize this function")
    - Refactoring ("rename this function to X")
    - Documentation ("add docstrings to this file")
    - Dependency management ("update requirements")
    """

    # Command patterns for routing
    PATTERNS = {
        "create_file": [
            r"create\s+(a\s+)?(?:new\s+)?(?:file|program|script|module|class)\s+(?:called\s+)?(.+)",
            r"write\s+(?:me\s+)?(?:a\s+)?(.+)\s+(?:file|program|script)",
            r"make\s+(?:a\s+)?(?:new\s+)?(.+)",
            r"generate\s+(?:a\s+)?(.+)",
        ],
        "rename": [
            r"rename\s+(?:this\s+)?(?:function|class|variable|method)\s+(?:to\s+)?(\w+)",
            r"change\s+(?:the\s+)?name\s+(?:of\s+)?(.+)\s+to\s+(\w+)",
        ],
        "improve": [
            r"improve\s+(?:this\s+)?(?:code|function|file)",
            r"optimize\s+(?:this\s+)?(?:code|function|file)",
            r"refactor\s+(?:this\s+)?(?:code|function|file)",
            r"make\s+(?:this\s+)?(?:code|function)\s+(?:better|faster|cleaner)",
        ],
        "document": [
            r"add\s+(?:docstrings?|documentation)\s+(?:to\s+)?(?:this\s+)?(?:file|code)?",
            r"document\s+(?:this\s+)?(?:file|code|function|class)",
            r"generate\s+(?:docstrings?|documentation)",
        ],
        "update_deps": [
            r"update\s+(?:the\s+)?requirements",
            r"add\s+(?:missing\s+)?dependencies",
            r"sync\s+(?:the\s+)?requirements",
        ],
        "analyze": [
            r"analyze\s+(?:this\s+)?(?:code|file)",
            r"check\s+(?:for\s+)?(?:issues|problems|bugs)",
            r"find\s+(?:issues|problems|bugs)\s+(?:in\s+)?(?:this\s+)?(?:code|file)?",
        ],
    }

    @dataclass
    class ParsedCommand:
        """Parsed voice command."""
        action: str
        parameters: Dict[str, Any]
        confidence: float
        raw_text: str

    def __init__(
        self,
        engine: Optional[Any] = None,
        refactoring_engine: Optional[AdvancedRefactoringEngine] = None,
        dependency_tracker: Optional[DependencyTracker] = None,
        suggestion_engine: Optional[ProactiveSuggestionEngine] = None,
        documentation_generator: Optional[DocumentationGenerator] = None,
    ):
        """
        Initialize voice command router.

        Args:
            engine: OuroborosEngine for file creation/improvement
            refactoring_engine: For rename/refactor operations
            dependency_tracker: For requirements management
            suggestion_engine: For code analysis
            documentation_generator: For docstring generation
        """
        self._engine = engine
        self._refactoring = refactoring_engine
        self._deps = dependency_tracker
        self._suggestions = suggestion_engine
        self._docs = documentation_generator

    def parse_command(self, text: str) -> Optional['VoiceCommandRouter.ParsedCommand']:
        """
        Parse a voice command into a structured format.

        Args:
            text: Raw voice command text

        Returns:
            ParsedCommand or None if not recognized
        """
        import re

        text_lower = text.lower().strip()

        for action, patterns in self.PATTERNS.items():
            for pattern in patterns:
                match = re.search(pattern, text_lower)
                if match:
                    params = {}
                    groups = match.groups()

                    if action == "create_file":
                        # Extract file description
                        desc = groups[-1] if groups else text
                        params["description"] = desc.strip()

                    elif action == "rename":
                        if len(groups) >= 2:
                            params["old_name"] = groups[0]
                            params["new_name"] = groups[1]
                        else:
                            params["new_name"] = groups[0] if groups else ""

                    return self.ParsedCommand(
                        action=action,
                        parameters=params,
                        confidence=0.8,  # Could use fuzzy matching score
                        raw_text=text,
                    )

        return None

    async def execute_command(
        self,
        command: 'VoiceCommandRouter.ParsedCommand',
        context: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Execute a parsed voice command.

        Args:
            command: Parsed command to execute
            context: Optional context (current file, selection, etc.)

        Returns:
            Result dict with 'success', 'message', and action-specific data
        """
        context = context or {}

        if command.action == "create_file":
            return await self._handle_create_file(command, context)
        elif command.action == "rename":
            return await self._handle_rename(command, context)
        elif command.action == "improve":
            return await self._handle_improve(command, context)
        elif command.action == "document":
            return await self._handle_document(command, context)
        elif command.action == "update_deps":
            return await self._handle_update_deps(command, context)
        elif command.action == "analyze":
            return await self._handle_analyze(command, context)

        return {
            "success": False,
            "message": f"Unknown command action: {command.action}",
        }

    async def _handle_create_file(
        self,
        command: 'VoiceCommandRouter.ParsedCommand',
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle file creation command."""
        if not self._engine:
            return {"success": False, "message": "Engine not available"}

        description = command.parameters.get("description", "")

        # Try to infer file path from description
        file_path = self._infer_file_path(description, context)

        # Import FileCreationRequest from engine
        try:
            from backend.core.ouroboros.engine import FileCreationRequest
            request = FileCreationRequest(
                target_path=Path(file_path),
                description=description,
                generate_tests=True,
            )
            result = await self._engine.create_file(request)

            return {
                "success": result.success,
                "message": f"Created {file_path}" if result.success else "Failed to create file",
                "file_path": file_path,
                "result": result,
            }
        except Exception as e:
            return {"success": False, "message": f"Error: {e}"}

    def _infer_file_path(
        self,
        description: str,
        context: Dict[str, Any],
    ) -> str:
        """Infer a file path from a description."""
        import re

        # Check if description contains a path
        path_match = re.search(r'(?:at|in|to)\s+([^\s]+\.py)', description.lower())
        if path_match:
            return path_match.group(1)

        # Generate from description
        name = description.lower()
        name = re.sub(r'[^\w\s]', '', name)
        name = re.sub(r'\s+', '_', name)
        name = name[:50]  # Limit length

        if not name.endswith('.py'):
            name += '.py'

        # Use context directory if available
        base_dir = context.get("current_dir", ".")
        return str(Path(base_dir) / name)

    async def _handle_rename(
        self,
        command: 'VoiceCommandRouter.ParsedCommand',
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle rename command."""
        if not self._refactoring:
            return {"success": False, "message": "Refactoring engine not available"}

        new_name = command.parameters.get("new_name")
        if not new_name:
            return {"success": False, "message": "New name not specified"}

        # Need context for current position
        file_path = context.get("current_file")
        line = context.get("cursor_line", 1)
        column = context.get("cursor_column", 1)

        if not file_path:
            return {"success": False, "message": "No file context available"}

        try:
            plan = await self._refactoring.rename_symbol(
                file_path, line, column, new_name, dry_run=False
            )
            return {
                "success": True,
                "message": f"Renamed to {new_name} in {len(plan.affected_files)} files",
                "plan": plan,
            }
        except Exception as e:
            return {"success": False, "message": f"Rename failed: {e}"}

    async def _handle_improve(
        self,
        command: 'VoiceCommandRouter.ParsedCommand',
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle code improvement command."""
        if not self._engine:
            return {"success": False, "message": "Engine not available"}

        file_path = context.get("current_file")
        if not file_path:
            return {"success": False, "message": "No file context available"}

        try:
            from backend.core.ouroboros.engine import ImprovementRequest
            request = ImprovementRequest(
                target_file=Path(file_path),
                goal="Improve code quality, performance, and readability",
            )
            result = await self._engine.improve(request)

            return {
                "success": result.success,
                "message": "Code improved" if result.success else "Improvement failed",
                "result": result,
            }
        except Exception as e:
            return {"success": False, "message": f"Error: {e}"}

    async def _handle_document(
        self,
        command: 'VoiceCommandRouter.ParsedCommand',
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle documentation command."""
        if not self._docs:
            self._docs = DocumentationGenerator()

        file_path = context.get("current_file")
        if not file_path:
            return {"success": False, "message": "No file context available"}

        try:
            result = await self._docs.document_file(file_path, dry_run=False)
            return {
                "success": True,
                "message": f"Added {len(result['changes'])} docstrings",
                "changes": result["changes"],
            }
        except Exception as e:
            return {"success": False, "message": f"Error: {e}"}

    async def _handle_update_deps(
        self,
        command: 'VoiceCommandRouter.ParsedCommand',
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle dependency update command."""
        if not self._deps:
            self._deps = DependencyTracker()

        try:
            changes = await self._deps.update_requirements_txt()
            return {
                "success": True,
                "message": f"Updated requirements.txt: {len(changes)} changes",
                "changes": changes,
            }
        except Exception as e:
            return {"success": False, "message": f"Error: {e}"}

    async def _handle_analyze(
        self,
        command: 'VoiceCommandRouter.ParsedCommand',
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Handle code analysis command."""
        if not self._suggestions:
            self._suggestions = ProactiveSuggestionEngine()

        file_path = context.get("current_file")

        try:
            if file_path:
                suggestions = await self._suggestions.analyze_file(file_path)
            else:
                suggestions = await self._suggestions.get_suggestions()

            return {
                "success": True,
                "message": f"Found {len(suggestions)} issues",
                "suggestions": suggestions,
            }
        except Exception as e:
            return {"success": False, "message": f"Error: {e}"}


# Global instances
_refactoring_engine: Optional[AdvancedRefactoringEngine] = None
_dependency_tracker: Optional[DependencyTracker] = None
_suggestion_engine: Optional[ProactiveSuggestionEngine] = None
_documentation_generator: Optional[DocumentationGenerator] = None
_voice_command_router: Optional[VoiceCommandRouter] = None


def get_refactoring_engine() -> AdvancedRefactoringEngine:
    """Get or create global refactoring engine."""
    global _refactoring_engine
    if _refactoring_engine is None:
        _refactoring_engine = AdvancedRefactoringEngine()
    return _refactoring_engine


def get_dependency_tracker(project_root: Optional[Path] = None) -> DependencyTracker:
    """Get or create global dependency tracker."""
    global _dependency_tracker
    if _dependency_tracker is None:
        _dependency_tracker = DependencyTracker(project_root)
    return _dependency_tracker


def get_suggestion_engine(project_root: Optional[Path] = None) -> ProactiveSuggestionEngine:
    """Get or create global suggestion engine."""
    global _suggestion_engine
    if _suggestion_engine is None:
        _suggestion_engine = ProactiveSuggestionEngine(project_root)
    return _suggestion_engine


def get_documentation_generator() -> DocumentationGenerator:
    """Get or create global documentation generator."""
    global _documentation_generator
    if _documentation_generator is None:
        _documentation_generator = DocumentationGenerator()
    return _documentation_generator


def get_voice_command_router(
    engine: Optional[Any] = None,
) -> VoiceCommandRouter:
    """Get or create global voice command router."""
    global _voice_command_router
    if _voice_command_router is None:
        _voice_command_router = VoiceCommandRouter(
            engine=engine,
            refactoring_engine=get_refactoring_engine(),
            dependency_tracker=get_dependency_tracker(),
            suggestion_engine=get_suggestion_engine(),
            documentation_generator=get_documentation_generator(),
        )
    return _voice_command_router


class MultiFileOrchestrator:
    """
    Orchestrates atomic multi-file editing sessions.

    Features:
    - Plan multi-file changes based on blast radius
    - Apply all changes atomically (all or nothing)
    - Track dependencies between files
    - Rollback on any failure
    """

    def __init__(self, session_memory: SessionMemoryManager):
        self._session_memory = session_memory
        self._pending_changes: Dict[str, Tuple[str, str]] = {}  # path -> (original, new)
        self._lock = asyncio.Lock()
        self._transaction_id: Optional[str] = None

    async def begin_transaction(self) -> str:
        """Begin a multi-file transaction."""
        async with self._lock:
            self._transaction_id = f"txn_{uuid.uuid4().hex[:12]}"
            self._pending_changes.clear()
            return self._transaction_id

    async def stage_change(
        self,
        file_path: str,
        original_content: str,
        new_content: str,
    ) -> None:
        """Stage a file change for the current transaction."""
        async with self._lock:
            if not self._transaction_id:
                raise RuntimeError("No active transaction - call begin_transaction() first")
            self._pending_changes[file_path] = (original_content, new_content)

    async def commit_transaction(self, goal: str) -> Tuple[bool, Optional[str]]:
        """
        Commit all staged changes atomically.

        Returns (success, error_message).
        """
        async with self._lock:
            if not self._transaction_id:
                return False, "No active transaction"

            if not self._pending_changes:
                return True, None  # Nothing to commit

            # Create backups
            backups: Dict[str, str] = {}
            applied: List[str] = []

            try:
                # Apply all changes
                for file_path, (original, new_content) in self._pending_changes.items():
                    path = Path(file_path)
                    if path.exists():
                        backups[file_path] = path.read_text(encoding="utf-8")

                    path.write_text(new_content, encoding="utf-8")
                    applied.append(file_path)

                    # Record in session memory
                    await self._session_memory.record_change(
                        file_path=file_path,
                        original_content=original,
                        new_content=new_content,
                        change_type="multi_file_edit",
                        goal=goal,
                        success=True,
                    )

                # Clear transaction state
                self._pending_changes.clear()
                self._transaction_id = None
                return True, None

            except Exception as e:
                # Rollback all applied changes
                for file_path in applied:
                    try:
                        if file_path in backups:
                            Path(file_path).write_text(backups[file_path], encoding="utf-8")
                    except Exception as rollback_error:
                        logger.error(f"Rollback failed for {file_path}: {rollback_error}")

                self._pending_changes.clear()
                self._transaction_id = None
                return False, str(e)

    async def abort_transaction(self) -> None:
        """Abort the current transaction without applying changes."""
        async with self._lock:
            self._pending_changes.clear()
            self._transaction_id = None


class IterativeRefinementLoop:
    """
    Provides user-directed iterative refinement of changes.

    Flow:
    1. Generate initial change
    2. User reviews and provides feedback ("make it smaller", "add error handling", etc.)
    3. Regenerate based on feedback
    4. Repeat until approved
    """

    def __init__(
        self,
        diff_preview_engine: DiffPreviewEngine,
        max_iterations: int = 5,
    ):
        self._diff_preview = diff_preview_engine
        self._max_iterations = max_iterations
        self._iteration_history: List[Dict[str, Any]] = []

    async def refine_with_feedback(
        self,
        original_content: str,
        current_content: str,
        feedback: str,
        goal: str,
        generate_improvement: Callable[[str, str, str], Awaitable[Optional[str]]],
    ) -> Tuple[Optional[str], int]:
        """
        Refine the change based on user feedback.

        Args:
            original_content: Original file content
            current_content: Current proposed change
            feedback: User feedback for refinement
            goal: Original improvement goal
            generate_improvement: Async function to generate improvements

        Returns:
            (refined_content, iteration_count) or (None, iteration_count) on failure
        """
        iteration = 0
        current = current_content

        # Incorporate feedback into the goal
        refined_goal = f"{goal}\n\nUser feedback: {feedback}"

        while iteration < self._max_iterations:
            iteration += 1

            # Generate refined version
            refined = await generate_improvement(original_content, refined_goal, current)

            if not refined:
                logger.warning(f"Refinement iteration {iteration} failed")
                continue

            self._iteration_history.append({
                "iteration": iteration,
                "feedback": feedback,
                "timestamp": time.time(),
                "content_hash": hashlib.md5(refined.encode()).hexdigest()[:12],
            })

            return refined, iteration

        return None, iteration

    def get_iteration_history(self) -> List[Dict[str, Any]]:
        """Get history of refinement iterations."""
        return list(self._iteration_history)


class ConnectionPoolManager:
    """
    Manages connection pools for API calls.

    Features:
    - Reusable HTTP sessions per provider
    - Automatic cleanup of idle connections
    - Health-aware routing
    """

    def __init__(self):
        self._sessions: Dict[str, Any] = {}  # endpoint -> aiohttp.ClientSession
        self._last_used: Dict[str, float] = {}
        self._lock = asyncio.Lock()
        self._cleanup_task: Optional[asyncio.Task] = None
        self._idle_timeout = 300.0  # 5 minutes

    async def get_session(self, endpoint: str) -> Any:
        """Get or create a session for the endpoint."""
        if not aiohttp:
            raise RuntimeError("aiohttp not available")

        async with self._lock:
            if endpoint in self._sessions:
                self._last_used[endpoint] = time.time()
                return self._sessions[endpoint]

            # Create new session with connection pooling
            connector = aiohttp.TCPConnector(
                limit=10,  # Max connections per host
                limit_per_host=5,
                ttl_dns_cache=300,
                keepalive_timeout=30,
            )
            session = aiohttp.ClientSession(connector=connector)
            self._sessions[endpoint] = session
            self._last_used[endpoint] = time.time()

            # Start cleanup task if not running
            if not self._cleanup_task or self._cleanup_task.done():
                self._cleanup_task = asyncio.create_task(self._cleanup_loop())

            return session

    async def _cleanup_loop(self) -> None:
        """Periodically clean up idle sessions."""
        while True:
            await asyncio.sleep(60)  # Check every minute
            await self._cleanup_idle_sessions()

    async def _cleanup_idle_sessions(self) -> None:
        """Close sessions that have been idle too long."""
        now = time.time()
        to_close = []

        async with self._lock:
            for endpoint, last_used in list(self._last_used.items()):
                if now - last_used > self._idle_timeout:
                    to_close.append(endpoint)

            for endpoint in to_close:
                session = self._sessions.pop(endpoint, None)
                self._last_used.pop(endpoint, None)
                if session:
                    await session.close()
                    logger.debug(f"Closed idle session for {endpoint}")

    async def close_all(self) -> None:
        """Close all sessions."""
        async with self._lock:
            for session in self._sessions.values():
                await session.close()
            self._sessions.clear()
            self._last_used.clear()

            if self._cleanup_task:
                self._cleanup_task.cancel()
                try:
                    await self._cleanup_task
                except asyncio.CancelledError:
                    pass


# =============================================================================
# ENHANCED NATIVE SELF-IMPROVEMENT WITH CLAUDE CODE BEHAVIORS
# =============================================================================

class EnhancedSelfImprovement(NativeSelfImprovement):
    """
    Enhanced self-improvement engine with Claude Code-like behaviors.

    Adds:
    - Diff preview before applying
    - Multi-file orchestration
    - Session memory
    - Streaming changes
    - Iterative refinement
    - Connection pooling
    """

    def __init__(self):
        super().__init__()

        # Claude Code-like components
        self._session_memory = SessionMemoryManager()
        self._diff_preview_engine = DiffPreviewEngine()
        self._connection_pool = ConnectionPoolManager()
        self._multi_file_orchestrator = MultiFileOrchestrator(self._session_memory)
        self._refinement_loop = IterativeRefinementLoop(self._diff_preview_engine)

        # Preview mode settings
        self._require_approval = bool(os.getenv("OUROBOROS_REQUIRE_APPROVAL", "false").lower() in ("true", "1"))
        self._auto_stream_diff = bool(os.getenv("OUROBOROS_STREAM_DIFF", "true").lower() in ("true", "1"))

    @property
    def session_memory(self) -> SessionMemoryManager:
        return self._session_memory

    @property
    def diff_preview_engine(self) -> DiffPreviewEngine:
        return self._diff_preview_engine

    @property
    def multi_file_orchestrator(self) -> MultiFileOrchestrator:
        return self._multi_file_orchestrator

    async def initialize(self) -> None:
        """Initialize with enhanced components."""
        await super().initialize()
        self.logger.info("Enhanced self-improvement components initialized")
        self.logger.info(f"  - Session ID: {self._session_memory.session_id}")
        self.logger.info(f"  - Require approval: {self._require_approval}")
        self.logger.info(f"  - Auto stream diff: {self._auto_stream_diff}")

    async def shutdown(self) -> None:
        """Shutdown with cleanup."""
        await self._connection_pool.close_all()
        await super().shutdown()

    async def execute_with_preview(
        self,
        target: Union[str, Path],
        goal: str,
        test_command: Optional[str] = None,
        context: Optional[str] = None,
        max_iterations: int = 5,
        require_approval: Optional[bool] = None,
    ) -> ImprovementResult:
        """
        Execute improvement with diff preview and approval workflow.

        This is the Claude Code-like interface:
        1. Generate improvement
        2. Create and stream diff preview
        3. Wait for user approval (if require_approval=True)
        4. Apply or iterate based on feedback
        """
        require = require_approval if require_approval is not None else self._require_approval

        # Phase 1: Generate improvement
        target_path = SecurityValidator.validate_path(target)
        original_content = await self._read_file_safe(target_path)

        improved_content, provider = await self._generate_improvement(
            original_content, goal, None, context
        )

        if not improved_content:
            return ImprovementResult(
                success=False,
                task_id=f"imp_{uuid.uuid4().hex[:12]}",
                target_file=str(target_path),
                goal=goal,
                iterations=1,
                total_time=0,
                error="Failed to generate improvement",
            )

        # Phase 2: Create diff preview
        preview = await self._diff_preview_engine.create_preview(
            original_content=original_content,
            modified_content=improved_content,
            file_path=str(target_path),
            goal=goal,
        )

        # Phase 3: Stream diff if enabled
        if self._auto_stream_diff:
            await self._diff_preview_engine.stream_preview(preview)

        # Phase 4: Wait for approval if required
        if require:
            approved, feedback = await self._diff_preview_engine.wait_for_approval(
                preview.id, timeout=300.0
            )

            if not approved:
                if feedback:
                    # User wants refinement
                    refined, iterations = await self._refinement_loop.refine_with_feedback(
                        original_content=original_content,
                        current_content=improved_content,
                        feedback=feedback,
                        goal=goal,
                        generate_improvement=lambda orig, g, _: self._generate_improvement(orig, g, None, context),
                    )
                    if refined:
                        improved_content = refined[0] if isinstance(refined, tuple) else refined
                    else:
                        return ImprovementResult(
                            success=False,
                            task_id=preview.id,
                            target_file=str(target_path),
                            goal=goal,
                            iterations=iterations,
                            total_time=0,
                            error=f"Refinement failed after {iterations} iterations",
                        )
                else:
                    # User rejected
                    return ImprovementResult(
                        success=False,
                        task_id=preview.id,
                        target_file=str(target_path),
                        goal=goal,
                        iterations=0,
                        total_time=0,
                        error="Changes rejected by user",
                    )

        # Phase 5: Apply changes
        await self._write_file_safe(target_path, improved_content)
        await self._session_memory.record_change(
            file_path=str(target_path),
            original_content=original_content,
            new_content=improved_content,
            change_type="improvement",
            goal=goal,
            success=True,
        )

        return ImprovementResult(
            success=True,
            task_id=preview.id,
            target_file=str(target_path),
            goal=goal,
            iterations=1,
            total_time=0,
            provider_used=provider,
            changes_applied=True,
            diff=preview.to_unified_diff(),
        )

    async def execute_multi_file_improvement(
        self,
        files_and_goals: List[Tuple[Union[str, Path], str]],
        shared_context: Optional[str] = None,
        require_approval: bool = True,
    ) -> Dict[str, ImprovementResult]:
        """
        Execute improvements on multiple files atomically.

        All files are modified together or none are modified.
        """
        results: Dict[str, ImprovementResult] = {}

        # Begin transaction
        txn_id = await self._multi_file_orchestrator.begin_transaction()

        try:
            # Generate improvements for each file
            for target, goal in files_and_goals:
                target_path = SecurityValidator.validate_path(target)
                original_content = await self._read_file_safe(target_path)

                improved_content, provider = await self._generate_improvement(
                    original_content, goal, None, shared_context
                )

                if not improved_content:
                    results[str(target_path)] = ImprovementResult(
                        success=False,
                        task_id=txn_id,
                        target_file=str(target_path),
                        goal=goal,
                        iterations=0,
                        total_time=0,
                        error="Failed to generate improvement",
                    )
                    # Abort on any failure
                    await self._multi_file_orchestrator.abort_transaction()
                    return results

                # Stage the change
                await self._multi_file_orchestrator.stage_change(
                    str(target_path), original_content, improved_content
                )

                results[str(target_path)] = ImprovementResult(
                    success=True,
                    task_id=txn_id,
                    target_file=str(target_path),
                    goal=goal,
                    iterations=1,
                    total_time=0,
                    provider_used=provider,
                )

            # Commit all changes atomically
            combined_goal = "; ".join(g for _, g in files_and_goals)
            success, error = await self._multi_file_orchestrator.commit_transaction(combined_goal)

            if not success:
                # Mark all as failed
                for path in results:
                    results[path] = ImprovementResult(
                        success=False,
                        task_id=txn_id,
                        target_file=path,
                        goal=results[path].goal,
                        iterations=0,
                        total_time=0,
                        error=f"Transaction commit failed: {error}",
                    )

            return results

        except Exception as e:
            await self._multi_file_orchestrator.abort_transaction()
            raise

    def get_status(self) -> Dict[str, Any]:
        """Get enhanced status."""
        base_status = super().get_status()
        base_status["enhanced"] = {
            "session_id": self._session_memory.session_id,
            "session_duration_seconds": self._session_memory.duration_seconds,
            "require_approval": self._require_approval,
            "pending_previews": len(self._diff_preview_engine.get_pending_previews()),
        }
        return base_status


# =============================================================================
# ENHANCED GLOBAL INSTANCE
# =============================================================================

_enhanced_engine: Optional[EnhancedSelfImprovement] = None


def get_enhanced_self_improvement() -> EnhancedSelfImprovement:
    """Get the enhanced self-improvement engine with Claude Code-like behaviors."""
    global _enhanced_engine
    if _enhanced_engine is None:
        _enhanced_engine = EnhancedSelfImprovement()
    return _enhanced_engine


async def execute_with_preview(
    target: Union[str, Path],
    goal: str,
    test_command: Optional[str] = None,
    context: Optional[str] = None,
    max_iterations: int = 5,
    require_approval: bool = False,
) -> ImprovementResult:
    """
    Execute improvement with diff preview - Claude Code-like interface.

    Example:
        result = await execute_with_preview(
            target="backend/core/utils.py",
            goal="Fix the race condition",
            require_approval=True  # Show diff and wait for approval
        )
    """
    engine = get_enhanced_self_improvement()
    if not engine._running:
        await engine.initialize()

    return await engine.execute_with_preview(
        target=target,
        goal=goal,
        test_command=test_command,
        context=context,
        max_iterations=max_iterations,
        require_approval=require_approval,
    )


async def execute_multi_file(
    files_and_goals: List[Tuple[Union[str, Path], str]],
    shared_context: Optional[str] = None,
    require_approval: bool = True,
) -> Dict[str, ImprovementResult]:
    """
    Execute improvements on multiple files atomically.

    Example:
        results = await execute_multi_file([
            ("backend/api/routes.py", "Add rate limiting"),
            ("backend/core/limiter.py", "Implement rate limiter"),
            ("tests/test_limiter.py", "Add rate limiter tests"),
        ])
    """
    engine = get_enhanced_self_improvement()
    if not engine._running:
        await engine.initialize()

    return await engine.execute_multi_file_improvement(
        files_and_goals=files_and_goals,
        shared_context=shared_context,
        require_approval=require_approval,
    )


# =============================================================================
# v4.0: AUTONOMOUS SELF-PROGRAMMING - THE MISSING 20%
# =============================================================================
# These components enable TRUE autonomous decision-making:
# - GoalDecompositionEngine: Breaks "Make yourself faster" into actionable tasks
# - TechnicalDebtDetector: Finds issues without being told
# - AutonomousSelfRefinementLoop: Runs continuously without prompts
# - DualAgentSystem: Architect generates, Reviewer critiques
# - CodeMemoryRAG: Oracle (graph) + ChromaDB (semantic) fusion
# - SystemFeedbackLoop: Hardware metrics trigger code improvements
# - AutoTestGenerator: Generates tests for untested existing code
# =============================================================================


class SubTaskPriority(Enum):
    """Priority levels for decomposed sub-tasks."""
    CRITICAL = 1      # Blocking issues, security vulnerabilities
    HIGH = 2          # Performance issues, missing tests
    MEDIUM = 3        # Code smells, minor improvements
    LOW = 4           # Style issues, documentation
    DEFERRED = 5      # Future improvements


class SubTaskType(Enum):
    """Types of sub-tasks from goal decomposition."""
    ANALYZE = "analyze"           # Investigate/profile code
    IMPLEMENT = "implement"       # Write new code
    REFACTOR = "refactor"         # Restructure existing code
    FIX = "fix"                   # Fix bugs/issues
    TEST = "test"                 # Write/run tests
    DOCUMENT = "document"         # Add documentation
    OPTIMIZE = "optimize"         # Performance improvements
    VALIDATE = "validate"         # Verify changes work
    BENCHMARK = "benchmark"       # Measure performance
    REVIEW = "review"             # Code review


class TechnicalDebtCategory(Enum):
    """Categories of technical debt."""
    COMPLEXITY = "complexity"         # High cyclomatic/cognitive complexity
    DUPLICATION = "duplication"       # Code duplication
    DEAD_CODE = "dead_code"           # Unused code
    CIRCULAR_DEPS = "circular_deps"   # Circular dependencies
    MISSING_TESTS = "missing_tests"   # Insufficient test coverage
    SECURITY = "security"             # Security vulnerabilities
    PERFORMANCE = "performance"       # Performance bottlenecks
    TYPE_SAFETY = "type_safety"       # Missing type hints
    ERROR_HANDLING = "error_handling" # Poor error handling
    DEPRECATED = "deprecated"         # Deprecated API usage
    COUPLING = "coupling"             # High coupling between modules


@dataclass
class SubTask:
    """
    A decomposed sub-task from a high-level goal.

    This is the atomic unit of work that can be assigned to the
    AgenticLoopOrchestrator for execution.
    """
    id: str
    description: str
    task_type: SubTaskType
    priority: SubTaskPriority
    target_files: List[str]
    dependencies: List[str] = field(default_factory=list)  # IDs of tasks that must complete first
    success_criteria: str = ""
    estimated_complexity: str = "medium"  # trivial, easy, medium, hard, expert
    context: Dict[str, Any] = field(default_factory=dict)
    parent_goal: str = ""
    created_at: float = field(default_factory=time.time)
    status: str = "pending"  # pending, in_progress, completed, failed, blocked
    result: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "id": self.id,
            "description": self.description,
            "task_type": self.task_type.value,
            "priority": self.priority.value,
            "target_files": self.target_files,
            "dependencies": self.dependencies,
            "success_criteria": self.success_criteria,
            "estimated_complexity": self.estimated_complexity,
            "context": self.context,
            "parent_goal": self.parent_goal,
            "status": self.status,
        }


@dataclass
class TechnicalDebtItem:
    """
    A single instance of technical debt detected in the codebase.
    """
    id: str
    category: TechnicalDebtCategory
    file_path: str
    line_start: int
    line_end: int
    severity: float  # 0.0 - 1.0
    description: str
    code_snippet: str = ""
    suggested_fix: str = ""
    estimated_effort: str = "medium"  # trivial, easy, medium, hard, expert
    detected_at: float = field(default_factory=time.time)
    metrics: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "id": self.id,
            "category": self.category.value,
            "file_path": self.file_path,
            "line_start": self.line_start,
            "line_end": self.line_end,
            "severity": self.severity,
            "description": self.description,
            "suggested_fix": self.suggested_fix,
            "estimated_effort": self.estimated_effort,
        }


@dataclass
class DecompositionResult:
    """Result of goal decomposition."""
    original_goal: str
    sub_tasks: List[SubTask]
    execution_order: List[str]  # Task IDs in order
    total_estimated_complexity: str
    reasoning: str
    dependencies_graph: Dict[str, List[str]]  # task_id -> [dependency_ids]
    created_at: float = field(default_factory=time.time)


@dataclass
class CodeMemoryResult:
    """Result from CodeMemoryRAG search."""
    query: str
    graph_results: List[Dict[str, Any]]  # From Oracle
    semantic_results: List[Dict[str, Any]]  # From ChromaDB
    merged_results: List[Dict[str, Any]]  # Combined and ranked
    relevance_scores: Dict[str, float]
    total_results: int


class GoalDecompositionEngine:
    """
    v4.0: Autonomous goal breakdown engine using LLM-powered analysis.

    This is the CRITICAL missing piece - the ability to take a high-level goal
    like "Make yourself faster" and decompose it into specific, actionable
    sub-tasks that can be executed by the AgenticLoopOrchestrator.

    The decomposition process:
    1. Analyze the goal using LLM to understand intent
    2. Query Oracle for relevant code structure
    3. Identify affected files and dependencies
    4. Generate ordered sub-tasks with dependencies
    5. Estimate complexity and priority for each

    Example:
        engine = GoalDecompositionEngine(oracle, llm_client)
        result = await engine.decompose(
            "Make the voice authentication faster",
            context={"codebase": "jarvis"}
        )
        # Returns: [
        #   SubTask("Profile voice_auth.py to find bottlenecks", ANALYZE, HIGH),
        #   SubTask("Cache speaker embeddings", IMPLEMENT, HIGH),
        #   SubTask("Optimize cosine similarity calculation", OPTIMIZE, MEDIUM),
        #   SubTask("Add benchmark tests", TEST, MEDIUM),
        # ]
    """

    # System prompt for goal decomposition
    DECOMPOSITION_SYSTEM_PROMPT = """You are an expert software architect specializing in breaking down complex improvement goals into actionable sub-tasks.

Your job is to analyze high-level goals and decompose them into specific, executable tasks that can be performed by an AI coding agent.

For each sub-task, you must specify:
1. A clear, actionable description
2. The type of task (analyze, implement, refactor, fix, test, document, optimize, validate, benchmark, review)
3. Priority (critical, high, medium, low)
4. Target files that will be affected
5. Dependencies on other tasks (which tasks must complete first)
6. Success criteria (how to verify the task is complete)
7. Estimated complexity (trivial, easy, medium, hard, expert)

Guidelines:
- Break goals into 3-10 specific sub-tasks
- Always start with analysis/profiling for optimization goals
- Always end with validation/testing tasks
- Consider dependencies between tasks (order matters)
- Be specific about file paths when possible
- Identify potential risks and blockers

Output your response as valid JSON with this structure:
{
    "reasoning": "Your analysis of the goal and approach",
    "sub_tasks": [
        {
            "description": "Task description",
            "task_type": "analyze|implement|refactor|fix|test|document|optimize|validate|benchmark|review",
            "priority": "critical|high|medium|low",
            "target_files": ["file1.py", "file2.py"],
            "dependencies": [],
            "success_criteria": "How to verify completion",
            "estimated_complexity": "trivial|easy|medium|hard|expert"
        }
    ],
    "execution_order": ["task_0", "task_1", ...],
    "total_complexity": "easy|medium|hard|expert"
}"""

    def __init__(
        self,
        oracle: Optional[Any] = None,
        llm_client: Optional[Any] = None,
        learning_db: Optional[Any] = None,
    ):
        """
        Initialize the goal decomposition engine.

        Args:
            oracle: The Oracle codebase knowledge graph
            llm_client: LLM client for goal analysis (Prime or Claude)
            learning_db: Learning database for pattern storage
        """
        self._oracle = oracle
        self._llm_client = llm_client
        self._learning_db = learning_db
        self._lock = asyncio.Lock()
        self._cache: Dict[str, DecompositionResult] = {}
        self._decomposition_history: List[DecompositionResult] = []

        # Configuration from environment
        self._max_sub_tasks = int(os.getenv("GOAL_DECOMP_MAX_TASKS", "10"))
        self._cache_ttl = int(os.getenv("GOAL_DECOMP_CACHE_TTL", "3600"))
        self._min_sub_tasks = int(os.getenv("GOAL_DECOMP_MIN_TASKS", "2"))

        logger.info("GoalDecompositionEngine initialized")

    async def set_oracle(self, oracle: Any) -> None:
        """Set the Oracle instance."""
        self._oracle = oracle

    async def set_llm_client(self, llm_client: Any) -> None:
        """Set the LLM client."""
        self._llm_client = llm_client

    async def decompose(
        self,
        goal: str,
        context: Optional[Dict[str, Any]] = None,
        target_files: Optional[List[str]] = None,
        force_refresh: bool = False,
    ) -> DecompositionResult:
        """
        Decompose a high-level goal into actionable sub-tasks.

        Args:
            goal: The high-level goal to decompose
            context: Additional context (codebase info, constraints, etc.)
            target_files: Optional list of specific files to focus on
            force_refresh: Bypass cache and regenerate

        Returns:
            DecompositionResult with ordered sub-tasks
        """
        # Check cache
        cache_key = hashlib.sha256(f"{goal}:{json.dumps(context or {})}".encode()).hexdigest()[:16]
        if not force_refresh and cache_key in self._cache:
            cached = self._cache[cache_key]
            if time.time() - cached.created_at < self._cache_ttl:
                logger.debug(f"Using cached decomposition for goal: {goal[:50]}...")
                return cached

        async with self._lock:
            # Gather context from Oracle
            oracle_context = await self._gather_oracle_context(goal, target_files)

            # Query learning DB for similar past decompositions
            historical_patterns = await self._get_historical_patterns(goal)

            # Build the prompt
            prompt = await self._build_decomposition_prompt(
                goal, context or {}, oracle_context, historical_patterns, target_files
            )

            # Call LLM for decomposition
            response = await self._call_llm(prompt)

            # Parse response into sub-tasks
            result = await self._parse_decomposition_response(goal, response)

            # Cache and store
            self._cache[cache_key] = result
            self._decomposition_history.append(result)

            # Record in learning DB
            if self._learning_db:
                await self._record_decomposition(result)

            logger.info(f"Decomposed goal '{goal[:50]}...' into {len(result.sub_tasks)} sub-tasks")
            return result

    async def _gather_oracle_context(
        self,
        goal: str,
        target_files: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """Gather relevant codebase context from Oracle."""
        if not self._oracle:
            return {"available": False}

        context = {"available": True}

        try:
            # Extract keywords from goal for file search
            keywords = self._extract_keywords(goal)

            # Find relevant files
            relevant_files = []
            for keyword in keywords:
                try:
                    nodes = await asyncio.to_thread(
                        lambda k=keyword: self._oracle.find_nodes_by_name(k, fuzzy=True)
                        if hasattr(self._oracle, 'find_nodes_by_name') else []
                    )
                    relevant_files.extend([str(n) for n in nodes[:5]])
                except Exception:
                    pass

            context["relevant_files"] = list(set(relevant_files))[:20]

            # Get dead code if optimization goal
            if any(kw in goal.lower() for kw in ["optimize", "performance", "speed", "fast"]):
                try:
                    dead_code = await asyncio.to_thread(
                        lambda: self._oracle.get_dead_code()
                        if hasattr(self._oracle, 'get_dead_code') else []
                    )
                    context["dead_code"] = [str(d) for d in list(dead_code)[:10]]
                except Exception:
                    pass

            # Get circular dependencies if refactor goal
            if any(kw in goal.lower() for kw in ["refactor", "clean", "structure", "architecture"]):
                try:
                    cycles = await asyncio.to_thread(
                        lambda: self._oracle.get_circular_dependencies()
                        if hasattr(self._oracle, 'get_circular_dependencies') else []
                    )
                    context["circular_deps"] = [str(c) for c in list(cycles)[:5]]
                except Exception:
                    pass

            # Get blast radius for target files
            if target_files:
                blast_radii = {}
                for file in target_files[:3]:
                    try:
                        radius = await asyncio.to_thread(
                            lambda f=file: self._oracle.get_blast_radius(f)
                            if hasattr(self._oracle, 'get_blast_radius') else None
                        )
                        if radius:
                            blast_radii[file] = {
                                "directly_affected": radius.directly_affected[:10]
                                if hasattr(radius, 'directly_affected') else [],
                                "risk_level": getattr(radius, 'risk_level', 'unknown'),
                            }
                    except Exception:
                        pass
                context["blast_radii"] = blast_radii

        except Exception as e:
            logger.warning(f"Error gathering Oracle context: {e}")
            context["error"] = str(e)

        return context

    def _extract_keywords(self, goal: str) -> List[str]:
        """Extract relevant keywords from goal for search."""
        # Remove common words and extract meaningful terms
        stop_words = {
            "the", "a", "an", "in", "on", "at", "to", "for", "of", "and", "or",
            "is", "are", "be", "been", "being", "have", "has", "had", "do", "does",
            "did", "will", "would", "could", "should", "may", "might", "must",
            "make", "it", "this", "that", "these", "those", "i", "you", "we",
            "more", "better", "improve", "fix", "add", "create", "update",
        }

        words = re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\b', goal.lower())
        keywords = [w for w in words if w not in stop_words and len(w) > 2]

        # Also extract potential class/function names (CamelCase or snake_case)
        camel_case = re.findall(r'[A-Z][a-z]+(?:[A-Z][a-z]+)*', goal)
        snake_case = re.findall(r'[a-z]+(?:_[a-z]+)+', goal)

        return list(set(keywords + camel_case + snake_case))[:10]

    async def _get_historical_patterns(self, goal: str) -> List[Dict[str, Any]]:
        """Get similar historical decompositions from learning DB."""
        if not self._learning_db:
            return []

        try:
            # Query for similar past decompositions
            if hasattr(self._learning_db, 'query_patterns'):
                patterns = await self._learning_db.query_patterns(
                    category="goal_decomposition",
                    limit=3,
                )
                return patterns
        except Exception as e:
            logger.debug(f"Error getting historical patterns: {e}")

        return []

    async def _build_decomposition_prompt(
        self,
        goal: str,
        context: Dict[str, Any],
        oracle_context: Dict[str, Any],
        historical_patterns: List[Dict[str, Any]],
        target_files: Optional[List[str]],
    ) -> str:
        """Build the prompt for LLM decomposition."""
        prompt_parts = [
            f"# Goal to Decompose\n{goal}\n",
        ]

        if context:
            prompt_parts.append(f"# Additional Context\n{json.dumps(context, indent=2)}\n")

        if oracle_context.get("available"):
            prompt_parts.append("# Codebase Analysis from Oracle")
            if oracle_context.get("relevant_files"):
                prompt_parts.append(f"Relevant files: {', '.join(oracle_context['relevant_files'][:10])}")
            if oracle_context.get("dead_code"):
                prompt_parts.append(f"Dead code detected: {', '.join(oracle_context['dead_code'][:5])}")
            if oracle_context.get("circular_deps"):
                prompt_parts.append(f"Circular dependencies: {', '.join(str(c) for c in oracle_context['circular_deps'][:3])}")
            if oracle_context.get("blast_radii"):
                prompt_parts.append(f"Blast radius info: {json.dumps(oracle_context['blast_radii'], indent=2)}")
            prompt_parts.append("")

        if target_files:
            prompt_parts.append(f"# Target Files to Focus On\n{', '.join(target_files)}\n")

        if historical_patterns:
            prompt_parts.append("# Similar Past Decompositions")
            for pattern in historical_patterns[:2]:
                if isinstance(pattern, dict):
                    prompt_parts.append(f"- Goal: {pattern.get('goal', 'N/A')[:100]}")
                    prompt_parts.append(f"  Tasks: {pattern.get('task_count', 'N/A')}")
            prompt_parts.append("")

        prompt_parts.append(
            "Please decompose this goal into specific, actionable sub-tasks. "
            "Return valid JSON as specified in the system prompt."
        )

        return "\n".join(prompt_parts)

    async def _call_llm(self, prompt: str) -> str:
        """Call the LLM for decomposition."""
        if not self._llm_client:
            # Fallback to basic decomposition without LLM
            return await self._basic_decomposition_fallback(prompt)

        try:
            # Try different LLM client interfaces
            if hasattr(self._llm_client, 'complete'):
                response = await self._llm_client.complete(
                    system=self.DECOMPOSITION_SYSTEM_PROMPT,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=2000,
                    temperature=0.3,
                )
                return response[0] if isinstance(response, tuple) else response
            elif hasattr(self._llm_client, 'generate'):
                response = await self._llm_client.generate(
                    prompt=f"{self.DECOMPOSITION_SYSTEM_PROMPT}\n\n{prompt}",
                    max_tokens=2000,
                )
                return response
            elif hasattr(self._llm_client, 'chat'):
                response = await self._llm_client.chat(
                    messages=[
                        {"role": "system", "content": self.DECOMPOSITION_SYSTEM_PROMPT},
                        {"role": "user", "content": prompt},
                    ],
                    max_tokens=2000,
                )
                return response.get("content", "") if isinstance(response, dict) else str(response)
            else:
                logger.warning("Unknown LLM client interface, using fallback")
                return await self._basic_decomposition_fallback(prompt)
        except Exception as e:
            logger.error(f"LLM call failed: {e}, using fallback")
            return await self._basic_decomposition_fallback(prompt)

    async def _basic_decomposition_fallback(self, prompt: str) -> str:
        """Basic rule-based decomposition when LLM is unavailable."""
        goal = prompt.split("\n")[1] if "\n" in prompt else prompt
        goal_lower = goal.lower()

        sub_tasks = []

        # Determine task types based on goal keywords
        if any(kw in goal_lower for kw in ["optimize", "performance", "speed", "fast"]):
            sub_tasks = [
                {"description": "Profile code to identify bottlenecks", "task_type": "analyze", "priority": "high", "target_files": [], "dependencies": [], "success_criteria": "Profiling report generated", "estimated_complexity": "easy"},
                {"description": "Implement optimizations based on profiling", "task_type": "optimize", "priority": "high", "target_files": [], "dependencies": ["task_0"], "success_criteria": "Optimizations applied", "estimated_complexity": "medium"},
                {"description": "Run benchmarks to verify improvements", "task_type": "benchmark", "priority": "medium", "target_files": [], "dependencies": ["task_1"], "success_criteria": "Performance improved", "estimated_complexity": "easy"},
            ]
        elif any(kw in goal_lower for kw in ["fix", "bug", "error", "issue"]):
            sub_tasks = [
                {"description": "Analyze error and identify root cause", "task_type": "analyze", "priority": "critical", "target_files": [], "dependencies": [], "success_criteria": "Root cause identified", "estimated_complexity": "medium"},
                {"description": "Implement fix for the issue", "task_type": "fix", "priority": "critical", "target_files": [], "dependencies": ["task_0"], "success_criteria": "Fix implemented", "estimated_complexity": "medium"},
                {"description": "Add tests to prevent regression", "task_type": "test", "priority": "high", "target_files": [], "dependencies": ["task_1"], "success_criteria": "Tests pass", "estimated_complexity": "easy"},
            ]
        elif any(kw in goal_lower for kw in ["refactor", "clean", "improve"]):
            sub_tasks = [
                {"description": "Analyze code structure and identify improvements", "task_type": "analyze", "priority": "high", "target_files": [], "dependencies": [], "success_criteria": "Analysis complete", "estimated_complexity": "easy"},
                {"description": "Refactor code to improve structure", "task_type": "refactor", "priority": "high", "target_files": [], "dependencies": ["task_0"], "success_criteria": "Code refactored", "estimated_complexity": "medium"},
                {"description": "Validate refactoring with existing tests", "task_type": "validate", "priority": "medium", "target_files": [], "dependencies": ["task_1"], "success_criteria": "All tests pass", "estimated_complexity": "easy"},
            ]
        else:
            sub_tasks = [
                {"description": f"Analyze requirements for: {goal}", "task_type": "analyze", "priority": "high", "target_files": [], "dependencies": [], "success_criteria": "Requirements understood", "estimated_complexity": "easy"},
                {"description": "Implement the requested changes", "task_type": "implement", "priority": "high", "target_files": [], "dependencies": ["task_0"], "success_criteria": "Implementation complete", "estimated_complexity": "medium"},
                {"description": "Test and validate changes", "task_type": "validate", "priority": "medium", "target_files": [], "dependencies": ["task_1"], "success_criteria": "Validation passed", "estimated_complexity": "easy"},
            ]

        return json.dumps({
            "reasoning": "Basic rule-based decomposition (LLM unavailable)",
            "sub_tasks": sub_tasks,
            "execution_order": [f"task_{i}" for i in range(len(sub_tasks))],
            "total_complexity": "medium",
        })

    async def _parse_decomposition_response(
        self,
        goal: str,
        response: str,
    ) -> DecompositionResult:
        """Parse the LLM response into a DecompositionResult."""
        try:
            # Extract JSON from response
            json_match = re.search(r'\{[\s\S]*\}', response)
            if not json_match:
                raise ValueError("No JSON found in response")

            data = json.loads(json_match.group())

            # Build sub-tasks
            sub_tasks = []
            for i, task_data in enumerate(data.get("sub_tasks", [])):
                task_id = f"task_{i}"

                # Parse enums safely
                try:
                    task_type = SubTaskType(task_data.get("task_type", "implement"))
                except ValueError:
                    task_type = SubTaskType.IMPLEMENT

                try:
                    priority_str = task_data.get("priority", "medium").upper()
                    priority = SubTaskPriority[priority_str] if priority_str in SubTaskPriority.__members__ else SubTaskPriority.MEDIUM
                except (KeyError, ValueError):
                    priority = SubTaskPriority.MEDIUM

                sub_task = SubTask(
                    id=task_id,
                    description=task_data.get("description", f"Task {i}"),
                    task_type=task_type,
                    priority=priority,
                    target_files=task_data.get("target_files", []),
                    dependencies=task_data.get("dependencies", []),
                    success_criteria=task_data.get("success_criteria", ""),
                    estimated_complexity=task_data.get("estimated_complexity", "medium"),
                    parent_goal=goal,
                )
                sub_tasks.append(sub_task)

            # Build dependency graph
            deps_graph = {task.id: task.dependencies for task in sub_tasks}

            return DecompositionResult(
                original_goal=goal,
                sub_tasks=sub_tasks,
                execution_order=data.get("execution_order", [t.id for t in sub_tasks]),
                total_estimated_complexity=data.get("total_complexity", "medium"),
                reasoning=data.get("reasoning", ""),
                dependencies_graph=deps_graph,
            )

        except Exception as e:
            logger.error(f"Failed to parse decomposition response: {e}")
            # Return minimal result
            return DecompositionResult(
                original_goal=goal,
                sub_tasks=[SubTask(
                    id="task_0",
                    description=goal,
                    task_type=SubTaskType.IMPLEMENT,
                    priority=SubTaskPriority.HIGH,
                    target_files=[],
                    parent_goal=goal,
                )],
                execution_order=["task_0"],
                total_estimated_complexity="medium",
                reasoning=f"Parse error: {e}",
                dependencies_graph={"task_0": []},
            )

    async def _record_decomposition(self, result: DecompositionResult) -> None:
        """Record decomposition in learning database."""
        if not self._learning_db:
            return

        try:
            if hasattr(self._learning_db, 'record_experience'):
                await self._learning_db.record_experience(
                    category="goal_decomposition",
                    data={
                        "goal": result.original_goal,
                        "task_count": len(result.sub_tasks),
                        "complexity": result.total_estimated_complexity,
                        "task_types": [t.task_type.value for t in result.sub_tasks],
                    }
                )
        except Exception as e:
            logger.debug(f"Error recording decomposition: {e}")

    def get_status(self) -> Dict[str, Any]:
        """Get engine status."""
        return {
            "cache_size": len(self._cache),
            "history_size": len(self._decomposition_history),
            "oracle_connected": self._oracle is not None,
            "llm_connected": self._llm_client is not None,
            "learning_db_connected": self._learning_db is not None,
        }


class TechnicalDebtDetector:
    """
    v4.0: Autonomous codebase scanner for technical debt detection.

    This component enables Ironcliw to find problems WITHOUT being told -
    it scans the codebase and identifies issues that need attention.

    Detection categories:
    - High complexity functions (cyclomatic, cognitive)
    - Code duplication
    - Dead/unused code
    - Circular dependencies
    - Missing test coverage
    - Security vulnerabilities
    - Performance bottlenecks
    - Type safety issues
    - Poor error handling
    - Deprecated API usage

    Integration:
    - Uses Oracle for structural analysis
    - Uses AST for complexity metrics
    - Uses pattern matching for security/performance issues
    - Prioritizes issues by severity and effort
    """

    # Complexity thresholds (configurable via environment)
    CYCLOMATIC_THRESHOLD = int(os.getenv("DEBT_CYCLOMATIC_THRESHOLD", "10"))
    COGNITIVE_THRESHOLD = int(os.getenv("DEBT_COGNITIVE_THRESHOLD", "15"))
    LINE_COUNT_THRESHOLD = int(os.getenv("DEBT_LINE_THRESHOLD", "200"))
    NESTING_THRESHOLD = int(os.getenv("DEBT_NESTING_THRESHOLD", "4"))

    # Security patterns to detect
    SECURITY_PATTERNS = [
        (r'eval\s*\(', "eval() usage - potential code injection", 0.9),
        (r'exec\s*\(', "exec() usage - potential code injection", 0.9),
        (r'__import__\s*\(', "Dynamic import - potential security risk", 0.7),
        (r'subprocess\..*shell\s*=\s*True', "Shell=True in subprocess - command injection risk", 0.85),
        (r'pickle\.loads?\s*\(', "Pickle usage - potential arbitrary code execution", 0.8),
        (r'yaml\.load\s*\([^,]+\)(?!.*Loader)', "yaml.load without Loader - arbitrary code execution", 0.85),
        (r'request\.args\.get|request\.form\.get', "Unsanitized user input", 0.6),
        (r'os\.system\s*\(', "os.system usage - prefer subprocess", 0.7),
        (r'\.format\s*\([^)]*request', "Format string with user input - potential injection", 0.75),
        (r'f["\'][^"\']*\{[^}]*request', "F-string with user input - potential injection", 0.75),
    ]

    # Performance patterns to detect
    PERFORMANCE_PATTERNS = [
        (r'for\s+\w+\s+in\s+range\(len\(', "range(len()) - use enumerate instead", 0.4),
        (r'\.append\([^)]+\)\s*$.*for\s+', "Append in loop - consider list comprehension", 0.3),
        (r'\+\s*=\s*["\']', "String concatenation in loop - use join()", 0.5),
        (r'time\.sleep\s*\(\s*[0-9]+\s*\)', "Blocking sleep - consider async sleep", 0.4),
        (r'open\s*\([^)]+\)(?!.*with)', "File open without context manager", 0.6),
        (r'except\s*:\s*$', "Bare except clause - catches SystemExit", 0.65),
        (r'except\s+Exception\s*:\s*$', "Broad exception catch", 0.5),
        (r'global\s+\w+', "Global variable usage - potential side effects", 0.4),
    ]

    def __init__(
        self,
        oracle: Optional[Any] = None,
        project_root: Optional[Path] = None,
    ):
        """
        Initialize technical debt detector.

        Args:
            oracle: The Oracle codebase knowledge graph
            project_root: Root directory of the project to scan
        """
        self._oracle = oracle
        self._project_root = project_root or Path(os.getenv(
            "Ironcliw_PATH",
            Path.home() / "Documents/repos/Ironcliw-AI-Agent"
        ))
        self._lock = asyncio.Lock()
        self._scan_results: List[TechnicalDebtItem] = []
        self._last_scan: Optional[float] = None
        self._scan_interval = int(os.getenv("DEBT_SCAN_INTERVAL", "3600"))  # 1 hour

        logger.info(f"TechnicalDebtDetector initialized for {self._project_root}")

    async def set_oracle(self, oracle: Any) -> None:
        """Set the Oracle instance."""
        self._oracle = oracle

    async def scan_codebase(
        self,
        target_dirs: Optional[List[str]] = None,
        categories: Optional[List[TechnicalDebtCategory]] = None,
        max_issues: int = 100,
        force_refresh: bool = False,
    ) -> List[TechnicalDebtItem]:
        """
        Scan the codebase for technical debt.

        Args:
            target_dirs: Specific directories to scan (default: entire project)
            categories: Specific categories to scan for (default: all)
            max_issues: Maximum number of issues to return
            force_refresh: Force rescan even if within scan interval

        Returns:
            List of detected technical debt items, sorted by severity
        """
        # Check if we should use cached results
        if not force_refresh and self._last_scan:
            if time.time() - self._last_scan < self._scan_interval:
                logger.debug("Using cached scan results")
                return self._filter_results(categories, max_issues)

        async with self._lock:
            self._scan_results = []

            # Determine directories to scan
            scan_dirs = []
            if target_dirs:
                scan_dirs = [self._project_root / d for d in target_dirs]
            else:
                scan_dirs = [self._project_root / "backend"]

            # Get all Python files
            python_files = []
            for scan_dir in scan_dirs:
                if scan_dir.exists():
                    python_files.extend(scan_dir.rglob("*.py"))

            # Filter out test files and cache
            python_files = [
                f for f in python_files
                if "__pycache__" not in str(f)
                and ".venv" not in str(f)
                and "node_modules" not in str(f)
            ]

            logger.info(f"Scanning {len(python_files)} Python files for technical debt...")

            # Scan in parallel
            tasks = []
            for file_path in python_files[:200]:  # Limit to avoid overwhelming
                tasks.append(self._scan_file(file_path, categories))

            results = await asyncio.gather(*tasks, return_exceptions=True)

            for result in results:
                if isinstance(result, list):
                    self._scan_results.extend(result)
                elif isinstance(result, Exception):
                    logger.debug(f"Scan error: {result}")

            # Add Oracle-based detections
            if self._oracle:
                await self._add_oracle_detections(categories)

            # Sort by severity
            self._scan_results.sort(key=lambda x: x.severity, reverse=True)
            self._last_scan = time.time()

            logger.info(f"Technical debt scan complete: {len(self._scan_results)} issues found")
            return self._filter_results(categories, max_issues)

    async def _scan_file(
        self,
        file_path: Path,
        categories: Optional[List[TechnicalDebtCategory]],
    ) -> List[TechnicalDebtItem]:
        """Scan a single file for technical debt."""
        items = []

        try:
            content = await asyncio.to_thread(file_path.read_text, encoding="utf-8")
            lines = content.splitlines()

            # Parse AST
            try:
                tree = ast.parse(content)
            except SyntaxError:
                return items

            # Check complexity
            if not categories or TechnicalDebtCategory.COMPLEXITY in categories:
                items.extend(await self._check_complexity(file_path, tree, lines))

            # Check security
            if not categories or TechnicalDebtCategory.SECURITY in categories:
                items.extend(await self._check_security(file_path, content, lines))

            # Check performance
            if not categories or TechnicalDebtCategory.PERFORMANCE in categories:
                items.extend(await self._check_performance(file_path, content, lines))

            # Check error handling
            if not categories or TechnicalDebtCategory.ERROR_HANDLING in categories:
                items.extend(await self._check_error_handling(file_path, tree, lines))

            # Check type safety
            if not categories or TechnicalDebtCategory.TYPE_SAFETY in categories:
                items.extend(await self._check_type_safety(file_path, tree, lines))

        except Exception as e:
            logger.debug(f"Error scanning {file_path}: {e}")

        return items

    async def _check_complexity(
        self,
        file_path: Path,
        tree: ast.AST,
        lines: List[str],
    ) -> List[TechnicalDebtItem]:
        """Check for complexity issues."""
        items = []

        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                # Calculate cyclomatic complexity
                cyclomatic = self._calculate_cyclomatic_complexity(node)

                # Calculate cognitive complexity
                cognitive = self._calculate_cognitive_complexity(node)

                # Check line count
                end_line = getattr(node, 'end_lineno', node.lineno + 10)
                line_count = end_line - node.lineno

                # Check nesting depth
                max_nesting = self._calculate_max_nesting(node)

                severity = 0.0
                issues = []

                if cyclomatic > self.CYCLOMATIC_THRESHOLD:
                    severity = max(severity, min(0.9, cyclomatic / 30))
                    issues.append(f"cyclomatic={cyclomatic}")

                if cognitive > self.COGNITIVE_THRESHOLD:
                    severity = max(severity, min(0.85, cognitive / 40))
                    issues.append(f"cognitive={cognitive}")

                if line_count > self.LINE_COUNT_THRESHOLD:
                    severity = max(severity, min(0.7, line_count / 500))
                    issues.append(f"lines={line_count}")

                if max_nesting > self.NESTING_THRESHOLD:
                    severity = max(severity, min(0.6, max_nesting / 8))
                    issues.append(f"nesting={max_nesting}")

                if severity > 0:
                    items.append(TechnicalDebtItem(
                        id=f"complexity_{uuid.uuid4().hex[:8]}",
                        category=TechnicalDebtCategory.COMPLEXITY,
                        file_path=str(file_path),
                        line_start=node.lineno,
                        line_end=end_line,
                        severity=severity,
                        description=f"High complexity in '{node.name}': {', '.join(issues)}",
                        code_snippet=lines[node.lineno - 1] if node.lineno <= len(lines) else "",
                        suggested_fix="Consider breaking this function into smaller, focused functions",
                        estimated_effort="medium" if severity < 0.7 else "hard",
                        metrics={
                            "cyclomatic": cyclomatic,
                            "cognitive": cognitive,
                            "line_count": line_count,
                            "max_nesting": max_nesting,
                        },
                    ))

        return items

    def _calculate_cyclomatic_complexity(self, node: ast.AST) -> int:
        """Calculate cyclomatic complexity of a function."""
        complexity = 1  # Base complexity

        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, (ast.And, ast.Or)):
                complexity += 1
            elif isinstance(child, ast.comprehension):
                complexity += 1
                if child.ifs:
                    complexity += len(child.ifs)
            elif isinstance(child, ast.Match):
                complexity += len(child.cases) - 1

        return complexity

    def _calculate_cognitive_complexity(self, node: ast.AST, depth: int = 0) -> int:
        """Calculate cognitive complexity of a function."""
        complexity = 0

        for child in ast.iter_child_nodes(node):
            increment = 0
            nesting_increment = 0

            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                increment = 1
                nesting_increment = depth
            elif isinstance(child, ast.ExceptHandler):
                increment = 1
                nesting_increment = depth
            elif isinstance(child, (ast.And, ast.Or)):
                increment = 1
            elif isinstance(child, ast.Lambda):
                increment = 1

            complexity += increment + nesting_increment

            # Recurse with increased depth for nesting structures
            new_depth = depth + 1 if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor, ast.Try)) else depth
            complexity += self._calculate_cognitive_complexity(child, new_depth)

        return complexity

    def _calculate_max_nesting(self, node: ast.AST, depth: int = 0) -> int:
        """Calculate maximum nesting depth."""
        max_depth = depth

        for child in ast.iter_child_nodes(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor, ast.Try, ast.With, ast.AsyncWith)):
                child_depth = self._calculate_max_nesting(child, depth + 1)
                max_depth = max(max_depth, child_depth)
            else:
                child_depth = self._calculate_max_nesting(child, depth)
                max_depth = max(max_depth, child_depth)

        return max_depth

    async def _check_security(
        self,
        file_path: Path,
        content: str,
        lines: List[str],
    ) -> List[TechnicalDebtItem]:
        """Check for security issues."""
        items = []

        for pattern, description, severity in self.SECURITY_PATTERNS:
            for match in re.finditer(pattern, content, re.MULTILINE):
                line_num = content[:match.start()].count('\n') + 1
                items.append(TechnicalDebtItem(
                    id=f"security_{uuid.uuid4().hex[:8]}",
                    category=TechnicalDebtCategory.SECURITY,
                    file_path=str(file_path),
                    line_start=line_num,
                    line_end=line_num,
                    severity=severity,
                    description=description,
                    code_snippet=lines[line_num - 1].strip() if line_num <= len(lines) else "",
                    suggested_fix="Review and apply security best practices",
                    estimated_effort="medium",
                ))

        return items

    async def _check_performance(
        self,
        file_path: Path,
        content: str,
        lines: List[str],
    ) -> List[TechnicalDebtItem]:
        """Check for performance issues."""
        items = []

        for pattern, description, severity in self.PERFORMANCE_PATTERNS:
            for match in re.finditer(pattern, content, re.MULTILINE):
                line_num = content[:match.start()].count('\n') + 1
                items.append(TechnicalDebtItem(
                    id=f"perf_{uuid.uuid4().hex[:8]}",
                    category=TechnicalDebtCategory.PERFORMANCE,
                    file_path=str(file_path),
                    line_start=line_num,
                    line_end=line_num,
                    severity=severity,
                    description=description,
                    code_snippet=lines[line_num - 1].strip() if line_num <= len(lines) else "",
                    suggested_fix="Apply performance optimization",
                    estimated_effort="easy",
                ))

        return items

    async def _check_error_handling(
        self,
        file_path: Path,
        tree: ast.AST,
        lines: List[str],
    ) -> List[TechnicalDebtItem]:
        """Check for error handling issues."""
        items = []

        for node in ast.walk(tree):
            if isinstance(node, ast.ExceptHandler):
                # Bare except
                if node.type is None:
                    items.append(TechnicalDebtItem(
                        id=f"error_{uuid.uuid4().hex[:8]}",
                        category=TechnicalDebtCategory.ERROR_HANDLING,
                        file_path=str(file_path),
                        line_start=node.lineno,
                        line_end=node.lineno,
                        severity=0.7,
                        description="Bare except clause catches all exceptions including SystemExit",
                        code_snippet=lines[node.lineno - 1].strip() if node.lineno <= len(lines) else "",
                        suggested_fix="Specify the exception type to catch",
                        estimated_effort="easy",
                    ))
                # Except Exception with pass
                elif isinstance(node.type, ast.Name) and node.type.id == "Exception":
                    if node.body and isinstance(node.body[0], ast.Pass):
                        items.append(TechnicalDebtItem(
                            id=f"error_{uuid.uuid4().hex[:8]}",
                            category=TechnicalDebtCategory.ERROR_HANDLING,
                            file_path=str(file_path),
                            line_start=node.lineno,
                            line_end=node.lineno,
                            severity=0.6,
                            description="Silently catching and ignoring all exceptions",
                            code_snippet=lines[node.lineno - 1].strip() if node.lineno <= len(lines) else "",
                            suggested_fix="Log the exception or handle it appropriately",
                            estimated_effort="easy",
                        ))

        return items

    async def _check_type_safety(
        self,
        file_path: Path,
        tree: ast.AST,
        lines: List[str],
    ) -> List[TechnicalDebtItem]:
        """Check for type safety issues (missing type hints)."""
        items = []

        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                # Skip private/magic methods and short functions
                if node.name.startswith('_') and not node.name.startswith('__'):
                    continue

                # Check for missing return type hint
                if node.returns is None and not node.name.startswith('__'):
                    # Check if function has return statements
                    has_return = any(
                        isinstance(n, ast.Return) and n.value is not None
                        for n in ast.walk(node)
                    )
                    if has_return:
                        items.append(TechnicalDebtItem(
                            id=f"type_{uuid.uuid4().hex[:8]}",
                            category=TechnicalDebtCategory.TYPE_SAFETY,
                            file_path=str(file_path),
                            line_start=node.lineno,
                            line_end=node.lineno,
                            severity=0.3,
                            description=f"Function '{node.name}' missing return type hint",
                            code_snippet=lines[node.lineno - 1].strip() if node.lineno <= len(lines) else "",
                            suggested_fix="Add return type annotation",
                            estimated_effort="trivial",
                        ))

                # Check for missing parameter type hints
                untyped_params = []
                for arg in node.args.args:
                    if arg.annotation is None and arg.arg != 'self' and arg.arg != 'cls':
                        untyped_params.append(arg.arg)

                if untyped_params and len(untyped_params) > 2:
                    items.append(TechnicalDebtItem(
                        id=f"type_{uuid.uuid4().hex[:8]}",
                        category=TechnicalDebtCategory.TYPE_SAFETY,
                        file_path=str(file_path),
                        line_start=node.lineno,
                        line_end=node.lineno,
                        severity=0.25,
                        description=f"Function '{node.name}' has {len(untyped_params)} untyped parameters: {', '.join(untyped_params[:3])}...",
                        code_snippet=lines[node.lineno - 1].strip() if node.lineno <= len(lines) else "",
                        suggested_fix="Add type annotations to parameters",
                        estimated_effort="easy",
                    ))

        return items

    async def _add_oracle_detections(
        self,
        categories: Optional[List[TechnicalDebtCategory]],
    ) -> None:
        """Add detections from Oracle analysis."""
        if not self._oracle:
            return

        try:
            # Dead code detection
            if not categories or TechnicalDebtCategory.DEAD_CODE in categories:
                if hasattr(self._oracle, 'get_dead_code'):
                    dead_code = await asyncio.to_thread(self._oracle.get_dead_code)
                    for node in list(dead_code)[:20]:
                        self._scan_results.append(TechnicalDebtItem(
                            id=f"dead_{uuid.uuid4().hex[:8]}",
                            category=TechnicalDebtCategory.DEAD_CODE,
                            file_path=str(node) if hasattr(node, '__str__') else "unknown",
                            line_start=getattr(node, 'line_number', 0),
                            line_end=getattr(node, 'line_number', 0),
                            severity=0.4,
                            description=f"Dead code detected: {node}",
                            suggested_fix="Remove unused code or add usage",
                            estimated_effort="easy",
                        ))

            # Circular dependency detection
            if not categories or TechnicalDebtCategory.CIRCULAR_DEPS in categories:
                if hasattr(self._oracle, 'get_circular_dependencies'):
                    cycles = await asyncio.to_thread(self._oracle.get_circular_dependencies)
                    for cycle in list(cycles)[:10]:
                        self._scan_results.append(TechnicalDebtItem(
                            id=f"circ_{uuid.uuid4().hex[:8]}",
                            category=TechnicalDebtCategory.CIRCULAR_DEPS,
                            file_path=str(cycle[0]) if cycle else "unknown",
                            line_start=0,
                            line_end=0,
                            severity=0.75,
                            description=f"Circular dependency: {' -> '.join(str(c) for c in cycle)}",
                            suggested_fix="Refactor to remove circular import",
                            estimated_effort="hard",
                        ))

        except Exception as e:
            logger.debug(f"Oracle detection error: {e}")

    def _filter_results(
        self,
        categories: Optional[List[TechnicalDebtCategory]],
        max_issues: int,
    ) -> List[TechnicalDebtItem]:
        """Filter and limit results."""
        results = self._scan_results

        if categories:
            results = [r for r in results if r.category in categories]

        return results[:max_issues]

    def get_status(self) -> Dict[str, Any]:
        """Get detector status."""
        return {
            "total_issues": len(self._scan_results),
            "last_scan": self._last_scan,
            "oracle_connected": self._oracle is not None,
            "issues_by_category": {
                cat.value: len([r for r in self._scan_results if r.category == cat])
                for cat in TechnicalDebtCategory
            },
        }


class AutonomousSelfRefinementLoop:
    """
    v4.0: The "Think-Loop" - Continuous autonomous self-improvement.

    This is the EXECUTIVE CONTROL that makes Ironcliw truly autonomous.
    It runs continuously in the background:
    1. Detects technical debt
    2. Decomposes issues into tasks
    3. Submits tasks to the orchestrator
    4. Learns from results

    The loop operates without human prompting - Ironcliw decides what
    to improve and when.

    Safety features:
    - Configurable improvement budget (max tasks per hour)
    - Risk thresholds (won't touch critical systems without approval)
    - Learning from failures (avoids repeating mistakes)
    - Circuit breaker for consecutive failures
    - Priority-based scheduling
    """

    def __init__(
        self,
        debt_detector: Optional[TechnicalDebtDetector] = None,
        goal_decomposer: Optional[GoalDecompositionEngine] = None,
        orchestrator: Optional[Any] = None,  # AgenticLoopOrchestrator
        learning_db: Optional[Any] = None,
    ):
        """
        Initialize the autonomous refinement loop.

        Args:
            debt_detector: Technical debt detector instance
            goal_decomposer: Goal decomposition engine
            orchestrator: AgenticLoopOrchestrator for task execution
            learning_db: Learning database for pattern storage
        """
        self._debt_detector = debt_detector
        self._goal_decomposer = goal_decomposer
        self._orchestrator = orchestrator
        self._learning_db = learning_db

        self._running = False
        self._paused = False
        self._task: Optional[asyncio.Task] = None
        self._lock = asyncio.Lock()

        # Configuration from environment
        self._scan_interval = int(os.getenv("AUTONOMOUS_SCAN_INTERVAL", "3600"))  # 1 hour
        self._max_tasks_per_hour = int(os.getenv("AUTONOMOUS_MAX_TASKS_HOUR", "5"))
        self._min_severity_threshold = float(os.getenv("AUTONOMOUS_MIN_SEVERITY", "0.5"))
        self._risk_threshold = float(os.getenv("AUTONOMOUS_RISK_THRESHOLD", "0.8"))
        self._consecutive_failure_limit = int(os.getenv("AUTONOMOUS_FAILURE_LIMIT", "3"))

        # State tracking
        self._tasks_this_hour: List[float] = []  # Timestamps of submitted tasks
        self._consecutive_failures = 0
        self._total_improvements = 0
        self._total_failures = 0
        self._avoided_issues: Set[str] = set()  # Issues we've learned to avoid

        # Metrics
        self._metrics = {
            "cycles_completed": 0,
            "issues_detected": 0,
            "tasks_submitted": 0,
            "improvements_successful": 0,
            "improvements_failed": 0,
            "issues_skipped_low_severity": 0,
            "issues_skipped_high_risk": 0,
            "issues_skipped_learned": 0,
        }

        logger.info("AutonomousSelfRefinementLoop initialized")

    async def start(self) -> None:
        """Start the autonomous refinement loop."""
        if self._running:
            logger.warning("AutonomousSelfRefinementLoop already running")
            return

        logger.info("Starting AutonomousSelfRefinementLoop...")
        self._running = True
        self._paused = False
        self._task = asyncio.create_task(self._refinement_loop())
        logger.info("AutonomousSelfRefinementLoop started")

    async def stop(self) -> None:
        """Stop the autonomous refinement loop."""
        if not self._running:
            return

        logger.info("Stopping AutonomousSelfRefinementLoop...")
        self._running = False

        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
            self._task = None

        logger.info("AutonomousSelfRefinementLoop stopped")

    async def pause(self) -> None:
        """Pause the refinement loop (keeps running but skips processing)."""
        self._paused = True
        logger.info("AutonomousSelfRefinementLoop paused")

    async def resume(self) -> None:
        """Resume the refinement loop."""
        self._paused = False
        logger.info("AutonomousSelfRefinementLoop resumed")

    async def trigger_scan_now(self) -> List[TechnicalDebtItem]:
        """Trigger an immediate scan and return results."""
        if not self._debt_detector:
            return []

        return await self._debt_detector.scan_codebase(force_refresh=True)

    async def _refinement_loop(self) -> None:
        """Main refinement loop that runs continuously."""
        logger.info("Refinement loop started")

        while self._running:
            try:
                # Check if paused
                if self._paused:
                    await asyncio.sleep(10)
                    continue

                # Check circuit breaker
                if self._consecutive_failures >= self._consecutive_failure_limit:
                    logger.warning(
                        f"Circuit breaker open: {self._consecutive_failures} consecutive failures. "
                        f"Waiting 5 minutes before retry."
                    )
                    await asyncio.sleep(300)  # 5 minute cooldown
                    self._consecutive_failures = 0
                    continue

                # Run one refinement cycle
                await self._run_refinement_cycle()

                self._metrics["cycles_completed"] += 1

                # Wait before next cycle
                await asyncio.sleep(self._scan_interval)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Refinement loop error: {e}")
                self._consecutive_failures += 1
                await asyncio.sleep(60)  # 1 minute cooldown on error

        logger.info("Refinement loop ended")

    async def _run_refinement_cycle(self) -> None:
        """Run a single refinement cycle."""
        logger.debug("Starting refinement cycle")

        # 1. Check task budget
        self._cleanup_old_tasks()
        remaining_budget = self._max_tasks_per_hour - len(self._tasks_this_hour)

        if remaining_budget <= 0:
            logger.debug("Task budget exhausted, skipping cycle")
            return

        # 2. Detect technical debt
        if not self._debt_detector:
            logger.warning("No debt detector configured")
            return

        issues = await self._debt_detector.scan_codebase()
        self._metrics["issues_detected"] += len(issues)

        if not issues:
            logger.debug("No technical debt detected")
            return

        # 3. Filter and prioritize issues
        actionable_issues = await self._filter_issues(issues)

        if not actionable_issues:
            logger.debug("No actionable issues after filtering")
            return

        logger.info(f"Found {len(actionable_issues)} actionable issues")

        # 4. Process top issues up to budget
        for issue in actionable_issues[:remaining_budget]:
            try:
                await self._process_issue(issue)
                self._consecutive_failures = 0  # Reset on success
            except Exception as e:
                logger.error(f"Failed to process issue {issue.id}: {e}")
                self._consecutive_failures += 1
                self._metrics["improvements_failed"] += 1

                # Learn from failure
                await self._record_failure(issue, str(e))

    async def _filter_issues(
        self,
        issues: List[TechnicalDebtItem],
    ) -> List[TechnicalDebtItem]:
        """Filter issues based on severity, risk, and learned patterns."""
        filtered = []

        for issue in issues:
            # Skip low severity
            if issue.severity < self._min_severity_threshold:
                self._metrics["issues_skipped_low_severity"] += 1
                continue

            # Skip high risk (security critical files, etc.)
            if await self._assess_risk(issue) > self._risk_threshold:
                self._metrics["issues_skipped_high_risk"] += 1
                continue

            # Skip issues we've learned to avoid
            issue_signature = self._get_issue_signature(issue)
            if issue_signature in self._avoided_issues:
                self._metrics["issues_skipped_learned"] += 1
                continue

            filtered.append(issue)

        # Sort by severity (highest first)
        filtered.sort(key=lambda x: x.severity, reverse=True)

        return filtered

    async def _assess_risk(self, issue: TechnicalDebtItem) -> float:
        """Assess the risk of fixing an issue."""
        risk = 0.0

        # Higher risk for certain files
        high_risk_patterns = [
            "security", "auth", "crypto", "payment", "billing",
            "database", "migration", "config", "secret",
        ]

        file_lower = issue.file_path.lower()
        for pattern in high_risk_patterns:
            if pattern in file_lower:
                risk += 0.2

        # Higher risk for high-severity security issues
        if issue.category == TechnicalDebtCategory.SECURITY:
            risk += 0.3

        # Use Oracle for blast radius if available
        if self._debt_detector and self._debt_detector._oracle:
            try:
                oracle = self._debt_detector._oracle
                if hasattr(oracle, 'get_blast_radius'):
                    blast = await asyncio.to_thread(
                        lambda: oracle.get_blast_radius(issue.file_path)
                    )
                    if blast and hasattr(blast, 'directly_affected'):
                        affected_count = len(blast.directly_affected)
                        risk += min(0.3, affected_count * 0.03)
            except Exception:
                pass

        return min(1.0, risk)

    def _get_issue_signature(self, issue: TechnicalDebtItem) -> str:
        """Get a unique signature for an issue (for learning)."""
        return f"{issue.category.value}:{issue.file_path}:{issue.line_start}"

    async def _process_issue(self, issue: TechnicalDebtItem) -> None:
        """Process a single issue by decomposing and submitting tasks."""
        logger.info(f"Processing issue: {issue.description[:50]}...")

        # Build improvement goal from issue
        goal = f"Fix {issue.category.value}: {issue.description}"

        # Decompose into sub-tasks
        if self._goal_decomposer:
            result = await self._goal_decomposer.decompose(
                goal=goal,
                context={
                    "issue_id": issue.id,
                    "category": issue.category.value,
                    "severity": issue.severity,
                    "file": issue.file_path,
                },
                target_files=[issue.file_path],
            )
            sub_tasks = result.sub_tasks
        else:
            # Fallback: single task
            sub_tasks = [SubTask(
                id=f"fix_{issue.id}",
                description=goal,
                task_type=SubTaskType.FIX,
                priority=SubTaskPriority.HIGH,
                target_files=[issue.file_path],
                parent_goal=goal,
            )]

        # Submit tasks to orchestrator
        if self._orchestrator:
            for sub_task in sub_tasks:
                await self._submit_task_to_orchestrator(sub_task, issue)
        else:
            logger.warning("No orchestrator configured, tasks not submitted")

        self._metrics["tasks_submitted"] += len(sub_tasks)
        self._tasks_this_hour.append(time.time())

    async def _submit_task_to_orchestrator(
        self,
        sub_task: SubTask,
        issue: TechnicalDebtItem,
    ) -> Optional[str]:
        """Submit a sub-task to the AgenticLoopOrchestrator."""
        if not self._orchestrator:
            return None

        try:
            # Map priority
            from enum import Enum

            # Import or create priority enum
            priority_value = 2  # Default to HIGH
            if sub_task.priority == SubTaskPriority.CRITICAL:
                priority_value = 1
            elif sub_task.priority == SubTaskPriority.MEDIUM:
                priority_value = 3
            elif sub_task.priority == SubTaskPriority.LOW:
                priority_value = 4

            # Find target file
            target_file = Path(issue.file_path)
            if not target_file.exists() and sub_task.target_files:
                target_file = Path(sub_task.target_files[0])

            if not target_file.exists():
                logger.warning(f"Target file not found: {target_file}")
                return None

            # Submit task
            if hasattr(self._orchestrator, 'submit_task'):
                task_id = await self._orchestrator.submit_task(
                    file_path=target_file,
                    goal=sub_task.description,
                    triggered_by="autonomous_refinement",
                )

                logger.info(f"Submitted task {task_id}: {sub_task.description[:50]}...")
                self._metrics["improvements_successful"] += 1
                return task_id

        except Exception as e:
            logger.error(f"Failed to submit task: {e}")
            return None

    async def _record_failure(self, issue: TechnicalDebtItem, error: str) -> None:
        """Record a failure for learning."""
        # Add to avoided issues if repeated failures
        issue_signature = self._get_issue_signature(issue)

        # Track in learning DB
        if self._learning_db and hasattr(self._learning_db, 'record_experience'):
            try:
                await self._learning_db.record_experience(
                    category="autonomous_refinement_failure",
                    data={
                        "issue_id": issue.id,
                        "category": issue.category.value,
                        "file": issue.file_path,
                        "error": error,
                        "signature": issue_signature,
                    }
                )
            except Exception:
                pass

        # After 2 failures on same issue, avoid it
        self._total_failures += 1
        if self._total_failures % 2 == 0:
            self._avoided_issues.add(issue_signature)

    def _cleanup_old_tasks(self) -> None:
        """Remove task timestamps older than 1 hour."""
        one_hour_ago = time.time() - 3600
        self._tasks_this_hour = [t for t in self._tasks_this_hour if t > one_hour_ago]

    def get_status(self) -> Dict[str, Any]:
        """Get loop status."""
        return {
            "running": self._running,
            "paused": self._paused,
            "consecutive_failures": self._consecutive_failures,
            "tasks_this_hour": len(self._tasks_this_hour),
            "max_tasks_per_hour": self._max_tasks_per_hour,
            "total_improvements": self._total_improvements,
            "avoided_issues_count": len(self._avoided_issues),
            "metrics": self._metrics.copy(),
        }


class DualAgentSystem:
    """
    v4.0: Architect/Reviewer dual-agent pattern for code improvements.

    This implements a two-phase approach:
    1. Architect agent generates the improvement
    2. Reviewer agent critiques and validates

    If the reviewer finds issues, the architect refines based on feedback.
    This catches errors before they're applied.

    Benefits:
    - Catches logical errors before application
    - Improves code quality through review
    - Reduces rollbacks from failed improvements
    - Enables different models for different roles
    """

    # Prompts for each role
    ARCHITECT_SYSTEM = """You are an expert software architect. Your job is to improve code based on goals while:
- Maintaining existing functionality
- Following best practices
- Writing clean, readable code
- Ensuring type safety
- Handling errors appropriately

Generate improved code that achieves the goal. Return the full improved code."""

    REVIEWER_SYSTEM = """You are a senior code reviewer. Analyze the proposed code changes and identify:
1. Logical errors or bugs
2. Security vulnerabilities
3. Performance issues
4. Code style violations
5. Missing error handling
6. Breaking changes

Be constructive and specific. If the code is good, say "APPROVED".
If issues found, list them clearly with specific line references.

Format:
VERDICT: APPROVED or NEEDS_REVISION
ISSUES: (if any)
- Issue 1 (line X): description
- Issue 2 (line Y): description
SUGGESTIONS: (optional improvements)"""

    def __init__(
        self,
        architect_client: Optional[Any] = None,
        reviewer_client: Optional[Any] = None,
        max_refinement_rounds: int = 3,
    ):
        """
        Initialize dual agent system.

        Args:
            architect_client: LLM client for architect agent
            reviewer_client: LLM client for reviewer agent (can be same)
            max_refinement_rounds: Maximum rounds of refinement
        """
        self._architect = architect_client
        self._reviewer = reviewer_client
        self._max_rounds = max_refinement_rounds
        self._lock = asyncio.Lock()

        # Metrics
        self._metrics = {
            "total_reviews": 0,
            "approved_first_try": 0,
            "approved_after_refinement": 0,
            "rejected": 0,
            "total_refinement_rounds": 0,
        }

        logger.info("DualAgentSystem initialized")

    async def improve_code(
        self,
        code: str,
        goal: str,
        context: Optional[Dict[str, Any]] = None,
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Improve code using architect/reviewer pattern.

        Args:
            code: Original code to improve
            goal: Improvement goal
            context: Additional context

        Returns:
            Tuple of (improved_code, metadata)
        """
        async with self._lock:
            self._metrics["total_reviews"] += 1

            current_code = code
            refinement_round = 0
            review_history = []

            while refinement_round < self._max_rounds:
                # 1. Architect generates improvement
                improved = await self._architect_generate(current_code, goal, context, review_history)

                if not improved or improved == current_code:
                    logger.warning("Architect produced no changes")
                    break

                # 2. Reviewer evaluates
                review = await self._reviewer_review(code, improved, goal)
                review_history.append(review)

                # 3. Check verdict
                if review.get("approved", False):
                    if refinement_round == 0:
                        self._metrics["approved_first_try"] += 1
                    else:
                        self._metrics["approved_after_refinement"] += 1

                    return improved, {
                        "approved": True,
                        "rounds": refinement_round + 1,
                        "review_history": review_history,
                    }

                # 4. Refinement needed
                refinement_round += 1
                self._metrics["total_refinement_rounds"] += 1
                current_code = improved

                logger.debug(f"Refinement round {refinement_round}: {review.get('issues', [])}")

            # Max rounds reached or no improvement
            self._metrics["rejected"] += 1
            return code, {  # Return original code
                "approved": False,
                "rounds": refinement_round,
                "review_history": review_history,
                "reason": "Max refinement rounds reached or no improvement",
            }

    async def _architect_generate(
        self,
        code: str,
        goal: str,
        context: Optional[Dict[str, Any]],
        review_history: List[Dict[str, Any]],
    ) -> str:
        """Generate improved code using architect agent."""
        if not self._architect:
            return code

        # Build prompt
        prompt_parts = [
            f"# Goal\n{goal}\n",
            f"# Original Code\n```python\n{code}\n```\n",
        ]

        if context:
            prompt_parts.append(f"# Context\n{json.dumps(context, indent=2)}\n")

        if review_history:
            prompt_parts.append("# Previous Review Feedback")
            for i, review in enumerate(review_history):
                if review.get("issues"):
                    prompt_parts.append(f"Round {i+1} issues: {review['issues']}")
            prompt_parts.append("")

        prompt_parts.append(
            "Generate improved code that achieves the goal and addresses any previous feedback. "
            "Return ONLY the improved code, no explanation."
        )

        prompt = "\n".join(prompt_parts)

        try:
            response = await self._call_llm(self._architect, self.ARCHITECT_SYSTEM, prompt)

            # Extract code from response
            code_match = re.search(r'```python\n(.*?)```', response, re.DOTALL)
            if code_match:
                return code_match.group(1).strip()
            return response.strip()

        except Exception as e:
            logger.error(f"Architect generation failed: {e}")
            return code

    async def _reviewer_review(
        self,
        original: str,
        improved: str,
        goal: str,
    ) -> Dict[str, Any]:
        """Review improved code using reviewer agent."""
        if not self._reviewer:
            return {"approved": True, "issues": []}

        prompt = f"""# Goal
{goal}

# Original Code
```python
{original}
```

# Proposed Improvement
```python
{improved}
```

Review the proposed changes. Is this improvement correct and safe to apply?"""

        try:
            response = await self._call_llm(self._reviewer, self.REVIEWER_SYSTEM, prompt)

            # Parse response
            approved = "APPROVED" in response.upper() and "NEEDS_REVISION" not in response.upper()

            # Extract issues
            issues = []
            if "ISSUES:" in response.upper():
                issues_section = response.split("ISSUES:")[1].split("SUGGESTIONS:")[0] if "SUGGESTIONS:" in response else response.split("ISSUES:")[1]
                issues = [line.strip() for line in issues_section.strip().split("\n") if line.strip().startswith("-")]

            return {
                "approved": approved,
                "issues": issues,
                "raw_response": response,
            }

        except Exception as e:
            logger.error(f"Reviewer evaluation failed: {e}")
            return {"approved": True, "issues": [], "error": str(e)}

    async def _call_llm(self, client: Any, system: str, prompt: str) -> str:
        """Call LLM with various client interfaces."""
        try:
            if hasattr(client, 'complete'):
                response = await client.complete(
                    system=system,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=4000,
                    temperature=0.3,
                )
                return response[0] if isinstance(response, tuple) else response
            elif hasattr(client, 'generate'):
                return await client.generate(
                    prompt=f"{system}\n\n{prompt}",
                    max_tokens=4000,
                )
            elif hasattr(client, 'chat'):
                response = await client.chat(
                    messages=[
                        {"role": "system", "content": system},
                        {"role": "user", "content": prompt},
                    ],
                    max_tokens=4000,
                )
                return response.get("content", "") if isinstance(response, dict) else str(response)
        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            return ""

        return ""

    def get_status(self) -> Dict[str, Any]:
        """Get system status."""
        return {
            "architect_connected": self._architect is not None,
            "reviewer_connected": self._reviewer is not None,
            "max_rounds": self._max_rounds,
            "metrics": self._metrics.copy(),
        }


class CodeMemoryRAG:
    """
    v4.0: Unified Oracle + ChromaDB code memory with GraphRAG.

    This fuses two memory systems:
    - Oracle (graph): Structural relationships (imports, calls, inheritance)
    - ChromaDB (semantic): Conceptual similarity and semantic search

    The combination enables:
    - "Find code similar to authentication" (semantic)
    - "Find code that calls the auth module" (graph)
    - "Find similar code that's structurally related" (hybrid)

    This is critical for intelligent code navigation and improvement.
    """

    def __init__(
        self,
        oracle: Optional[Any] = None,
        chromadb_client: Optional[Any] = None,
        embedding_model: Optional[Any] = None,
    ):
        """
        Initialize unified code memory.

        Args:
            oracle: The Oracle codebase knowledge graph
            chromadb_client: ChromaDB client for semantic memory
            embedding_model: Model for generating embeddings
        """
        self._oracle = oracle
        self._chromadb = chromadb_client
        self._embedding_model = embedding_model
        self._lock = asyncio.Lock()
        self._collection_name = os.getenv("CODE_MEMORY_COLLECTION", "jarvis_code_memory")

        logger.info("CodeMemoryRAG initialized")

    async def remember_code(
        self,
        file_path: str,
        code: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        Index code in both Oracle (structure) and ChromaDB (semantics).

        Args:
            file_path: Path to the file
            code: Code content
            metadata: Additional metadata
        """
        async with self._lock:
            # Index in Oracle (structural)
            if self._oracle:
                try:
                    if hasattr(self._oracle, 'index_file'):
                        await asyncio.to_thread(
                            lambda: self._oracle.index_file(Path(file_path))
                        )
                except Exception as e:
                    logger.debug(f"Oracle indexing error: {e}")

            # Index in ChromaDB (semantic)
            if self._chromadb:
                try:
                    await self._index_in_chromadb(file_path, code, metadata)
                except Exception as e:
                    logger.debug(f"ChromaDB indexing error: {e}")

    async def _index_in_chromadb(
        self,
        file_path: str,
        code: str,
        metadata: Optional[Dict[str, Any]],
    ) -> None:
        """Index code in ChromaDB."""
        # Get or create collection
        if hasattr(self._chromadb, 'get_or_create_collection'):
            collection = self._chromadb.get_or_create_collection(
                name=self._collection_name,
            )
        else:
            return

        # Split code into chunks
        chunks = self._chunk_code(code)

        # Generate embeddings
        embeddings = []
        if self._embedding_model:
            for chunk in chunks:
                try:
                    if hasattr(self._embedding_model, 'encode'):
                        emb = self._embedding_model.encode(chunk)
                        embeddings.append(emb.tolist() if hasattr(emb, 'tolist') else emb)
                except Exception:
                    pass

        # Store in ChromaDB
        for i, chunk in enumerate(chunks):
            doc_id = f"{hashlib.sha256(file_path.encode()).hexdigest()[:12]}_{i}"
            chunk_metadata = {
                "file_path": file_path,
                "chunk_index": i,
                "total_chunks": len(chunks),
                **(metadata or {}),
            }

            try:
                if embeddings and i < len(embeddings):
                    collection.upsert(
                        ids=[doc_id],
                        documents=[chunk],
                        metadatas=[chunk_metadata],
                        embeddings=[embeddings[i]],
                    )
                else:
                    collection.upsert(
                        ids=[doc_id],
                        documents=[chunk],
                        metadatas=[chunk_metadata],
                    )
            except Exception as e:
                logger.debug(f"ChromaDB upsert error: {e}")

    def _chunk_code(self, code: str, chunk_size: int = 500) -> List[str]:
        """Split code into semantic chunks (at function/class boundaries)."""
        chunks = []

        try:
            tree = ast.parse(code)
            lines = code.splitlines()

            for node in ast.iter_child_nodes(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                    start = node.lineno - 1
                    end = getattr(node, 'end_lineno', start + 10)
                    chunk = "\n".join(lines[start:end])
                    if chunk.strip():
                        chunks.append(chunk)

            # Add module-level code
            if not chunks:
                # No functions/classes, chunk by lines
                for i in range(0, len(lines), chunk_size // 50):
                    chunk = "\n".join(lines[i:i + chunk_size // 50])
                    if chunk.strip():
                        chunks.append(chunk)

        except SyntaxError:
            # Fallback to simple line-based chunking
            lines = code.splitlines()
            for i in range(0, len(lines), chunk_size // 50):
                chunk = "\n".join(lines[i:i + chunk_size // 50])
                if chunk.strip():
                    chunks.append(chunk)

        return chunks or [code]

    async def find_related_code(
        self,
        query: str,
        limit: int = 10,
        use_graph: bool = True,
        use_semantic: bool = True,
    ) -> CodeMemoryResult:
        """
        Find code related to a query using hybrid search.

        Args:
            query: Search query (natural language or code)
            limit: Maximum results to return
            use_graph: Include Oracle graph search
            use_semantic: Include ChromaDB semantic search

        Returns:
            CodeMemoryResult with merged results
        """
        graph_results = []
        semantic_results = []

        # Graph search (Oracle)
        if use_graph and self._oracle:
            graph_results = await self._search_oracle(query, limit)

        # Semantic search (ChromaDB)
        if use_semantic and self._chromadb:
            semantic_results = await self._search_chromadb(query, limit)

        # Merge results
        merged, scores = self._merge_results(graph_results, semantic_results)

        return CodeMemoryResult(
            query=query,
            graph_results=graph_results,
            semantic_results=semantic_results,
            merged_results=merged[:limit],
            relevance_scores=scores,
            total_results=len(merged),
        )

    async def _search_oracle(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Search Oracle graph database."""
        results = []

        try:
            # Extract keywords
            keywords = re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\b', query)

            for keyword in keywords[:5]:
                if hasattr(self._oracle, 'find_nodes_by_name'):
                    nodes = await asyncio.to_thread(
                        lambda k=keyword: self._oracle.find_nodes_by_name(k, fuzzy=True)
                    )
                    for node in nodes[:limit // 2]:
                        results.append({
                            "source": "oracle",
                            "type": "graph",
                            "node": str(node),
                            "keyword": keyword,
                        })
        except Exception as e:
            logger.debug(f"Oracle search error: {e}")

        return results

    async def _search_chromadb(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Search ChromaDB semantic memory."""
        results = []

        try:
            if hasattr(self._chromadb, 'get_or_create_collection'):
                collection = self._chromadb.get_or_create_collection(
                    name=self._collection_name
                )

                # Generate query embedding
                query_embedding = None
                if self._embedding_model and hasattr(self._embedding_model, 'encode'):
                    emb = self._embedding_model.encode(query)
                    query_embedding = emb.tolist() if hasattr(emb, 'tolist') else emb

                # Search
                if query_embedding:
                    search_results = collection.query(
                        query_embeddings=[query_embedding],
                        n_results=limit,
                    )
                else:
                    search_results = collection.query(
                        query_texts=[query],
                        n_results=limit,
                    )

                # Format results
                if search_results and search_results.get("documents"):
                    for i, doc in enumerate(search_results["documents"][0]):
                        metadata = search_results.get("metadatas", [[]])[0]
                        meta = metadata[i] if i < len(metadata) else {}
                        results.append({
                            "source": "chromadb",
                            "type": "semantic",
                            "content": doc[:200] + "..." if len(doc) > 200 else doc,
                            "file_path": meta.get("file_path", "unknown"),
                            "score": search_results.get("distances", [[]])[0][i]
                            if search_results.get("distances") else 0,
                        })

        except Exception as e:
            logger.debug(f"ChromaDB search error: {e}")

        return results

    def _merge_results(
        self,
        graph_results: List[Dict[str, Any]],
        semantic_results: List[Dict[str, Any]],
    ) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
        """Merge and rank results from both sources."""
        merged = []
        scores = {}

        # Score and deduplicate
        seen_files = set()

        # Add semantic results (higher priority for similarity)
        for result in semantic_results:
            file_path = result.get("file_path", "")
            if file_path and file_path not in seen_files:
                seen_files.add(file_path)
                score = 1.0 - result.get("score", 0.5)  # ChromaDB returns distance
                scores[file_path] = score
                result["combined_score"] = score
                merged.append(result)

        # Add graph results
        for result in graph_results:
            node = result.get("node", "")
            if node not in seen_files:
                seen_files.add(node)
                score = 0.7  # Base score for graph matches
                scores[node] = score
                result["combined_score"] = score
                merged.append(result)

        # Sort by score
        merged.sort(key=lambda x: x.get("combined_score", 0), reverse=True)

        return merged, scores

    def get_status(self) -> Dict[str, Any]:
        """Get memory status."""
        return {
            "oracle_connected": self._oracle is not None,
            "chromadb_connected": self._chromadb is not None,
            "embedding_model_connected": self._embedding_model is not None,
            "collection_name": self._collection_name,
        }


class SystemFeedbackLoop:
    """
    v4.0: Hardware metrics → code improvements feedback loop.

    Monitors system performance and triggers code improvements
    when resource usage exceeds thresholds.

    Example:
    - Memory > 85% → Trigger memory optimization task
    - CPU > 90% → Trigger performance optimization
    - Slow API responses → Profile and optimize
    """

    def __init__(
        self,
        orchestrator: Optional[Any] = None,
        goal_decomposer: Optional[GoalDecompositionEngine] = None,
    ):
        """
        Initialize system feedback loop.

        Args:
            orchestrator: AgenticLoopOrchestrator for task submission
            goal_decomposer: Goal decomposition engine
        """
        self._orchestrator = orchestrator
        self._goal_decomposer = goal_decomposer
        self._running = False
        self._task: Optional[asyncio.Task] = None
        self._lock = asyncio.Lock()

        # Thresholds (from environment)
        self._memory_threshold = float(os.getenv("FEEDBACK_MEMORY_THRESHOLD", "0.85"))
        self._cpu_threshold = float(os.getenv("FEEDBACK_CPU_THRESHOLD", "0.90"))
        self._disk_threshold = float(os.getenv("FEEDBACK_DISK_THRESHOLD", "0.90"))
        self._check_interval = int(os.getenv("FEEDBACK_CHECK_INTERVAL", "60"))

        # Cooldown to prevent spam
        self._last_memory_alert: Optional[float] = None
        self._last_cpu_alert: Optional[float] = None
        self._alert_cooldown = 3600  # 1 hour

        # Metrics
        self._metrics = {
            "checks_performed": 0,
            "memory_alerts": 0,
            "cpu_alerts": 0,
            "tasks_triggered": 0,
        }

        logger.info("SystemFeedbackLoop initialized")

    async def start(self) -> None:
        """Start the feedback monitoring loop."""
        if self._running:
            return

        self._running = True
        self._task = asyncio.create_task(self._monitor_loop())
        logger.info("SystemFeedbackLoop started")

    async def stop(self) -> None:
        """Stop the feedback monitoring loop."""
        if not self._running:
            return

        self._running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass

        logger.info("SystemFeedbackLoop stopped")

    async def _monitor_loop(self) -> None:
        """Main monitoring loop."""
        while self._running:
            try:
                metrics = await self._get_system_metrics()
                self._metrics["checks_performed"] += 1

                # Check memory
                if metrics.get("memory_percent", 0) > self._memory_threshold * 100:
                    await self._handle_high_memory(metrics)

                # Check CPU
                if metrics.get("cpu_percent", 0) > self._cpu_threshold * 100:
                    await self._handle_high_cpu(metrics)

                await asyncio.sleep(self._check_interval)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Monitor loop error: {e}")
                await asyncio.sleep(60)

    async def _get_system_metrics(self) -> Dict[str, Any]:
        """Get current system metrics."""
        metrics = {}

        try:
            import psutil

            # Memory
            memory = psutil.virtual_memory()
            metrics["memory_percent"] = memory.percent
            metrics["memory_available_gb"] = memory.available / (1024**3)

            # CPU
            metrics["cpu_percent"] = psutil.cpu_percent(interval=1)

            # Disk
            disk = psutil.disk_usage('/')
            metrics["disk_percent"] = disk.percent

        except ImportError:
            # Fallback without psutil
            metrics["memory_percent"] = 0
            metrics["cpu_percent"] = 0
            metrics["disk_percent"] = 0
        except Exception as e:
            logger.debug(f"Metrics collection error: {e}")

        return metrics

    async def _handle_high_memory(self, metrics: Dict[str, Any]) -> None:
        """Handle high memory usage."""
        now = time.time()

        # Check cooldown
        if self._last_memory_alert and now - self._last_memory_alert < self._alert_cooldown:
            return

        self._last_memory_alert = now
        self._metrics["memory_alerts"] += 1

        logger.warning(f"High memory usage: {metrics.get('memory_percent', 0):.1f}%")

        # Submit optimization task
        if self._orchestrator:
            goal = f"Optimize memory usage - currently at {metrics.get('memory_percent', 0):.1f}%"

            if self._goal_decomposer:
                result = await self._goal_decomposer.decompose(goal)
                for task in result.sub_tasks:
                    if task.target_files:
                        await self._submit_task(task.target_files[0], task.description)
            else:
                await self._submit_task(
                    "backend/core/ouroboros/integration.py",
                    goal,
                )

    async def _handle_high_cpu(self, metrics: Dict[str, Any]) -> None:
        """Handle high CPU usage."""
        now = time.time()

        # Check cooldown
        if self._last_cpu_alert and now - self._last_cpu_alert < self._alert_cooldown:
            return

        self._last_cpu_alert = now
        self._metrics["cpu_alerts"] += 1

        logger.warning(f"High CPU usage: {metrics.get('cpu_percent', 0):.1f}%")

        # Submit optimization task
        if self._orchestrator:
            goal = f"Optimize CPU usage - currently at {metrics.get('cpu_percent', 0):.1f}%"
            await self._submit_task(
                "backend/core/ouroboros/integration.py",
                goal,
            )

    async def _submit_task(self, file_path: str, goal: str) -> None:
        """Submit optimization task to orchestrator."""
        if not self._orchestrator or not hasattr(self._orchestrator, 'submit_task'):
            return

        try:
            await self._orchestrator.submit_task(
                file_path=Path(file_path),
                goal=goal,
                triggered_by="system_feedback",
            )
            self._metrics["tasks_triggered"] += 1
        except Exception as e:
            logger.error(f"Failed to submit task: {e}")

    def get_status(self) -> Dict[str, Any]:
        """Get loop status."""
        return {
            "running": self._running,
            "orchestrator_connected": self._orchestrator is not None,
            "thresholds": {
                "memory": self._memory_threshold,
                "cpu": self._cpu_threshold,
                "disk": self._disk_threshold,
            },
            "metrics": self._metrics.copy(),
        }


class AutoTestGenerator:
    """
    v4.0: Automatic test generation for existing untested code.

    Scans the codebase for functions/classes without tests and
    generates pytest tests for them.

    Features:
    - Identifies untested code through coverage analysis
    - Generates meaningful test cases using LLM
    - Validates tests actually pass
    - Integrates with existing test suites
    """

    def __init__(
        self,
        project_root: Optional[Path] = None,
        llm_client: Optional[Any] = None,
    ):
        """
        Initialize auto test generator.

        Args:
            project_root: Root directory of the project
            llm_client: LLM client for test generation
        """
        self._project_root = project_root or Path(os.getenv(
            "Ironcliw_PATH",
            Path.home() / "Documents/repos/Ironcliw-AI-Agent"
        ))
        self._llm_client = llm_client
        self._lock = asyncio.Lock()

        logger.info("AutoTestGenerator initialized")

    async def find_untested_code(
        self,
        target_dir: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        Find functions and classes without test coverage.

        Args:
            target_dir: Directory to scan (default: backend/)

        Returns:
            List of untested items with metadata
        """
        scan_dir = self._project_root / (target_dir or "backend")
        test_dir = self._project_root / "tests"

        untested = []

        # Get all Python files
        source_files = list(scan_dir.rglob("*.py"))
        source_files = [f for f in source_files if "__pycache__" not in str(f)]

        # Get all test files
        test_files = list(test_dir.rglob("test_*.py")) if test_dir.exists() else []

        # Build set of tested items
        tested_items = set()
        for test_file in test_files:
            try:
                content = await asyncio.to_thread(test_file.read_text)
                # Extract tested function/class names
                matches = re.findall(r'def test_(\w+)', content)
                matches += re.findall(r'class Test(\w+)', content)
                tested_items.update(m.lower() for m in matches)
            except Exception:
                pass

        # Find untested items
        for source_file in source_files[:50]:  # Limit to avoid overwhelming
            try:
                content = await asyncio.to_thread(source_file.read_text)
                tree = ast.parse(content)

                for node in ast.walk(tree):
                    if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                        # Skip private and magic methods
                        if node.name.startswith('_'):
                            continue

                        # Check if tested
                        if node.name.lower() not in tested_items:
                            untested.append({
                                "type": "function",
                                "name": node.name,
                                "file": str(source_file),
                                "line": node.lineno,
                                "signature": self._get_signature(node),
                                "docstring": ast.get_docstring(node),
                            })

                    elif isinstance(node, ast.ClassDef):
                        if node.name.startswith('_'):
                            continue

                        if node.name.lower() not in tested_items:
                            untested.append({
                                "type": "class",
                                "name": node.name,
                                "file": str(source_file),
                                "line": node.lineno,
                                "docstring": ast.get_docstring(node),
                            })
            except Exception:
                pass

        return untested

    def _get_signature(self, node: ast.FunctionDef) -> str:
        """Extract function signature."""
        args = []
        for arg in node.args.args:
            arg_str = arg.arg
            if arg.annotation:
                try:
                    arg_str += f": {ast.unparse(arg.annotation)}"
                except Exception:
                    pass
            args.append(arg_str)

        return_hint = ""
        if node.returns:
            try:
                return_hint = f" -> {ast.unparse(node.returns)}"
            except Exception:
                pass

        return f"def {node.name}({', '.join(args)}){return_hint}"

    async def generate_tests(
        self,
        item: Dict[str, Any],
        dry_run: bool = True,
    ) -> str:
        """
        Generate tests for an untested item.

        Args:
            item: Untested item metadata
            dry_run: If True, only return tests without saving

        Returns:
            Generated test code
        """
        if not self._llm_client:
            return self._generate_basic_test(item)

        # Read source code
        try:
            source_file = Path(item["file"])
            content = await asyncio.to_thread(source_file.read_text)
        except Exception:
            content = ""

        # Build prompt
        prompt = f"""Generate pytest tests for the following {item['type']}:

Name: {item['name']}
Signature: {item.get('signature', 'N/A')}
Docstring: {item.get('docstring', 'No documentation')}

Source file: {item['file']}

Source code context:
```python
{content[:3000]}
```

Generate comprehensive pytest tests including:
1. Happy path test cases
2. Edge cases
3. Error handling tests (if applicable)

Use pytest fixtures where appropriate.
Include docstrings for each test.
Return only the test code, no explanation."""

        try:
            system = "You are a test engineering expert. Generate comprehensive, clean pytest tests."

            if hasattr(self._llm_client, 'complete'):
                response = await self._llm_client.complete(
                    system=system,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=2000,
                )
                test_code = response[0] if isinstance(response, tuple) else response
            else:
                test_code = self._generate_basic_test(item)

            # Extract code from response
            code_match = re.search(r'```python\n(.*?)```', test_code, re.DOTALL)
            if code_match:
                test_code = code_match.group(1)

            # Save if not dry run
            if not dry_run:
                await self._save_tests(item, test_code)

            return test_code

        except Exception as e:
            logger.error(f"Test generation failed: {e}")
            return self._generate_basic_test(item)

    def _generate_basic_test(self, item: Dict[str, Any]) -> str:
        """Generate basic test template without LLM."""
        name = item["name"]
        item_type = item["type"]

        if item_type == "function":
            return f'''import pytest
from {self._get_import_path(item["file"])} import {name}


def test_{name}_basic():
    """Basic test for {name}."""
    # TODO: Implement test
    pass


def test_{name}_edge_case():
    """Edge case test for {name}."""
    # TODO: Implement test
    pass
'''
        else:  # class
            return f'''import pytest
from {self._get_import_path(item["file"])} import {name}


class Test{name}:
    """Tests for {name} class."""

    def test_init(self):
        """Test {name} initialization."""
        # TODO: Implement test
        pass

    def test_basic_usage(self):
        """Test basic {name} usage."""
        # TODO: Implement test
        pass
'''

    def _get_import_path(self, file_path: str) -> str:
        """Convert file path to import path."""
        rel_path = Path(file_path).relative_to(self._project_root)
        import_path = str(rel_path).replace("/", ".").replace("\\", ".").replace(".py", "")
        return import_path

    async def _save_tests(self, item: Dict[str, Any], test_code: str) -> None:
        """Save generated tests to file."""
        source_file = Path(item["file"])
        rel_path = source_file.relative_to(self._project_root)

        # Create test file path
        test_path = self._project_root / "tests" / f"test_{rel_path.stem}.py"
        test_path.parent.mkdir(parents=True, exist_ok=True)

        # Append or create
        if test_path.exists():
            existing = await asyncio.to_thread(test_path.read_text)
            if item["name"] not in existing:
                test_code = f"\n\n{test_code}"
                await asyncio.to_thread(
                    lambda: test_path.write_text(existing + test_code)
                )
        else:
            await asyncio.to_thread(
                lambda: test_path.write_text(test_code)
            )

        logger.info(f"Saved tests to {test_path}")

    def get_status(self) -> Dict[str, Any]:
        """Get generator status."""
        return {
            "project_root": str(self._project_root),
            "llm_connected": self._llm_client is not None,
        }


# =============================================================================
# v4.0: GLOBAL INSTANCES AND FACTORY FUNCTIONS
# =============================================================================

_goal_decomposer: Optional[GoalDecompositionEngine] = None
_debt_detector: Optional[TechnicalDebtDetector] = None
_refinement_loop: Optional[AutonomousSelfRefinementLoop] = None
_dual_agent_system: Optional[DualAgentSystem] = None
_code_memory_rag: Optional[CodeMemoryRAG] = None
_system_feedback_loop: Optional[SystemFeedbackLoop] = None
_auto_test_generator: Optional[AutoTestGenerator] = None


def get_goal_decomposer(
    oracle: Optional[Any] = None,
    llm_client: Optional[Any] = None,
) -> GoalDecompositionEngine:
    """Get or create the goal decomposition engine."""
    global _goal_decomposer
    if _goal_decomposer is None:
        _goal_decomposer = GoalDecompositionEngine(
            oracle=oracle,
            llm_client=llm_client,
        )
    return _goal_decomposer


def get_debt_detector(
    oracle: Optional[Any] = None,
    project_root: Optional[Path] = None,
) -> TechnicalDebtDetector:
    """Get or create the technical debt detector."""
    global _debt_detector
    if _debt_detector is None:
        _debt_detector = TechnicalDebtDetector(
            oracle=oracle,
            project_root=project_root,
        )
    return _debt_detector


def get_refinement_loop(
    debt_detector: Optional[TechnicalDebtDetector] = None,
    goal_decomposer: Optional[GoalDecompositionEngine] = None,
    orchestrator: Optional[Any] = None,
) -> AutonomousSelfRefinementLoop:
    """Get or create the autonomous refinement loop."""
    global _refinement_loop
    if _refinement_loop is None:
        _refinement_loop = AutonomousSelfRefinementLoop(
            debt_detector=debt_detector,
            goal_decomposer=goal_decomposer,
            orchestrator=orchestrator,
        )
    return _refinement_loop


def get_dual_agent_system(
    architect_client: Optional[Any] = None,
    reviewer_client: Optional[Any] = None,
) -> DualAgentSystem:
    """Get or create the dual agent system."""
    global _dual_agent_system
    if _dual_agent_system is None:
        _dual_agent_system = DualAgentSystem(
            architect_client=architect_client,
            reviewer_client=reviewer_client,
        )
    return _dual_agent_system


def get_code_memory_rag(
    oracle: Optional[Any] = None,
    chromadb_client: Optional[Any] = None,
) -> CodeMemoryRAG:
    """Get or create the code memory RAG."""
    global _code_memory_rag
    if _code_memory_rag is None:
        _code_memory_rag = CodeMemoryRAG(
            oracle=oracle,
            chromadb_client=chromadb_client,
        )
    return _code_memory_rag


def get_system_feedback_loop(
    orchestrator: Optional[Any] = None,
    goal_decomposer: Optional[GoalDecompositionEngine] = None,
) -> SystemFeedbackLoop:
    """Get or create the system feedback loop."""
    global _system_feedback_loop
    if _system_feedback_loop is None:
        _system_feedback_loop = SystemFeedbackLoop(
            orchestrator=orchestrator,
            goal_decomposer=goal_decomposer,
        )
    return _system_feedback_loop


def get_auto_test_generator(
    project_root: Optional[Path] = None,
    llm_client: Optional[Any] = None,
) -> AutoTestGenerator:
    """Get or create the auto test generator."""
    global _auto_test_generator
    if _auto_test_generator is None:
        _auto_test_generator = AutoTestGenerator(
            project_root=project_root,
            llm_client=llm_client,
        )
    return _auto_test_generator


async def initialize_autonomous_self_programming(
    oracle: Optional[Any] = None,
    orchestrator: Optional[Any] = None,
    llm_client: Optional[Any] = None,
    chromadb_client: Optional[Any] = None,
    start_loops: bool = False,
) -> Dict[str, Any]:
    """
    Initialize all autonomous self-programming components.

    This is the master initialization function that sets up:
    - GoalDecompositionEngine
    - TechnicalDebtDetector
    - AutonomousSelfRefinementLoop
    - DualAgentSystem
    - CodeMemoryRAG
    - SystemFeedbackLoop
    - AutoTestGenerator

    Args:
        oracle: The Oracle codebase knowledge graph
        orchestrator: AgenticLoopOrchestrator for task execution
        llm_client: LLM client for goal decomposition and test generation
        chromadb_client: ChromaDB client for semantic memory
        start_loops: Whether to start background loops

    Returns:
        Dictionary with all initialized components
    """
    logger.info("Initializing autonomous self-programming components...")

    # Initialize components
    goal_decomposer = get_goal_decomposer(oracle=oracle, llm_client=llm_client)
    debt_detector = get_debt_detector(oracle=oracle)
    dual_agent = get_dual_agent_system(architect_client=llm_client, reviewer_client=llm_client)
    code_memory = get_code_memory_rag(oracle=oracle, chromadb_client=chromadb_client)

    # Initialize loops (but don't start yet)
    refinement_loop = get_refinement_loop(
        debt_detector=debt_detector,
        goal_decomposer=goal_decomposer,
        orchestrator=orchestrator,
    )

    system_feedback = get_system_feedback_loop(
        orchestrator=orchestrator,
        goal_decomposer=goal_decomposer,
    )

    test_generator = get_auto_test_generator(llm_client=llm_client)

    # Start background loops if requested
    if start_loops:
        await refinement_loop.start()
        await system_feedback.start()

    components = {
        "goal_decomposer": goal_decomposer,
        "debt_detector": debt_detector,
        "refinement_loop": refinement_loop,
        "dual_agent_system": dual_agent,
        "code_memory_rag": code_memory,
        "system_feedback_loop": system_feedback,
        "auto_test_generator": test_generator,
    }

    logger.info(f"Autonomous self-programming initialized with {len(components)} components")
    return components


async def shutdown_autonomous_self_programming() -> None:
    """Shutdown all autonomous self-programming components."""
    logger.info("Shutting down autonomous self-programming components...")

    global _refinement_loop, _system_feedback_loop

    if _refinement_loop:
        await _refinement_loop.stop()

    if _system_feedback_loop:
        await _system_feedback_loop.stop()

    logger.info("Autonomous self-programming shutdown complete")


# =============================================================================
# v5.0: Web Search Integration for Autonomous Self-Programming
# =============================================================================
# This module provides web search capabilities for the self-programming system:
# - WebSearchExtractor: Multi-provider web search with fallback
# - DocumentationLookup: Official library documentation retrieval
# - StackOverflowSearcher: Error solutions from Stack Overflow
# - CodeExampleScraper: Code examples from GitHub
# - OuroborosWebIntegration: Integration with the improvement loop
# =============================================================================


class SearchProvider(Enum):
    """Supported web search providers."""
    DUCKDUCKGO = "duckduckgo"
    BRAVE = "brave"
    BING = "bing"
    GOOGLE = "google"
    SEARXNG = "searxng"


class SearchResultType(Enum):
    """Types of search results."""
    WEB_PAGE = "web_page"
    DOCUMENTATION = "documentation"
    STACK_OVERFLOW = "stack_overflow"
    GITHUB_CODE = "github_code"
    GITHUB_REPO = "github_repo"
    API_REFERENCE = "api_reference"
    TUTORIAL = "tutorial"
    BLOG_POST = "blog_post"


class DocumentationType(Enum):
    """Types of documentation sources."""
    OFFICIAL = "official"
    COMMUNITY = "community"
    API_REFERENCE = "api_reference"
    TUTORIAL = "tutorial"
    CHANGELOG = "changelog"
    README = "readme"


@dataclass
class SearchResult:
    """A single search result."""
    title: str
    url: str
    snippet: str
    provider: SearchProvider
    result_type: SearchResultType
    relevance_score: float  # 0.0 to 1.0
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)
    cached: bool = False
    cache_expires: Optional[datetime] = None


@dataclass
class DocumentationResult:
    """Documentation lookup result."""
    library_name: str
    version: Optional[str]
    doc_type: DocumentationType
    title: str
    url: str
    content: str
    code_examples: List[str] = field(default_factory=list)
    related_topics: List[str] = field(default_factory=list)
    last_updated: Optional[datetime] = None
    relevance_score: float = 0.0


@dataclass
class StackOverflowResult:
    """Stack Overflow search result."""
    question_id: int
    question_title: str
    question_url: str
    question_score: int
    answer_count: int
    is_answered: bool
    accepted_answer_id: Optional[int]
    tags: List[str]
    question_body: str
    answers: List[Dict[str, Any]] = field(default_factory=list)
    relevance_score: float = 0.0


@dataclass
class CodeExample:
    """Code example from GitHub or other sources."""
    source: str  # 'github', 'gist', 'docs', etc.
    repo_name: Optional[str]
    file_path: Optional[str]
    code: str
    language: str
    context: str  # Surrounding description
    stars: int = 0
    url: str = ""
    relevance_score: float = 0.0


@dataclass
class WebSearchConfig:
    """Configuration for web search components."""
    # Provider configuration
    primary_provider: SearchProvider = SearchProvider.DUCKDUCKGO
    fallback_providers: List[SearchProvider] = field(
        default_factory=lambda: [SearchProvider.BRAVE, SearchProvider.BING]
    )

    # Rate limiting (requests per minute per provider)
    rate_limits: Dict[SearchProvider, int] = field(default_factory=lambda: {
        SearchProvider.DUCKDUCKGO: 30,
        SearchProvider.BRAVE: 60,
        SearchProvider.BING: 100,
        SearchProvider.GOOGLE: 100,
        SearchProvider.SEARXNG: 120,
    })

    # Caching
    cache_ttl_seconds: int = 3600  # 1 hour
    max_cache_size: int = 10000

    # Retry configuration
    max_retries: int = 3
    retry_base_delay: float = 1.0
    retry_max_delay: float = 30.0
    retry_exponential_base: float = 2.0

    # Circuit breaker
    circuit_breaker_threshold: int = 5  # failures before opening
    circuit_breaker_timeout: float = 60.0  # seconds to wait before retry

    # Timeouts
    request_timeout: float = 30.0
    connection_timeout: float = 10.0

    # Result limits
    max_results_per_query: int = 20
    max_parallel_requests: int = 5

    # API Keys (loaded from environment)
    brave_api_key: Optional[str] = None
    bing_api_key: Optional[str] = None
    google_api_key: Optional[str] = None
    google_cx_id: Optional[str] = None
    github_token: Optional[str] = None
    stackoverflow_key: Optional[str] = None

    def __post_init__(self):
        """Load API keys from environment."""
        self.brave_api_key = os.getenv("BRAVE_SEARCH_API_KEY", self.brave_api_key)
        self.bing_api_key = os.getenv("BING_SEARCH_API_KEY", self.bing_api_key)
        self.google_api_key = os.getenv("GOOGLE_SEARCH_API_KEY", self.google_api_key)
        self.google_cx_id = os.getenv("GOOGLE_SEARCH_CX_ID", self.google_cx_id)
        self.github_token = os.getenv("GITHUB_TOKEN", self.github_token)
        self.stackoverflow_key = os.getenv("STACKOVERFLOW_API_KEY", self.stackoverflow_key)


class RateLimiter:
    """Token bucket rate limiter for API calls."""

    def __init__(self, rate_per_minute: int):
        self.rate = rate_per_minute
        self.tokens = float(rate_per_minute)
        self.max_tokens = float(rate_per_minute)
        self.last_update = time.monotonic()
        self._lock = asyncio.Lock()

    async def acquire(self) -> bool:
        """Acquire a token, waiting if necessary."""
        async with self._lock:
            now = time.monotonic()
            elapsed = now - self.last_update
            self.last_update = now

            # Replenish tokens
            self.tokens = min(
                self.max_tokens,
                self.tokens + elapsed * (self.rate / 60.0)
            )

            if self.tokens >= 1.0:
                self.tokens -= 1.0
                return True

            # Calculate wait time
            wait_time = (1.0 - self.tokens) * (60.0 / self.rate)
            await asyncio.sleep(wait_time)
            self.tokens = 0.0
            return True

    def available(self) -> float:
        """Get available tokens without consuming."""
        now = time.monotonic()
        elapsed = now - self.last_update
        return min(
            self.max_tokens,
            self.tokens + elapsed * (self.rate / 60.0)
        )


class CircuitBreaker:
    """Circuit breaker for handling provider failures."""

    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout: float = 60.0,
    ):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failures = 0
        self.last_failure_time: Optional[float] = None
        self.state = "closed"  # closed, open, half-open
        self._lock = asyncio.Lock()

    async def call(self, func: Callable, *args, **kwargs) -> Any:
        """Execute function with circuit breaker protection."""
        async with self._lock:
            if self.state == "open":
                if self.last_failure_time:
                    elapsed = time.monotonic() - self.last_failure_time
                    if elapsed >= self.recovery_timeout:
                        self.state = "half-open"
                    else:
                        raise Exception(
                            f"Circuit breaker open, retry in {self.recovery_timeout - elapsed:.1f}s"
                        )

        try:
            if asyncio.iscoroutinefunction(func):
                result = await func(*args, **kwargs)
            else:
                result = func(*args, **kwargs)

            async with self._lock:
                self.failures = 0
                self.state = "closed"

            return result

        except Exception as e:
            async with self._lock:
                self.failures += 1
                self.last_failure_time = time.monotonic()

                if self.failures >= self.failure_threshold:
                    self.state = "open"
                    logger.warning(
                        f"Circuit breaker opened after {self.failures} failures"
                    )
            raise

    @property
    def is_open(self) -> bool:
        """Check if circuit is open."""
        return self.state == "open"


class SearchCache:
    """LRU cache with TTL for search results."""

    def __init__(self, max_size: int = 10000, ttl_seconds: int = 3600):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self._cache: Dict[str, Tuple[Any, datetime]] = {}
        self._access_order: List[str] = []
        self._lock = asyncio.Lock()

    def _make_key(self, query: str, provider: Optional[str] = None) -> str:
        """Generate cache key."""
        key_data = f"{query}:{provider or 'default'}"
        return hashlib.sha256(key_data.encode()).hexdigest()[:32]

    async def get(self, query: str, provider: Optional[str] = None) -> Optional[Any]:
        """Get cached result if not expired."""
        async with self._lock:
            key = self._make_key(query, provider)
            if key in self._cache:
                value, timestamp = self._cache[key]
                if (datetime.now() - timestamp).total_seconds() < self.ttl_seconds:
                    # Move to end of access order
                    if key in self._access_order:
                        self._access_order.remove(key)
                    self._access_order.append(key)
                    return value
                else:
                    # Expired, remove
                    del self._cache[key]
                    if key in self._access_order:
                        self._access_order.remove(key)
            return None

    async def set(self, query: str, value: Any, provider: Optional[str] = None) -> None:
        """Cache a result."""
        async with self._lock:
            key = self._make_key(query, provider)

            # Evict if at capacity
            while len(self._cache) >= self.max_size and self._access_order:
                oldest_key = self._access_order.pop(0)
                if oldest_key in self._cache:
                    del self._cache[oldest_key]

            self._cache[key] = (value, datetime.now())
            self._access_order.append(key)

    async def invalidate(self, query: str, provider: Optional[str] = None) -> None:
        """Invalidate a cached entry."""
        async with self._lock:
            key = self._make_key(query, provider)
            if key in self._cache:
                del self._cache[key]
            if key in self._access_order:
                self._access_order.remove(key)

    async def clear(self) -> None:
        """Clear all cached entries."""
        async with self._lock:
            self._cache.clear()
            self._access_order.clear()

    def stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        return {
            "size": len(self._cache),
            "max_size": self.max_size,
            "ttl_seconds": self.ttl_seconds,
        }


class WebSearchExtractor:
    """
    Multi-provider web search extractor with fallback, rate limiting, and caching.

    Supports:
    - DuckDuckGo (free, no API key required)
    - Brave Search (API key optional for limited use)
    - Bing Search (API key required)
    - Google Custom Search (API key required)
    - SearXNG (self-hosted, configurable)
    """

    def __init__(self, config: Optional[WebSearchConfig] = None):
        self.config = config or WebSearchConfig()
        self._cache = SearchCache(
            max_size=self.config.max_cache_size,
            ttl_seconds=self.config.cache_ttl_seconds,
        )
        self._rate_limiters: Dict[SearchProvider, RateLimiter] = {}
        self._circuit_breakers: Dict[SearchProvider, CircuitBreaker] = {}
        self._session: Optional[aiohttp.ClientSession] = None
        self._initialized = False

        # Initialize rate limiters and circuit breakers for each provider
        for provider, rate in self.config.rate_limits.items():
            self._rate_limiters[provider] = RateLimiter(rate)
            self._circuit_breakers[provider] = CircuitBreaker(
                failure_threshold=self.config.circuit_breaker_threshold,
                recovery_timeout=self.config.circuit_breaker_timeout,
            )

    async def initialize(self) -> None:
        """Initialize HTTP session and validate configuration."""
        if self._initialized:
            return

        connector = aiohttp.TCPConnector(
            limit=self.config.max_parallel_requests,
            limit_per_host=10,
        )

        timeout = aiohttp.ClientTimeout(
            total=self.config.request_timeout,
            connect=self.config.connection_timeout,
        )

        self._session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={"User-Agent": "Ironcliw-AI-Agent/1.0 (Self-Programming System)"},
        )

        self._initialized = True
        logger.info("WebSearchExtractor initialized")

    async def shutdown(self) -> None:
        """Cleanup resources."""
        if self._session:
            await self._session.close()
            self._session = None
        self._initialized = False
        logger.info("WebSearchExtractor shutdown complete")

    async def search(
        self,
        query: str,
        max_results: int = 10,
        result_types: Optional[List[SearchResultType]] = None,
        use_cache: bool = True,
    ) -> List[SearchResult]:
        """
        Perform web search across providers with fallback.

        Args:
            query: Search query string
            max_results: Maximum results to return
            result_types: Filter by result types (None = all)
            use_cache: Whether to use cached results

        Returns:
            List of search results sorted by relevance
        """
        if not self._initialized:
            await self.initialize()

        # Check cache first
        if use_cache:
            cached = await self._cache.get(query)
            if cached:
                logger.debug(f"Cache hit for query: {query[:50]}...")
                results = cached
                for r in results:
                    r.cached = True
                return results[:max_results]

        # Try providers in order
        providers = [self.config.primary_provider] + self.config.fallback_providers
        last_error = None

        for provider in providers:
            if self._circuit_breakers[provider].is_open:
                logger.debug(f"Skipping {provider.value} - circuit breaker open")
                continue

            try:
                # Rate limit
                await self._rate_limiters[provider].acquire()

                # Execute search with circuit breaker
                results = await self._circuit_breakers[provider].call(
                    self._search_provider,
                    provider,
                    query,
                    max_results * 2,  # Get extra for filtering
                )

                # Cache results
                await self._cache.set(query, results)

                # Filter and sort
                if result_types:
                    results = [r for r in results if r.result_type in result_types]

                results.sort(key=lambda r: r.relevance_score, reverse=True)

                logger.info(
                    f"Search '{query[:30]}...' returned {len(results)} results from {provider.value}"
                )
                return results[:max_results]

            except Exception as e:
                logger.warning(f"Search failed with {provider.value}: {e}")
                last_error = e
                continue

        # All providers failed
        if last_error:
            logger.error(f"All search providers failed for query: {query}")
            raise last_error

        return []

    async def _search_provider(
        self,
        provider: SearchProvider,
        query: str,
        max_results: int,
    ) -> List[SearchResult]:
        """Execute search on specific provider."""
        if provider == SearchProvider.DUCKDUCKGO:
            return await self._search_duckduckgo(query, max_results)
        elif provider == SearchProvider.BRAVE:
            return await self._search_brave(query, max_results)
        elif provider == SearchProvider.BING:
            return await self._search_bing(query, max_results)
        elif provider == SearchProvider.GOOGLE:
            return await self._search_google(query, max_results)
        elif provider == SearchProvider.SEARXNG:
            return await self._search_searxng(query, max_results)
        else:
            raise ValueError(f"Unsupported provider: {provider}")

    async def _search_duckduckgo(
        self,
        query: str,
        max_results: int,
    ) -> List[SearchResult]:
        """Search using DuckDuckGo HTML (no API key required)."""
        results = []

        # Use DuckDuckGo Lite (HTML) for reliability
        url = "https://lite.duckduckgo.com/lite/"
        params = {"q": query, "kl": "us-en"}

        async with self._session.post(url, data=params) as response:
            response.raise_for_status()
            html = await response.text()

        # Parse HTML results
        # DuckDuckGo Lite uses simple HTML structure
        soup = None
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, "html.parser")
        except ImportError:
            # Fallback to regex parsing
            import re

            # Extract links from HTML
            link_pattern = r'<a[^>]+href="([^"]+)"[^>]*class="result-link"[^>]*>([^<]+)</a>'
            snippet_pattern = r'<td[^>]+class="result-snippet"[^>]*>([^<]+)</td>'

            links = re.findall(link_pattern, html)
            snippets = re.findall(snippet_pattern, html)

            for i, (link_url, title) in enumerate(links[:max_results]):
                snippet = snippets[i] if i < len(snippets) else ""
                results.append(SearchResult(
                    title=title.strip(),
                    url=link_url,
                    snippet=snippet.strip(),
                    provider=SearchProvider.DUCKDUCKGO,
                    result_type=self._classify_result_type(link_url, title),
                    relevance_score=1.0 - (i * 0.05),
                ))

        if soup:
            # Parse with BeautifulSoup
            for i, link in enumerate(soup.select("a.result-link")[:max_results]):
                link_url = link.get("href", "")
                title = link.get_text(strip=True)

                # Get snippet from sibling
                snippet_td = link.find_parent("tr")
                snippet = ""
                if snippet_td:
                    snippet_elem = snippet_td.find_next_sibling("tr")
                    if snippet_elem:
                        snippet = snippet_elem.get_text(strip=True)

                results.append(SearchResult(
                    title=title,
                    url=link_url,
                    snippet=snippet,
                    provider=SearchProvider.DUCKDUCKGO,
                    result_type=self._classify_result_type(link_url, title),
                    relevance_score=1.0 - (i * 0.05),
                ))

        return results

    async def _search_brave(
        self,
        query: str,
        max_results: int,
    ) -> List[SearchResult]:
        """Search using Brave Search API."""
        if not self.config.brave_api_key:
            raise ValueError("Brave API key not configured")

        results = []
        url = "https://api.search.brave.com/res/v1/web/search"
        headers = {
            "X-Subscription-Token": self.config.brave_api_key,
            "Accept": "application/json",
        }
        params = {
            "q": query,
            "count": min(max_results, 20),
            "freshness": "py",  # Past year
        }

        async with self._session.get(url, headers=headers, params=params) as response:
            response.raise_for_status()
            data = await response.json()

        web_results = data.get("web", {}).get("results", [])

        for i, item in enumerate(web_results):
            results.append(SearchResult(
                title=item.get("title", ""),
                url=item.get("url", ""),
                snippet=item.get("description", ""),
                provider=SearchProvider.BRAVE,
                result_type=self._classify_result_type(
                    item.get("url", ""),
                    item.get("title", ""),
                ),
                relevance_score=1.0 - (i * 0.04),
                metadata={
                    "published": item.get("age"),
                    "family_friendly": item.get("family_friendly"),
                },
            ))

        return results

    async def _search_bing(
        self,
        query: str,
        max_results: int,
    ) -> List[SearchResult]:
        """Search using Bing Web Search API."""
        if not self.config.bing_api_key:
            raise ValueError("Bing API key not configured")

        results = []
        url = "https://api.bing.microsoft.com/v7.0/search"
        headers = {"Ocp-Apim-Subscription-Key": self.config.bing_api_key}
        params = {
            "q": query,
            "count": min(max_results, 50),
            "mkt": "en-US",
        }

        async with self._session.get(url, headers=headers, params=params) as response:
            response.raise_for_status()
            data = await response.json()

        web_pages = data.get("webPages", {}).get("value", [])

        for i, item in enumerate(web_pages):
            results.append(SearchResult(
                title=item.get("name", ""),
                url=item.get("url", ""),
                snippet=item.get("snippet", ""),
                provider=SearchProvider.BING,
                result_type=self._classify_result_type(
                    item.get("url", ""),
                    item.get("name", ""),
                ),
                relevance_score=1.0 - (i * 0.03),
                metadata={
                    "date_last_crawled": item.get("dateLastCrawled"),
                    "language": item.get("language"),
                },
            ))

        return results

    async def _search_google(
        self,
        query: str,
        max_results: int,
    ) -> List[SearchResult]:
        """Search using Google Custom Search API."""
        if not self.config.google_api_key or not self.config.google_cx_id:
            raise ValueError("Google API key or CX ID not configured")

        results = []
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": self.config.google_api_key,
            "cx": self.config.google_cx_id,
            "q": query,
            "num": min(max_results, 10),
        }

        async with self._session.get(url, params=params) as response:
            response.raise_for_status()
            data = await response.json()

        items = data.get("items", [])

        for i, item in enumerate(items):
            results.append(SearchResult(
                title=item.get("title", ""),
                url=item.get("link", ""),
                snippet=item.get("snippet", ""),
                provider=SearchProvider.GOOGLE,
                result_type=self._classify_result_type(
                    item.get("link", ""),
                    item.get("title", ""),
                ),
                relevance_score=1.0 - (i * 0.05),
                metadata={
                    "display_link": item.get("displayLink"),
                    "cache_id": item.get("cacheId"),
                },
            ))

        return results

    async def _search_searxng(
        self,
        query: str,
        max_results: int,
    ) -> List[SearchResult]:
        """Search using self-hosted SearXNG instance."""
        searxng_url = os.getenv("SEARXNG_URL", "http://localhost:8888")

        results = []
        url = f"{searxng_url}/search"
        params = {
            "q": query,
            "format": "json",
            "engines": "google,bing,duckduckgo",
        }

        async with self._session.get(url, params=params) as response:
            response.raise_for_status()
            data = await response.json()

        for i, item in enumerate(data.get("results", [])[:max_results]):
            results.append(SearchResult(
                title=item.get("title", ""),
                url=item.get("url", ""),
                snippet=item.get("content", ""),
                provider=SearchProvider.SEARXNG,
                result_type=self._classify_result_type(
                    item.get("url", ""),
                    item.get("title", ""),
                ),
                relevance_score=item.get("score", 1.0 - (i * 0.05)),
                metadata={
                    "engine": item.get("engine"),
                    "parsed_url": item.get("parsed_url"),
                },
            ))

        return results

    def _classify_result_type(self, url: str, title: str) -> SearchResultType:
        """Classify search result type based on URL and title."""
        url_lower = url.lower()
        title_lower = title.lower()

        # Documentation sites
        doc_patterns = [
            "docs.", "documentation", ".readthedocs.", "/docs/",
            "developer.", "reference/", "api-reference",
        ]
        if any(p in url_lower for p in doc_patterns):
            return SearchResultType.DOCUMENTATION

        # Stack Overflow
        if "stackoverflow.com" in url_lower or "stackexchange.com" in url_lower:
            return SearchResultType.STACK_OVERFLOW

        # GitHub
        if "github.com" in url_lower:
            if "/blob/" in url_lower or "/tree/" in url_lower:
                return SearchResultType.GITHUB_CODE
            return SearchResultType.GITHUB_REPO

        # API references
        if "api" in url_lower and ("reference" in url_lower or "docs" in url_lower):
            return SearchResultType.API_REFERENCE

        # Tutorials
        tutorial_patterns = ["tutorial", "guide", "how-to", "getting-started"]
        if any(p in url_lower or p in title_lower for p in tutorial_patterns):
            return SearchResultType.TUTORIAL

        # Blog posts
        blog_patterns = ["blog", "medium.com", "dev.to", "hashnode"]
        if any(p in url_lower for p in blog_patterns):
            return SearchResultType.BLOG_POST

        return SearchResultType.WEB_PAGE

    async def search_for_error(
        self,
        error_message: str,
        language: str = "python",
        max_results: int = 10,
    ) -> List[SearchResult]:
        """Search specifically for error solutions."""
        # Construct targeted query
        query = f"{language} {error_message} solution"

        results = await self.search(
            query,
            max_results=max_results * 2,
            result_types=[
                SearchResultType.STACK_OVERFLOW,
                SearchResultType.DOCUMENTATION,
                SearchResultType.BLOG_POST,
            ],
        )

        # Boost Stack Overflow results
        for r in results:
            if r.result_type == SearchResultType.STACK_OVERFLOW:
                r.relevance_score *= 1.5

        results.sort(key=lambda r: r.relevance_score, reverse=True)
        return results[:max_results]

    async def search_for_library(
        self,
        library_name: str,
        topic: str = "usage",
        max_results: int = 10,
    ) -> List[SearchResult]:
        """Search for library documentation and usage."""
        query = f"{library_name} {topic} documentation"

        results = await self.search(
            query,
            max_results=max_results * 2,
            result_types=[
                SearchResultType.DOCUMENTATION,
                SearchResultType.API_REFERENCE,
                SearchResultType.TUTORIAL,
            ],
        )

        # Boost official docs
        for r in results:
            if "official" in r.title.lower() or library_name.lower() in r.url.lower():
                r.relevance_score *= 1.3

        results.sort(key=lambda r: r.relevance_score, reverse=True)
        return results[:max_results]


class DocumentationLookup:
    """
    Library documentation lookup with official docs priority.

    Maintains mapping of popular libraries to their official documentation URLs
    and provides structured documentation retrieval.
    """

    # Dynamic documentation source registry
    DOCUMENTATION_SOURCES: Dict[str, Dict[str, str]] = {
        # Python standard library and popular packages
        "python": {"base": "https://docs.python.org/3/", "search": "search.html?q="},
        "asyncio": {"base": "https://docs.python.org/3/library/asyncio.html"},
        "typing": {"base": "https://docs.python.org/3/library/typing.html"},

        # Web frameworks
        "fastapi": {"base": "https://fastapi.tiangolo.com/", "search": "?q="},
        "flask": {"base": "https://flask.palletsprojects.com/", "search": "search/?q="},
        "django": {"base": "https://docs.djangoproject.com/", "search": "search/?q="},
        "starlette": {"base": "https://www.starlette.io/"},

        # Async/networking
        "aiohttp": {"base": "https://docs.aiohttp.org/en/stable/"},
        "httpx": {"base": "https://www.python-httpx.org/"},
        "requests": {"base": "https://requests.readthedocs.io/en/latest/"},

        # Data science
        "numpy": {"base": "https://numpy.org/doc/stable/", "search": "search.html?q="},
        "pandas": {"base": "https://pandas.pydata.org/docs/", "search": "search.html?q="},
        "scipy": {"base": "https://docs.scipy.org/doc/scipy/", "search": "search.html?q="},
        "matplotlib": {"base": "https://matplotlib.org/stable/", "search": "search.html?q="},

        # ML/AI
        "torch": {"base": "https://pytorch.org/docs/stable/", "search": "search.html?q="},
        "pytorch": {"base": "https://pytorch.org/docs/stable/", "search": "search.html?q="},
        "tensorflow": {"base": "https://www.tensorflow.org/api_docs/python/"},
        "transformers": {"base": "https://huggingface.co/docs/transformers/"},
        "langchain": {"base": "https://python.langchain.com/docs/"},
        "openai": {"base": "https://platform.openai.com/docs/"},
        "anthropic": {"base": "https://docs.anthropic.com/"},

        # Database
        "sqlalchemy": {"base": "https://docs.sqlalchemy.org/en/20/"},
        "psycopg2": {"base": "https://www.psycopg.org/docs/"},
        "redis": {"base": "https://redis.io/docs/"},
        "chromadb": {"base": "https://docs.trychroma.com/"},

        # Testing
        "pytest": {"base": "https://docs.pytest.org/en/stable/", "search": "search.html?q="},
        "unittest": {"base": "https://docs.python.org/3/library/unittest.html"},

        # DevOps/Cloud
        "docker": {"base": "https://docs.docker.com/"},
        "kubernetes": {"base": "https://kubernetes.io/docs/"},
        "gcp": {"base": "https://cloud.google.com/docs/"},
        "aws": {"base": "https://docs.aws.amazon.com/"},

        # Misc
        "pydantic": {"base": "https://docs.pydantic.dev/latest/"},
        "click": {"base": "https://click.palletsprojects.com/"},
        "rich": {"base": "https://rich.readthedocs.io/en/stable/"},
    }

    def __init__(
        self,
        search_extractor: Optional[WebSearchExtractor] = None,
        config: Optional[WebSearchConfig] = None,
    ):
        self.search_extractor = search_extractor
        self.config = config or WebSearchConfig()
        self._session: Optional[aiohttp.ClientSession] = None
        self._cache = SearchCache(
            max_size=self.config.max_cache_size,
            ttl_seconds=self.config.cache_ttl_seconds * 2,  # Docs change less often
        )

    async def initialize(self) -> None:
        """Initialize HTTP session."""
        if self._session is None:
            self._session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30),
                headers={"User-Agent": "Ironcliw-AI-Agent/1.0 (Documentation Lookup)"},
            )

    async def shutdown(self) -> None:
        """Cleanup resources."""
        if self._session:
            await self._session.close()
            self._session = None

    def register_documentation_source(
        self,
        library: str,
        base_url: str,
        search_path: Optional[str] = None,
    ) -> None:
        """Dynamically register a new documentation source."""
        self.DOCUMENTATION_SOURCES[library.lower()] = {
            "base": base_url,
            **({"search": search_path} if search_path else {}),
        }

    async def lookup(
        self,
        library: str,
        topic: Optional[str] = None,
        version: Optional[str] = None,
    ) -> List[DocumentationResult]:
        """
        Look up documentation for a library.

        Args:
            library: Library name (e.g., 'fastapi', 'numpy')
            topic: Specific topic to search for
            version: Specific version (if applicable)

        Returns:
            List of documentation results
        """
        await self.initialize()

        library_lower = library.lower()
        results = []

        # Check if we have a known documentation source
        if library_lower in self.DOCUMENTATION_SOURCES:
            source = self.DOCUMENTATION_SOURCES[library_lower]
            base_url = source["base"]

            # Direct documentation URL
            results.append(DocumentationResult(
                library_name=library,
                version=version,
                doc_type=DocumentationType.OFFICIAL,
                title=f"{library} Official Documentation",
                url=base_url,
                content="",  # Will be fetched on demand
                relevance_score=1.0,
            ))

            # If topic specified and search path exists
            if topic and "search" in source:
                search_url = f"{base_url}{source['search']}{topic}"
                results.append(DocumentationResult(
                    library_name=library,
                    version=version,
                    doc_type=DocumentationType.OFFICIAL,
                    title=f"{library} - {topic}",
                    url=search_url,
                    content="",
                    relevance_score=0.95,
                ))

        # Fall back to web search if needed
        if self.search_extractor and (not results or topic):
            query = f"{library} {topic or 'documentation'} official docs"
            search_results = await self.search_extractor.search_for_library(
                library,
                topic=topic or "documentation",
                max_results=5,
            )

            for sr in search_results:
                if sr.result_type in [
                    SearchResultType.DOCUMENTATION,
                    SearchResultType.API_REFERENCE,
                ]:
                    results.append(DocumentationResult(
                        library_name=library,
                        version=version,
                        doc_type=self._classify_doc_type(sr),
                        title=sr.title,
                        url=sr.url,
                        content=sr.snippet,
                        relevance_score=sr.relevance_score * 0.9,
                    ))

        return results

    async def fetch_content(
        self,
        doc_result: DocumentationResult,
        extract_examples: bool = True,
    ) -> DocumentationResult:
        """
        Fetch and parse documentation content.

        Args:
            doc_result: Documentation result to fetch
            extract_examples: Whether to extract code examples

        Returns:
            Updated documentation result with content
        """
        await self.initialize()

        try:
            async with self._session.get(doc_result.url) as response:
                response.raise_for_status()
                html = await response.text()

            # Parse HTML
            try:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html, "html.parser")

                # Extract main content
                main_content = soup.find("main") or soup.find("article") or soup.find("body")
                if main_content:
                    doc_result.content = main_content.get_text(separator="\n", strip=True)

                # Extract code examples
                if extract_examples:
                    code_blocks = soup.find_all(["code", "pre"])
                    for block in code_blocks:
                        code = block.get_text(strip=True)
                        if len(code) > 20:  # Skip tiny snippets
                            doc_result.code_examples.append(code)

                # Extract related topics
                links = soup.find_all("a", href=True)
                for link in links[:20]:
                    href = link.get("href", "")
                    text = link.get_text(strip=True)
                    if href.startswith(doc_result.url) and text:
                        doc_result.related_topics.append(text)

            except ImportError:
                # Fallback: Basic content extraction
                import re
                # Remove HTML tags
                doc_result.content = re.sub(r"<[^>]+>", " ", html)
                doc_result.content = re.sub(r"\s+", " ", doc_result.content).strip()

                # Extract code blocks with regex
                code_pattern = r"<(?:code|pre)[^>]*>(.*?)</(?:code|pre)>"
                matches = re.findall(code_pattern, html, re.DOTALL | re.IGNORECASE)
                doc_result.code_examples = [
                    re.sub(r"<[^>]+>", "", m).strip()
                    for m in matches if len(m) > 20
                ]

        except Exception as e:
            logger.warning(f"Failed to fetch documentation: {e}")

        return doc_result

    def _classify_doc_type(self, search_result: SearchResult) -> DocumentationType:
        """Classify documentation type from search result."""
        url_lower = search_result.url.lower()
        title_lower = search_result.title.lower()

        if "api" in url_lower or "api" in title_lower:
            return DocumentationType.API_REFERENCE
        elif "tutorial" in url_lower or "tutorial" in title_lower:
            return DocumentationType.TUTORIAL
        elif "changelog" in url_lower or "release" in url_lower:
            return DocumentationType.CHANGELOG
        elif "readme" in url_lower:
            return DocumentationType.README
        elif any(x in url_lower for x in ["docs.", "documentation", ".readthedocs."]):
            return DocumentationType.OFFICIAL
        else:
            return DocumentationType.COMMUNITY


class StackOverflowSearcher:
    """
    Stack Overflow integration for finding solutions to errors and coding questions.

    Supports both API and web scraping fallback.
    """

    API_BASE = "https://api.stackexchange.com/2.3"
    SITE = "stackoverflow"

    def __init__(self, config: Optional[WebSearchConfig] = None):
        self.config = config or WebSearchConfig()
        self._session: Optional[aiohttp.ClientSession] = None
        self._rate_limiter = RateLimiter(30)  # SO API limit
        self._cache = SearchCache(
            max_size=self.config.max_cache_size,
            ttl_seconds=self.config.cache_ttl_seconds,
        )

    async def initialize(self) -> None:
        """Initialize HTTP session."""
        if self._session is None:
            self._session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30),
            )

    async def shutdown(self) -> None:
        """Cleanup resources."""
        if self._session:
            await self._session.close()
            self._session = None

    async def search(
        self,
        query: str,
        tags: Optional[List[str]] = None,
        max_results: int = 10,
        sort: str = "relevance",
    ) -> List[StackOverflowResult]:
        """
        Search Stack Overflow for questions matching query.

        Args:
            query: Search query
            tags: Filter by tags (e.g., ['python', 'asyncio'])
            max_results: Maximum results to return
            sort: Sort order ('relevance', 'votes', 'creation', 'activity')

        Returns:
            List of Stack Overflow results
        """
        await self.initialize()

        # Check cache
        cache_key = f"{query}:{','.join(tags or [])}:{sort}"
        cached = await self._cache.get(cache_key)
        if cached:
            return cached[:max_results]

        await self._rate_limiter.acquire()

        results = []

        # Build API request
        params = {
            "order": "desc",
            "sort": sort,
            "intitle": query,
            "site": self.SITE,
            "pagesize": min(max_results, 30),
            "filter": "withbody",  # Include question body
        }

        if tags:
            params["tagged"] = ";".join(tags)

        if self.config.stackoverflow_key:
            params["key"] = self.config.stackoverflow_key

        try:
            async with self._session.get(
                f"{self.API_BASE}/search/advanced",
                params=params,
            ) as response:
                response.raise_for_status()
                data = await response.json()

            for item in data.get("items", []):
                result = StackOverflowResult(
                    question_id=item["question_id"],
                    question_title=item["title"],
                    question_url=item["link"],
                    question_score=item["score"],
                    answer_count=item["answer_count"],
                    is_answered=item["is_answered"],
                    accepted_answer_id=item.get("accepted_answer_id"),
                    tags=item.get("tags", []),
                    question_body=item.get("body", ""),
                    relevance_score=self._calculate_relevance(item, query),
                )
                results.append(result)

            # Cache results
            await self._cache.set(cache_key, results)

        except Exception as e:
            logger.warning(f"Stack Overflow API search failed: {e}")
            # Fallback could be implemented here

        return results[:max_results]

    async def get_answers(
        self,
        question_id: int,
        max_answers: int = 5,
    ) -> List[Dict[str, Any]]:
        """
        Get answers for a specific question.

        Args:
            question_id: Stack Overflow question ID
            max_answers: Maximum answers to fetch

        Returns:
            List of answer dictionaries
        """
        await self.initialize()
        await self._rate_limiter.acquire()

        params = {
            "order": "desc",
            "sort": "votes",
            "site": self.SITE,
            "pagesize": max_answers,
            "filter": "withbody",
        }

        if self.config.stackoverflow_key:
            params["key"] = self.config.stackoverflow_key

        try:
            async with self._session.get(
                f"{self.API_BASE}/questions/{question_id}/answers",
                params=params,
            ) as response:
                response.raise_for_status()
                data = await response.json()

            return [
                {
                    "answer_id": a["answer_id"],
                    "score": a["score"],
                    "is_accepted": a.get("is_accepted", False),
                    "body": a.get("body", ""),
                    "creation_date": a.get("creation_date"),
                }
                for a in data.get("items", [])
            ]

        except Exception as e:
            logger.warning(f"Failed to fetch answers: {e}")
            return []

    async def search_for_error(
        self,
        error_message: str,
        language: str = "python",
        max_results: int = 5,
    ) -> List[StackOverflowResult]:
        """
        Search for solutions to a specific error.

        Args:
            error_message: The error message to search for
            language: Programming language tag
            max_results: Maximum results

        Returns:
            List of relevant Stack Overflow results
        """
        # Clean up error message for search
        import re

        # Remove file paths and line numbers
        clean_error = re.sub(r'File "[^"]+", line \d+', '', error_message)
        # Remove memory addresses
        clean_error = re.sub(r'0x[a-fA-F0-9]+', '', clean_error)
        # Get core error type and message
        clean_error = clean_error.strip()[:200]

        results = await self.search(
            query=clean_error,
            tags=[language],
            max_results=max_results,
            sort="relevance",
        )

        # Fetch answers for top results
        for result in results[:3]:
            result.answers = await self.get_answers(
                result.question_id,
                max_answers=3,
            )

        return results

    def _calculate_relevance(
        self,
        item: Dict[str, Any],
        query: str,
    ) -> float:
        """Calculate relevance score for a result."""
        score = 0.5  # Base score

        # Boost for answered questions
        if item.get("is_answered"):
            score += 0.2

        # Boost for accepted answers
        if item.get("accepted_answer_id"):
            score += 0.1

        # Boost for high vote count
        votes = item.get("score", 0)
        if votes > 100:
            score += 0.15
        elif votes > 50:
            score += 0.1
        elif votes > 10:
            score += 0.05

        # Title match bonus
        title_lower = item.get("title", "").lower()
        query_words = query.lower().split()
        matching_words = sum(1 for w in query_words if w in title_lower)
        score += min(0.15, matching_words * 0.03)

        return min(1.0, score)


class CodeExampleScraper:
    """
    Code example scraper for GitHub repositories and gists.

    Finds relevant code examples for implementation patterns.
    """

    GITHUB_API = "https://api.github.com"

    def __init__(self, config: Optional[WebSearchConfig] = None):
        self.config = config or WebSearchConfig()
        self._session: Optional[aiohttp.ClientSession] = None
        self._rate_limiter = RateLimiter(30)  # GitHub API limit
        self._cache = SearchCache(
            max_size=self.config.max_cache_size,
            ttl_seconds=self.config.cache_ttl_seconds * 2,
        )

    async def initialize(self) -> None:
        """Initialize HTTP session."""
        if self._session is None:
            headers = {
                "Accept": "application/vnd.github.v3+json",
                "User-Agent": "Ironcliw-AI-Agent/1.0",
            }
            if self.config.github_token:
                headers["Authorization"] = f"token {self.config.github_token}"

            self._session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=30),
                headers=headers,
            )

    async def shutdown(self) -> None:
        """Cleanup resources."""
        if self._session:
            await self._session.close()
            self._session = None

    async def search_code(
        self,
        query: str,
        language: str = "python",
        max_results: int = 10,
    ) -> List[CodeExample]:
        """
        Search GitHub for code examples.

        Args:
            query: Code search query
            language: Programming language filter
            max_results: Maximum results

        Returns:
            List of code examples
        """
        await self.initialize()

        # Check cache
        cache_key = f"{query}:{language}"
        cached = await self._cache.get(cache_key)
        if cached:
            return cached[:max_results]

        await self._rate_limiter.acquire()

        results = []

        # Build search query
        search_query = f"{query} language:{language}"

        params = {
            "q": search_query,
            "per_page": min(max_results, 30),
            "sort": "indexed",
        }

        try:
            async with self._session.get(
                f"{self.GITHUB_API}/search/code",
                params=params,
            ) as response:
                if response.status == 403:
                    # Rate limited
                    logger.warning("GitHub API rate limited")
                    return []

                response.raise_for_status()
                data = await response.json()

            for item in data.get("items", []):
                # Fetch file content
                content = await self._fetch_file_content(item["url"])

                if content:
                    results.append(CodeExample(
                        source="github",
                        repo_name=item["repository"]["full_name"],
                        file_path=item["path"],
                        code=content,
                        language=language,
                        context=item.get("name", ""),
                        stars=0,  # Would need additional API call
                        url=item["html_url"],
                        relevance_score=item.get("score", 0.5),
                    ))

            # Cache results
            await self._cache.set(cache_key, results)

        except Exception as e:
            logger.warning(f"GitHub code search failed: {e}")

        return results[:max_results]

    async def search_repos(
        self,
        query: str,
        language: str = "python",
        max_results: int = 5,
    ) -> List[Dict[str, Any]]:
        """
        Search for GitHub repositories.

        Args:
            query: Repository search query
            language: Language filter
            max_results: Maximum results

        Returns:
            List of repository info dictionaries
        """
        await self.initialize()
        await self._rate_limiter.acquire()

        params = {
            "q": f"{query} language:{language}",
            "sort": "stars",
            "per_page": max_results,
        }

        try:
            async with self._session.get(
                f"{self.GITHUB_API}/search/repositories",
                params=params,
            ) as response:
                response.raise_for_status()
                data = await response.json()

            return [
                {
                    "name": r["full_name"],
                    "description": r.get("description", ""),
                    "url": r["html_url"],
                    "stars": r["stargazers_count"],
                    "forks": r["forks_count"],
                    "language": r.get("language"),
                    "topics": r.get("topics", []),
                }
                for r in data.get("items", [])
            ]

        except Exception as e:
            logger.warning(f"GitHub repo search failed: {e}")
            return []

    async def get_readme(self, repo_name: str) -> Optional[str]:
        """
        Get README content for a repository.

        Args:
            repo_name: Repository full name (owner/repo)

        Returns:
            README content or None
        """
        await self.initialize()
        await self._rate_limiter.acquire()

        try:
            async with self._session.get(
                f"{self.GITHUB_API}/repos/{repo_name}/readme",
                headers={"Accept": "application/vnd.github.v3.raw"},
            ) as response:
                if response.status == 200:
                    return await response.text()
        except Exception as e:
            logger.debug(f"Failed to fetch README: {e}")

        return None

    async def _fetch_file_content(self, content_url: str) -> Optional[str]:
        """Fetch raw file content from GitHub API."""
        try:
            async with self._session.get(
                content_url,
                headers={"Accept": "application/vnd.github.v3.raw"},
            ) as response:
                if response.status == 200:
                    return await response.text()
        except Exception as e:
            logger.debug(f"Failed to fetch file content: {e}")

        return None


class OuroborosWebIntegration:
    """
    Integrates web search capabilities into the Ouroboros improvement loop.

    This class bridges the gap between autonomous self-programming and
    external knowledge sources, enabling:
    - Error resolution via Stack Overflow
    - Library documentation lookup during implementation
    - Code example discovery for implementation patterns
    - Best practices research before refactoring
    """

    def __init__(
        self,
        search_extractor: Optional[WebSearchExtractor] = None,
        doc_lookup: Optional[DocumentationLookup] = None,
        stackoverflow: Optional[StackOverflowSearcher] = None,
        code_scraper: Optional[CodeExampleScraper] = None,
        oracle: Optional[Any] = None,
        orchestrator: Optional[Any] = None,
        config: Optional[WebSearchConfig] = None,
    ):
        self.config = config or WebSearchConfig()

        # Initialize components
        self.search_extractor = search_extractor or WebSearchExtractor(self.config)
        self.doc_lookup = doc_lookup or DocumentationLookup(
            search_extractor=self.search_extractor,
            config=self.config,
        )
        self.stackoverflow = stackoverflow or StackOverflowSearcher(self.config)
        self.code_scraper = code_scraper or CodeExampleScraper(self.config)

        # Ouroboros integration
        self.oracle = oracle
        self.orchestrator = orchestrator

        # Event handlers
        self._error_handlers: List[Callable] = []
        self._improvement_handlers: List[Callable] = []

        # Metrics
        self._searches_performed = 0
        self._errors_resolved = 0
        self._docs_fetched = 0
        self._examples_found = 0

    async def initialize(self) -> None:
        """Initialize all web integration components."""
        await asyncio.gather(
            self.search_extractor.initialize(),
            self.doc_lookup.initialize(),
            self.stackoverflow.initialize(),
            self.code_scraper.initialize(),
        )
        logger.info("OuroborosWebIntegration initialized")

    async def shutdown(self) -> None:
        """Shutdown all components."""
        await asyncio.gather(
            self.search_extractor.shutdown(),
            self.doc_lookup.shutdown(),
            self.stackoverflow.shutdown(),
            self.code_scraper.shutdown(),
        )
        logger.info("OuroborosWebIntegration shutdown complete")

    def register_error_handler(self, handler: Callable) -> None:
        """Register handler for error resolution events."""
        self._error_handlers.append(handler)

    def register_improvement_handler(self, handler: Callable) -> None:
        """Register handler for improvement research events."""
        self._improvement_handlers.append(handler)

    async def resolve_error(
        self,
        error_message: str,
        error_type: str = "",
        file_path: Optional[str] = None,
        code_context: Optional[str] = None,
        language: str = "python",
    ) -> Dict[str, Any]:
        """
        Attempt to resolve an error using web resources.

        Args:
            error_message: The error message
            error_type: Error type (e.g., 'TypeError', 'ImportError')
            file_path: File where error occurred
            code_context: Surrounding code context
            language: Programming language

        Returns:
            Resolution result with solutions and confidence
        """
        self._searches_performed += 1

        result = {
            "error_message": error_message,
            "error_type": error_type,
            "solutions": [],
            "documentation": [],
            "code_examples": [],
            "confidence": 0.0,
            "resolution_status": "pending",
        }

        # Parallel search across sources
        tasks = [
            self.stackoverflow.search_for_error(error_message, language),
            self.search_extractor.search_for_error(error_message, language),
        ]

        # If we know the library involved, get docs
        detected_library = self._detect_library_from_error(error_message)
        if detected_library:
            tasks.append(self.doc_lookup.lookup(detected_library, error_type))

        search_results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process Stack Overflow results
        if not isinstance(search_results[0], Exception):
            so_results = search_results[0]
            for so in so_results:
                if so.is_answered and so.answers:
                    result["solutions"].append({
                        "source": "stackoverflow",
                        "question": so.question_title,
                        "url": so.question_url,
                        "score": so.question_score,
                        "answer": so.answers[0].get("body", "") if so.answers else "",
                        "relevance": so.relevance_score,
                    })

        # Process web search results
        if not isinstance(search_results[1], Exception):
            web_results = search_results[1]
            for wr in web_results:
                if wr.result_type == SearchResultType.DOCUMENTATION:
                    result["documentation"].append({
                        "title": wr.title,
                        "url": wr.url,
                        "snippet": wr.snippet,
                        "relevance": wr.relevance_score,
                    })

        # Process documentation results
        if len(search_results) > 2 and not isinstance(search_results[2], Exception):
            doc_results = search_results[2]
            for doc in doc_results:
                result["documentation"].append({
                    "library": doc.library_name,
                    "title": doc.title,
                    "url": doc.url,
                    "type": doc.doc_type.value,
                    "relevance": doc.relevance_score,
                })

        # Calculate confidence
        if result["solutions"]:
            best_solution = max(result["solutions"], key=lambda s: s["relevance"])
            result["confidence"] = best_solution["relevance"]
            result["best_solution"] = best_solution

            if result["confidence"] > 0.7:
                result["resolution_status"] = "likely_resolved"
                self._errors_resolved += 1
            else:
                result["resolution_status"] = "needs_review"
        else:
            result["resolution_status"] = "no_solution_found"

        # Notify handlers
        for handler in self._error_handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler(result)
                else:
                    handler(result)
            except Exception as e:
                logger.warning(f"Error handler failed: {e}")

        return result

    async def research_improvement(
        self,
        topic: str,
        improvement_type: str = "refactoring",
        current_implementation: Optional[str] = None,
        target_libraries: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        Research best practices and patterns for an improvement task.

        Args:
            topic: The improvement topic (e.g., 'async error handling')
            improvement_type: Type of improvement (refactoring, optimization, etc.)
            current_implementation: Current code for context
            target_libraries: Specific libraries to research

        Returns:
            Research results with patterns, examples, and recommendations
        """
        self._searches_performed += 1

        result = {
            "topic": topic,
            "improvement_type": improvement_type,
            "best_practices": [],
            "patterns": [],
            "code_examples": [],
            "documentation": [],
            "recommendations": [],
        }

        # Build search queries
        queries = [
            f"python {topic} best practices",
            f"python {improvement_type} {topic} pattern",
        ]

        if target_libraries:
            for lib in target_libraries:
                queries.append(f"{lib} {topic} example")

        # Execute parallel searches
        search_tasks = [
            self.search_extractor.search(q, max_results=5)
            for q in queries[:3]  # Limit parallel queries
        ]

        # Also search for code examples
        code_task = self.code_scraper.search_code(
            topic,
            language="python",
            max_results=5,
        )
        search_tasks.append(code_task)

        # Get documentation if libraries specified
        doc_tasks = []
        if target_libraries:
            for lib in target_libraries[:3]:
                doc_tasks.append(self.doc_lookup.lookup(lib, topic))

        all_results = await asyncio.gather(
            *search_tasks,
            *doc_tasks,
            return_exceptions=True,
        )

        # Process search results
        for i, res in enumerate(all_results[:len(search_tasks)]):
            if isinstance(res, Exception):
                continue

            if isinstance(res, list):
                if res and isinstance(res[0], CodeExample):
                    # Code examples
                    for ex in res:
                        result["code_examples"].append({
                            "source": ex.source,
                            "repo": ex.repo_name,
                            "path": ex.file_path,
                            "code": ex.code[:2000],  # Limit size
                            "url": ex.url,
                            "relevance": ex.relevance_score,
                        })
                        self._examples_found += 1
                else:
                    # Search results
                    for sr in res:
                        if sr.result_type == SearchResultType.TUTORIAL:
                            result["best_practices"].append({
                                "title": sr.title,
                                "url": sr.url,
                                "snippet": sr.snippet,
                            })
                        elif sr.result_type in [
                            SearchResultType.DOCUMENTATION,
                            SearchResultType.API_REFERENCE,
                        ]:
                            result["documentation"].append({
                                "title": sr.title,
                                "url": sr.url,
                                "snippet": sr.snippet,
                            })

        # Process documentation results
        for res in all_results[len(search_tasks):]:
            if isinstance(res, Exception):
                continue
            if isinstance(res, list):
                for doc in res:
                    result["documentation"].append({
                        "library": doc.library_name,
                        "title": doc.title,
                        "url": doc.url,
                        "type": doc.doc_type.value,
                    })
                    self._docs_fetched += 1

        # Generate recommendations based on findings
        result["recommendations"] = self._generate_recommendations(result)

        # Notify handlers
        for handler in self._improvement_handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler(result)
                else:
                    handler(result)
            except Exception as e:
                logger.warning(f"Improvement handler failed: {e}")

        return result

    async def get_library_usage(
        self,
        library: str,
        feature: str,
    ) -> Dict[str, Any]:
        """
        Get usage examples and documentation for a specific library feature.

        Args:
            library: Library name
            feature: Feature or function name

        Returns:
            Usage information with examples and documentation
        """
        result = {
            "library": library,
            "feature": feature,
            "documentation": [],
            "examples": [],
            "related_functions": [],
        }

        # Get documentation
        docs = await self.doc_lookup.lookup(library, feature)
        for doc in docs:
            fetched = await self.doc_lookup.fetch_content(doc)
            result["documentation"].append({
                "title": fetched.title,
                "url": fetched.url,
                "content": fetched.content[:3000],
                "examples": fetched.code_examples[:5],
            })

        # Get code examples
        examples = await self.code_scraper.search_code(
            f"{library} {feature}",
            max_results=5,
        )
        for ex in examples:
            result["examples"].append({
                "source": ex.source,
                "repo": ex.repo_name,
                "code": ex.code[:1500],
                "url": ex.url,
            })

        return result

    def _detect_library_from_error(self, error_message: str) -> Optional[str]:
        """Detect which library an error originated from."""
        # Common library error patterns
        patterns = {
            "aiohttp": ["aiohttp", "ClientSession", "ClientError"],
            "fastapi": ["fastapi", "HTTPException", "RequestValidation"],
            "pydantic": ["pydantic", "ValidationError", "validator"],
            "sqlalchemy": ["sqlalchemy", "IntegrityError", "OperationalError"],
            "requests": ["requests.exceptions", "ConnectionError", "HTTPError"],
            "numpy": ["numpy", "ndarray", "AxisError"],
            "pandas": ["pandas", "DataFrame", "KeyError"],
            "torch": ["torch", "cuda", "RuntimeError: CUDA"],
        }

        error_lower = error_message.lower()
        for lib, keywords in patterns.items():
            if any(kw.lower() in error_lower for kw in keywords):
                return lib

        return None

    def _generate_recommendations(
        self,
        research_result: Dict[str, Any],
    ) -> List[str]:
        """Generate recommendations based on research findings."""
        recommendations = []

        if research_result["code_examples"]:
            recommendations.append(
                f"Found {len(research_result['code_examples'])} relevant code examples. "
                "Review top-rated implementations for proven patterns."
            )

        if research_result["documentation"]:
            recommendations.append(
                f"Official documentation available for {len(research_result['documentation'])} topics. "
                "Consult docs before implementation."
            )

        if research_result["best_practices"]:
            recommendations.append(
                f"Found {len(research_result['best_practices'])} best practice guides. "
                "Consider established patterns."
            )

        if not any([
            research_result["code_examples"],
            research_result["documentation"],
            research_result["best_practices"],
        ]):
            recommendations.append(
                "Limited external resources found. Consider searching with different terms "
                "or consulting internal documentation."
            )

        return recommendations

    def get_metrics(self) -> Dict[str, Any]:
        """Get web integration metrics."""
        return {
            "searches_performed": self._searches_performed,
            "errors_resolved": self._errors_resolved,
            "docs_fetched": self._docs_fetched,
            "examples_found": self._examples_found,
            "resolution_rate": (
                self._errors_resolved / max(1, self._searches_performed)
            ),
        }


# =============================================================================
# Web Integration Factory Functions and Initialization
# =============================================================================

# Global instances
_web_search_extractor: Optional[WebSearchExtractor] = None
_doc_lookup: Optional[DocumentationLookup] = None
_stackoverflow_searcher: Optional[StackOverflowSearcher] = None
_code_example_scraper: Optional[CodeExampleScraper] = None
_ouroboros_web_integration: Optional[OuroborosWebIntegration] = None


def get_web_search_extractor(
    config: Optional[WebSearchConfig] = None,
) -> WebSearchExtractor:
    """Get or create the web search extractor."""
    global _web_search_extractor
    if _web_search_extractor is None:
        _web_search_extractor = WebSearchExtractor(config)
    return _web_search_extractor


def get_doc_lookup(
    search_extractor: Optional[WebSearchExtractor] = None,
    config: Optional[WebSearchConfig] = None,
) -> DocumentationLookup:
    """Get or create the documentation lookup."""
    global _doc_lookup
    if _doc_lookup is None:
        _doc_lookup = DocumentationLookup(
            search_extractor=search_extractor or get_web_search_extractor(config),
            config=config,
        )
    return _doc_lookup


def get_stackoverflow_searcher(
    config: Optional[WebSearchConfig] = None,
) -> StackOverflowSearcher:
    """Get or create the Stack Overflow searcher."""
    global _stackoverflow_searcher
    if _stackoverflow_searcher is None:
        _stackoverflow_searcher = StackOverflowSearcher(config)
    return _stackoverflow_searcher


def get_code_example_scraper(
    config: Optional[WebSearchConfig] = None,
) -> CodeExampleScraper:
    """Get or create the code example scraper."""
    global _code_example_scraper
    if _code_example_scraper is None:
        _code_example_scraper = CodeExampleScraper(config)
    return _code_example_scraper


def get_ouroboros_web_integration(
    oracle: Optional[Any] = None,
    orchestrator: Optional[Any] = None,
    config: Optional[WebSearchConfig] = None,
) -> OuroborosWebIntegration:
    """Get or create the Ouroboros web integration."""
    global _ouroboros_web_integration
    if _ouroboros_web_integration is None:
        _ouroboros_web_integration = OuroborosWebIntegration(
            search_extractor=get_web_search_extractor(config),
            doc_lookup=get_doc_lookup(config=config),
            stackoverflow=get_stackoverflow_searcher(config),
            code_scraper=get_code_example_scraper(config),
            oracle=oracle,
            orchestrator=orchestrator,
            config=config,
        )
    return _ouroboros_web_integration


async def initialize_web_integration(
    oracle: Optional[Any] = None,
    orchestrator: Optional[Any] = None,
    config: Optional[WebSearchConfig] = None,
) -> Dict[str, Any]:
    """
    Initialize all web integration components.

    Args:
        oracle: Oracle codebase knowledge graph
        orchestrator: AgenticLoopOrchestrator for task execution
        config: Web search configuration

    Returns:
        Dictionary with all initialized components
    """
    logger.info("Initializing web integration components...")

    web_integration = get_ouroboros_web_integration(
        oracle=oracle,
        orchestrator=orchestrator,
        config=config,
    )

    await web_integration.initialize()

    components = {
        "web_search_extractor": web_integration.search_extractor,
        "doc_lookup": web_integration.doc_lookup,
        "stackoverflow_searcher": web_integration.stackoverflow,
        "code_example_scraper": web_integration.code_scraper,
        "ouroboros_web_integration": web_integration,
    }

    logger.info(f"Web integration initialized with {len(components)} components")
    return components


async def shutdown_web_integration() -> None:
    """Shutdown all web integration components."""
    logger.info("Shutting down web integration components...")

    global _ouroboros_web_integration

    if _ouroboros_web_integration:
        await _ouroboros_web_integration.shutdown()

    # Reset global instances
    global _web_search_extractor, _doc_lookup, _stackoverflow_searcher, _code_example_scraper
    _web_search_extractor = None
    _doc_lookup = None
    _stackoverflow_searcher = None
    _code_example_scraper = None
    _ouroboros_web_integration = None

    logger.info("Web integration shutdown complete")


# =============================================================================
# v6.0: Complete Autonomous Self-Programming System
# =============================================================================
# This module completes the autonomous self-programming capabilities:
# - CodeSanitizer: Validates and sanitizes scraped code
# - DependencyAutoInstaller: Auto-installs missing packages
# - FileLockManager: Detects user edits and prevents conflicts
# - ReactorCoreFeedbackReceiver: Bidirectional Reactor Core integration
# - PrimeTrainingIntegration: Training feedback to Ironcliw Prime
# - ModelUpdateNotifier: Notifications when models change
# - AutonomousLoopController: Master controller for all loops
# - CrossRepoSyncManager: Syncs state across repos
# =============================================================================


class SecurityRisk(Enum):
    """Security risk levels for code validation."""
    SAFE = "safe"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class DependencySource(Enum):
    """Sources for dependency resolution."""
    PYPI = "pypi"
    CONDA = "conda"
    SYSTEM = "system"
    LOCAL = "local"


@dataclass
class CodeValidationResult:
    """Result of code validation and sanitization."""
    original_code: str
    sanitized_code: str
    is_safe: bool
    risk_level: SecurityRisk
    issues_found: List[Dict[str, Any]]
    fixes_applied: List[str]
    confidence: float
    source_url: Optional[str] = None
    validation_time: float = 0.0


@dataclass
class DependencyInfo:
    """Information about a package dependency."""
    name: str
    version_spec: Optional[str] = None
    source: DependencySource = DependencySource.PYPI
    is_installed: bool = False
    install_command: Optional[str] = None
    alternatives: List[str] = field(default_factory=list)


@dataclass
class FileLockInfo:
    """Information about a file lock."""
    file_path: str
    locked_by: str  # 'user', 'jarvis', 'external'
    locked_at: datetime
    lock_type: str  # 'read', 'write', 'exclusive'
    expires_at: Optional[datetime] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ReactorFeedback:
    """Feedback from Reactor Core training."""
    model_id: str
    training_run_id: str
    metrics: Dict[str, float]  # accuracy, loss, f1, perplexity
    experience_ids: List[str]  # Which experiences contributed
    timestamp: datetime
    status: str  # 'success', 'failed', 'partial'
    insights: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)


@dataclass
class ModelUpdateEvent:
    """Event for model updates."""
    model_id: str
    old_version: Optional[str]
    new_version: str
    update_type: str  # 'new', 'upgrade', 'rollback', 'deprecation'
    capabilities_changed: Dict[str, Any]
    performance_delta: Dict[str, float]
    timestamp: datetime
    source: str  # 'prime', 'reactor', 'external'


class CodeSanitizer:
    """
    Validates and sanitizes code from external sources (web search, GitHub, etc.).

    Security features:
    - Detects dangerous patterns (eval, exec, os.system, etc.)
    - Removes or neutralizes malicious code
    - Validates syntax before accepting
    - Tracks code provenance
    - Applies automatic fixes for common issues
    """

    # Dangerous patterns with their risk levels
    DANGEROUS_PATTERNS: Dict[str, Tuple[str, SecurityRisk, str]] = {
        r'\beval\s*\(': ("eval() execution", SecurityRisk.CRITICAL, "Remove eval()"),
        r'\bexec\s*\(': ("exec() execution", SecurityRisk.CRITICAL, "Remove exec()"),
        r'\bcompile\s*\([^)]*exec': ("compile/exec", SecurityRisk.CRITICAL, "Remove compile/exec"),
        r'__import__\s*\(': ("Dynamic import", SecurityRisk.HIGH, "Use static import"),
        r'os\.system\s*\(': ("Shell command", SecurityRisk.CRITICAL, "Use subprocess safely"),
        r'subprocess\..*shell\s*=\s*True': ("Shell=True", SecurityRisk.HIGH, "Use shell=False"),
        r'pickle\.loads?\s*\(': ("Pickle deserialization", SecurityRisk.HIGH, "Use json instead"),
        r'yaml\.load\s*\([^)]*Loader\s*=\s*None': ("Unsafe YAML", SecurityRisk.HIGH, "Use safe_load"),
        r'yaml\.unsafe_load': ("Unsafe YAML", SecurityRisk.HIGH, "Use safe_load"),
        r'input\s*\([^)]*\)\s*$': ("Raw input", SecurityRisk.MEDIUM, "Validate input"),
        r'requests\.get\([^)]*verify\s*=\s*False': ("SSL disabled", SecurityRisk.MEDIUM, "Enable SSL"),
        r'open\s*\([^)]*,\s*[\'"]w': ("File write", SecurityRisk.LOW, "Validate path"),
        r'shutil\.rmtree': ("Directory deletion", SecurityRisk.MEDIUM, "Add safeguards"),
        r'__builtins__': ("Builtins access", SecurityRisk.HIGH, "Remove builtins access"),
        r'globals\s*\(\s*\)': ("Globals access", SecurityRisk.MEDIUM, "Limit scope"),
        r'locals\s*\(\s*\)': ("Locals access", SecurityRisk.LOW, "Consider alternatives"),
        r'getattr\s*\([^,]+,\s*[^)]*input': ("Dynamic getattr", SecurityRisk.MEDIUM, "Validate attr"),
        r'setattr\s*\(': ("Dynamic setattr", SecurityRisk.MEDIUM, "Validate carefully"),
        r'delattr\s*\(': ("Dynamic delattr", SecurityRisk.MEDIUM, "Validate carefully"),
        r'ctypes\.': ("C types access", SecurityRisk.HIGH, "Review carefully"),
        r'sys\.modules': ("Module manipulation", SecurityRisk.HIGH, "Avoid if possible"),
    }

    # Auto-fix patterns
    AUTO_FIXES: Dict[str, Tuple[str, str]] = {
        r'print\s+([^(])': (r'print(\1)', "Python 2 print to Python 3"),
        r'except\s*:': (r'except Exception:', "Bare except to Exception"),
        r'\.has_key\s*\(': (r' in ', "has_key to 'in'"),
        r'xrange\s*\(': (r'range(', "xrange to range"),
        r'\.iteritems\s*\(': (r'.items(', "iteritems to items"),
        r'\.itervalues\s*\(': (r'.values(', "itervalues to values"),
        r'\.iterkeys\s*\(': (r'.keys(', "iterkeys to keys"),
        r'raw_input\s*\(': (r'input(', "raw_input to input"),
        r'unicode\s*\(': (r'str(', "unicode to str"),
    }

    def __init__(
        self,
        max_code_length: int = 50000,
        allow_medium_risk: bool = True,
        allow_low_risk: bool = True,
        auto_fix: bool = True,
    ):
        self.max_code_length = max_code_length
        self.allow_medium_risk = allow_medium_risk
        self.allow_low_risk = allow_low_risk
        self.auto_fix = auto_fix
        self._validation_cache: Dict[str, CodeValidationResult] = {}
        self._lock = asyncio.Lock()

    async def validate_and_sanitize(
        self,
        code: str,
        source_url: Optional[str] = None,
        context: Optional[str] = None,
    ) -> CodeValidationResult:
        """
        Validate and sanitize code from external source.

        Args:
            code: The code to validate
            source_url: Where the code came from
            context: Additional context about the code

        Returns:
            CodeValidationResult with sanitized code and issues
        """
        start_time = time.monotonic()

        # Check cache
        cache_key = hashlib.sha256(code.encode()).hexdigest()[:16]
        if cache_key in self._validation_cache:
            cached = self._validation_cache[cache_key]
            cached.validation_time = time.monotonic() - start_time
            return cached

        issues_found = []
        fixes_applied = []
        highest_risk = SecurityRisk.SAFE
        sanitized_code = code

        # Check code length
        if len(code) > self.max_code_length:
            issues_found.append({
                "type": "length",
                "message": f"Code exceeds max length ({len(code)} > {self.max_code_length})",
                "risk": SecurityRisk.MEDIUM.value,
            })
            highest_risk = SecurityRisk.MEDIUM

        # Check for dangerous patterns
        for pattern, (description, risk, fix_hint) in self.DANGEROUS_PATTERNS.items():
            matches = re.findall(pattern, sanitized_code, re.IGNORECASE)
            if matches:
                issues_found.append({
                    "type": "dangerous_pattern",
                    "pattern": pattern,
                    "description": description,
                    "risk": risk.value,
                    "fix_hint": fix_hint,
                    "occurrences": len(matches),
                })
                if self._risk_level_value(risk) > self._risk_level_value(highest_risk):
                    highest_risk = risk

                # Remove critical patterns
                if risk == SecurityRisk.CRITICAL:
                    sanitized_code = re.sub(pattern, "# REMOVED: " + description, sanitized_code)
                    fixes_applied.append(f"Removed {description}")

        # Apply auto-fixes if enabled
        if self.auto_fix:
            for pattern, (replacement, description) in self.AUTO_FIXES.items():
                if re.search(pattern, sanitized_code):
                    sanitized_code = re.sub(pattern, replacement, sanitized_code)
                    fixes_applied.append(description)

        # Validate syntax
        syntax_valid, syntax_error = self._validate_syntax(sanitized_code)
        if not syntax_valid:
            issues_found.append({
                "type": "syntax_error",
                "message": str(syntax_error),
                "risk": SecurityRisk.MEDIUM.value,
            })
            if self._risk_level_value(SecurityRisk.MEDIUM) > self._risk_level_value(highest_risk):
                highest_risk = SecurityRisk.MEDIUM

        # Determine if code is safe
        is_safe = self._is_acceptable_risk(highest_risk)

        # Calculate confidence
        confidence = self._calculate_confidence(issues_found, fixes_applied, syntax_valid)

        result = CodeValidationResult(
            original_code=code,
            sanitized_code=sanitized_code,
            is_safe=is_safe,
            risk_level=highest_risk,
            issues_found=issues_found,
            fixes_applied=fixes_applied,
            confidence=confidence,
            source_url=source_url,
            validation_time=time.monotonic() - start_time,
        )

        # Cache result
        async with self._lock:
            if len(self._validation_cache) > 1000:
                # Evict oldest entries
                keys_to_remove = list(self._validation_cache.keys())[:100]
                for key in keys_to_remove:
                    del self._validation_cache[key]
            self._validation_cache[cache_key] = result

        return result

    def _validate_syntax(self, code: str) -> Tuple[bool, Optional[str]]:
        """Validate Python syntax."""
        try:
            ast.parse(code)
            return True, None
        except SyntaxError as e:
            return False, str(e)

    def _risk_level_value(self, risk: SecurityRisk) -> int:
        """Get numeric value for risk level."""
        return {
            SecurityRisk.SAFE: 0,
            SecurityRisk.LOW: 1,
            SecurityRisk.MEDIUM: 2,
            SecurityRisk.HIGH: 3,
            SecurityRisk.CRITICAL: 4,
        }[risk]

    def _is_acceptable_risk(self, risk: SecurityRisk) -> bool:
        """Determine if risk level is acceptable."""
        if risk == SecurityRisk.SAFE:
            return True
        if risk == SecurityRisk.LOW and self.allow_low_risk:
            return True
        if risk == SecurityRisk.MEDIUM and self.allow_medium_risk:
            return True
        return False

    def _calculate_confidence(
        self,
        issues: List[Dict],
        fixes: List[str],
        syntax_valid: bool,
    ) -> float:
        """Calculate confidence score for sanitized code."""
        confidence = 1.0

        # Reduce for issues
        for issue in issues:
            risk = issue.get("risk", "low")
            if risk == "critical":
                confidence -= 0.3
            elif risk == "high":
                confidence -= 0.2
            elif risk == "medium":
                confidence -= 0.1
            elif risk == "low":
                confidence -= 0.05

        # Increase for fixes
        confidence += len(fixes) * 0.02

        # Reduce for syntax errors
        if not syntax_valid:
            confidence -= 0.3

        return max(0.0, min(1.0, confidence))


class DependencyAutoInstaller:
    """
    Automatically detects and installs missing dependencies.

    Features:
    - Parses imports from code
    - Maps imports to package names
    - Checks if packages are installed
    - Auto-installs using pip/conda
    - Supports virtual environments
    - Caches package availability
    """

    # Import name to package name mapping (for non-obvious mappings)
    IMPORT_TO_PACKAGE: Dict[str, str] = {
        "PIL": "Pillow",
        "cv2": "opencv-python",
        "sklearn": "scikit-learn",
        "yaml": "PyYAML",
        "bs4": "beautifulsoup4",
        "dotenv": "python-dotenv",
        "jwt": "PyJWT",
        "dateutil": "python-dateutil",
        "magic": "python-magic",
        "Crypto": "pycryptodome",
        "OpenSSL": "pyOpenSSL",
        "wx": "wxPython",
        "gi": "PyGObject",
        "skimage": "scikit-image",
        "usb": "pyusb",
        "serial": "pyserial",
    }

    # Standard library modules (should not be installed)
    STDLIB_MODULES: Set[str] = {
        "abc", "aifc", "argparse", "array", "ast", "asynchat", "asyncio",
        "asyncore", "atexit", "audioop", "base64", "bdb", "binascii",
        "binhex", "bisect", "builtins", "bz2", "calendar", "cgi", "cgitb",
        "chunk", "cmath", "cmd", "code", "codecs", "codeop", "collections",
        "colorsys", "compileall", "concurrent", "configparser", "contextlib",
        "contextvars", "copy", "copyreg", "cProfile", "crypt", "csv",
        "ctypes", "curses", "dataclasses", "datetime", "dbm", "decimal",
        "difflib", "dis", "distutils", "doctest", "email", "encodings",
        "enum", "errno", "faulthandler", "fcntl", "filecmp", "fileinput",
        "fnmatch", "fractions", "ftplib", "functools", "gc", "getopt",
        "getpass", "gettext", "glob", "graphlib", "grp", "gzip", "hashlib",
        "heapq", "hmac", "html", "http", "imaplib", "imghdr", "imp",
        "importlib", "inspect", "io", "ipaddress", "itertools", "json",
        "keyword", "lib2to3", "linecache", "locale", "logging", "lzma",
        "mailbox", "mailcap", "marshal", "math", "mimetypes", "mmap",
        "modulefinder", "multiprocessing", "netrc", "nis", "nntplib",
        "numbers", "operator", "optparse", "os", "ossaudiodev", "pathlib",
        "pdb", "pickle", "pickletools", "pipes", "pkgutil", "platform",
        "plistlib", "poplib", "posix", "posixpath", "pprint", "profile",
        "pstats", "pty", "pwd", "py_compile", "pyclbr", "pydoc", "queue",
        "quopri", "random", "re", "readline", "reprlib", "resource",
        "rlcompleter", "runpy", "sched", "secrets", "select", "selectors",
        "shelve", "shlex", "shutil", "signal", "site", "smtpd", "smtplib",
        "sndhdr", "socket", "socketserver", "spwd", "sqlite3", "ssl",
        "stat", "statistics", "string", "stringprep", "struct", "subprocess",
        "sunau", "symtable", "sys", "sysconfig", "syslog", "tabnanny",
        "tarfile", "telnetlib", "tempfile", "termios", "test", "textwrap",
        "threading", "time", "timeit", "tkinter", "token", "tokenize",
        "trace", "traceback", "tracemalloc", "tty", "turtle", "turtledemo",
        "types", "typing", "unicodedata", "unittest", "urllib", "uu",
        "uuid", "venv", "warnings", "wave", "weakref", "webbrowser",
        "winreg", "winsound", "wsgiref", "xdrlib", "xml", "xmlrpc",
        "zipapp", "zipfile", "zipimport", "zlib", "_thread",
    }

    def __init__(
        self,
        auto_install: bool = True,
        use_venv: bool = True,
        max_concurrent_installs: int = 3,
    ):
        self.auto_install = auto_install
        self.use_venv = use_venv
        self.max_concurrent_installs = max_concurrent_installs
        self._install_semaphore = asyncio.Semaphore(max_concurrent_installs)
        self._installed_cache: Set[str] = set()
        self._failed_cache: Set[str] = set()
        self._lock = asyncio.Lock()

    async def analyze_and_install(
        self,
        code: str,
        dry_run: bool = False,
    ) -> Dict[str, Any]:
        """
        Analyze code for imports and install missing dependencies.

        Args:
            code: The code to analyze
            dry_run: If True, don't actually install

        Returns:
            Dictionary with analysis results
        """
        result = {
            "imports_found": [],
            "missing_packages": [],
            "installed_packages": [],
            "failed_packages": [],
            "skipped_packages": [],
        }

        # Parse imports
        imports = self._extract_imports(code)
        result["imports_found"] = imports

        # Check which are missing
        for import_name in imports:
            # Skip stdlib
            root_module = import_name.split(".")[0]
            if root_module in self.STDLIB_MODULES:
                continue

            # Map to package name
            package_name = self.IMPORT_TO_PACKAGE.get(root_module, root_module)

            # Check if installed
            is_installed = await self._check_installed(package_name)
            if is_installed:
                continue

            # Check if already failed
            if package_name in self._failed_cache:
                result["skipped_packages"].append({
                    "name": package_name,
                    "reason": "Previously failed",
                })
                continue

            result["missing_packages"].append(package_name)

            # Install if enabled
            if self.auto_install and not dry_run:
                success = await self._install_package(package_name)
                if success:
                    result["installed_packages"].append(package_name)
                else:
                    result["failed_packages"].append(package_name)

        return result

    def _extract_imports(self, code: str) -> List[str]:
        """Extract import names from code."""
        imports = set()

        try:
            tree = ast.parse(code)
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.add(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        imports.add(node.module)
        except SyntaxError:
            # Fallback to regex
            import_pattern = r'^(?:from\s+(\S+)\s+)?import\s+(\S+)'
            for match in re.finditer(import_pattern, code, re.MULTILINE):
                if match.group(1):
                    imports.add(match.group(1))
                if match.group(2):
                    imports.add(match.group(2).split(",")[0].strip())

        return list(imports)

    async def _check_installed(self, package_name: str) -> bool:
        """Check if a package is installed."""
        if package_name in self._installed_cache:
            return True

        try:
            # Use importlib to check
            import importlib.util
            spec = importlib.util.find_spec(package_name.replace("-", "_"))
            if spec is not None:
                async with self._lock:
                    self._installed_cache.add(package_name)
                return True
        except (ImportError, ModuleNotFoundError, ValueError):
            pass

        # Try pip show
        try:
            proc = await asyncio.create_subprocess_exec(
                sys.executable, "-m", "pip", "show", package_name,
                stdout=asyncio.subprocess.DEVNULL,
                stderr=asyncio.subprocess.DEVNULL,
            )
            await proc.wait()
            if proc.returncode == 0:
                async with self._lock:
                    self._installed_cache.add(package_name)
                return True
        except Exception:
            pass

        return False

    async def _install_package(self, package_name: str) -> bool:
        """Install a package using pip."""
        async with self._install_semaphore:
            logger.info(f"Installing missing package: {package_name}")

            try:
                proc = await asyncio.create_subprocess_exec(
                    sys.executable, "-m", "pip", "install", package_name,
                    "--quiet", "--disable-pip-version-check",
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )
                stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=120)

                if proc.returncode == 0:
                    async with self._lock:
                        self._installed_cache.add(package_name)
                    logger.info(f"Successfully installed: {package_name}")
                    return True
                else:
                    async with self._lock:
                        self._failed_cache.add(package_name)
                    logger.warning(f"Failed to install {package_name}: {stderr.decode()}")
                    return False

            except asyncio.TimeoutError:
                logger.warning(f"Timeout installing {package_name}")
                async with self._lock:
                    self._failed_cache.add(package_name)
                return False
            except Exception as e:
                logger.warning(f"Error installing {package_name}: {e}")
                async with self._lock:
                    self._failed_cache.add(package_name)
                return False


class FileLockManager:
    """
    Manages file locks to prevent conflicts during autonomous editing.

    Features:
    - Detects when user is editing a file
    - Monitors file modification times
    - Uses file system events (watchdog-like)
    - Prevents Ironcliw from editing files user is modifying
    - Supports lock acquisition with timeout
    - Handles stale locks
    """

    def __init__(
        self,
        lock_timeout: float = 300.0,  # 5 minutes
        stale_threshold: float = 600.0,  # 10 minutes
        check_interval: float = 1.0,
    ):
        self.lock_timeout = lock_timeout
        self.stale_threshold = stale_threshold
        self.check_interval = check_interval
        self._locks: Dict[str, FileLockInfo] = {}
        self._file_mtimes: Dict[str, float] = {}
        self._lock = asyncio.Lock()
        self._monitoring = False
        self._monitor_task: Optional[asyncio.Task] = None

    async def start_monitoring(self) -> None:
        """Start monitoring files for changes."""
        if self._monitoring:
            return

        self._monitoring = True
        self._monitor_task = asyncio.create_task(self._monitor_loop())
        logger.info("FileLockManager monitoring started")

    async def stop_monitoring(self) -> None:
        """Stop monitoring files."""
        self._monitoring = False
        if self._monitor_task:
            self._monitor_task.cancel()
            try:
                await self._monitor_task
            except asyncio.CancelledError:
                pass
        logger.info("FileLockManager monitoring stopped")

    async def acquire_lock(
        self,
        file_path: str,
        lock_type: str = "write",
        timeout: Optional[float] = None,
    ) -> bool:
        """
        Acquire a lock on a file.

        Args:
            file_path: Path to the file
            lock_type: Type of lock ('read', 'write', 'exclusive')
            timeout: How long to wait for lock

        Returns:
            True if lock acquired, False otherwise
        """
        timeout = timeout or self.lock_timeout
        abs_path = os.path.abspath(file_path)
        start_time = time.monotonic()

        while time.monotonic() - start_time < timeout:
            async with self._lock:
                existing = self._locks.get(abs_path)

                # Check if locked by user
                if existing and existing.locked_by == "user":
                    # User is editing, don't interfere
                    await asyncio.sleep(self.check_interval)
                    continue

                # Check for stale lock
                if existing:
                    if self._is_stale_lock(existing):
                        del self._locks[abs_path]
                        existing = None

                # Check if we can acquire
                if existing is None or (
                    existing.lock_type == "read" and lock_type == "read"
                ):
                    # Record current mtime
                    try:
                        self._file_mtimes[abs_path] = os.path.getmtime(abs_path)
                    except OSError:
                        self._file_mtimes[abs_path] = 0

                    self._locks[abs_path] = FileLockInfo(
                        file_path=abs_path,
                        locked_by="jarvis",
                        locked_at=datetime.now(),
                        lock_type=lock_type,
                        expires_at=datetime.now() + timedelta(seconds=self.lock_timeout),
                    )
                    return True

            await asyncio.sleep(self.check_interval)

        return False

    async def release_lock(self, file_path: str) -> None:
        """Release a lock on a file."""
        abs_path = os.path.abspath(file_path)

        async with self._lock:
            if abs_path in self._locks:
                lock_info = self._locks[abs_path]
                if lock_info.locked_by == "jarvis":
                    del self._locks[abs_path]

    async def is_file_locked_by_user(self, file_path: str) -> bool:
        """Check if a file is being edited by user."""
        abs_path = os.path.abspath(file_path)

        async with self._lock:
            # Check explicit lock
            lock_info = self._locks.get(abs_path)
            if lock_info and lock_info.locked_by == "user":
                return True

            # Check if file was modified since we last saw it
            try:
                current_mtime = os.path.getmtime(abs_path)
                last_mtime = self._file_mtimes.get(abs_path, 0)

                if current_mtime > last_mtime:
                    # File changed externally, assume user edit
                    self._locks[abs_path] = FileLockInfo(
                        file_path=abs_path,
                        locked_by="user",
                        locked_at=datetime.now(),
                        lock_type="write",
                        expires_at=datetime.now() + timedelta(seconds=60),
                    )
                    self._file_mtimes[abs_path] = current_mtime
                    return True

            except OSError:
                pass

        return False

    async def _monitor_loop(self) -> None:
        """Background loop to monitor files."""
        while self._monitoring:
            try:
                await self._cleanup_stale_locks()
                await asyncio.sleep(self.check_interval * 5)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.debug(f"File monitor error: {e}")

    async def _cleanup_stale_locks(self) -> None:
        """Clean up stale locks."""
        async with self._lock:
            stale_paths = [
                path for path, info in self._locks.items()
                if self._is_stale_lock(info)
            ]
            for path in stale_paths:
                del self._locks[path]

    def _is_stale_lock(self, lock_info: FileLockInfo) -> bool:
        """Check if a lock is stale."""
        if lock_info.expires_at and datetime.now() > lock_info.expires_at:
            return True

        age = (datetime.now() - lock_info.locked_at).total_seconds()
        return age > self.stale_threshold

    def get_locked_files(self) -> List[FileLockInfo]:
        """Get list of currently locked files."""
        return list(self._locks.values())


class ReactorCoreFeedbackReceiver:
    """
    Receives feedback from Reactor Core training pipeline.

    Creates a bidirectional loop:
    Ironcliw → Experiences → Reactor Core → Training → Feedback → Ironcliw

    Features:
    - Watches for MODEL_READY events
    - Processes training metrics
    - Updates experience quality scores
    - Triggers model hot-swap
    - Learns from training results
    """

    def __init__(
        self,
        events_dir: Optional[Path] = None,
        poll_interval: float = 5.0,
    ):
        self.events_dir = events_dir or Path.home() / ".jarvis" / "reactor" / "events"
        self.poll_interval = poll_interval
        self._running = False
        self._poll_task: Optional[asyncio.Task] = None
        self._processed_events: Set[str] = set()
        self._feedback_handlers: List[Callable] = []
        self._model_ready_handlers: List[Callable] = []
        self._metrics: Dict[str, Any] = {
            "events_received": 0,
            "feedback_processed": 0,
            "models_updated": 0,
        }

    async def start(self) -> None:
        """Start receiving feedback from Reactor Core."""
        if self._running:
            return

        # Ensure events directory exists
        self.events_dir.mkdir(parents=True, exist_ok=True)

        self._running = True
        self._poll_task = asyncio.create_task(self._poll_loop())
        logger.info(f"ReactorCoreFeedbackReceiver started, watching: {self.events_dir}")

    async def stop(self) -> None:
        """Stop receiving feedback."""
        self._running = False
        if self._poll_task:
            self._poll_task.cancel()
            try:
                await self._poll_task
            except asyncio.CancelledError:
                pass
        logger.info("ReactorCoreFeedbackReceiver stopped")

    def register_feedback_handler(self, handler: Callable) -> None:
        """Register handler for training feedback."""
        self._feedback_handlers.append(handler)

    def register_model_ready_handler(self, handler: Callable) -> None:
        """Register handler for model ready events."""
        self._model_ready_handlers.append(handler)

    async def _poll_loop(self) -> None:
        """Poll for new events from Reactor Core."""
        while self._running:
            try:
                await self._process_events()
                await asyncio.sleep(self.poll_interval)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.debug(f"Reactor feedback poll error: {e}")
                await asyncio.sleep(self.poll_interval * 2)

    async def _process_events(self) -> None:
        """Process new events in the events directory."""
        if not self.events_dir.exists():
            return

        for event_file in self.events_dir.glob("*.json"):
            event_id = event_file.stem

            if event_id in self._processed_events:
                continue

            try:
                content = await asyncio.to_thread(event_file.read_text)
                event_data = json.loads(content)

                self._metrics["events_received"] += 1
                self._processed_events.add(event_id)

                await self._handle_event(event_data)

                # Remove processed event
                await asyncio.to_thread(event_file.unlink, missing_ok=True)

            except Exception as e:
                logger.debug(f"Error processing Reactor event {event_id}: {e}")

    async def _handle_event(self, event_data: Dict[str, Any]) -> None:
        """Handle a Reactor Core event."""
        event_type = event_data.get("type", "")

        if event_type == "MODEL_READY":
            await self._handle_model_ready(event_data)
        elif event_type == "TRAINING_COMPLETE":
            await self._handle_training_complete(event_data)
        elif event_type == "TRAINING_FEEDBACK":
            await self._handle_training_feedback(event_data)
        elif event_type == "EXPERIENCE_ACK":
            await self._handle_experience_ack(event_data)

    async def _handle_model_ready(self, event_data: Dict[str, Any]) -> None:
        """Handle MODEL_READY event."""
        model_id = event_data.get("model_id", "")
        version = event_data.get("version", "")
        metrics = event_data.get("metrics", {})

        logger.info(f"Reactor Core model ready: {model_id} v{version}")

        update_event = ModelUpdateEvent(
            model_id=model_id,
            old_version=None,
            new_version=version,
            update_type="new",
            capabilities_changed=event_data.get("capabilities", {}),
            performance_delta=metrics,
            timestamp=datetime.now(),
            source="reactor",
        )

        self._metrics["models_updated"] += 1

        for handler in self._model_ready_handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler(update_event)
                else:
                    handler(update_event)
            except Exception as e:
                logger.debug(f"Model ready handler error: {e}")

    async def _handle_training_complete(self, event_data: Dict[str, Any]) -> None:
        """Handle TRAINING_COMPLETE event."""
        training_run_id = event_data.get("run_id", "")
        metrics = event_data.get("metrics", {})
        experience_ids = event_data.get("experience_ids", [])

        feedback = ReactorFeedback(
            model_id=event_data.get("model_id", ""),
            training_run_id=training_run_id,
            metrics=metrics,
            experience_ids=experience_ids,
            timestamp=datetime.now(),
            status="success" if metrics.get("loss", 1) < 0.5 else "partial",
            insights=event_data.get("insights", []),
            recommendations=event_data.get("recommendations", []),
        )

        self._metrics["feedback_processed"] += 1

        for handler in self._feedback_handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler(feedback)
                else:
                    handler(feedback)
            except Exception as e:
                logger.debug(f"Feedback handler error: {e}")

    async def _handle_training_feedback(self, event_data: Dict[str, Any]) -> None:
        """Handle TRAINING_FEEDBACK event (detailed metrics)."""
        feedback = ReactorFeedback(
            model_id=event_data.get("model_id", ""),
            training_run_id=event_data.get("run_id", ""),
            metrics=event_data.get("metrics", {}),
            experience_ids=event_data.get("experience_ids", []),
            timestamp=datetime.now(),
            status=event_data.get("status", "unknown"),
            insights=event_data.get("insights", []),
            recommendations=event_data.get("recommendations", []),
        )

        self._metrics["feedback_processed"] += 1

        for handler in self._feedback_handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler(feedback)
                else:
                    handler(feedback)
            except Exception as e:
                logger.debug(f"Training feedback handler error: {e}")

    async def _handle_experience_ack(self, event_data: Dict[str, Any]) -> None:
        """Handle EXPERIENCE_ACK event (confirmation of experience receipt)."""
        experience_ids = event_data.get("experience_ids", [])
        logger.debug(f"Reactor Core acknowledged {len(experience_ids)} experiences")

    def get_metrics(self) -> Dict[str, Any]:
        """Get receiver metrics."""
        return self._metrics.copy()


class PrimeTrainingIntegration:
    """
    Integrates Ironcliw improvements with Ironcliw Prime training.

    Creates training feedback loop:
    Ironcliw Improvements → Training Data → Prime Training → Updated Models

    Features:
    - Formats improvements as training examples
    - Batches training data
    - Triggers Prime retraining
    - Monitors training progress
    - Validates new models
    """

    def __init__(
        self,
        prime_api_base: Optional[str] = None,
        batch_size: int = 100,
        min_quality_score: float = 0.7,
    ):
        self.prime_api_base = prime_api_base or os.getenv(
            "Ironcliw_PRIME_API_BASE",
            "http://localhost:8080"
        )
        self.batch_size = batch_size
        self.min_quality_score = min_quality_score
        self._training_queue: List[Dict[str, Any]] = []
        self._session: Optional[aiohttp.ClientSession] = None
        self._lock = asyncio.Lock()
        self._metrics = {
            "examples_queued": 0,
            "batches_sent": 0,
            "training_triggers": 0,
        }

    async def initialize(self) -> None:
        """Initialize HTTP session."""
        if self._session is None:
            self._session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=60),
            )

    async def shutdown(self) -> None:
        """Cleanup resources."""
        if self._session:
            await self._session.close()
            self._session = None

    async def record_improvement(
        self,
        original_code: str,
        improved_code: str,
        improvement_type: str,
        success: bool,
        quality_score: float,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """
        Record an improvement for Prime training.

        Args:
            original_code: Original code before improvement
            improved_code: Improved code after changes
            improvement_type: Type of improvement (refactor, bugfix, etc.)
            success: Whether the improvement was successful
            quality_score: Quality score of the improvement
            metadata: Additional metadata
        """
        if not success or quality_score < self.min_quality_score:
            return

        example = {
            "input": original_code,
            "output": improved_code,
            "type": improvement_type,
            "quality": quality_score,
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {},
        }

        async with self._lock:
            self._training_queue.append(example)
            self._metrics["examples_queued"] += 1

            if len(self._training_queue) >= self.batch_size:
                await self._flush_batch()

    async def _flush_batch(self) -> None:
        """Send batch to Prime for training."""
        if not self._training_queue:
            return

        await self.initialize()

        batch = self._training_queue[:self.batch_size]
        self._training_queue = self._training_queue[self.batch_size:]

        try:
            async with self._session.post(
                f"{self.prime_api_base}/v1/training/batch",
                json={"examples": batch},
            ) as response:
                if response.status == 200:
                    self._metrics["batches_sent"] += 1
                    logger.info(f"Sent {len(batch)} training examples to Prime")
                else:
                    # Re-queue on failure
                    self._training_queue = batch + self._training_queue
                    logger.warning(f"Failed to send batch to Prime: {response.status}")

        except Exception as e:
            # Re-queue on error
            self._training_queue = batch + self._training_queue
            logger.debug(f"Error sending batch to Prime: {e}")

    async def trigger_training(
        self,
        model_id: Optional[str] = None,
        priority: str = "normal",
    ) -> Optional[str]:
        """
        Trigger Prime to start training.

        Args:
            model_id: Specific model to train (or default)
            priority: Training priority (low, normal, high)

        Returns:
            Training run ID if triggered, None otherwise
        """
        await self.initialize()

        # Flush any pending batches first
        await self._flush_batch()

        try:
            async with self._session.post(
                f"{self.prime_api_base}/v1/training/trigger",
                json={
                    "model_id": model_id,
                    "priority": priority,
                },
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    self._metrics["training_triggers"] += 1
                    return data.get("run_id")
                else:
                    logger.warning(f"Failed to trigger Prime training: {response.status}")
                    return None

        except Exception as e:
            logger.debug(f"Error triggering Prime training: {e}")
            return None

    async def get_training_status(self, run_id: str) -> Optional[Dict[str, Any]]:
        """Get status of a training run."""
        await self.initialize()

        try:
            async with self._session.get(
                f"{self.prime_api_base}/v1/training/status/{run_id}",
            ) as response:
                if response.status == 200:
                    return await response.json()
        except Exception as e:
            logger.debug(f"Error getting training status: {e}")

        return None

    def get_metrics(self) -> Dict[str, Any]:
        """Get integration metrics."""
        return {
            **self._metrics,
            "queue_size": len(self._training_queue),
        }


class ModelUpdateNotifier:
    """
    Notifies system components when models are updated.

    Channels:
    - Internal event bus
    - WebSocket broadcast
    - Voice announcement (optional)
    - Log notification

    Features:
    - Multi-channel notification
    - Deduplication
    - Priority levels
    - User preferences
    """

    def __init__(
        self,
        enable_voice: bool = False,
        enable_websocket: bool = True,
        cooldown_seconds: float = 60.0,
    ):
        self.enable_voice = enable_voice
        self.enable_websocket = enable_websocket
        self.cooldown_seconds = cooldown_seconds
        self._last_notifications: Dict[str, datetime] = {}
        self._handlers: List[Callable] = []
        self._lock = asyncio.Lock()

    def register_handler(self, handler: Callable) -> None:
        """Register notification handler."""
        self._handlers.append(handler)

    async def notify_model_update(
        self,
        event: ModelUpdateEvent,
        priority: str = "normal",
    ) -> bool:
        """
        Notify about a model update.

        Args:
            event: The model update event
            priority: Notification priority (low, normal, high, critical)

        Returns:
            True if notification sent, False if deduplicated
        """
        # Check cooldown
        key = f"{event.model_id}:{event.new_version}"
        async with self._lock:
            last = self._last_notifications.get(key)
            if last and (datetime.now() - last).total_seconds() < self.cooldown_seconds:
                return False  # Deduplicated

            self._last_notifications[key] = datetime.now()

        # Format message
        message = self._format_message(event)

        # Notify handlers
        for handler in self._handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler(event, message, priority)
                else:
                    handler(event, message, priority)
            except Exception as e:
                logger.debug(f"Notification handler error: {e}")

        # Log notification
        log_level = {
            "low": logging.DEBUG,
            "normal": logging.INFO,
            "high": logging.WARNING,
            "critical": logging.ERROR,
        }.get(priority, logging.INFO)

        logger.log(log_level, f"Model Update: {message}")

        return True

    def _format_message(self, event: ModelUpdateEvent) -> str:
        """Format notification message."""
        type_verbs = {
            "new": "is now available",
            "upgrade": "has been upgraded",
            "rollback": "has been rolled back",
            "deprecation": "has been deprecated",
        }

        verb = type_verbs.get(event.update_type, "was updated")

        parts = [f"Model {event.model_id} {verb} to version {event.new_version}"]

        if event.performance_delta:
            if "accuracy" in event.performance_delta:
                delta = event.performance_delta["accuracy"]
                parts.append(f"Accuracy: {delta:+.1%}")

        return ". ".join(parts)


class AutonomousLoopController:
    """
    Master controller for all autonomous self-programming loops.

    Manages:
    - GoalDecompositionEngine
    - TechnicalDebtDetector
    - AutonomousSelfRefinementLoop
    - SystemFeedbackLoop
    - ReactorCoreFeedbackReceiver

    Features:
    - Coordinated start/stop
    - Health monitoring
    - Load balancing
    - Emergency shutdown
    - Status reporting
    """

    def __init__(self):
        self._components: Dict[str, Any] = {}
        self._running = False
        self._health_task: Optional[asyncio.Task] = None
        self._lock = asyncio.Lock()
        self._status = "stopped"
        self._metrics = {
            "start_time": None,
            "cycles_completed": 0,
            "errors": 0,
            "last_health_check": None,
        }

    async def register_component(
        self,
        name: str,
        component: Any,
        auto_start: bool = True,
    ) -> None:
        """Register a component for management."""
        async with self._lock:
            self._components[name] = {
                "instance": component,
                "auto_start": auto_start,
                "status": "registered",
                "last_error": None,
            }

    async def start_all(self) -> Dict[str, str]:
        """Start all registered components."""
        if self._running:
            return {"status": "already_running"}

        self._running = True
        self._status = "starting"
        self._metrics["start_time"] = datetime.now().isoformat()

        results = {}

        async with self._lock:
            for name, info in self._components.items():
                if not info["auto_start"]:
                    results[name] = "skipped"
                    continue

                component = info["instance"]

                try:
                    if hasattr(component, "start"):
                        await component.start()
                        info["status"] = "running"
                        results[name] = "started"
                    elif hasattr(component, "initialize"):
                        await component.initialize()
                        info["status"] = "initialized"
                        results[name] = "initialized"
                    else:
                        info["status"] = "ready"
                        results[name] = "ready"

                except Exception as e:
                    info["status"] = "error"
                    info["last_error"] = str(e)
                    results[name] = f"error: {e}"
                    self._metrics["errors"] += 1

        # Start health monitoring
        self._health_task = asyncio.create_task(self._health_monitor())
        self._status = "running"

        logger.info(f"AutonomousLoopController started: {results}")
        return results

    async def stop_all(self) -> Dict[str, str]:
        """Stop all components gracefully."""
        if not self._running:
            return {"status": "not_running"}

        self._running = False
        self._status = "stopping"

        # Stop health monitor
        if self._health_task:
            self._health_task.cancel()
            try:
                await self._health_task
            except asyncio.CancelledError:
                pass

        results = {}

        async with self._lock:
            for name, info in self._components.items():
                component = info["instance"]

                try:
                    if hasattr(component, "stop"):
                        await component.stop()
                    elif hasattr(component, "shutdown"):
                        await component.shutdown()

                    info["status"] = "stopped"
                    results[name] = "stopped"

                except Exception as e:
                    info["status"] = "error"
                    results[name] = f"error: {e}"

        self._status = "stopped"
        logger.info(f"AutonomousLoopController stopped: {results}")
        return results

    async def _health_monitor(self) -> None:
        """Monitor health of all components."""
        while self._running:
            try:
                self._metrics["last_health_check"] = datetime.now().isoformat()

                async with self._lock:
                    for name, info in self._components.items():
                        component = info["instance"]

                        if hasattr(component, "get_status"):
                            try:
                                status = component.get_status()
                                info["last_status"] = status
                            except Exception as e:
                                info["last_error"] = str(e)

                self._metrics["cycles_completed"] += 1
                await asyncio.sleep(30)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.debug(f"Health monitor error: {e}")
                await asyncio.sleep(60)

    def get_status(self) -> Dict[str, Any]:
        """Get comprehensive status."""
        return {
            "status": self._status,
            "running": self._running,
            "metrics": self._metrics.copy(),
            "components": {
                name: {
                    "status": info["status"],
                    "last_error": info.get("last_error"),
                }
                for name, info in self._components.items()
            },
        }


class CrossRepoSyncManager:
    """
    Manages state synchronization across Ironcliw, Prime, and Reactor Core.

    Features:
    - Heartbeat monitoring across repos
    - State propagation
    - Conflict resolution
    - Event broadcasting
    - Recovery coordination
    """

    def __init__(
        self,
        jarvis_path: Optional[Path] = None,
        prime_path: Optional[Path] = None,
        reactor_path: Optional[Path] = None,
    ):
        self.jarvis_path = jarvis_path or Path(os.getenv(
            "Ironcliw_PATH",
            Path.home() / "Documents/repos/Ironcliw-AI-Agent"
        ))
        self.prime_path = prime_path or Path(os.getenv(
            "Ironcliw_PRIME_PATH",
            Path.home() / "Documents/repos/jarvis-prime"
        ))
        self.reactor_path = reactor_path or Path(os.getenv(
            "REACTOR_CORE_PATH",
            Path.home() / "Documents/repos/reactor-core"
        ))

        self._sync_dir = Path.home() / ".jarvis" / "cross_repo" / "sync"
        self._running = False
        self._sync_task: Optional[asyncio.Task] = None
        self._state: Dict[str, Any] = {
            "jarvis": {"status": "unknown", "last_seen": None},
            "prime": {"status": "unknown", "last_seen": None},
            "reactor": {"status": "unknown", "last_seen": None},
        }
        self._event_handlers: Dict[str, List[Callable]] = defaultdict(list)
        self._registered_repos: Dict[str, Path] = {}
        self._heartbeat_running = False
        self._heartbeat_task: Optional[asyncio.Task] = None

    async def register_repo(self, repo_name: str, repo_path: str) -> bool:
        """
        v93.0: Register a repository for cross-repo synchronization.

        This enables the sync manager to monitor and coordinate with
        the specified repository.

        Args:
            repo_name: Name identifier for the repo (e.g., 'jarvis', 'prime', 'reactor')
            repo_path: Filesystem path to the repository

        Returns:
            True if registration successful
        """
        path = Path(repo_path)

        if not path.exists():
            logger.warning(f"[CrossRepoSyncManager] Repo path does not exist: {repo_path}")
            return False

        self._registered_repos[repo_name] = path

        # Initialize state for this repo
        if repo_name not in self._state:
            self._state[repo_name] = {
                "status": "registered",
                "last_seen": time.time(),
                "path": str(path),
            }
        else:
            self._state[repo_name]["status"] = "registered"
            self._state[repo_name]["path"] = str(path)
            self._state[repo_name]["last_seen"] = time.time()

        logger.info(f"[CrossRepoSyncManager] Registered repo: {repo_name} at {repo_path}")
        return True

    async def start_heartbeat(self) -> None:
        """
        v93.0: Start heartbeat monitoring for registered repositories.

        This initiates periodic health checks across all registered repos,
        enabling automatic detection of disconnected or unhealthy services.
        """
        if self._heartbeat_running:
            logger.debug("[CrossRepoSyncManager] Heartbeat already running")
            return

        self._heartbeat_running = True
        self._heartbeat_task = asyncio.create_task(self._heartbeat_loop())
        logger.info("[CrossRepoSyncManager] Heartbeat monitoring started")

    async def stop_heartbeat(self) -> None:
        """v93.0: Stop heartbeat monitoring."""
        self._heartbeat_running = False
        if self._heartbeat_task:
            self._heartbeat_task.cancel()
            try:
                await self._heartbeat_task
            except asyncio.CancelledError:
                pass
            self._heartbeat_task = None
        logger.info("[CrossRepoSyncManager] Heartbeat monitoring stopped")

    async def _heartbeat_loop(self) -> None:
        """v93.0: Internal heartbeat monitoring loop."""
        while self._heartbeat_running:
            try:
                now = time.time()

                for repo_name, repo_path in self._registered_repos.items():
                    # Check if repo is accessible
                    if repo_path.exists():
                        self._state[repo_name]["status"] = "healthy"
                        self._state[repo_name]["last_seen"] = now
                    else:
                        self._state[repo_name]["status"] = "unreachable"

                    # Check for heartbeat file (if repos write their own)
                    heartbeat_file = repo_path / ".jarvis" / "heartbeat.json"
                    if heartbeat_file.exists():
                        try:
                            data = json.loads(heartbeat_file.read_text())
                            last_beat = data.get("timestamp", 0)
                            if now - last_beat < 60:  # Within 1 minute
                                self._state[repo_name]["status"] = "active"
                        except Exception:
                            pass

                await asyncio.sleep(10)  # Check every 10 seconds

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.debug(f"[CrossRepoSyncManager] Heartbeat error: {e}")
                await asyncio.sleep(10)

    def get_registered_repos(self) -> Dict[str, str]:
        """v93.0: Get all registered repositories."""
        return {name: str(path) for name, path in self._registered_repos.items()}

    async def start(self) -> None:
        """Start cross-repo synchronization."""
        if self._running:
            return

        self._sync_dir.mkdir(parents=True, exist_ok=True)
        self._running = True
        self._sync_task = asyncio.create_task(self._sync_loop())
        logger.info("CrossRepoSyncManager started")

    async def stop(self) -> None:
        """Stop synchronization."""
        self._running = False
        if self._sync_task:
            self._sync_task.cancel()
            try:
                await self._sync_task
            except asyncio.CancelledError:
                pass
        logger.info("CrossRepoSyncManager stopped")

    def register_event_handler(self, event_type: str, handler: Callable) -> None:
        """Register handler for cross-repo events."""
        self._event_handlers[event_type].append(handler)

    async def broadcast_event(
        self,
        event_type: str,
        payload: Dict[str, Any],
        targets: Optional[List[str]] = None,
    ) -> None:
        """Broadcast an event to other repos."""
        targets = targets or ["prime", "reactor"]

        event = {
            "type": event_type,
            "source": "jarvis",
            "timestamp": datetime.now().isoformat(),
            "payload": payload,
        }

        event_file = self._sync_dir / f"{event_type}_{int(time.time() * 1000)}.json"

        await asyncio.to_thread(
            event_file.write_text,
            json.dumps(event, indent=2)
        )

    async def get_repo_status(self, repo: str) -> Dict[str, Any]:
        """Get status of a specific repo."""
        return self._state.get(repo, {"status": "unknown"})

    async def _sync_loop(self) -> None:
        """Main synchronization loop."""
        while self._running:
            try:
                # Update heartbeats
                await self._send_heartbeat()

                # Check other repos
                await self._check_repos()

                # Process incoming events
                await self._process_events()

                await asyncio.sleep(5)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.debug(f"Sync loop error: {e}")
                await asyncio.sleep(10)

    async def _send_heartbeat(self) -> None:
        """Send heartbeat to sync directory."""
        heartbeat = {
            "source": "jarvis",
            "timestamp": datetime.now().isoformat(),
            "status": "running",
        }

        heartbeat_file = self._sync_dir / "jarvis_heartbeat.json"
        await asyncio.to_thread(
            heartbeat_file.write_text,
            json.dumps(heartbeat)
        )

    async def _check_repos(self) -> None:
        """Check status of other repos."""
        for repo in ["prime", "reactor"]:
            heartbeat_file = self._sync_dir / f"{repo}_heartbeat.json"

            try:
                if heartbeat_file.exists():
                    content = await asyncio.to_thread(heartbeat_file.read_text)
                    data = json.loads(content)

                    last_time = datetime.fromisoformat(data["timestamp"])
                    age = (datetime.now() - last_time).total_seconds()

                    if age < 30:
                        self._state[repo] = {
                            "status": "online",
                            "last_seen": data["timestamp"],
                        }
                    elif age < 120:
                        self._state[repo] = {
                            "status": "stale",
                            "last_seen": data["timestamp"],
                        }
                    else:
                        self._state[repo] = {
                            "status": "offline",
                            "last_seen": data["timestamp"],
                        }
                else:
                    self._state[repo] = {"status": "unknown", "last_seen": None}

            except Exception as e:
                self._state[repo] = {"status": "error", "error": str(e)}

    async def _process_events(self) -> None:
        """Process incoming events from other repos."""
        for event_file in self._sync_dir.glob("*.json"):
            if event_file.name.endswith("_heartbeat.json"):
                continue

            try:
                content = await asyncio.to_thread(event_file.read_text)
                event = json.loads(content)

                # Skip our own events
                if event.get("source") == "jarvis":
                    continue

                event_type = event.get("type", "")
                handlers = self._event_handlers.get(event_type, [])

                for handler in handlers:
                    try:
                        if asyncio.iscoroutinefunction(handler):
                            await handler(event)
                        else:
                            handler(event)
                    except Exception as e:
                        logger.debug(f"Event handler error: {e}")

                # Remove processed event
                await asyncio.to_thread(event_file.unlink, missing_ok=True)

            except Exception as e:
                logger.debug(f"Event processing error: {e}")

    def get_sync_status(self) -> Dict[str, Any]:
        """Get synchronization status."""
        return {
            "running": self._running,
            "repos": self._state.copy(),
            "sync_dir": str(self._sync_dir),
        }


# =============================================================================
# v6.0 Factory Functions and Global Instances
# =============================================================================

_code_sanitizer: Optional[CodeSanitizer] = None
_dependency_installer: Optional[DependencyAutoInstaller] = None
_file_lock_manager: Optional[FileLockManager] = None
_reactor_feedback_receiver: Optional[ReactorCoreFeedbackReceiver] = None
_prime_training_integration: Optional[PrimeTrainingIntegration] = None
_model_update_notifier: Optional[ModelUpdateNotifier] = None
_autonomous_loop_controller: Optional[AutonomousLoopController] = None
_cross_repo_sync_manager: Optional[CrossRepoSyncManager] = None


def get_code_sanitizer() -> CodeSanitizer:
    """Get or create the code sanitizer."""
    global _code_sanitizer
    if _code_sanitizer is None:
        _code_sanitizer = CodeSanitizer()
    return _code_sanitizer


def get_dependency_installer() -> DependencyAutoInstaller:
    """Get or create the dependency auto-installer."""
    global _dependency_installer
    if _dependency_installer is None:
        _dependency_installer = DependencyAutoInstaller()
    return _dependency_installer


def get_file_lock_manager() -> FileLockManager:
    """Get or create the file lock manager."""
    global _file_lock_manager
    if _file_lock_manager is None:
        _file_lock_manager = FileLockManager()
    return _file_lock_manager


def get_reactor_feedback_receiver() -> ReactorCoreFeedbackReceiver:
    """Get or create the Reactor Core feedback receiver."""
    global _reactor_feedback_receiver
    if _reactor_feedback_receiver is None:
        _reactor_feedback_receiver = ReactorCoreFeedbackReceiver()
    return _reactor_feedback_receiver


def get_prime_training_integration() -> PrimeTrainingIntegration:
    """Get or create the Prime training integration."""
    global _prime_training_integration
    if _prime_training_integration is None:
        _prime_training_integration = PrimeTrainingIntegration()
    return _prime_training_integration


def get_model_update_notifier() -> ModelUpdateNotifier:
    """Get or create the model update notifier."""
    global _model_update_notifier
    if _model_update_notifier is None:
        _model_update_notifier = ModelUpdateNotifier()
    return _model_update_notifier


def get_autonomous_loop_controller() -> AutonomousLoopController:
    """Get or create the autonomous loop controller."""
    global _autonomous_loop_controller
    if _autonomous_loop_controller is None:
        _autonomous_loop_controller = AutonomousLoopController()
    return _autonomous_loop_controller


def get_cross_repo_sync_manager() -> CrossRepoSyncManager:
    """Get or create the cross-repo sync manager."""
    global _cross_repo_sync_manager
    if _cross_repo_sync_manager is None:
        _cross_repo_sync_manager = CrossRepoSyncManager()
    return _cross_repo_sync_manager


async def initialize_autonomous_system_v6(
    start_loops: bool = True,
    enable_web_search: bool = True,
    enable_reactor_feedback: bool = True,
    enable_prime_training: bool = True,
    enable_file_locking: bool = True,
) -> Dict[str, Any]:
    """
    Initialize the complete v6.0 autonomous self-programming system.

    This activates:
    - Code sanitization for external code
    - Dependency auto-installation
    - File locking to prevent conflicts
    - Reactor Core bidirectional feedback
    - Prime training integration
    - Model update notifications
    - Autonomous loop controller
    - Cross-repo synchronization

    Args:
        start_loops: Whether to start background loops
        enable_web_search: Enable web search integration
        enable_reactor_feedback: Enable Reactor Core feedback
        enable_prime_training: Enable Prime training integration
        enable_file_locking: Enable file lock management

    Returns:
        Dictionary with all initialized components
    """
    logger.info("🚀 Initializing Autonomous System v6.0...")
    start_time = time.monotonic()

    components: Dict[str, Any] = {}

    try:
        # Get the master controller
        controller = get_autonomous_loop_controller()
        components["autonomous_loop_controller"] = controller

        # Initialize code sanitizer
        sanitizer = get_code_sanitizer()
        components["code_sanitizer"] = sanitizer

        # Initialize dependency installer
        installer = get_dependency_installer()
        components["dependency_installer"] = installer

        # Initialize file lock manager
        if enable_file_locking:
            lock_manager = get_file_lock_manager()
            await lock_manager.start_monitoring()
            components["file_lock_manager"] = lock_manager
            await controller.register_component("file_lock_manager", lock_manager)

        # Initialize Reactor Core feedback receiver
        if enable_reactor_feedback:
            reactor_receiver = get_reactor_feedback_receiver()
            components["reactor_feedback_receiver"] = reactor_receiver
            await controller.register_component(
                "reactor_feedback_receiver",
                reactor_receiver,
                auto_start=start_loops,
            )

        # Initialize Prime training integration
        if enable_prime_training:
            prime_training = get_prime_training_integration()
            await prime_training.initialize()
            components["prime_training_integration"] = prime_training

        # Initialize model update notifier
        notifier = get_model_update_notifier()
        components["model_update_notifier"] = notifier

        # Wire up reactor feedback to Prime training
        if enable_reactor_feedback and enable_prime_training:
            reactor_receiver.register_feedback_handler(
                lambda fb: logger.info(
                    f"Training feedback received: {fb.status}, "
                    f"metrics={fb.metrics}"
                )
            )
            reactor_receiver.register_model_ready_handler(
                lambda evt: notifier.notify_model_update(evt)
            )

        # Initialize cross-repo sync manager
        sync_manager = get_cross_repo_sync_manager()
        components["cross_repo_sync_manager"] = sync_manager
        await controller.register_component(
            "cross_repo_sync_manager",
            sync_manager,
            auto_start=start_loops,
        )

        # Start all components if requested
        if start_loops:
            await controller.start_all()

        elapsed = time.monotonic() - start_time
        logger.info(f"✅ Autonomous System v6.0 initialized in {elapsed:.2f}s")
        logger.info(f"   Components: {list(components.keys())}")

        return components

    except Exception as e:
        logger.error(f"❌ Autonomous System v6.0 initialization failed: {e}")
        raise


async def shutdown_autonomous_system_v6() -> None:
    """Shutdown all v6.0 autonomous system components."""
    logger.info("Shutting down Autonomous System v6.0...")

    global _autonomous_loop_controller, _file_lock_manager
    global _reactor_feedback_receiver, _prime_training_integration
    global _cross_repo_sync_manager

    # Stop the controller (stops all registered components)
    if _autonomous_loop_controller:
        await _autonomous_loop_controller.stop_all()

    # Additional cleanup
    if _file_lock_manager:
        await _file_lock_manager.stop_monitoring()

    if _prime_training_integration:
        await _prime_training_integration.shutdown()

    # Reset globals
    _code_sanitizer = None
    _dependency_installer = None
    _file_lock_manager = None
    _reactor_feedback_receiver = None
    _prime_training_integration = None
    _model_update_notifier = None
    _autonomous_loop_controller = None
    _cross_repo_sync_manager = None

    logger.info("✅ Autonomous System v6.0 shutdown complete")


# =============================================================================
# v7.0: ENTERPRISE-GRADE AUTONOMOUS SELF-PROGRAMMING ENHANCEMENTS
# =============================================================================
# Addresses ALL remaining gaps:
# 1. Adaptive file lock sensitivity with ML-based tuning
# 2. Code sanitization whitelist mechanism
# 3. Dependency version conflict detection and resolution
# 4. Multi-format Reactor Core event handling
# 5. Model hot-swap mechanism with zero-downtime switching
# 6. Cross-repo sync with exponential backoff retry
# 7. Event-based health monitoring with circuit breakers
# 8. Import path auto-updater
# 9. Large file intelligent chunking system
# 10. Comprehensive status dashboard
# =============================================================================


class AdaptiveSensitivityLevel(Enum):
    """Adaptive sensitivity levels for file lock detection."""
    ULTRA_LOW = 0.1      # Almost never triggers (for frequent auto-saves)
    LOW = 0.3            # Rarely triggers
    MEDIUM = 0.5         # Balanced (default)
    HIGH = 0.7           # More sensitive
    ULTRA_HIGH = 0.9     # Very sensitive (for critical files)


@dataclass
class FileLockMetrics:
    """Metrics for adaptive file lock tuning."""
    file_path: str
    false_positive_count: int = 0
    false_negative_count: int = 0
    true_positive_count: int = 0
    true_negative_count: int = 0
    avg_edit_interval_ms: float = 0.0
    typical_edit_size_bytes: int = 0
    is_ide_managed: bool = False
    last_calibration: Optional[datetime] = None


@dataclass
class SanitizationWhitelistEntry:
    """Whitelist entry for code sanitization."""
    pattern: str
    reason: str
    source_urls: Set[str] = field(default_factory=set)
    added_by: str = "system"
    added_at: datetime = field(default_factory=datetime.now)
    expires_at: Optional[datetime] = None
    usage_count: int = 0


@dataclass
class DependencyConflict:
    """Represents a dependency version conflict."""
    package: str
    required_version: str
    installed_version: str
    conflicting_packages: List[str]
    resolution_strategy: str  # "upgrade", "downgrade", "pin", "skip"
    auto_resolvable: bool = True


@dataclass
class ReactorEventFormat:
    """Reactor Core event format specification."""
    format_version: str
    field_mappings: Dict[str, str]
    required_fields: Set[str]
    optional_fields: Set[str]
    transform_functions: Dict[str, Callable]


@dataclass
class ModelHotSwapConfig:
    """Configuration for model hot-swapping."""
    model_id: str
    old_version: str
    new_version: str
    rollback_on_failure: bool = True
    warmup_requests: int = 3
    graceful_timeout_s: float = 30.0
    health_check_interval_s: float = 5.0


@dataclass
class ImportPathMapping:
    """Mapping for import path updates."""
    old_path: str
    new_path: str
    reason: str
    auto_update: bool = True
    affected_files: List[str] = field(default_factory=list)


@dataclass
class ChunkingStrategy:
    """Strategy for intelligent file chunking."""
    chunk_size_lines: int = 500
    overlap_lines: int = 50
    context_window_tokens: int = 32000
    semantic_boundaries: bool = True  # Split at class/function boundaries
    preserve_imports: bool = True
    max_chunk_tokens: int = 8000


class AdaptiveFileLockManager:
    """
    v7.0: Adaptive file lock manager with ML-based sensitivity tuning.

    Features:
    - Learns from false positives/negatives to auto-tune sensitivity
    - Detects IDE patterns (auto-save, format-on-save)
    - Per-file sensitivity based on edit history
    - Configurable whitelist for known safe patterns
    """

    def __init__(self):
        self._metrics: Dict[str, FileLockMetrics] = {}
        self._sensitivity_cache: Dict[str, float] = {}
        self._ide_patterns = self._load_ide_patterns()
        self._lock = asyncio.Lock()
        self._calibration_interval_s = 300  # Recalibrate every 5 minutes
        self._last_global_calibration = datetime.now()

    def _load_ide_patterns(self) -> Dict[str, Dict[str, Any]]:
        """Load known IDE edit patterns."""
        return {
            "vscode": {
                "auto_save_interval_ms": 1000,
                "format_on_save": True,
                "temp_file_patterns": [r".*\.tmp$", r".*~$"],
                "backup_patterns": [r".*\.bak$"],
            },
            "pycharm": {
                "auto_save_interval_ms": 15000,
                "format_on_save": True,
                "temp_file_patterns": [r".*\.___jb_tmp___$"],
                "backup_patterns": [r".*\.___jb_old___$"],
            },
            "vim": {
                "auto_save_interval_ms": 0,  # Manual save
                "format_on_save": False,
                "temp_file_patterns": [r".*\.swp$", r".*\.swo$"],
                "backup_patterns": [r".*~$"],
            },
            "sublime": {
                "auto_save_interval_ms": 0,
                "format_on_save": False,
                "temp_file_patterns": [],
                "backup_patterns": [],
            },
        }

    async def get_adaptive_sensitivity(self, file_path: str) -> float:
        """
        Get adaptive sensitivity for a file based on its edit history.

        Returns sensitivity between 0.0 (never lock) and 1.0 (always lock).
        """
        async with self._lock:
            # Check cache
            if file_path in self._sensitivity_cache:
                cached_time = self._metrics.get(file_path, FileLockMetrics(file_path)).last_calibration
                if cached_time and (datetime.now() - cached_time).seconds < self._calibration_interval_s:
                    return self._sensitivity_cache[file_path]

            # Calculate adaptive sensitivity
            metrics = self._metrics.get(file_path, FileLockMetrics(file_path))
            sensitivity = self._calculate_sensitivity(metrics)

            # Cache result
            self._sensitivity_cache[file_path] = sensitivity
            metrics.last_calibration = datetime.now()
            self._metrics[file_path] = metrics

            return sensitivity

    def _calculate_sensitivity(self, metrics: FileLockMetrics) -> float:
        """Calculate sensitivity based on historical metrics."""
        # Base sensitivity
        base = 0.5

        # Adjust for false positives (too many → lower sensitivity)
        if metrics.false_positive_count > 0:
            fp_adjustment = -0.1 * min(metrics.false_positive_count, 5)
            base += fp_adjustment

        # Adjust for false negatives (too many → higher sensitivity)
        if metrics.false_negative_count > 0:
            fn_adjustment = 0.15 * min(metrics.false_negative_count, 5)
            base += fn_adjustment

        # Adjust for IDE auto-save patterns
        if metrics.is_ide_managed and metrics.avg_edit_interval_ms > 0:
            if metrics.avg_edit_interval_ms < 2000:  # Very frequent saves
                base -= 0.2

        # Clamp to valid range
        return max(0.1, min(0.95, base))

    async def record_feedback(
        self,
        file_path: str,
        was_user_edit: bool,
        we_blocked: bool,
    ) -> None:
        """Record feedback for adaptive learning."""
        async with self._lock:
            metrics = self._metrics.get(file_path, FileLockMetrics(file_path))

            if was_user_edit and we_blocked:
                metrics.true_positive_count += 1
            elif was_user_edit and not we_blocked:
                metrics.false_negative_count += 1
                logger.warning(f"False negative: missed user edit on {file_path}")
            elif not was_user_edit and we_blocked:
                metrics.false_positive_count += 1
                logger.debug(f"False positive: blocked non-user edit on {file_path}")
            else:
                metrics.true_negative_count += 1

            self._metrics[file_path] = metrics

            # Invalidate cache to recalculate
            if file_path in self._sensitivity_cache:
                del self._sensitivity_cache[file_path]

    async def is_ide_auto_save(self, file_path: str, edit_interval_ms: float) -> bool:
        """Detect if this is an IDE auto-save pattern."""
        for ide_name, patterns in self._ide_patterns.items():
            auto_save_interval = patterns.get("auto_save_interval_ms", 0)
            if auto_save_interval > 0:
                # Allow 20% variance
                if abs(edit_interval_ms - auto_save_interval) < auto_save_interval * 0.2:
                    return True
        return False

    def get_metrics_summary(self) -> Dict[str, Any]:
        """Get summary of all file lock metrics."""
        total_fp = sum(m.false_positive_count for m in self._metrics.values())
        total_fn = sum(m.false_negative_count for m in self._metrics.values())
        total_tp = sum(m.true_positive_count for m in self._metrics.values())
        total_tn = sum(m.true_negative_count for m in self._metrics.values())

        total = total_fp + total_fn + total_tp + total_tn
        accuracy = (total_tp + total_tn) / total if total > 0 else 1.0

        return {
            "total_files_tracked": len(self._metrics),
            "true_positives": total_tp,
            "true_negatives": total_tn,
            "false_positives": total_fp,
            "false_negatives": total_fn,
            "accuracy": accuracy,
            "precision": total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 1.0,
            "recall": total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 1.0,
        }


class CodeSanitizationWhitelist:
    """
    v7.0: Whitelist mechanism for code sanitization.

    Allows legitimate uses of potentially dangerous patterns when:
    - Source is trusted (e.g., official documentation)
    - Pattern is well-known safe idiom
    - Admin has explicitly approved
    """

    def __init__(self):
        self._entries: Dict[str, SanitizationWhitelistEntry] = {}
        self._lock = asyncio.Lock()
        self._whitelist_file = Path.home() / ".jarvis" / "sanitization_whitelist.json"
        self._load_persistent_whitelist()

    def _load_persistent_whitelist(self) -> None:
        """Load whitelist from persistent storage."""
        if self._whitelist_file.exists():
            try:
                data = json.loads(self._whitelist_file.read_text())
                for pattern, entry_data in data.items():
                    self._entries[pattern] = SanitizationWhitelistEntry(
                        pattern=pattern,
                        reason=entry_data.get("reason", ""),
                        source_urls=set(entry_data.get("source_urls", [])),
                        added_by=entry_data.get("added_by", "system"),
                        added_at=datetime.fromisoformat(entry_data.get("added_at", datetime.now().isoformat())),
                        expires_at=datetime.fromisoformat(entry_data["expires_at"]) if entry_data.get("expires_at") else None,
                        usage_count=entry_data.get("usage_count", 0),
                    )
            except Exception as e:
                logger.warning(f"Failed to load whitelist: {e}")

    async def _save_persistent_whitelist(self) -> None:
        """Save whitelist to persistent storage."""
        try:
            self._whitelist_file.parent.mkdir(parents=True, exist_ok=True)
            data = {}
            for pattern, entry in self._entries.items():
                data[pattern] = {
                    "reason": entry.reason,
                    "source_urls": list(entry.source_urls),
                    "added_by": entry.added_by,
                    "added_at": entry.added_at.isoformat(),
                    "expires_at": entry.expires_at.isoformat() if entry.expires_at else None,
                    "usage_count": entry.usage_count,
                }
            self._whitelist_file.write_text(json.dumps(data, indent=2))
        except Exception as e:
            logger.error(f"Failed to save whitelist: {e}")

    async def add_entry(
        self,
        pattern: str,
        reason: str,
        source_url: Optional[str] = None,
        added_by: str = "system",
        expires_in_days: Optional[int] = None,
    ) -> None:
        """Add a whitelist entry."""
        async with self._lock:
            expires_at = None
            if expires_in_days:
                expires_at = datetime.now() + timedelta(days=expires_in_days)

            entry = SanitizationWhitelistEntry(
                pattern=pattern,
                reason=reason,
                source_urls={source_url} if source_url else set(),
                added_by=added_by,
                expires_at=expires_at,
            )
            self._entries[pattern] = entry
            await self._save_persistent_whitelist()
            logger.info(f"Added whitelist entry: {pattern[:50]}... (reason: {reason})")

    async def is_whitelisted(self, code: str, source_url: Optional[str] = None) -> Tuple[bool, Optional[str]]:
        """
        Check if code matches any whitelist entry.

        Returns (is_whitelisted, reason).
        """
        async with self._lock:
            now = datetime.now()
            for pattern, entry in list(self._entries.items()):
                # Check expiration
                if entry.expires_at and now > entry.expires_at:
                    del self._entries[pattern]
                    continue

                # Check pattern match
                if re.search(pattern, code, re.IGNORECASE | re.MULTILINE):
                    # If source URL provided, verify it's in allowed sources
                    if source_url and entry.source_urls:
                        if not any(allowed in source_url for allowed in entry.source_urls):
                            continue

                    # Record usage
                    entry.usage_count += 1
                    return True, entry.reason

            return False, None

    async def get_builtin_whitelisted_patterns(self) -> List[Dict[str, str]]:
        """Get built-in safe patterns that should never be blocked."""
        return [
            {
                "pattern": r"getattr\s*\(\s*\w+\s*,\s*['\"][\w_]+['\"]\s*\)",
                "reason": "getattr with literal attribute name is safe",
            },
            {
                "pattern": r"setattr\s*\(\s*self\s*,",
                "reason": "setattr on self in __init__ is common pattern",
            },
            {
                "pattern": r"eval\s*\(\s*['\"][\d\+\-\*\/\s]+['\"]\s*\)",
                "reason": "eval of simple math expression is safe",
            },
            {
                "pattern": r"ast\.literal_eval\s*\(",
                "reason": "ast.literal_eval is safe alternative to eval",
            },
            {
                "pattern": r"json\.loads?\s*\(",
                "reason": "json.load/loads is safe deserialization",
            },
            {
                "pattern": r"subprocess\.run\s*\([^)]*shell\s*=\s*False",
                "reason": "subprocess with shell=False is safe",
            },
            {
                "pattern": r"importlib\.import_module\s*\(\s*['\"][\w\.]+['\"]\s*\)",
                "reason": "importlib with literal module name is safe",
            },
        ]


class DependencyConflictResolver:
    """
    v7.0: Intelligent dependency conflict detection and resolution.

    Features:
    - Detects version conflicts before installation
    - Finds compatible version ranges
    - Supports multiple resolution strategies
    - Rollback on failure
    """

    def __init__(self):
        self._conflict_history: List[DependencyConflict] = []
        self._lock = asyncio.Lock()
        self._pip_cache: Dict[str, Dict[str, Any]] = {}

    async def check_conflicts(
        self,
        package: str,
        required_version: Optional[str] = None,
    ) -> Optional[DependencyConflict]:
        """Check for conflicts before installing a package."""
        try:
            # Get currently installed version
            result = await asyncio.to_thread(
                subprocess.run,
                [sys.executable, "-m", "pip", "show", package],
                capture_output=True,
                text=True,
            )

            installed_version = None
            if result.returncode == 0:
                for line in result.stdout.split("\n"):
                    if line.startswith("Version:"):
                        installed_version = line.split(":")[1].strip()
                        break

            if not installed_version:
                return None  # Not installed, no conflict

            # If required version specified, check compatibility
            if required_version:
                if not self._versions_compatible(installed_version, required_version):
                    # Find conflicting packages
                    conflicting = await self._find_conflicting_packages(package)
                    return DependencyConflict(
                        package=package,
                        required_version=required_version,
                        installed_version=installed_version,
                        conflicting_packages=conflicting,
                        resolution_strategy=self._determine_strategy(
                            installed_version, required_version
                        ),
                    )

            return None

        except Exception as e:
            logger.error(f"Conflict check failed for {package}: {e}")
            return None

    def _versions_compatible(self, installed: str, required: str) -> bool:
        """Check if versions are compatible."""
        # Handle version specifiers
        if required.startswith(">="):
            req_version = required[2:]
            return self._compare_versions(installed, req_version) >= 0
        elif required.startswith("<="):
            req_version = required[2:]
            return self._compare_versions(installed, req_version) <= 0
        elif required.startswith("=="):
            req_version = required[2:]
            return installed == req_version
        elif required.startswith("~="):
            # Compatible release
            req_version = required[2:]
            return self._is_compatible_release(installed, req_version)
        else:
            # Exact match or any version
            return required == "" or installed == required

    def _compare_versions(self, v1: str, v2: str) -> int:
        """Compare two version strings. Returns -1, 0, or 1."""
        try:
            parts1 = [int(x) for x in re.split(r"[.\-_]", v1.split("+")[0]) if x.isdigit()]
            parts2 = [int(x) for x in re.split(r"[.\-_]", v2.split("+")[0]) if x.isdigit()]

            # Pad shorter version
            while len(parts1) < len(parts2):
                parts1.append(0)
            while len(parts2) < len(parts1):
                parts2.append(0)

            for p1, p2 in zip(parts1, parts2):
                if p1 < p2:
                    return -1
                elif p1 > p2:
                    return 1
            return 0
        except Exception:
            return 0

    def _is_compatible_release(self, installed: str, required: str) -> bool:
        """Check if installed is a compatible release of required."""
        try:
            inst_parts = installed.split(".")
            req_parts = required.split(".")

            # Must match major and minor, patch can differ
            return inst_parts[0] == req_parts[0] and inst_parts[1] == req_parts[1]
        except Exception:
            return False

    async def _find_conflicting_packages(self, package: str) -> List[str]:
        """Find packages that depend on different versions."""
        conflicting = []
        try:
            result = await asyncio.to_thread(
                subprocess.run,
                [sys.executable, "-m", "pip", "check"],
                capture_output=True,
                text=True,
            )

            if result.returncode != 0:
                for line in result.stdout.split("\n"):
                    if package.lower() in line.lower():
                        # Extract package name from conflict message
                        match = re.match(r"(\S+)\s+\d+", line)
                        if match:
                            conflicting.append(match.group(1))
        except Exception as e:
            logger.debug(f"Could not find conflicting packages: {e}")

        return conflicting

    def _determine_strategy(self, installed: str, required: str) -> str:
        """Determine best resolution strategy."""
        comparison = self._compare_versions(installed, required.lstrip(">=<~="))

        if comparison < 0:
            return "upgrade"
        elif comparison > 0:
            return "downgrade"
        else:
            return "pin"

    async def resolve_conflict(
        self,
        conflict: DependencyConflict,
        dry_run: bool = True,
    ) -> Dict[str, Any]:
        """Resolve a dependency conflict."""
        result = {
            "package": conflict.package,
            "strategy": conflict.resolution_strategy,
            "success": False,
            "actions_taken": [],
        }

        if dry_run:
            result["dry_run"] = True
            result["would_do"] = f"{conflict.resolution_strategy} {conflict.package} to {conflict.required_version}"
            return result

        try:
            if conflict.resolution_strategy == "upgrade":
                cmd = [sys.executable, "-m", "pip", "install", "--upgrade",
                       f"{conflict.package}{conflict.required_version}"]
            elif conflict.resolution_strategy == "downgrade":
                cmd = [sys.executable, "-m", "pip", "install",
                       f"{conflict.package}=={conflict.required_version.lstrip('>=<~=')}"]
            elif conflict.resolution_strategy == "pin":
                cmd = [sys.executable, "-m", "pip", "install",
                       f"{conflict.package}=={conflict.installed_version}"]
            else:
                result["error"] = f"Unknown strategy: {conflict.resolution_strategy}"
                return result

            proc_result = await asyncio.to_thread(
                subprocess.run, cmd, capture_output=True, text=True
            )

            result["success"] = proc_result.returncode == 0
            result["actions_taken"].append(" ".join(cmd))
            if not result["success"]:
                result["error"] = proc_result.stderr

        except Exception as e:
            result["error"] = str(e)

        return result


class MultiFormatReactorEventHandler:
    """
    v7.0: Handle multiple Reactor Core event formats.

    Supports:
    - v1.0 format (original)
    - v2.0 format (enhanced with metrics)
    - Custom formats via plugins
    - Auto-detection of format version
    """

    def __init__(self):
        self._formats: Dict[str, ReactorEventFormat] = {}
        self._register_builtin_formats()
        self._lock = asyncio.Lock()

    def _register_builtin_formats(self) -> None:
        """Register built-in event formats."""
        # v1.0 format - simple
        self._formats["1.0"] = ReactorEventFormat(
            format_version="1.0",
            field_mappings={
                "model": "model_id",
                "status": "status",
                "time": "timestamp",
            },
            required_fields={"model", "status"},
            optional_fields={"time", "metrics"},
            transform_functions={
                "time": lambda x: datetime.fromisoformat(x) if isinstance(x, str) else x,
            },
        )

        # v2.0 format - enhanced
        self._formats["2.0"] = ReactorEventFormat(
            format_version="2.0",
            field_mappings={
                "model_id": "model_id",
                "training_status": "status",
                "timestamp": "timestamp",
                "training_metrics": "metrics",
                "model_version": "version",
            },
            required_fields={"model_id", "training_status"},
            optional_fields={"timestamp", "training_metrics", "model_version", "checksum"},
            transform_functions={
                "timestamp": lambda x: datetime.fromisoformat(x) if isinstance(x, str) else x,
                "training_metrics": lambda x: x if isinstance(x, dict) else {},
            },
        )

        # v3.0 format - with provenance
        self._formats["3.0"] = ReactorEventFormat(
            format_version="3.0",
            field_mappings={
                "id": "event_id",
                "model": "model_id",
                "state": "status",
                "ts": "timestamp",
                "data": "metrics",
                "ver": "version",
                "source": "source_repo",
            },
            required_fields={"id", "model", "state"},
            optional_fields={"ts", "data", "ver", "source", "parent_id"},
            transform_functions={
                "ts": lambda x: datetime.fromtimestamp(x) if isinstance(x, (int, float)) else datetime.fromisoformat(x) if isinstance(x, str) else x,
            },
        )

    async def detect_format(self, event_data: Dict[str, Any]) -> Optional[str]:
        """Auto-detect the format version of an event."""
        # Check for explicit version field
        if "format_version" in event_data:
            return event_data["format_version"]
        if "version" in event_data and isinstance(event_data["version"], str) and event_data["version"].count(".") == 1:
            return event_data["version"]

        # Detect by field presence
        if "model_id" in event_data and "training_status" in event_data:
            return "2.0"
        elif "id" in event_data and "state" in event_data:
            return "3.0"
        elif "model" in event_data and "status" in event_data:
            return "1.0"

        return None

    async def normalize_event(
        self,
        event_data: Dict[str, Any],
        target_format: str = "2.0",
    ) -> Optional[Dict[str, Any]]:
        """Normalize event to a standard format."""
        source_format = await self.detect_format(event_data)
        if not source_format:
            logger.warning(f"Could not detect event format: {list(event_data.keys())}")
            return None

        if source_format not in self._formats:
            logger.warning(f"Unknown format version: {source_format}")
            return None

        source_spec = self._formats[source_format]
        target_spec = self._formats.get(target_format, self._formats["2.0"])

        # Create normalized event
        normalized = {}

        # Map fields from source to canonical names
        for source_field, canonical_name in source_spec.field_mappings.items():
            if source_field in event_data:
                value = event_data[source_field]
                # Apply transform if defined
                if source_field in source_spec.transform_functions:
                    try:
                        value = source_spec.transform_functions[source_field](value)
                    except Exception as e:
                        logger.debug(f"Transform failed for {source_field}: {e}")
                normalized[canonical_name] = value

        # Add metadata
        normalized["_source_format"] = source_format
        normalized["_normalized_at"] = datetime.now().isoformat()

        return normalized

    async def parse_event(self, event_data: Dict[str, Any]) -> Optional[ReactorFeedback]:
        """Parse an event into a ReactorFeedback object."""
        normalized = await self.normalize_event(event_data)
        if not normalized:
            return None

        try:
            return ReactorFeedback(
                model_id=normalized.get("model_id", "unknown"),
                status=normalized.get("status", "unknown"),
                metrics=normalized.get("metrics", {}),
                timestamp=normalized.get("timestamp", datetime.now()),
                source_format=normalized.get("_source_format", "unknown"),
            )
        except Exception as e:
            logger.error(f"Failed to parse event: {e}")
            return None


class ModelHotSwapManager:
    """
    v7.0: Zero-downtime model hot-swapping.

    Features:
    - Graceful transition between model versions
    - Health checks during swap
    - Automatic rollback on failure
    - Warmup with test requests
    """

    def __init__(self):
        self._active_swaps: Dict[str, ModelHotSwapConfig] = {}
        self._model_health: Dict[str, Dict[str, Any]] = {}
        self._lock = asyncio.Lock()
        self._swap_history: List[Dict[str, Any]] = []

    async def initiate_swap(
        self,
        model_id: str,
        new_version: str,
        old_version: Optional[str] = None,
        rollback_on_failure: bool = True,
    ) -> Dict[str, Any]:
        """Initiate a hot-swap operation."""
        async with self._lock:
            if model_id in self._active_swaps:
                return {
                    "success": False,
                    "error": f"Swap already in progress for {model_id}",
                }

            config = ModelHotSwapConfig(
                model_id=model_id,
                old_version=old_version or "unknown",
                new_version=new_version,
                rollback_on_failure=rollback_on_failure,
            )
            self._active_swaps[model_id] = config

        result = {
            "model_id": model_id,
            "old_version": config.old_version,
            "new_version": new_version,
            "phases": [],
            "success": False,
        }

        try:
            # Phase 1: Pre-swap health check
            result["phases"].append({"phase": "pre_health_check", "status": "started"})
            pre_health = await self._check_model_health(model_id, config.old_version)
            result["phases"][-1]["result"] = pre_health
            result["phases"][-1]["status"] = "completed"

            # Phase 2: Load new model
            result["phases"].append({"phase": "load_new_model", "status": "started"})
            load_result = await self._load_model_version(model_id, new_version)
            result["phases"][-1]["result"] = load_result
            result["phases"][-1]["status"] = "completed" if load_result.get("success") else "failed"

            if not load_result.get("success"):
                raise Exception(f"Failed to load new model: {load_result.get('error')}")

            # Phase 3: Warmup new model
            result["phases"].append({"phase": "warmup", "status": "started"})
            warmup_result = await self._warmup_model(model_id, new_version, config.warmup_requests)
            result["phases"][-1]["result"] = warmup_result
            result["phases"][-1]["status"] = "completed" if warmup_result.get("success") else "failed"

            if not warmup_result.get("success"):
                raise Exception(f"Warmup failed: {warmup_result.get('error')}")

            # Phase 4: Switch traffic
            result["phases"].append({"phase": "switch_traffic", "status": "started"})
            switch_result = await self._switch_traffic(model_id, config.old_version, new_version)
            result["phases"][-1]["result"] = switch_result
            result["phases"][-1]["status"] = "completed" if switch_result.get("success") else "failed"

            if not switch_result.get("success"):
                raise Exception(f"Traffic switch failed: {switch_result.get('error')}")

            # Phase 5: Post-swap health check
            result["phases"].append({"phase": "post_health_check", "status": "started"})
            post_health = await self._check_model_health(model_id, new_version)
            result["phases"][-1]["result"] = post_health
            result["phases"][-1]["status"] = "completed"

            if not post_health.get("healthy"):
                raise Exception(f"Post-swap health check failed")

            # Phase 6: Cleanup old model
            result["phases"].append({"phase": "cleanup", "status": "started"})
            await self._cleanup_old_model(model_id, config.old_version)
            result["phases"][-1]["status"] = "completed"

            result["success"] = True
            logger.info(f"Hot-swap completed: {model_id} {config.old_version} → {new_version}")

        except Exception as e:
            result["error"] = str(e)
            logger.error(f"Hot-swap failed for {model_id}: {e}")

            # Rollback if configured
            if rollback_on_failure and config.old_version != "unknown":
                result["rollback"] = await self._rollback(model_id, config)

        finally:
            async with self._lock:
                if model_id in self._active_swaps:
                    del self._active_swaps[model_id]
                self._swap_history.append({
                    "model_id": model_id,
                    "result": result,
                    "timestamp": datetime.now().isoformat(),
                })

        return result

    async def _check_model_health(self, model_id: str, version: str) -> Dict[str, Any]:
        """Check health of a specific model version."""
        # This would integrate with the actual model serving infrastructure
        return {
            "model_id": model_id,
            "version": version,
            "healthy": True,
            "latency_ms": 50,
            "memory_mb": 1024,
            "checked_at": datetime.now().isoformat(),
        }

    async def _load_model_version(self, model_id: str, version: str) -> Dict[str, Any]:
        """Load a specific model version."""
        # This would integrate with Ironcliw Prime model loading
        return {
            "success": True,
            "model_id": model_id,
            "version": version,
            "load_time_ms": 500,
        }

    async def _warmup_model(
        self,
        model_id: str,
        version: str,
        num_requests: int,
    ) -> Dict[str, Any]:
        """Warmup model with test requests."""
        results = []
        for i in range(num_requests):
            # Simulated warmup request
            results.append({"request": i, "latency_ms": 100 + i * 10})
            await asyncio.sleep(0.1)

        return {
            "success": True,
            "requests": num_requests,
            "avg_latency_ms": sum(r["latency_ms"] for r in results) / len(results),
        }

    async def _switch_traffic(
        self,
        model_id: str,
        old_version: str,
        new_version: str,
    ) -> Dict[str, Any]:
        """Switch traffic from old to new model."""
        # This would update routing configuration
        return {
            "success": True,
            "old_version": old_version,
            "new_version": new_version,
            "switched_at": datetime.now().isoformat(),
        }

    async def _cleanup_old_model(self, model_id: str, version: str) -> None:
        """Cleanup old model version."""
        # Keep in memory for potential rollback, but mark as inactive
        logger.debug(f"Marked {model_id}:{version} for cleanup")

    async def _rollback(
        self,
        model_id: str,
        config: ModelHotSwapConfig,
    ) -> Dict[str, Any]:
        """Rollback to previous version."""
        logger.warning(f"Rolling back {model_id} to {config.old_version}")

        try:
            switch_result = await self._switch_traffic(
                model_id, config.new_version, config.old_version
            )
            return {
                "success": switch_result.get("success", False),
                "rolled_back_to": config.old_version,
            }
        except Exception as e:
            return {"success": False, "error": str(e)}


class CrossRepoSyncRetryManager:
    """
    v7.0: Cross-repo synchronization with exponential backoff retry.

    Features:
    - Exponential backoff with jitter
    - Per-repo failure tracking
    - Circuit breaker for persistent failures
    - Automatic recovery detection
    """

    def __init__(self):
        self._retry_counts: Dict[str, int] = {}
        self._last_success: Dict[str, datetime] = {}
        self._circuit_breakers: Dict[str, Dict[str, Any]] = {}
        self._lock = asyncio.Lock()

        # Configuration
        self._max_retries = 5
        self._base_delay_s = 1.0
        self._max_delay_s = 60.0
        self._jitter_factor = 0.3
        self._circuit_breaker_threshold = 10
        self._circuit_breaker_reset_s = 300

    async def execute_with_retry(
        self,
        repo_id: str,
        operation: Callable[[], Awaitable[Any]],
        operation_name: str = "sync",
    ) -> Dict[str, Any]:
        """Execute an operation with retry logic."""
        # Check circuit breaker
        if await self._is_circuit_open(repo_id):
            return {
                "success": False,
                "error": "Circuit breaker open",
                "repo_id": repo_id,
                "retry_after_s": await self._get_circuit_reset_time(repo_id),
            }

        result = {
            "repo_id": repo_id,
            "operation": operation_name,
            "attempts": 0,
            "success": False,
        }

        async with self._lock:
            retry_count = self._retry_counts.get(repo_id, 0)

        for attempt in range(self._max_retries):
            result["attempts"] = attempt + 1

            try:
                op_result = await operation()
                result["success"] = True
                result["result"] = op_result

                # Reset retry count on success
                async with self._lock:
                    self._retry_counts[repo_id] = 0
                    self._last_success[repo_id] = datetime.now()

                return result

            except Exception as e:
                result["last_error"] = str(e)
                logger.warning(f"Retry {attempt + 1}/{self._max_retries} for {repo_id} {operation_name}: {e}")

                # Calculate delay with exponential backoff and jitter
                delay = self._calculate_delay(attempt)
                result["next_retry_delay_s"] = delay

                if attempt < self._max_retries - 1:
                    await asyncio.sleep(delay)

        # All retries exhausted
        async with self._lock:
            self._retry_counts[repo_id] = self._retry_counts.get(repo_id, 0) + 1

            # Check if we should open circuit breaker
            if self._retry_counts[repo_id] >= self._circuit_breaker_threshold:
                await self._open_circuit(repo_id)
                result["circuit_opened"] = True

        return result

    def _calculate_delay(self, attempt: int) -> float:
        """Calculate delay with exponential backoff and jitter."""
        delay = min(
            self._base_delay_s * (2 ** attempt),
            self._max_delay_s
        )

        # Add jitter
        jitter = delay * self._jitter_factor * (2 * random.random() - 1)
        return max(0.1, delay + jitter)

    async def _is_circuit_open(self, repo_id: str) -> bool:
        """Check if circuit breaker is open."""
        async with self._lock:
            cb = self._circuit_breakers.get(repo_id)
            if not cb:
                return False

            if cb.get("open"):
                # Check if reset time has passed
                opened_at = cb.get("opened_at")
                if opened_at:
                    elapsed = (datetime.now() - opened_at).total_seconds()
                    if elapsed >= self._circuit_breaker_reset_s:
                        # Half-open: allow one attempt
                        cb["half_open"] = True
                        return False
                return True
            return False

    async def _open_circuit(self, repo_id: str) -> None:
        """Open circuit breaker."""
        async with self._lock:
            self._circuit_breakers[repo_id] = {
                "open": True,
                "opened_at": datetime.now(),
                "half_open": False,
            }
            logger.warning(f"Circuit breaker opened for {repo_id}")

    async def _get_circuit_reset_time(self, repo_id: str) -> float:
        """Get time until circuit resets."""
        async with self._lock:
            cb = self._circuit_breakers.get(repo_id)
            if cb and cb.get("opened_at"):
                elapsed = (datetime.now() - cb["opened_at"]).total_seconds()
                return max(0, self._circuit_breaker_reset_s - elapsed)
            return 0

    async def close_circuit(self, repo_id: str) -> None:
        """Manually close circuit breaker."""
        async with self._lock:
            if repo_id in self._circuit_breakers:
                del self._circuit_breakers[repo_id]
            self._retry_counts[repo_id] = 0
            logger.info(f"Circuit breaker closed for {repo_id}")

    def get_status(self) -> Dict[str, Any]:
        """Get retry manager status."""
        return {
            "retry_counts": dict(self._retry_counts),
            "last_success": {k: v.isoformat() for k, v in self._last_success.items()},
            "circuit_breakers": {
                k: {
                    "open": v.get("open"),
                    "opened_at": v.get("opened_at").isoformat() if v.get("opened_at") else None,
                }
                for k, v in self._circuit_breakers.items()
            },
        }


class EventBasedHealthMonitor:
    """
    v7.0: Event-based health monitoring with circuit breakers.

    Features:
    - Real-time event monitoring (not polling)
    - Automatic failure detection
    - Component restart on failure
    - Health metrics aggregation
    """

    def __init__(self):
        self._components: Dict[str, Dict[str, Any]] = {}
        self._health_events: Deque[Dict[str, Any]] = deque(maxlen=1000)
        self._event_handlers: Dict[str, List[Callable]] = defaultdict(list)
        self._lock = asyncio.Lock()
        self._running = False
        self._monitor_task: Optional[asyncio.Task] = None

        # v125.1: Configuration with environment variable overrides
        # Increased defaults for startup resilience
        self._failure_threshold = int(os.environ.get("HEALTH_FAILURE_THRESHOLD", "10"))
        self._recovery_threshold = int(os.environ.get("HEALTH_RECOVERY_THRESHOLD", "2"))
        self._health_check_interval_s = float(os.environ.get("HEALTH_CHECK_INTERVAL", "5.0"))

    async def register_component(
        self,
        name: str,
        component: Any,
        health_check: Optional[Callable[[], Awaitable[bool]]] = None,
        restart_func: Optional[Callable[[], Awaitable[None]]] = None,
    ) -> None:
        """Register a component for health monitoring."""
        async with self._lock:
            self._components[name] = {
                "component": component,
                "health_check": health_check or self._default_health_check,
                "restart_func": restart_func,
                "status": "healthy",
                "consecutive_failures": 0,
                "consecutive_successes": 0,
                "last_check": None,
                "total_failures": 0,
                "total_checks": 0,
            }
        logger.debug(f"Registered component for health monitoring: {name}")

    async def _default_health_check(self) -> bool:
        """Default health check - always healthy."""
        return True

    async def start(self) -> None:
        """Start health monitoring."""
        if self._running:
            return

        self._running = True
        self._monitor_task = asyncio.create_task(self._monitor_loop())
        logger.info("Event-based health monitor started")

    async def stop(self) -> None:
        """Stop health monitoring."""
        self._running = False
        if self._monitor_task:
            self._monitor_task.cancel()
            try:
                await self._monitor_task
            except asyncio.CancelledError:
                pass
        logger.info("Event-based health monitor stopped")

    async def _monitor_loop(self) -> None:
        """Main monitoring loop."""
        while self._running:
            try:
                # Check all components
                async with self._lock:
                    component_names = list(self._components.keys())

                # Run health checks in parallel
                checks = [
                    self._check_component(name)
                    for name in component_names
                ]
                await asyncio.gather(*checks, return_exceptions=True)

                await asyncio.sleep(self._health_check_interval_s)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Health monitor error: {e}")
                await asyncio.sleep(1.0)

    async def _check_component(self, name: str) -> None:
        """Check health of a single component."""
        async with self._lock:
            comp_data = self._components.get(name)
            if not comp_data:
                return

        try:
            health_check = comp_data["health_check"]
            is_healthy = await health_check()

            await self._record_health_event(name, is_healthy)

        except Exception as e:
            logger.debug(f"Health check failed for {name}: {e}")
            await self._record_health_event(name, False, error=str(e))

    async def _record_health_event(
        self,
        name: str,
        is_healthy: bool,
        error: Optional[str] = None,
    ) -> None:
        """Record a health event and update component status."""
        event = {
            "component": name,
            "healthy": is_healthy,
            "timestamp": datetime.now().isoformat(),
            "error": error,
        }

        async with self._lock:
            self._health_events.append(event)

            comp_data = self._components.get(name)
            if not comp_data:
                return

            comp_data["total_checks"] += 1
            comp_data["last_check"] = datetime.now()

            if is_healthy:
                comp_data["consecutive_failures"] = 0
                comp_data["consecutive_successes"] += 1

                # Check for recovery
                if comp_data["status"] == "unhealthy":
                    if comp_data["consecutive_successes"] >= self._recovery_threshold:
                        comp_data["status"] = "healthy"
                        await self._emit_event("component_recovered", name)
                        logger.info(f"Component recovered: {name}")

            else:
                comp_data["consecutive_successes"] = 0
                comp_data["consecutive_failures"] += 1
                comp_data["total_failures"] += 1

                # Check for failure threshold
                if comp_data["status"] == "healthy":
                    if comp_data["consecutive_failures"] >= self._failure_threshold:
                        comp_data["status"] = "unhealthy"
                        await self._emit_event("component_failed", name)
                        logger.warning(f"Component failed: {name}")

                        # Attempt restart if configured
                        restart_func = comp_data.get("restart_func")
                        if restart_func:
                            await self._attempt_restart(name, restart_func)

    async def _attempt_restart(
        self,
        name: str,
        restart_func: Callable[[], Awaitable[None]],
    ) -> None:
        """Attempt to restart a failed component."""
        logger.info(f"Attempting to restart component: {name}")
        try:
            await restart_func()
            await self._emit_event("component_restarted", name)
            logger.info(f"Component restarted successfully: {name}")
        except Exception as e:
            logger.error(f"Failed to restart component {name}: {e}")
            await self._emit_event("restart_failed", name, error=str(e))

    async def _emit_event(
        self,
        event_type: str,
        component: str,
        **kwargs,
    ) -> None:
        """Emit a health event to registered handlers."""
        event = {
            "type": event_type,
            "component": component,
            "timestamp": datetime.now().isoformat(),
            **kwargs,
        }

        for handler in self._event_handlers.get(event_type, []):
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler(event)
                else:
                    handler(event)
            except Exception as e:
                logger.debug(f"Event handler error: {e}")

    def on_event(self, event_type: str, handler: Callable) -> None:
        """Register an event handler."""
        self._event_handlers[event_type].append(handler)

    def get_status(self) -> Dict[str, Any]:
        """Get health monitor status."""
        return {
            "running": self._running,
            "components": {
                name: {
                    "status": data["status"],
                    "consecutive_failures": data["consecutive_failures"],
                    "consecutive_successes": data["consecutive_successes"],
                    "total_failures": data["total_failures"],
                    "total_checks": data["total_checks"],
                    "last_check": data["last_check"].isoformat() if data["last_check"] else None,
                }
                for name, data in self._components.items()
            },
            "recent_events": list(self._health_events)[-10:],
        }


class ImportPathAutoUpdater:
    """
    v7.0: Automatic import path updates when files move.

    Features:
    - Detects file moves/renames
    - Updates all import statements
    - Handles relative and absolute imports
    - Supports multi-repo updates
    """

    def __init__(self):
        self._mappings: List[ImportPathMapping] = []
        self._lock = asyncio.Lock()
        self._cache: Dict[str, Set[str]] = {}  # file -> set of import paths

    async def register_move(
        self,
        old_path: str,
        new_path: str,
        reason: str = "file moved",
    ) -> ImportPathMapping:
        """Register a file move for import updates."""
        # Convert file paths to import paths
        old_import = self._path_to_import(old_path)
        new_import = self._path_to_import(new_path)

        mapping = ImportPathMapping(
            old_path=old_import,
            new_path=new_import,
            reason=reason,
        )

        async with self._lock:
            self._mappings.append(mapping)

        return mapping

    def _path_to_import(self, file_path: str) -> str:
        """Convert file path to import path."""
        # Remove .py extension
        path = file_path.replace(".py", "")
        # Convert slashes to dots
        path = path.replace("/", ".").replace("\\", ".")
        # Remove leading dots
        path = path.lstrip(".")
        return path

    async def find_affected_files(
        self,
        mapping: ImportPathMapping,
        search_dirs: Optional[List[Path]] = None,
    ) -> List[str]:
        """Find all files affected by an import path change."""
        if search_dirs is None:
            search_dirs = [Path.cwd()]

        affected = []
        pattern = re.compile(
            rf"(?:from|import)\s+{re.escape(mapping.old_path)}(?:\s|\.|\n|$)"
        )

        for search_dir in search_dirs:
            for py_file in search_dir.rglob("*.py"):
                try:
                    content = await asyncio.to_thread(py_file.read_text)
                    if pattern.search(content):
                        affected.append(str(py_file))
                except Exception as e:
                    logger.debug(f"Could not read {py_file}: {e}")

        mapping.affected_files = affected
        return affected

    async def update_imports(
        self,
        mapping: ImportPathMapping,
        dry_run: bool = True,
    ) -> Dict[str, Any]:
        """Update imports in affected files."""
        result = {
            "mapping": {
                "old": mapping.old_path,
                "new": mapping.new_path,
            },
            "files_updated": [],
            "errors": [],
            "dry_run": dry_run,
        }

        if not mapping.affected_files:
            await self.find_affected_files(mapping)

        for file_path in mapping.affected_files:
            try:
                updated = await self._update_file_imports(
                    file_path, mapping, dry_run
                )
                if updated:
                    result["files_updated"].append(file_path)
            except Exception as e:
                result["errors"].append({"file": file_path, "error": str(e)})

        return result

    async def _update_file_imports(
        self,
        file_path: str,
        mapping: ImportPathMapping,
        dry_run: bool,
    ) -> bool:
        """Update imports in a single file."""
        content = await asyncio.to_thread(Path(file_path).read_text)

        # Pattern for "from X import Y" style
        from_pattern = rf"(from\s+){re.escape(mapping.old_path)}(\s+import)"
        new_content = re.sub(
            from_pattern,
            rf"\g<1>{mapping.new_path}\g<2>",
            content
        )

        # Pattern for "import X" style
        import_pattern = rf"(import\s+){re.escape(mapping.old_path)}(\s|$|\n)"
        new_content = re.sub(
            import_pattern,
            rf"\g<1>{mapping.new_path}\g<2>",
            new_content
        )

        if new_content != content:
            if not dry_run:
                await asyncio.to_thread(Path(file_path).write_text, new_content)
            return True
        return False

    async def auto_update_on_move(
        self,
        old_path: str,
        new_path: str,
        search_dirs: Optional[List[Path]] = None,
        dry_run: bool = False,
    ) -> Dict[str, Any]:
        """Automatically update all imports when a file moves."""
        mapping = await self.register_move(old_path, new_path)
        await self.find_affected_files(mapping, search_dirs)
        return await self.update_imports(mapping, dry_run)


class IntelligentFileChunker:
    """
    v7.0: Intelligent file chunking for large files.

    Features:
    - Semantic boundary detection (class/function)
    - Token-aware chunking
    - Context preservation across chunks
    - Overlap management
    """

    def __init__(self, strategy: Optional[ChunkingStrategy] = None):
        self._strategy = strategy or ChunkingStrategy()
        self._tokenizer_cache: Dict[str, int] = {}

    async def chunk_file(
        self,
        file_path: str,
        max_tokens: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """Chunk a file intelligently."""
        content = await asyncio.to_thread(Path(file_path).read_text)
        return await self.chunk_content(content, file_path, max_tokens)

    async def chunk_content(
        self,
        content: str,
        file_path: Optional[str] = None,
        max_tokens: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """Chunk content intelligently."""
        max_tokens = max_tokens or self._strategy.max_chunk_tokens
        lines = content.split("\n")

        if self._strategy.semantic_boundaries:
            chunks = await self._chunk_by_semantics(lines, max_tokens)
        else:
            chunks = await self._chunk_by_lines(lines, max_tokens)

        # Add metadata to chunks
        for i, chunk in enumerate(chunks):
            chunk["index"] = i
            chunk["total_chunks"] = len(chunks)
            chunk["file_path"] = file_path

        return chunks

    async def _chunk_by_semantics(
        self,
        lines: List[str],
        max_tokens: int,
    ) -> List[Dict[str, Any]]:
        """Chunk by semantic boundaries (class/function definitions)."""
        chunks = []
        current_chunk_lines: List[str] = []
        current_chunk_start = 0
        current_tokens = 0

        # Patterns for semantic boundaries
        boundary_patterns = [
            r"^class\s+\w+",           # Class definition
            r"^def\s+\w+",             # Function definition
            r"^async\s+def\s+\w+",     # Async function
            r"^@\w+",                  # Decorator (often precedes class/function)
        ]
        boundary_regex = re.compile("|".join(boundary_patterns))

        # Extract imports for preservation
        imports = []
        if self._strategy.preserve_imports:
            for line in lines:
                if line.strip().startswith(("import ", "from ")):
                    imports.append(line)
                elif line.strip() and not line.strip().startswith("#"):
                    break  # Stop at first non-import, non-comment, non-empty line

        for i, line in enumerate(lines):
            line_tokens = await self._estimate_tokens(line)

            # Check if this line is a semantic boundary
            is_boundary = bool(boundary_regex.match(line.strip()))

            # Should we start a new chunk?
            should_split = (
                is_boundary and
                current_chunk_lines and
                current_tokens > max_tokens * 0.3  # At least 30% full
            ) or (current_tokens + line_tokens > max_tokens)

            if should_split:
                # Save current chunk
                chunks.append({
                    "content": "\n".join(current_chunk_lines),
                    "start_line": current_chunk_start,
                    "end_line": i - 1,
                    "tokens": current_tokens,
                })

                # Start new chunk
                current_chunk_lines = []
                current_chunk_start = i
                current_tokens = 0

                # Add imports to new chunk if preserving
                if self._strategy.preserve_imports and imports:
                    for imp in imports:
                        current_chunk_lines.append(imp)
                        current_tokens += await self._estimate_tokens(imp)
                    current_chunk_lines.append("")  # Empty line after imports

                # Add overlap lines from previous chunk
                if self._strategy.overlap_lines > 0 and chunks:
                    prev_lines = chunks[-1]["content"].split("\n")
                    overlap = prev_lines[-self._strategy.overlap_lines:]
                    for ol in overlap:
                        if ol not in imports:  # Don't duplicate imports
                            current_chunk_lines.append(f"# [overlap] {ol}")

            current_chunk_lines.append(line)
            current_tokens += line_tokens

        # Don't forget the last chunk
        if current_chunk_lines:
            chunks.append({
                "content": "\n".join(current_chunk_lines),
                "start_line": current_chunk_start,
                "end_line": len(lines) - 1,
                "tokens": current_tokens,
            })

        return chunks

    async def _chunk_by_lines(
        self,
        lines: List[str],
        max_tokens: int,
    ) -> List[Dict[str, Any]]:
        """Simple line-based chunking."""
        chunks = []
        current_chunk_lines: List[str] = []
        current_chunk_start = 0
        current_tokens = 0

        for i, line in enumerate(lines):
            line_tokens = await self._estimate_tokens(line)

            if current_tokens + line_tokens > max_tokens:
                # Save current chunk
                chunks.append({
                    "content": "\n".join(current_chunk_lines),
                    "start_line": current_chunk_start,
                    "end_line": i - 1,
                    "tokens": current_tokens,
                })

                # Start new chunk with overlap
                overlap_start = max(0, len(current_chunk_lines) - self._strategy.overlap_lines)
                current_chunk_lines = current_chunk_lines[overlap_start:]
                current_chunk_start = i - len(current_chunk_lines)
                current_tokens = sum(
                    self._estimate_tokens_sync(l) for l in current_chunk_lines
                )

            current_chunk_lines.append(line)
            current_tokens += line_tokens

        # Last chunk
        if current_chunk_lines:
            chunks.append({
                "content": "\n".join(current_chunk_lines),
                "start_line": current_chunk_start,
                "end_line": len(lines) - 1,
                "tokens": current_tokens,
            })

        return chunks

    async def _estimate_tokens(self, text: str) -> int:
        """Estimate token count for text."""
        # Cache check
        if text in self._tokenizer_cache:
            return self._tokenizer_cache[text]

        # Simple estimation: ~4 characters per token
        tokens = len(text) // 4 + 1

        # Cache result
        if len(self._tokenizer_cache) < 10000:
            self._tokenizer_cache[text] = tokens

        return tokens

    def _estimate_tokens_sync(self, text: str) -> int:
        """Synchronous token estimation."""
        return len(text) // 4 + 1


class AutonomousSystemDashboard:
    """
    v7.0: Comprehensive status dashboard for autonomous system.

    Provides real-time visibility into:
    - Component health
    - Sync status
    - Recent events
    - Performance metrics
    """

    def __init__(self):
        self._start_time = datetime.now()
        self._metrics: Dict[str, Any] = defaultdict(int)
        self._lock = asyncio.Lock()

    async def get_full_status(self) -> Dict[str, Any]:
        """Get comprehensive system status."""
        status = {
            "timestamp": datetime.now().isoformat(),
            "uptime_seconds": (datetime.now() - self._start_time).total_seconds(),
            "version": "7.0.0",
            "components": {},
            "cross_repo": {},
            "health": {},
            "metrics": {},
        }

        # Get component status
        try:
            controller = get_autonomous_loop_controller()
            status["components"]["loop_controller"] = controller.get_status()
        except Exception as e:
            status["components"]["loop_controller"] = {"error": str(e)}

        try:
            sync_manager = get_cross_repo_sync_manager()
            status["cross_repo"]["sync_manager"] = await sync_manager.get_status()
        except Exception as e:
            status["cross_repo"]["sync_manager"] = {"error": str(e)}

        # Get health monitor status
        global _health_monitor
        if _health_monitor:
            status["health"] = _health_monitor.get_status()

        # Get retry manager status
        global _retry_manager
        if _retry_manager:
            status["cross_repo"]["retry_manager"] = _retry_manager.get_status()

        # Get file lock metrics
        global _adaptive_lock_manager
        if _adaptive_lock_manager:
            status["components"]["file_lock_manager"] = _adaptive_lock_manager.get_metrics_summary()

        # Get hot-swap status
        global _hot_swap_manager
        if _hot_swap_manager:
            status["components"]["hot_swap_manager"] = {
                "active_swaps": len(_hot_swap_manager._active_swaps),
                "swap_history_count": len(_hot_swap_manager._swap_history),
            }

        # Add metrics
        async with self._lock:
            status["metrics"] = dict(self._metrics)

        return status

    async def record_metric(self, name: str, value: Any) -> None:
        """Record a metric."""
        async with self._lock:
            self._metrics[name] = value

    async def increment_counter(self, name: str, delta: int = 1) -> None:
        """Increment a counter metric."""
        async with self._lock:
            self._metrics[name] = self._metrics.get(name, 0) + delta

    def get_summary(self) -> str:
        """Get a text summary of system status."""
        lines = [
            "=" * 60,
            "Ironcliw Autonomous System v7.0 Status",
            "=" * 60,
            f"Uptime: {(datetime.now() - self._start_time).total_seconds():.0f}s",
            "",
        ]

        # Add more summary info as needed
        return "\n".join(lines)


# =============================================================================
# v7.0: GLOBAL INSTANCES
# =============================================================================

_adaptive_lock_manager: Optional[AdaptiveFileLockManager] = None
_sanitization_whitelist: Optional[CodeSanitizationWhitelist] = None
_conflict_resolver: Optional[DependencyConflictResolver] = None
_multi_format_handler: Optional[MultiFormatReactorEventHandler] = None
_hot_swap_manager: Optional[ModelHotSwapManager] = None
_retry_manager: Optional[CrossRepoSyncRetryManager] = None
_health_monitor: Optional[EventBasedHealthMonitor] = None
_import_updater: Optional[ImportPathAutoUpdater] = None
_file_chunker: Optional[IntelligentFileChunker] = None
_dashboard: Optional[AutonomousSystemDashboard] = None


# =============================================================================
# v7.0: FACTORY FUNCTIONS
# =============================================================================

def get_adaptive_lock_manager() -> AdaptiveFileLockManager:
    """Get or create the adaptive lock manager."""
    global _adaptive_lock_manager
    if _adaptive_lock_manager is None:
        _adaptive_lock_manager = AdaptiveFileLockManager()
    return _adaptive_lock_manager


def get_sanitization_whitelist() -> CodeSanitizationWhitelist:
    """Get or create the sanitization whitelist."""
    global _sanitization_whitelist
    if _sanitization_whitelist is None:
        _sanitization_whitelist = CodeSanitizationWhitelist()
    return _sanitization_whitelist


def get_conflict_resolver() -> DependencyConflictResolver:
    """Get or create the dependency conflict resolver."""
    global _conflict_resolver
    if _conflict_resolver is None:
        _conflict_resolver = DependencyConflictResolver()
    return _conflict_resolver


def get_multi_format_handler() -> MultiFormatReactorEventHandler:
    """Get or create the multi-format event handler."""
    global _multi_format_handler
    if _multi_format_handler is None:
        _multi_format_handler = MultiFormatReactorEventHandler()
    return _multi_format_handler


def get_hot_swap_manager() -> ModelHotSwapManager:
    """Get or create the model hot-swap manager."""
    global _hot_swap_manager
    if _hot_swap_manager is None:
        _hot_swap_manager = ModelHotSwapManager()
    return _hot_swap_manager


def get_retry_manager() -> CrossRepoSyncRetryManager:
    """Get or create the retry manager."""
    global _retry_manager
    if _retry_manager is None:
        _retry_manager = CrossRepoSyncRetryManager()
    return _retry_manager


def get_health_monitor() -> EventBasedHealthMonitor:
    """Get or create the health monitor."""
    global _health_monitor
    if _health_monitor is None:
        _health_monitor = EventBasedHealthMonitor()
    return _health_monitor


def get_import_updater() -> ImportPathAutoUpdater:
    """Get or create the import path updater."""
    global _import_updater
    if _import_updater is None:
        _import_updater = ImportPathAutoUpdater()
    return _import_updater


def get_file_chunker(strategy: Optional[ChunkingStrategy] = None) -> IntelligentFileChunker:
    """Get or create the file chunker."""
    global _file_chunker
    if _file_chunker is None:
        _file_chunker = IntelligentFileChunker(strategy)
    return _file_chunker


def get_dashboard() -> AutonomousSystemDashboard:
    """Get or create the status dashboard."""
    global _dashboard
    if _dashboard is None:
        _dashboard = AutonomousSystemDashboard()
    return _dashboard


# =============================================================================
# v7.0: MASTER INITIALIZATION
# =============================================================================

async def initialize_autonomous_system_v7(
    start_loops: bool = True,
    enable_adaptive_locking: bool = True,
    enable_sanitization_whitelist: bool = True,
    enable_conflict_resolution: bool = True,
    enable_multi_format_events: bool = True,
    enable_hot_swap: bool = True,
    enable_retry_manager: bool = True,
    enable_health_monitor: bool = True,
    enable_import_updater: bool = True,
    enable_file_chunker: bool = True,
    enable_dashboard: bool = True,
) -> Dict[str, Any]:
    """
    Initialize the complete v7.0 autonomous self-programming system.

    This builds on v6.0 and adds:
    - Adaptive file lock sensitivity with ML-based tuning
    - Code sanitization whitelist mechanism
    - Dependency version conflict detection and resolution
    - Multi-format Reactor Core event handling
    - Model hot-swap mechanism with zero-downtime switching
    - Cross-repo sync with exponential backoff retry
    - Event-based health monitoring with circuit breakers
    - Import path auto-updater
    - Large file intelligent chunking system
    - Comprehensive status dashboard

    Args:
        start_loops: Whether to start background loops
        enable_*: Feature flags for individual components

    Returns:
        Dictionary with all initialized components
    """
    logger.info("🚀 Initializing Autonomous System v7.0...")
    start_time = time.monotonic()

    # First initialize v6.0
    v6_components = await initialize_autonomous_system_v6(
        start_loops=start_loops,
        enable_web_search=True,
        enable_reactor_feedback=True,
        enable_prime_training=True,
        enable_file_locking=True,
    )

    components: Dict[str, Any] = dict(v6_components)

    try:
        # Initialize v7.0 components in parallel where possible
        async def init_adaptive_lock():
            if enable_adaptive_locking:
                return ("adaptive_lock_manager", get_adaptive_lock_manager())
            return None

        async def init_whitelist():
            if enable_sanitization_whitelist:
                whitelist = get_sanitization_whitelist()
                # Add built-in safe patterns
                for pattern_info in await whitelist.get_builtin_whitelisted_patterns():
                    await whitelist.add_entry(
                        pattern_info["pattern"],
                        pattern_info["reason"],
                        added_by="system_builtin",
                    )
                return ("sanitization_whitelist", whitelist)
            return None

        async def init_conflict_resolver():
            if enable_conflict_resolution:
                return ("conflict_resolver", get_conflict_resolver())
            return None

        async def init_multi_format():
            if enable_multi_format_events:
                return ("multi_format_handler", get_multi_format_handler())
            return None

        async def init_hot_swap():
            if enable_hot_swap:
                return ("hot_swap_manager", get_hot_swap_manager())
            return None

        async def init_retry():
            if enable_retry_manager:
                return ("retry_manager", get_retry_manager())
            return None

        async def init_health():
            if enable_health_monitor:
                monitor = get_health_monitor()
                # Register v6.0 components for monitoring
                for name, comp in v6_components.items():
                    if hasattr(comp, 'get_status'):
                        async def health_check(c=comp):
                            try:
                                status = c.get_status()
                                return status.get("running", True)
                            except Exception:
                                return False
                        await monitor.register_component(name, comp, health_check)
                if start_loops:
                    await monitor.start()
                return ("health_monitor", monitor)
            return None

        async def init_import_updater():
            if enable_import_updater:
                return ("import_updater", get_import_updater())
            return None

        async def init_chunker():
            if enable_file_chunker:
                return ("file_chunker", get_file_chunker())
            return None

        async def init_dashboard():
            if enable_dashboard:
                return ("dashboard", get_dashboard())
            return None

        # Run all initializations in parallel
        results = await asyncio.gather(
            init_adaptive_lock(),
            init_whitelist(),
            init_conflict_resolver(),
            init_multi_format(),
            init_hot_swap(),
            init_retry(),
            init_health(),
            init_import_updater(),
            init_chunker(),
            init_dashboard(),
            return_exceptions=True,
        )

        # Process results
        for result in results:
            if isinstance(result, tuple) and result is not None:
                name, comp = result
                components[name] = comp
                logger.info(f"  ✅ {name} initialized (v7.0)")
            elif isinstance(result, Exception):
                logger.warning(f"  ⚠️ Component initialization failed: {result}")

        # Wire v7.0 components together
        await _wire_v7_components(components)

        elapsed = time.monotonic() - start_time
        logger.info(f"✅ Autonomous System v7.0 initialized in {elapsed:.2f}s")
        logger.info(f"   Components: {list(components.keys())}")

        return components

    except Exception as e:
        logger.error(f"❌ Autonomous System v7.0 initialization failed: {e}")
        raise


async def _wire_v7_components(components: Dict[str, Any]) -> None:
    """Wire v7.0 components together."""

    # Wire adaptive lock manager into file lock manager
    adaptive_lock = components.get("adaptive_lock_manager")
    file_lock = components.get("file_lock_manager")
    if adaptive_lock and file_lock:
        # Override the file lock's sensitivity with adaptive sensitivity
        original_check = file_lock._should_block_edit if hasattr(file_lock, '_should_block_edit') else None
        if original_check:
            async def adaptive_check(file_path: str) -> bool:
                sensitivity = await adaptive_lock.get_adaptive_sensitivity(file_path)
                # Use sensitivity to adjust blocking threshold
                return random.random() < sensitivity
            file_lock._should_block_edit = adaptive_check
        logger.debug("  Wired adaptive_lock_manager → file_lock_manager")

    # Wire sanitization whitelist into code sanitizer
    whitelist = components.get("sanitization_whitelist")
    sanitizer = components.get("code_sanitizer")
    if whitelist and sanitizer:
        # Add whitelist check before sanitization
        original_validate = sanitizer.validate_and_sanitize
        async def validate_with_whitelist(code: str, source_url: Optional[str] = None):
            # Check whitelist first
            is_whitelisted, reason = await whitelist.is_whitelisted(code, source_url)
            if is_whitelisted:
                logger.debug(f"Code whitelisted: {reason}")
                return CodeValidationResult(
                    is_safe=True,
                    risk_level=SecurityRisk.SAFE,
                    issues_found=[],
                    sanitized_code=code,
                    blocked_patterns=[],
                    auto_fixed=[],
                )
            return await original_validate(code, source_url)
        sanitizer.validate_and_sanitize = validate_with_whitelist
        logger.debug("  Wired sanitization_whitelist → code_sanitizer")

    # Wire conflict resolver into dependency installer
    resolver = components.get("conflict_resolver")
    installer = components.get("dependency_installer")
    if resolver and installer:
        # Add conflict check before installation
        original_install = installer.analyze_and_install
        async def install_with_conflict_check(code: str, dry_run: bool = False):
            result = await original_install(code, dry_run)
            # Check for conflicts on installed packages
            for pkg in result.get("installed", []):
                conflict = await resolver.check_conflicts(pkg)
                if conflict:
                    resolution = await resolver.resolve_conflict(conflict, dry_run)
                    result.setdefault("conflict_resolutions", []).append(resolution)
            return result
        installer.analyze_and_install = install_with_conflict_check
        logger.debug("  Wired conflict_resolver → dependency_installer")

    # Wire multi-format handler into reactor feedback receiver
    multi_format = components.get("multi_format_handler")
    reactor = components.get("reactor_feedback_receiver")
    if multi_format and reactor:
        # Override event parsing to use multi-format handler
        reactor._parse_event = multi_format.parse_event
        logger.debug("  Wired multi_format_handler → reactor_feedback_receiver")

    # Wire retry manager into cross-repo sync manager
    retry = components.get("retry_manager")
    sync = components.get("cross_repo_sync_manager")
    if retry and sync:
        # Wrap sync operations with retry logic
        original_broadcast = sync.broadcast_event
        async def broadcast_with_retry(event_type: str, payload: Dict[str, Any]):
            async def do_broadcast():
                return await original_broadcast(event_type, payload)
            return await retry.execute_with_retry("broadcast", do_broadcast, event_type)
        sync.broadcast_event = broadcast_with_retry
        logger.debug("  Wired retry_manager → cross_repo_sync_manager")

    # Wire health monitor events to dashboard
    health = components.get("health_monitor")
    dashboard = components.get("dashboard")
    if health and dashboard:
        health.on_event("component_failed", lambda e: asyncio.create_task(
            dashboard.increment_counter("component_failures")
        ))
        health.on_event("component_recovered", lambda e: asyncio.create_task(
            dashboard.increment_counter("component_recoveries")
        ))
        logger.debug("  Wired health_monitor → dashboard")

    logger.info("  ✅ v7.0 component wiring complete")


async def shutdown_autonomous_system_v7() -> None:
    """Shutdown all v7.0 autonomous system components."""
    logger.info("Shutting down Autonomous System v7.0...")

    global _health_monitor, _dashboard

    # Stop health monitor first
    if _health_monitor:
        await _health_monitor.stop()

    # Shutdown v6.0
    await shutdown_autonomous_system_v6()

    # Reset v7.0 globals
    global _adaptive_lock_manager, _sanitization_whitelist, _conflict_resolver
    global _multi_format_handler, _hot_swap_manager, _retry_manager
    global _import_updater, _file_chunker

    _adaptive_lock_manager = None
    _sanitization_whitelist = None
    _conflict_resolver = None
    _multi_format_handler = None
    _hot_swap_manager = None
    _retry_manager = None
    _health_monitor = None
    _import_updater = None
    _file_chunker = None
    _dashboard = None

    logger.info("✅ Autonomous System v7.0 shutdown complete")


# =============================================================================
# v8.0: "Ironcliw, IMPROVE YOURSELF" - INTELLIGENT AUTONOMOUS SELF-IMPROVEMENT
# =============================================================================
# This implements the complete "improve yourself" command with:
# 1. Intelligent file selection (picks best files to improve)
# 2. Goal decomposition (vague command → specific tasks)
# 3. Priority scoring (which improvements matter most)
# 4. Autonomous scheduling (continuous improvement)
# 5. Cross-repo intelligence (Ironcliw + Prime + Reactor)
# 6. Voice command integration
# 7. Learning from past improvements
# =============================================================================


class ImprovementPriority(Enum):
    """Priority levels for improvements."""
    CRITICAL = 100    # Security, crashes, data loss
    HIGH = 75         # Bugs, performance issues
    MEDIUM = 50       # Code smells, technical debt
    LOW = 25          # Style, minor improvements
    COSMETIC = 10     # Comments, formatting


class ImprovementCategory(Enum):
    """Categories of improvements."""
    SECURITY = "security"
    PERFORMANCE = "performance"
    RELIABILITY = "reliability"
    MAINTAINABILITY = "maintainability"
    TESTABILITY = "testability"
    READABILITY = "readability"
    DOCUMENTATION = "documentation"
    ARCHITECTURE = "architecture"
    ERROR_HANDLING = "error_handling"
    ASYNC_PATTERNS = "async_patterns"


@dataclass
class FileImprovementCandidate:
    """A candidate file for improvement."""
    file_path: str
    priority_score: float
    categories: List[ImprovementCategory]
    issues_found: List[Dict[str, Any]]
    estimated_impact: str  # "high", "medium", "low"
    estimated_complexity: str  # "simple", "moderate", "complex"
    last_modified: Optional[datetime] = None
    last_improved: Optional[datetime] = None
    improvement_count: int = 0
    lines_of_code: int = 0
    cyclomatic_complexity: float = 0.0
    test_coverage: float = 0.0


@dataclass
class ImprovementGoal:
    """A decomposed improvement goal."""
    id: str
    description: str
    target_files: List[str]
    category: ImprovementCategory
    priority: ImprovementPriority
    subtasks: List["ImprovementGoal"] = field(default_factory=list)
    dependencies: List[str] = field(default_factory=list)
    estimated_effort: str = "unknown"  # "trivial", "small", "medium", "large"
    success_criteria: List[str] = field(default_factory=list)
    status: str = "pending"  # "pending", "in_progress", "completed", "failed"
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class ImprovementResult:
    """Result of an improvement attempt."""
    goal_id: str
    file_path: str
    success: bool
    changes_made: List[str]
    tests_passed: bool
    performance_impact: Optional[Dict[str, float]] = None
    error: Optional[str] = None
    diff_summary: Optional[str] = None
    rollback_available: bool = True
    completed_at: datetime = field(default_factory=datetime.now)


class IntelligentFileSelector:
    """
    v8.0: Intelligent file selection for "improve yourself" command.

    Uses multiple signals to select the best files to improve:
    - Technical debt score
    - Code complexity
    - Recent error history
    - Test coverage gaps
    - Commit frequency (hot files)
    - Dependency importance
    - Security vulnerability indicators
    """

    def __init__(self):
        self._file_scores: Dict[str, FileImprovementCandidate] = {}
        self._lock = asyncio.Lock()
        self._cache_ttl_s = 300  # 5 minute cache
        self._last_scan: Optional[datetime] = None

        # Weights for scoring
        self._weights = {
            "technical_debt": 0.25,
            "complexity": 0.15,
            "error_frequency": 0.20,
            "test_coverage_gap": 0.15,
            "hotness": 0.10,
            "dependency_importance": 0.10,
            "security_risk": 0.05,
        }

        # Patterns that indicate improvement opportunities
        self._improvement_patterns = {
            # Security issues
            r"eval\s*\(": (ImprovementCategory.SECURITY, ImprovementPriority.CRITICAL),
            r"exec\s*\(": (ImprovementCategory.SECURITY, ImprovementPriority.CRITICAL),
            r"pickle\.loads": (ImprovementCategory.SECURITY, ImprovementPriority.HIGH),
            r"shell\s*=\s*True": (ImprovementCategory.SECURITY, ImprovementPriority.HIGH),

            # Error handling
            r"except\s*:": (ImprovementCategory.ERROR_HANDLING, ImprovementPriority.MEDIUM),
            r"pass\s*$": (ImprovementCategory.ERROR_HANDLING, ImprovementPriority.LOW),

            # Performance
            r"time\.sleep\s*\(\s*\d+\s*\)": (ImprovementCategory.PERFORMANCE, ImprovementPriority.MEDIUM),
            r"for\s+\w+\s+in\s+\w+\.items\(\)": (ImprovementCategory.PERFORMANCE, ImprovementPriority.LOW),

            # Async patterns
            r"asyncio\.run\s*\(": (ImprovementCategory.ASYNC_PATTERNS, ImprovementPriority.MEDIUM),
            r"loop\.run_until_complete": (ImprovementCategory.ASYNC_PATTERNS, ImprovementPriority.MEDIUM),

            # Code smells
            r"# ?TODO": (ImprovementCategory.MAINTAINABILITY, ImprovementPriority.LOW),
            r"# ?FIXME": (ImprovementCategory.MAINTAINABILITY, ImprovementPriority.MEDIUM),
            r"# ?HACK": (ImprovementCategory.MAINTAINABILITY, ImprovementPriority.MEDIUM),
            r"# ?XXX": (ImprovementCategory.MAINTAINABILITY, ImprovementPriority.MEDIUM),

            # Documentation
            r'"""[^"]*"""': (ImprovementCategory.DOCUMENTATION, ImprovementPriority.COSMETIC),
        }

    async def select_files_to_improve(
        self,
        project_root: Optional[Path] = None,
        max_files: int = 5,
        categories: Optional[List[ImprovementCategory]] = None,
        min_priority: ImprovementPriority = ImprovementPriority.LOW,
        exclude_patterns: Optional[List[str]] = None,
    ) -> List[FileImprovementCandidate]:
        """
        Select the best files to improve.

        Args:
            project_root: Root directory to scan
            max_files: Maximum number of files to return
            categories: Filter by improvement categories
            min_priority: Minimum priority threshold
            exclude_patterns: Regex patterns to exclude

        Returns:
            List of files sorted by improvement priority
        """
        if project_root is None:
            project_root = Path.cwd()

        # Check cache
        async with self._lock:
            if self._last_scan and (datetime.now() - self._last_scan).seconds < self._cache_ttl_s:
                candidates = list(self._file_scores.values())
            else:
                # Scan for candidates
                candidates = await self._scan_for_candidates(project_root, exclude_patterns)
                self._last_scan = datetime.now()

        # Filter by categories
        if categories:
            candidates = [
                c for c in candidates
                if any(cat in categories for cat in c.categories)
            ]

        # Filter by priority
        min_score = min_priority.value
        candidates = [c for c in candidates if c.priority_score >= min_score]

        # Sort by priority score (descending)
        candidates.sort(key=lambda x: x.priority_score, reverse=True)

        return candidates[:max_files]

    async def _scan_for_candidates(
        self,
        project_root: Path,
        exclude_patterns: Optional[List[str]] = None,
    ) -> List[FileImprovementCandidate]:
        """Scan project for improvement candidates."""
        candidates = []
        exclude_patterns = exclude_patterns or [
            r"__pycache__",
            r"\.git",
            r"\.venv",
            r"venv",
            r"node_modules",
            r"\.pyc$",
            r"test_.*\.py$",
            r".*_test\.py$",
        ]

        exclude_regex = re.compile("|".join(exclude_patterns))

        # Scan Python files
        for py_file in project_root.rglob("*.py"):
            file_path = str(py_file)

            # Skip excluded patterns
            if exclude_regex.search(file_path):
                continue

            try:
                candidate = await self._analyze_file(py_file)
                if candidate:
                    candidates.append(candidate)
                    self._file_scores[file_path] = candidate
            except Exception as e:
                logger.debug(f"Failed to analyze {py_file}: {e}")

        return candidates

    async def _analyze_file(self, file_path: Path) -> Optional[FileImprovementCandidate]:
        """Analyze a single file for improvement opportunities."""
        try:
            content = await asyncio.to_thread(file_path.read_text)
        except Exception:
            return None

        lines = content.split("\n")
        issues_found = []
        categories_found = set()
        max_priority = ImprovementPriority.COSMETIC

        # Check for improvement patterns
        for pattern, (category, priority) in self._improvement_patterns.items():
            matches = list(re.finditer(pattern, content, re.MULTILINE))
            if matches:
                categories_found.add(category)
                if priority.value > max_priority.value:
                    max_priority = priority

                for match in matches[:3]:  # Limit to 3 examples
                    line_num = content[:match.start()].count("\n") + 1
                    issues_found.append({
                        "pattern": pattern,
                        "category": category.value,
                        "priority": priority.value,
                        "line": line_num,
                        "snippet": match.group(0)[:50],
                    })

        # Calculate metrics
        loc = len([l for l in lines if l.strip() and not l.strip().startswith("#")])
        complexity = await self._estimate_complexity(content)

        # Calculate priority score
        score = self._calculate_priority_score(
            issues_found=issues_found,
            loc=loc,
            complexity=complexity,
            max_priority=max_priority,
        )

        # Estimate impact and complexity
        impact = "high" if max_priority.value >= 75 else "medium" if max_priority.value >= 50 else "low"
        effort = "complex" if complexity > 20 else "moderate" if complexity > 10 else "simple"

        return FileImprovementCandidate(
            file_path=str(file_path),
            priority_score=score,
            categories=list(categories_found),
            issues_found=issues_found,
            estimated_impact=impact,
            estimated_complexity=effort,
            last_modified=datetime.fromtimestamp(file_path.stat().st_mtime),
            lines_of_code=loc,
            cyclomatic_complexity=complexity,
        )

    async def _estimate_complexity(self, code: str) -> float:
        """Estimate cyclomatic complexity."""
        # Simple estimation based on control flow keywords
        control_keywords = [
            r"\bif\b", r"\belif\b", r"\belse\b",
            r"\bfor\b", r"\bwhile\b",
            r"\btry\b", r"\bexcept\b",
            r"\band\b", r"\bor\b",
            r"\bwith\b",
        ]

        complexity = 1  # Base complexity
        for keyword in control_keywords:
            complexity += len(re.findall(keyword, code))

        return complexity

    def _calculate_priority_score(
        self,
        issues_found: List[Dict],
        loc: int,
        complexity: float,
        max_priority: ImprovementPriority,
    ) -> float:
        """Calculate overall priority score."""
        # Base score from issues
        issue_score = sum(issue["priority"] for issue in issues_found)

        # Normalize by LOC (larger files may have more issues)
        if loc > 0:
            issue_density = min(100, (issue_score / loc) * 100)
        else:
            issue_density = 0

        # Complexity factor (higher complexity = higher priority)
        complexity_factor = min(100, complexity * 3)

        # Combine scores
        score = (
            issue_density * 0.4 +
            max_priority.value * 0.4 +
            complexity_factor * 0.2
        )

        return min(100, score)


class GoalDecomposer:
    """
    v8.0: Decomposes vague improvement goals into specific tasks.

    "Improve yourself" → [
        "Fix security issues in auth.py",
        "Add error handling to api.py",
        "Improve test coverage for utils.py",
        ...
    ]
    """

    def __init__(self, llm_client: Optional[Any] = None, oracle: Optional[Any] = None):
        self._llm_client = llm_client
        self._oracle = oracle
        self._lock = asyncio.Lock()
        self._goal_templates = self._load_goal_templates()
        self._initialized = False

    async def set_oracle(self, oracle: Any) -> None:
        """
        v93.0: Set the oracle for enhanced goal decomposition.

        The oracle provides code analysis capabilities for more intelligent
        goal decomposition based on actual codebase state.

        Args:
            oracle: Oracle instance for code analysis
        """
        async with self._lock:
            self._oracle = oracle
            logger.debug("[GoalDecomposer] Oracle configured")

    async def set_llm_client(self, llm_client: Any) -> None:
        """
        v93.0: Set the LLM client for AI-powered goal decomposition.

        The LLM client enables natural language understanding and
        sophisticated goal analysis beyond template matching.

        Args:
            llm_client: LLM client instance (Claude, etc.)
        """
        async with self._lock:
            self._llm_client = llm_client
            logger.debug("[GoalDecomposer] LLM client configured")

    async def initialize(self) -> bool:
        """
        v93.0: Initialize the goal decomposer with async resources.

        Returns:
            True if initialization successful
        """
        if self._initialized:
            return True

        async with self._lock:
            # Validate oracle connection if present
            if self._oracle:
                try:
                    # Attempt a simple query to validate oracle
                    if hasattr(self._oracle, 'is_ready'):
                        await self._oracle.is_ready()
                except Exception as e:
                    logger.warning(f"[GoalDecomposer] Oracle validation failed: {e}")

            self._initialized = True
            logger.info("[GoalDecomposer] Initialized successfully")
            return True

    def _load_goal_templates(self) -> Dict[str, List[str]]:
        """Load goal decomposition templates."""
        return {
            "improve yourself": [
                "Fix critical security vulnerabilities",
                "Improve error handling and resilience",
                "Optimize performance bottlenecks",
                "Increase test coverage",
                "Refactor complex code for maintainability",
                "Update deprecated patterns",
            ],
            "improve performance": [
                "Profile and identify bottlenecks",
                "Optimize database queries",
                "Add caching where beneficial",
                "Use async patterns for I/O",
                "Reduce memory allocations",
            ],
            "improve security": [
                "Remove unsafe eval/exec usage",
                "Sanitize user inputs",
                "Update vulnerable dependencies",
                "Add input validation",
                "Implement proper authentication",
            ],
            "improve code quality": [
                "Add type hints",
                "Improve docstrings",
                "Reduce code complexity",
                "Remove code duplication",
                "Apply consistent formatting",
            ],
            "improve reliability": [
                "Add proper error handling",
                "Implement retry logic",
                "Add circuit breakers",
                "Improve logging",
                "Add health checks",
            ],
        }

    async def decompose_goal(
        self,
        goal: str,
        file_selector: IntelligentFileSelector,
        project_root: Optional[Path] = None,
    ) -> List[ImprovementGoal]:
        """
        Decompose a vague goal into specific improvement tasks.

        Args:
            goal: The vague goal (e.g., "improve yourself")
            file_selector: File selector to find relevant files
            project_root: Project root directory

        Returns:
            List of specific improvement goals
        """
        goals = []
        goal_lower = goal.lower().strip()

        # Find matching template
        template_key = None
        for key in self._goal_templates:
            if key in goal_lower or goal_lower in key:
                template_key = key
                break

        # Default to "improve yourself" if no match
        if template_key is None:
            template_key = "improve yourself"

        template_goals = self._goal_templates[template_key]

        # Get improvement candidates
        candidates = await file_selector.select_files_to_improve(
            project_root=project_root,
            max_files=10,
        )

        # Map goals to files
        for i, goal_desc in enumerate(template_goals):
            # Find relevant files for this goal type
            category = self._goal_to_category(goal_desc)
            relevant_files = [
                c.file_path for c in candidates
                if category in c.categories or not c.categories
            ][:3]  # Max 3 files per goal

            if not relevant_files:
                # Use top priority files if no category match
                relevant_files = [c.file_path for c in candidates[:2]]

            goal_obj = ImprovementGoal(
                id=f"goal_{i}_{int(datetime.now().timestamp())}",
                description=goal_desc,
                target_files=relevant_files,
                category=category,
                priority=self._estimate_priority(goal_desc),
                estimated_effort=self._estimate_effort(goal_desc),
                success_criteria=self._generate_success_criteria(goal_desc),
            )
            goals.append(goal_obj)

        # Sort by priority
        goals.sort(key=lambda g: g.priority.value, reverse=True)

        return goals

    def _goal_to_category(self, goal_desc: str) -> ImprovementCategory:
        """Map goal description to category."""
        goal_lower = goal_desc.lower()

        if any(w in goal_lower for w in ["security", "vulnerab", "unsafe"]):
            return ImprovementCategory.SECURITY
        elif any(w in goal_lower for w in ["performance", "optim", "cache", "bottleneck"]):
            return ImprovementCategory.PERFORMANCE
        elif any(w in goal_lower for w in ["error", "exception", "retry", "resilience"]):
            return ImprovementCategory.ERROR_HANDLING
        elif any(w in goal_lower for w in ["test", "coverage"]):
            return ImprovementCategory.TESTABILITY
        elif any(w in goal_lower for w in ["async", "concurrent"]):
            return ImprovementCategory.ASYNC_PATTERNS
        elif any(w in goal_lower for w in ["document", "docstring", "comment"]):
            return ImprovementCategory.DOCUMENTATION
        elif any(w in goal_lower for w in ["refactor", "complex", "maintain"]):
            return ImprovementCategory.MAINTAINABILITY
        else:
            return ImprovementCategory.MAINTAINABILITY

    def _estimate_priority(self, goal_desc: str) -> ImprovementPriority:
        """Estimate priority from goal description."""
        goal_lower = goal_desc.lower()

        if any(w in goal_lower for w in ["critical", "security", "vulnerab"]):
            return ImprovementPriority.CRITICAL
        elif any(w in goal_lower for w in ["error", "bug", "fix"]):
            return ImprovementPriority.HIGH
        elif any(w in goal_lower for w in ["performance", "optimize"]):
            return ImprovementPriority.MEDIUM
        elif any(w in goal_lower for w in ["test", "refactor"]):
            return ImprovementPriority.LOW
        else:
            return ImprovementPriority.LOW

    def _estimate_effort(self, goal_desc: str) -> str:
        """Estimate effort from goal description."""
        goal_lower = goal_desc.lower()

        if any(w in goal_lower for w in ["refactor", "architect", "redesign"]):
            return "large"
        elif any(w in goal_lower for w in ["optimize", "improve", "update"]):
            return "medium"
        elif any(w in goal_lower for w in ["add", "fix", "remove"]):
            return "small"
        else:
            return "medium"

    def _generate_success_criteria(self, goal_desc: str) -> List[str]:
        """Generate success criteria for a goal."""
        goal_lower = goal_desc.lower()

        criteria = ["All tests pass after changes"]

        if "security" in goal_lower:
            criteria.extend([
                "No eval/exec calls remain",
                "All user inputs are validated",
            ])
        elif "performance" in goal_lower:
            criteria.extend([
                "No regression in benchmark results",
                "Memory usage not increased",
            ])
        elif "error" in goal_lower:
            criteria.extend([
                "All exceptions have specific handlers",
                "No bare except clauses",
            ])
        elif "test" in goal_lower:
            criteria.extend([
                "Test coverage increased",
                "New tests for uncovered code",
            ])

        return criteria


class AutonomousImprovementEngine:
    """
    v8.0: The core engine for "Ironcliw, improve yourself".

    Orchestrates:
    1. File selection
    2. Goal decomposition
    3. Improvement execution
    4. Validation and rollback
    5. Learning from results
    """

    def __init__(
        self,
        file_selector: Optional[IntelligentFileSelector] = None,
        goal_decomposer: Optional[GoalDecomposer] = None,
        llm_client: Optional[Any] = None,
        oracle: Optional[Any] = None,
    ):
        self._file_selector = file_selector or IntelligentFileSelector()
        self._goal_decomposer = goal_decomposer or GoalDecomposer(llm_client)
        self._llm_client = llm_client
        self._oracle = oracle

        self._lock = asyncio.Lock()
        self._running = False
        self._current_task: Optional[asyncio.Task] = None

        # State
        self._improvement_history: List[ImprovementResult] = []
        self._active_goals: Dict[str, ImprovementGoal] = {}
        self._queued_goals: Deque[ImprovementGoal] = deque(maxlen=100)

        # Configuration
        self._max_concurrent_improvements = 1  # Serial for safety
        self._improvement_timeout_s = 300  # 5 minutes per improvement
        self._auto_rollback_on_test_failure = True

        # Callbacks for voice/UI feedback
        self._progress_callbacks: List[Callable] = []
        self._completion_callbacks: List[Callable] = []

    def register_progress_callback(self, callback: Callable) -> None:
        """Register callback for progress updates."""
        self._progress_callbacks.append(callback)

    def register_completion_callback(self, callback: Callable) -> None:
        """Register callback for completion events."""
        self._completion_callbacks.append(callback)

    async def _notify_progress(self, message: str, progress: float = 0.0) -> None:
        """Notify progress to all callbacks."""
        for callback in self._progress_callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(message, progress)
                else:
                    callback(message, progress)
            except Exception as e:
                logger.debug(f"Progress callback error: {e}")

    async def _notify_completion(self, result: ImprovementResult) -> None:
        """Notify completion to all callbacks."""
        for callback in self._completion_callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(result)
                else:
                    callback(result)
            except Exception as e:
                logger.debug(f"Completion callback error: {e}")

    async def improve_yourself(
        self,
        project_root: Optional[Path] = None,
        max_improvements: int = 5,
        dry_run: bool = False,
        categories: Optional[List[ImprovementCategory]] = None,
    ) -> List[ImprovementResult]:
        """
        Execute "Ironcliw, improve yourself" command.

        This is the main entry point for autonomous self-improvement.

        Args:
            project_root: Project root to improve
            max_improvements: Maximum number of improvements to make
            dry_run: If True, show what would be done without making changes
            categories: Filter to specific improvement categories

        Returns:
            List of improvement results
        """
        logger.info("🚀 Starting 'Improve Yourself' process...")
        await self._notify_progress("Analyzing codebase...", 0.1)

        if project_root is None:
            project_root = Path.cwd()

        results = []

        try:
            # Step 1: Select files to improve
            await self._notify_progress("Selecting files to improve...", 0.2)
            candidates = await self._file_selector.select_files_to_improve(
                project_root=project_root,
                max_files=max_improvements * 2,  # Get extra for filtering
                categories=categories,
            )

            if not candidates:
                await self._notify_progress("No improvement opportunities found.", 1.0)
                logger.info("No improvement candidates found")
                return results

            logger.info(f"Found {len(candidates)} improvement candidates")

            # Step 2: Decompose into goals
            await self._notify_progress("Planning improvements...", 0.3)
            goals = await self._goal_decomposer.decompose_goal(
                "improve yourself",
                self._file_selector,
                project_root,
            )

            logger.info(f"Created {len(goals)} improvement goals")

            # Step 3: Execute improvements
            for i, goal in enumerate(goals[:max_improvements]):
                progress = 0.3 + (0.6 * (i / max_improvements))
                await self._notify_progress(
                    f"Improving: {goal.description}...",
                    progress,
                )

                if dry_run:
                    result = ImprovementResult(
                        goal_id=goal.id,
                        file_path=goal.target_files[0] if goal.target_files else "",
                        success=True,
                        changes_made=[f"[DRY RUN] Would: {goal.description}"],
                        tests_passed=True,
                    )
                else:
                    result = await self._execute_improvement(goal)

                results.append(result)
                await self._notify_completion(result)

                if not result.success:
                    logger.warning(f"Improvement failed: {goal.description}")
                else:
                    logger.info(f"Improvement succeeded: {goal.description}")

            # Step 4: Summary
            successful = sum(1 for r in results if r.success)
            await self._notify_progress(
                f"Completed: {successful}/{len(results)} improvements",
                1.0,
            )

            logger.info(f"✅ 'Improve Yourself' complete: {successful}/{len(results)} successful")

            return results

        except Exception as e:
            logger.error(f"'Improve Yourself' failed: {e}")
            await self._notify_progress(f"Error: {e}", 1.0)
            raise

    async def improve_file(
        self,
        file_path: str,
        goal: Optional[str] = None,
        dry_run: bool = False,
    ) -> ImprovementResult:
        """
        Improve a specific file.

        Args:
            file_path: Path to the file to improve
            goal: Optional specific improvement goal
            dry_run: If True, show what would be done

        Returns:
            Improvement result
        """
        logger.info(f"Improving file: {file_path}")
        await self._notify_progress(f"Analyzing {Path(file_path).name}...", 0.2)

        # Analyze the file
        candidate = await self._file_selector._analyze_file(Path(file_path))

        if not candidate:
            return ImprovementResult(
                goal_id="manual",
                file_path=file_path,
                success=False,
                changes_made=[],
                tests_passed=False,
                error="Could not analyze file",
            )

        # Create improvement goal
        improvement_goal = ImprovementGoal(
            id=f"file_improve_{int(datetime.now().timestamp())}",
            description=goal or f"Improve {Path(file_path).name}",
            target_files=[file_path],
            category=candidate.categories[0] if candidate.categories else ImprovementCategory.MAINTAINABILITY,
            priority=ImprovementPriority.MEDIUM,
        )

        if dry_run:
            return ImprovementResult(
                goal_id=improvement_goal.id,
                file_path=file_path,
                success=True,
                changes_made=[f"[DRY RUN] Would improve: {candidate.issues_found}"],
                tests_passed=True,
            )

        return await self._execute_improvement(improvement_goal)

    async def _execute_improvement(
        self,
        goal: ImprovementGoal,
    ) -> ImprovementResult:
        """Execute a single improvement goal."""
        if not goal.target_files:
            return ImprovementResult(
                goal_id=goal.id,
                file_path="",
                success=False,
                changes_made=[],
                tests_passed=False,
                error="No target files specified",
            )

        file_path = goal.target_files[0]
        changes_made = []

        try:
            # Read the file
            content = await asyncio.to_thread(Path(file_path).read_text)

            # Generate improvement using LLM
            improved_content = await self._generate_improvement(
                content,
                goal,
                file_path,
            )

            if improved_content == content:
                return ImprovementResult(
                    goal_id=goal.id,
                    file_path=file_path,
                    success=True,
                    changes_made=["No changes needed"],
                    tests_passed=True,
                )

            # Backup original
            backup_path = Path(file_path + ".bak")
            await asyncio.to_thread(backup_path.write_text, content)

            # Apply changes
            await asyncio.to_thread(Path(file_path).write_text, improved_content)
            changes_made.append(f"Applied improvements to {file_path}")

            # Run tests
            tests_passed = await self._run_tests(file_path)

            if not tests_passed and self._auto_rollback_on_test_failure:
                # Rollback
                await asyncio.to_thread(Path(file_path).write_text, content)
                changes_made.append("Rolled back due to test failure")

                return ImprovementResult(
                    goal_id=goal.id,
                    file_path=file_path,
                    success=False,
                    changes_made=changes_made,
                    tests_passed=False,
                    error="Tests failed after improvement",
                    rollback_available=False,
                )

            # Remove backup on success
            if backup_path.exists():
                backup_path.unlink()

            return ImprovementResult(
                goal_id=goal.id,
                file_path=file_path,
                success=True,
                changes_made=changes_made,
                tests_passed=tests_passed,
            )

        except Exception as e:
            logger.error(f"Improvement execution failed: {e}")
            return ImprovementResult(
                goal_id=goal.id,
                file_path=file_path,
                success=False,
                changes_made=changes_made,
                tests_passed=False,
                error=str(e),
            )

    async def _generate_improvement(
        self,
        content: str,
        goal: ImprovementGoal,
        file_path: str,
    ) -> str:
        """Generate improved code using LLM."""
        if not self._llm_client:
            # Without LLM, use pattern-based improvements
            return await self._apply_pattern_improvements(content, goal)

        # Use LLM for intelligent improvements
        prompt = f"""Improve the following Python code with this goal: {goal.description}

Category: {goal.category.value}
Success criteria: {', '.join(goal.success_criteria)}

Code:
```python
{content}
```

Return ONLY the improved Python code, no explanations."""

        try:
            response = await self._llm_client.generate(prompt)
            # Extract code from response
            if "```python" in response:
                code = response.split("```python")[1].split("```")[0]
            elif "```" in response:
                code = response.split("```")[1].split("```")[0]
            else:
                code = response

            return code.strip()
        except Exception as e:
            logger.warning(f"LLM improvement failed, using pattern-based: {e}")
            return await self._apply_pattern_improvements(content, goal)

    async def _apply_pattern_improvements(
        self,
        content: str,
        goal: ImprovementGoal,
    ) -> str:
        """Apply pattern-based improvements without LLM."""
        improved = content

        # Security improvements
        if goal.category == ImprovementCategory.SECURITY:
            # Replace bare eval with safer alternatives
            improved = re.sub(
                r'\beval\s*\(\s*([^)]+)\s*\)',
                r'ast.literal_eval(\1)',
                improved,
            )

        # Error handling improvements
        elif goal.category == ImprovementCategory.ERROR_HANDLING:
            # Replace bare except with specific exception
            improved = re.sub(
                r'except:\s*\n(\s+)pass',
                r'except Exception as e:\n\1logger.debug(f"Error: {e}")',
                improved,
            )

        # Async pattern improvements
        elif goal.category == ImprovementCategory.ASYNC_PATTERNS:
            # Replace asyncio.run in async context
            improved = re.sub(
                r'asyncio\.run\(([^)]+)\)',
                r'await \1',
                improved,
            )

        return improved

    async def _run_tests(self, file_path: str) -> bool:
        """Run tests related to a file."""
        try:
            # Try to find and run related tests
            test_file = Path(file_path).parent / f"test_{Path(file_path).name}"

            if test_file.exists():
                result = await asyncio.to_thread(
                    subprocess.run,
                    [sys.executable, "-m", "pytest", str(test_file), "-v"],
                    capture_output=True,
                    text=True,
                    timeout=60,
                )
                return result.returncode == 0

            # No specific test file, try general syntax check
            result = await asyncio.to_thread(
                subprocess.run,
                [sys.executable, "-m", "py_compile", file_path],
                capture_output=True,
                text=True,
                timeout=30,
            )
            return result.returncode == 0

        except Exception as e:
            logger.warning(f"Test execution failed: {e}")
            return True  # Assume passing if we can't run tests

    def get_status(self) -> Dict[str, Any]:
        """Get engine status."""
        return {
            "running": self._running,
            "active_goals": len(self._active_goals),
            "queued_goals": len(self._queued_goals),
            "improvement_history": len(self._improvement_history),
            "recent_improvements": [
                {
                    "file": r.file_path,
                    "success": r.success,
                    "completed": r.completed_at.isoformat(),
                }
                for r in self._improvement_history[-5:]
            ],
        }


class VoiceCommandHandler:
    """
    v8.0: Handles voice commands for self-improvement.

    Maps voice commands to improvement actions:
    - "Ironcliw, improve yourself" → improve_yourself()
    - "Ironcliw, improve [file]" → improve_file(file)
    - "Ironcliw, fix [file]" → improve_file(file, goal="fix")
    - "Ironcliw, optimize [file]" → improve_file(file, goal="optimize")
    """

    def __init__(self, improvement_engine: AutonomousImprovementEngine):
        self._engine = improvement_engine
        self._command_patterns = self._compile_patterns()

    def _compile_patterns(self) -> List[Tuple[re.Pattern, str, Callable]]:
        """Compile voice command patterns for natural language commands."""
        # Patterns are tried in order - more specific first
        return [
            # "Ironcliw, improve yourself" / "improve yourself"
            (
                re.compile(r"(?:jarvis[,\s]+)?improve\s+yourself", re.IGNORECASE),
                "improve_yourself",
                self._handle_improve_yourself,
            ),
            # "Make yourself better" / "Ironcliw, make yourself better"
            (
                re.compile(r"(?:jarvis[,\s]+)?make\s+yourself\s+better", re.IGNORECASE),
                "improve_yourself",
                self._handle_improve_yourself,
            ),
            # "Upgrade yourself"
            (
                re.compile(r"(?:jarvis[,\s]+)?upgrade\s+yourself", re.IGNORECASE),
                "improve_yourself",
                self._handle_improve_yourself,
            ),
            # "Self-improve" / "Self improve"
            (
                re.compile(r"(?:jarvis[,\s]+)?self[- ]?improv", re.IGNORECASE),
                "improve_yourself",
                self._handle_improve_yourself,
            ),
            # "Improve your code" / "Improve your system"
            (
                re.compile(r"(?:jarvis[,\s]+)?improve\s+your\s+(?:code|system|codebase)", re.IGNORECASE),
                "improve_yourself",
                self._handle_improve_yourself,
            ),
            # "Fix bugs" / "Fix bugs in the system"
            (
                re.compile(r"(?:jarvis[,\s]+)?fix\s+(?:the\s+)?bugs?(?:\s+in\s+(?:the\s+)?(?:system|code|codebase))?", re.IGNORECASE),
                "improve_yourself",
                self._handle_improve_yourself,
            ),
            # "Improve security"
            (
                re.compile(r"(?:jarvis[,\s]+)?improve\s+security", re.IGNORECASE),
                "improve_security",
                self._handle_improve_category,
            ),
            # "Improve performance" / "Make the code faster"
            (
                re.compile(r"(?:jarvis[,\s]+)?(?:improve\s+performance|make\s+(?:the\s+)?code\s+faster|optimize\s+performance)", re.IGNORECASE),
                "improve_performance",
                self._handle_improve_category,
            ),
            # "Make it better" (vague - default to improve_yourself)
            (
                re.compile(r"(?:jarvis[,\s]+)?make\s+(?:it\s+)?better", re.IGNORECASE),
                "improve_yourself",
                self._handle_improve_yourself,
            ),
            # "Improve specific file" - matches "[improve] path/file.py"
            (
                re.compile(r"(?:jarvis[,\s]+)?improve\s+(.+\.py)", re.IGNORECASE),
                "improve_file",
                self._handle_improve_file,
            ),
            # "Fix specific file"
            (
                re.compile(r"(?:jarvis[,\s]+)?fix\s+(.+\.py)", re.IGNORECASE),
                "fix_file",
                self._handle_fix_file,
            ),
            # "Optimize specific file"
            (
                re.compile(r"(?:jarvis[,\s]+)?optimize\s+(.+\.py)", re.IGNORECASE),
                "optimize_file",
                self._handle_optimize_file,
            ),
            # "Refactor specific file"
            (
                re.compile(r"(?:jarvis[,\s]+)?refactor\s+(.+\.py)", re.IGNORECASE),
                "refactor_file",
                self._handle_refactor_file,
            ),
        ]

    async def handle_command(self, command: str) -> Dict[str, Any]:
        """
        Handle a voice command for improvement.

        Args:
            command: The voice command text

        Returns:
            Result dictionary with response and actions taken
        """
        command = command.strip()

        for pattern, cmd_type, handler in self._command_patterns:
            match = pattern.search(command)
            if match:
                return await handler(match, command)

        # No pattern matched
        return {
            "success": False,
            "response": "I couldn't understand that improvement command. "
                       "Try 'improve yourself' or 'improve [filename].py'",
            "command_type": "unknown",
        }

    async def _handle_improve_yourself(
        self,
        match: re.Match,
        command: str,
    ) -> Dict[str, Any]:
        """Handle 'improve yourself' command."""
        try:
            results = await self._engine.improve_yourself(
                max_improvements=5,
                dry_run=False,
            )

            successful = sum(1 for r in results if r.success)

            return {
                "success": True,
                "response": f"I've made {successful} improvements to myself, Sir. "
                           f"Check the logs for details.",
                "command_type": "improve_yourself",
                "results": [
                    {"file": r.file_path, "success": r.success}
                    for r in results
                ],
            }

        except Exception as e:
            return {
                "success": False,
                "response": f"I encountered an error during self-improvement: {e}",
                "command_type": "improve_yourself",
                "error": str(e),
            }

    async def _handle_improve_file(
        self,
        match: re.Match,
        command: str,
    ) -> Dict[str, Any]:
        """Handle 'improve [file]' command."""
        file_path = match.group(1)

        # Search for the file
        found_path = await self._find_file(file_path)

        if not found_path:
            return {
                "success": False,
                "response": f"I couldn't find {file_path}. "
                           "Please provide the full path.",
                "command_type": "improve_file",
            }

        try:
            result = await self._engine.improve_file(found_path)

            if result.success:
                return {
                    "success": True,
                    "response": f"I've improved {Path(found_path).name}, Sir.",
                    "command_type": "improve_file",
                    "file": found_path,
                    "changes": result.changes_made,
                }
            else:
                return {
                    "success": False,
                    "response": f"I wasn't able to improve {Path(found_path).name}. "
                               f"Reason: {result.error}",
                    "command_type": "improve_file",
                    "file": found_path,
                    "error": result.error,
                }

        except Exception as e:
            return {
                "success": False,
                "response": f"Error improving {file_path}: {e}",
                "command_type": "improve_file",
                "error": str(e),
            }

    async def _handle_fix_file(
        self,
        match: re.Match,
        command: str,
    ) -> Dict[str, Any]:
        """Handle 'fix [file]' command."""
        file_path = match.group(1)
        found_path = await self._find_file(file_path)

        if not found_path:
            return {
                "success": False,
                "response": f"I couldn't find {file_path}.",
                "command_type": "fix_file",
            }

        result = await self._engine.improve_file(
            found_path,
            goal="Fix bugs and errors",
        )

        return {
            "success": result.success,
            "response": f"I've fixed issues in {Path(found_path).name}, Sir."
                       if result.success else
                       f"I couldn't fix {Path(found_path).name}: {result.error}",
            "command_type": "fix_file",
            "file": found_path,
        }

    async def _handle_optimize_file(
        self,
        match: re.Match,
        command: str,
    ) -> Dict[str, Any]:
        """Handle 'optimize [file]' command."""
        file_path = match.group(1)
        found_path = await self._find_file(file_path)

        if not found_path:
            return {
                "success": False,
                "response": f"I couldn't find {file_path}.",
                "command_type": "optimize_file",
            }

        result = await self._engine.improve_file(
            found_path,
            goal="Optimize performance",
        )

        return {
            "success": result.success,
            "response": f"I've optimized {Path(found_path).name}, Sir."
                       if result.success else
                       f"I couldn't optimize {Path(found_path).name}: {result.error}",
            "command_type": "optimize_file",
            "file": found_path,
        }

    async def _handle_refactor_file(
        self,
        match: re.Match,
        command: str,
    ) -> Dict[str, Any]:
        """Handle 'refactor [file]' command."""
        file_path = match.group(1)
        found_path = await self._find_file(file_path)

        if not found_path:
            return {
                "success": False,
                "response": f"I couldn't find {file_path}.",
                "command_type": "refactor_file",
            }

        result = await self._engine.improve_file(
            found_path,
            goal="Refactor for better maintainability and readability",
        )

        return {
            "success": result.success,
            "response": f"I've refactored {Path(found_path).name}, Sir."
                       if result.success else
                       f"I couldn't refactor {Path(found_path).name}: {result.error}",
            "command_type": "refactor_file",
            "file": found_path,
        }

    async def _handle_improve_category(
        self,
        match: re.Match,
        command: str,
    ) -> Dict[str, Any]:
        """Handle category-specific improvement commands like 'improve security'."""
        # Determine category from command
        command_lower = command.lower()

        if "security" in command_lower:
            from backend.core.ouroboros.native_integration import ImprovementCategory
            categories = [ImprovementCategory.SECURITY]
            category_name = "security"
        elif "performance" in command_lower or "faster" in command_lower:
            from backend.core.ouroboros.native_integration import ImprovementCategory
            categories = [ImprovementCategory.PERFORMANCE]
            category_name = "performance"
        elif "reliability" in command_lower:
            from backend.core.ouroboros.native_integration import ImprovementCategory
            categories = [ImprovementCategory.RELIABILITY]
            category_name = "reliability"
        elif "maintainability" in command_lower:
            from backend.core.ouroboros.native_integration import ImprovementCategory
            categories = [ImprovementCategory.MAINTAINABILITY]
            category_name = "maintainability"
        else:
            categories = None
            category_name = "general"

        try:
            results = await self._engine.improve_yourself(
                max_improvements=5,
                categories=categories,
                dry_run=False,
            )

            successful = sum(1 for r in results if r.success)

            return {
                "success": True,
                "response": f"I've made {successful} {category_name} improvements, Sir. "
                           f"Check the logs for details.",
                "command_type": f"improve_{category_name}",
                "improvements": len(results),
                "successful": successful,
            }
        except Exception as e:
            return {
                "success": False,
                "response": f"I encountered an error during {category_name} improvement: {e}",
                "command_type": f"improve_{category_name}",
                "error": str(e),
            }

    async def _find_file(self, file_path: str) -> Optional[str]:
        """Find a file by name or partial path."""
        # Try exact path first
        if Path(file_path).exists():
            return str(Path(file_path).absolute())

        # Search in project
        project_root = Path.cwd()
        for py_file in project_root.rglob("*.py"):
            if file_path in str(py_file) or py_file.name == file_path:
                return str(py_file)

        return None


# =============================================================================
# v8.0: GLOBAL INSTANCES
# =============================================================================

_file_selector: Optional[IntelligentFileSelector] = None
_goal_decomposer: Optional[GoalDecomposer] = None
_improvement_engine: Optional[AutonomousImprovementEngine] = None
_voice_handler: Optional[VoiceCommandHandler] = None


# =============================================================================
# v8.0: FACTORY FUNCTIONS
# =============================================================================

def get_intelligent_file_selector() -> IntelligentFileSelector:
    """Get or create the intelligent file selector."""
    global _file_selector
    if _file_selector is None:
        _file_selector = IntelligentFileSelector()
    return _file_selector


def get_goal_decomposer(
    llm_client: Optional[Any] = None,
    oracle: Optional[Any] = None,  # v92.0: Accept oracle for backwards compatibility
) -> GoalDecomposer:
    """
    Get or create the goal decomposer.

    v92.0: Added oracle parameter for backwards compatibility with callers
    that pass oracle=... kwarg. The oracle is not used by GoalDecomposer
    but accepting it prevents TypeError when called with that argument.
    """
    global _goal_decomposer
    if _goal_decomposer is None:
        _goal_decomposer = GoalDecomposer(llm_client)
    return _goal_decomposer


def get_improvement_engine(
    llm_client: Optional[Any] = None,
    oracle: Optional[Any] = None,
) -> AutonomousImprovementEngine:
    """Get or create the autonomous improvement engine."""
    global _improvement_engine
    if _improvement_engine is None:
        _improvement_engine = AutonomousImprovementEngine(
            file_selector=get_intelligent_file_selector(),
            goal_decomposer=get_goal_decomposer(llm_client),
            llm_client=llm_client,
            oracle=oracle,
        )
    return _improvement_engine


def get_voice_command_handler(
    llm_client: Optional[Any] = None,
    oracle: Optional[Any] = None,
) -> VoiceCommandHandler:
    """Get or create the voice command handler."""
    global _voice_handler
    if _voice_handler is None:
        _voice_handler = VoiceCommandHandler(
            get_improvement_engine(llm_client, oracle)
        )
    return _voice_handler


# =============================================================================
# v8.0: MASTER INITIALIZATION
# =============================================================================

async def initialize_improve_yourself_system(
    llm_client: Optional[Any] = None,
    oracle: Optional[Any] = None,
    start_scheduler: bool = False,
) -> Dict[str, Any]:
    """
    Initialize the complete "Improve Yourself" system.

    This sets up:
    - Intelligent file selection
    - Goal decomposition
    - Autonomous improvement engine
    - Voice command handling

    Args:
        llm_client: LLM client for intelligent improvements
        oracle: Oracle for code understanding
        start_scheduler: Whether to start autonomous improvement scheduler

    Returns:
        Dictionary with initialized components
    """
    logger.info("🚀 Initializing 'Improve Yourself' System v8.0...")
    start_time = time.monotonic()

    components = {}

    try:
        # Initialize file selector
        file_selector = get_intelligent_file_selector()
        components["file_selector"] = file_selector

        # Initialize goal decomposer
        goal_decomposer = get_goal_decomposer(llm_client)
        components["goal_decomposer"] = goal_decomposer

        # Initialize improvement engine
        engine = get_improvement_engine(llm_client, oracle)
        components["improvement_engine"] = engine

        # Initialize voice handler
        voice_handler = get_voice_command_handler(llm_client, oracle)
        components["voice_handler"] = voice_handler

        elapsed = time.monotonic() - start_time
        logger.info(f"✅ 'Improve Yourself' System initialized in {elapsed:.2f}s")

        return components

    except Exception as e:
        logger.error(f"❌ 'Improve Yourself' System initialization failed: {e}")
        raise


async def shutdown_improve_yourself_system() -> None:
    """Shutdown the 'Improve Yourself' system."""
    logger.info("Shutting down 'Improve Yourself' System...")

    global _file_selector, _goal_decomposer, _improvement_engine, _voice_handler

    _file_selector = None
    _goal_decomposer = None
    _improvement_engine = None
    _voice_handler = None

    logger.info("✅ 'Improve Yourself' System shutdown complete")


# =============================================================================
# v8.0: CONVENIENCE FUNCTION
# =============================================================================

async def jarvis_improve_yourself(
    project_root: Optional[Path] = None,
    max_improvements: int = 5,
    dry_run: bool = False,
    voice_feedback: bool = True,
) -> List[ImprovementResult]:
    """
    Execute "Ironcliw, improve yourself" command.

    This is the main entry point that can be called from anywhere.

    Usage:
        results = await jarvis_improve_yourself()

    Args:
        project_root: Project root directory
        max_improvements: Maximum improvements to make
        dry_run: If True, show what would be done
        voice_feedback: Whether to provide voice feedback

    Returns:
        List of improvement results
    """
    engine = get_improvement_engine()

    # Register voice feedback if requested
    if voice_feedback:
        async def voice_progress(message: str, progress: float):
            # This would integrate with Ironcliw voice system
            logger.info(f"[Ironcliw] {message} ({progress*100:.0f}%)")

        engine.register_progress_callback(voice_progress)

    return await engine.improve_yourself(
        project_root=project_root,
        max_improvements=max_improvements,
        dry_run=dry_run,
    )


async def jarvis_improve_file(
    file_path: str,
    goal: Optional[str] = None,
    dry_run: bool = False,
) -> ImprovementResult:
    """
    Execute "Ironcliw, improve [file]" command.

    Usage:
        result = await jarvis_improve_file("backend/api/main.py")

    Args:
        file_path: Path to the file to improve
        goal: Optional specific goal
        dry_run: If True, show what would be done

    Returns:
        Improvement result
    """
    engine = get_improvement_engine()
    return await engine.improve_file(file_path, goal, dry_run)


async def handle_jarvis_voice_command(command: str) -> Dict[str, Any]:
    """
    Handle a Ironcliw voice command for improvement.

    Usage:
        result = await handle_jarvis_voice_command("improve yourself")
        result = await handle_jarvis_voice_command("improve main.py")

    Args:
        command: The voice command text

    Returns:
        Result dictionary with response
    """
    handler = get_voice_command_handler()
    return await handler.handle_command(command)


# =============================================================================
# v9.0: MULTI-LANGUAGE SUPPORT SYSTEM
# =============================================================================
#
# This module provides comprehensive multi-language support for Ironcliw:
# - Language detection and registration for 15+ programming languages
# - Universal AST parsing with tree-sitter integration
# - Cross-language symbol tracking for API contracts
# - Cross-language refactoring with symbol propagation
# - Language-specific code analysis and best practices
#
# Supported Languages:
#   Python, JavaScript, TypeScript, Go, Rust, Java, C, C++, Ruby, PHP,
#   Swift, Kotlin, Scala, C#, Lua, Shell/Bash, SQL, HTML, CSS, YAML, JSON
#
# =============================================================================


class LanguageType(Enum):
    """Supported programming languages."""
    PYTHON = "python"
    JAVASCRIPT = "javascript"
    TYPESCRIPT = "typescript"
    GO = "go"
    RUST = "rust"
    JAVA = "java"
    C = "c"
    CPP = "cpp"
    RUBY = "ruby"
    PHP = "php"
    SWIFT = "swift"
    KOTLIN = "kotlin"
    SCALA = "scala"
    CSHARP = "csharp"
    LUA = "lua"
    SHELL = "shell"
    SQL = "sql"
    HTML = "html"
    CSS = "css"
    YAML = "yaml"
    JSON = "json"
    MARKDOWN = "markdown"
    UNKNOWN = "unknown"


@dataclass
class LanguageConfig:
    """Configuration for a programming language."""
    language_type: LanguageType
    extensions: Set[str]
    comment_single: str
    comment_multi_start: Optional[str] = None
    comment_multi_end: Optional[str] = None
    string_delimiters: Set[str] = field(default_factory=lambda: {'"', "'"})
    shebangs: Set[str] = field(default_factory=set)
    keywords: Set[str] = field(default_factory=set)
    function_patterns: List[str] = field(default_factory=list)
    class_patterns: List[str] = field(default_factory=list)
    import_patterns: List[str] = field(default_factory=list)
    export_patterns: List[str] = field(default_factory=list)
    variable_patterns: List[str] = field(default_factory=list)
    type_patterns: List[str] = field(default_factory=list)
    async_patterns: List[str] = field(default_factory=list)
    tree_sitter_grammar: Optional[str] = None
    supports_types: bool = False
    supports_async: bool = False
    supports_classes: bool = True
    indentation_sensitive: bool = False


@dataclass
class SymbolLocation:
    """Location of a symbol in source code."""
    file_path: str
    line: int
    column: int
    end_line: Optional[int] = None
    end_column: Optional[int] = None
    language: LanguageType = LanguageType.UNKNOWN


@dataclass
class CrossLanguageSymbol:
    """A symbol that may be referenced across languages."""
    name: str
    symbol_type: str  # function, class, variable, type, constant, api_endpoint
    language: LanguageType
    file_path: str
    line: int
    column: int
    signature: Optional[str] = None
    docstring: Optional[str] = None
    references: List[SymbolLocation] = field(default_factory=list)
    exported: bool = False
    api_endpoint: Optional[str] = None  # For REST/GraphQL endpoints
    parameter_types: Dict[str, str] = field(default_factory=dict)
    return_type: Optional[str] = None
    cross_language_refs: List[SymbolLocation] = field(default_factory=list)


@dataclass
class ASTNode:
    """Universal AST node representation across languages."""
    node_type: str
    name: Optional[str]
    start_line: int
    start_column: int
    end_line: int
    end_column: int
    language: LanguageType
    children: List['ASTNode'] = field(default_factory=list)
    properties: Dict[str, Any] = field(default_factory=dict)
    raw_text: Optional[str] = None


@dataclass
class RefactoringChange:
    """A single refactoring change."""
    file_path: str
    language: LanguageType
    old_text: str
    new_text: str
    start_line: int
    end_line: int
    change_type: str  # rename, extract, inline, move, delete
    description: str
    confidence: float = 1.0


@dataclass
class CrossLanguageRefactoringResult:
    """Result of a cross-language refactoring operation."""
    success: bool
    changes: List[RefactoringChange]
    affected_files: Dict[LanguageType, List[str]]
    symbols_updated: int
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    rollback_available: bool = True


class LanguageRegistry:
    """
    v9.0: Dynamic language detection and registration.

    Provides:
    - Language detection from file extension, shebang, and content
    - Language-specific configuration management
    - Pattern matching for symbols across languages
    - Extensible language support via plugins
    """

    def __init__(self):
        self._languages: Dict[LanguageType, LanguageConfig] = {}
        self._extension_map: Dict[str, LanguageType] = {}
        self._shebang_map: Dict[str, LanguageType] = {}
        self._lock = asyncio.Lock()
        self._initialized = False

        # Register built-in languages
        self._register_builtin_languages()

    def _register_builtin_languages(self) -> None:
        """Register all built-in language configurations."""
        # Python
        self.register_language(LanguageConfig(
            language_type=LanguageType.PYTHON,
            extensions={".py", ".pyw", ".pyx", ".pxd", ".pyi"},
            comment_single="#",
            comment_multi_start='"""',
            comment_multi_end='"""',
            string_delimiters={'"', "'", '"""', "'''"},
            shebangs={"python", "python3", "python2"},
            keywords={
                "def", "class", "import", "from", "as", "if", "elif", "else",
                "for", "while", "try", "except", "finally", "with", "async",
                "await", "yield", "return", "raise", "pass", "break", "continue",
                "lambda", "global", "nonlocal", "assert", "del", "in", "is",
                "and", "or", "not", "True", "False", "None",
            },
            function_patterns=[
                r"(?:async\s+)?def\s+(\w+)\s*\(",
            ],
            class_patterns=[
                r"class\s+(\w+)\s*(?:\([^)]*\))?\s*:",
            ],
            import_patterns=[
                r"import\s+([\w.]+)",
                r"from\s+([\w.]+)\s+import\s+(.+)",
            ],
            variable_patterns=[
                r"(\w+)\s*(?::\s*[\w\[\],\s]+)?\s*=",
            ],
            type_patterns=[
                r":\s*([\w\[\],\s|]+)\s*(?:=|$|\))",
                r"->\s*([\w\[\],\s|]+)",
            ],
            async_patterns=[
                r"async\s+def\s+\w+",
                r"await\s+\w+",
            ],
            tree_sitter_grammar="python",
            supports_types=True,
            supports_async=True,
            supports_classes=True,
            indentation_sensitive=True,
        ))

        # JavaScript
        self.register_language(LanguageConfig(
            language_type=LanguageType.JAVASCRIPT,
            extensions={".js", ".mjs", ".cjs", ".jsx"},
            comment_single="//",
            comment_multi_start="/*",
            comment_multi_end="*/",
            string_delimiters={'"', "'", "`"},
            shebangs={"node", "nodejs", "bun", "deno"},
            keywords={
                "function", "const", "let", "var", "class", "extends", "import",
                "export", "from", "if", "else", "for", "while", "do", "switch",
                "case", "break", "continue", "return", "throw", "try", "catch",
                "finally", "async", "await", "yield", "new", "this", "super",
                "typeof", "instanceof", "in", "of", "true", "false", "null",
                "undefined", "static", "get", "set",
            },
            function_patterns=[
                r"(?:async\s+)?function\s*(\w*)\s*\(",
                r"(?:const|let|var)\s+(\w+)\s*=\s*(?:async\s+)?\([^)]*\)\s*=>",
                r"(?:const|let|var)\s+(\w+)\s*=\s*(?:async\s+)?function",
                r"(\w+)\s*\([^)]*\)\s*\{",  # Method shorthand
            ],
            class_patterns=[
                r"class\s+(\w+)(?:\s+extends\s+\w+)?",
            ],
            import_patterns=[
                r"import\s+(?:\{[^}]+\}|[\w*]+)\s+from\s+['\"]([^'\"]+)['\"]",
                r"require\s*\(\s*['\"]([^'\"]+)['\"]\s*\)",
            ],
            export_patterns=[
                r"export\s+(?:default\s+)?(?:function|class|const|let|var)\s+(\w+)",
                r"export\s*\{\s*([^}]+)\s*\}",
                r"module\.exports\s*=",
            ],
            variable_patterns=[
                r"(?:const|let|var)\s+(\w+)\s*=",
            ],
            async_patterns=[
                r"async\s+function",
                r"async\s+\(",
                r"await\s+\w+",
            ],
            tree_sitter_grammar="javascript",
            supports_types=False,
            supports_async=True,
            supports_classes=True,
        ))

        # TypeScript
        self.register_language(LanguageConfig(
            language_type=LanguageType.TYPESCRIPT,
            extensions={".ts", ".tsx", ".mts", ".cts", ".d.ts"},
            comment_single="//",
            comment_multi_start="/*",
            comment_multi_end="*/",
            string_delimiters={'"', "'", "`"},
            keywords={
                "function", "const", "let", "var", "class", "extends", "implements",
                "import", "export", "from", "if", "else", "for", "while", "do",
                "switch", "case", "break", "continue", "return", "throw", "try",
                "catch", "finally", "async", "await", "yield", "new", "this",
                "super", "typeof", "instanceof", "in", "of", "true", "false",
                "null", "undefined", "static", "get", "set", "interface", "type",
                "enum", "namespace", "module", "declare", "abstract", "readonly",
                "private", "protected", "public", "as", "is", "keyof", "infer",
                "never", "unknown", "any", "void", "boolean", "number", "string",
            },
            function_patterns=[
                r"(?:async\s+)?function\s*(\w*)\s*(?:<[^>]+>)?\s*\(",
                r"(?:const|let|var)\s+(\w+)\s*(?::\s*[^=]+)?\s*=\s*(?:async\s+)?\([^)]*\)\s*(?::\s*[^=]+)?\s*=>",
                r"(\w+)\s*(?:<[^>]+>)?\s*\([^)]*\)\s*(?::\s*[^{]+)?\s*\{",
            ],
            class_patterns=[
                r"(?:abstract\s+)?class\s+(\w+)(?:<[^>]+>)?(?:\s+extends\s+\w+)?(?:\s+implements\s+[\w,\s]+)?",
            ],
            import_patterns=[
                r"import\s+(?:type\s+)?(?:\{[^}]+\}|[\w*]+)\s+from\s+['\"]([^'\"]+)['\"]",
                r"import\s*\(\s*['\"]([^'\"]+)['\"]\s*\)",
            ],
            export_patterns=[
                r"export\s+(?:default\s+)?(?:type\s+)?(?:function|class|const|let|var|interface|type|enum)\s+(\w+)",
                r"export\s*\{\s*([^}]+)\s*\}",
            ],
            type_patterns=[
                r":\s*([\w<>\[\]|&\s,]+)(?:\s*[=;,)\]]|$)",
                r"interface\s+(\w+)",
                r"type\s+(\w+)\s*=",
            ],
            variable_patterns=[
                r"(?:const|let|var)\s+(\w+)\s*(?::\s*[^=]+)?\s*=",
            ],
            async_patterns=[
                r"async\s+function",
                r"async\s+\(",
                r"await\s+\w+",
                r"Promise<",
            ],
            tree_sitter_grammar="typescript",
            supports_types=True,
            supports_async=True,
            supports_classes=True,
        ))

        # Go
        self.register_language(LanguageConfig(
            language_type=LanguageType.GO,
            extensions={".go"},
            comment_single="//",
            comment_multi_start="/*",
            comment_multi_end="*/",
            string_delimiters={'"', "`"},
            keywords={
                "break", "case", "chan", "const", "continue", "default", "defer",
                "else", "fallthrough", "for", "func", "go", "goto", "if", "import",
                "interface", "map", "package", "range", "return", "select", "struct",
                "switch", "type", "var", "true", "false", "nil", "iota",
            },
            function_patterns=[
                r"func\s*(?:\([^)]+\))?\s*(\w+)\s*\(",
            ],
            class_patterns=[
                r"type\s+(\w+)\s+struct\s*\{",
                r"type\s+(\w+)\s+interface\s*\{",
            ],
            import_patterns=[
                r"import\s+[\"']([^\"']+)[\"']",
                r"import\s+\w+\s+[\"']([^\"']+)[\"']",
            ],
            type_patterns=[
                r"type\s+(\w+)\s+",
                r"func\s*\([^)]+\)\s*\w+\s*\([^)]*\)\s*(\w+)",
                r"func\s+\w+\s*\([^)]*\)\s*(\w+)",
            ],
            variable_patterns=[
                r"(?:var|const)\s+(\w+)",
                r"(\w+)\s*:=",
            ],
            tree_sitter_grammar="go",
            supports_types=True,
            supports_async=True,  # goroutines
            supports_classes=False,  # structs instead
        ))

        # Rust
        self.register_language(LanguageConfig(
            language_type=LanguageType.RUST,
            extensions={".rs"},
            comment_single="//",
            comment_multi_start="/*",
            comment_multi_end="*/",
            string_delimiters={'"'},
            keywords={
                "as", "async", "await", "break", "const", "continue", "crate",
                "dyn", "else", "enum", "extern", "false", "fn", "for", "if",
                "impl", "in", "let", "loop", "match", "mod", "move", "mut",
                "pub", "ref", "return", "self", "Self", "static", "struct",
                "super", "trait", "true", "type", "unsafe", "use", "where",
                "while", "async", "await", "try",
            },
            function_patterns=[
                r"(?:pub\s+)?(?:async\s+)?fn\s+(\w+)\s*(?:<[^>]+>)?\s*\(",
            ],
            class_patterns=[
                r"(?:pub\s+)?struct\s+(\w+)",
                r"(?:pub\s+)?enum\s+(\w+)",
                r"(?:pub\s+)?trait\s+(\w+)",
            ],
            import_patterns=[
                r"use\s+([\w:]+)",
                r"extern\s+crate\s+(\w+)",
            ],
            type_patterns=[
                r":\s*(\w+(?:<[^>]+>)?)",
                r"->\s*(\w+(?:<[^>]+>)?)",
            ],
            variable_patterns=[
                r"let\s+(?:mut\s+)?(\w+)",
                r"const\s+(\w+)",
                r"static\s+(?:mut\s+)?(\w+)",
            ],
            async_patterns=[
                r"async\s+fn",
                r"\.await",
            ],
            tree_sitter_grammar="rust",
            supports_types=True,
            supports_async=True,
            supports_classes=True,
        ))

        # Java
        self.register_language(LanguageConfig(
            language_type=LanguageType.JAVA,
            extensions={".java"},
            comment_single="//",
            comment_multi_start="/*",
            comment_multi_end="*/",
            string_delimiters={'"'},
            keywords={
                "abstract", "assert", "boolean", "break", "byte", "case", "catch",
                "char", "class", "const", "continue", "default", "do", "double",
                "else", "enum", "extends", "final", "finally", "float", "for",
                "goto", "if", "implements", "import", "instanceof", "int",
                "interface", "long", "native", "new", "package", "private",
                "protected", "public", "return", "short", "static", "strictfp",
                "super", "switch", "synchronized", "this", "throw", "throws",
                "transient", "try", "void", "volatile", "while", "true", "false",
                "null", "var", "record", "sealed", "permits", "yield",
            },
            function_patterns=[
                r"(?:public|private|protected|static|final|synchronized|native|abstract|\s)+[\w<>\[\],\s]+\s+(\w+)\s*\([^)]*\)\s*(?:throws\s+[\w,\s]+)?\s*\{",
            ],
            class_patterns=[
                r"(?:public|private|protected|abstract|final|\s)*class\s+(\w+)",
                r"(?:public|private|protected|\s)*interface\s+(\w+)",
                r"(?:public|private|protected|\s)*enum\s+(\w+)",
                r"(?:public|private|protected|\s)*record\s+(\w+)",
            ],
            import_patterns=[
                r"import\s+(?:static\s+)?([\w.*]+);",
            ],
            type_patterns=[
                r":\s*(\w+(?:<[^>]+>)?)",
                r"(?:public|private|protected|static|final|\s)+(\w+(?:<[^>]+>)?)\s+\w+\s*[=;]",
            ],
            variable_patterns=[
                r"(?:public|private|protected|static|final|\s)+[\w<>\[\],\s]+\s+(\w+)\s*[=;]",
            ],
            tree_sitter_grammar="java",
            supports_types=True,
            supports_async=True,  # CompletableFuture
            supports_classes=True,
        ))

        # C
        self.register_language(LanguageConfig(
            language_type=LanguageType.C,
            extensions={".c", ".h"},
            comment_single="//",
            comment_multi_start="/*",
            comment_multi_end="*/",
            string_delimiters={'"'},
            keywords={
                "auto", "break", "case", "char", "const", "continue", "default",
                "do", "double", "else", "enum", "extern", "float", "for", "goto",
                "if", "inline", "int", "long", "register", "restrict", "return",
                "short", "signed", "sizeof", "static", "struct", "switch",
                "typedef", "union", "unsigned", "void", "volatile", "while",
                "_Bool", "_Complex", "_Imaginary",
            },
            function_patterns=[
                r"(?:static\s+)?(?:inline\s+)?[\w*\s]+\s+(\w+)\s*\([^)]*\)\s*\{",
            ],
            class_patterns=[
                r"struct\s+(\w+)\s*\{",
                r"typedef\s+struct\s*(?:\w+)?\s*\{[^}]+\}\s*(\w+);",
                r"enum\s+(\w+)\s*\{",
            ],
            import_patterns=[
                r"#include\s*[<\"]([^>\"]+)[>\"]",
            ],
            type_patterns=[
                r"typedef\s+[\w\s*]+\s+(\w+);",
            ],
            variable_patterns=[
                r"(?:static\s+)?(?:const\s+)?[\w*\s]+\s+(\w+)\s*[=;]",
            ],
            tree_sitter_grammar="c",
            supports_types=True,
            supports_async=False,
            supports_classes=False,
        ))

        # C++
        self.register_language(LanguageConfig(
            language_type=LanguageType.CPP,
            extensions={".cpp", ".cc", ".cxx", ".c++", ".hpp", ".hh", ".hxx", ".h++"},
            comment_single="//",
            comment_multi_start="/*",
            comment_multi_end="*/",
            string_delimiters={'"'},
            keywords={
                "alignas", "alignof", "and", "and_eq", "asm", "auto", "bitand",
                "bitor", "bool", "break", "case", "catch", "char", "char8_t",
                "char16_t", "char32_t", "class", "compl", "concept", "const",
                "consteval", "constexpr", "constinit", "const_cast", "continue",
                "co_await", "co_return", "co_yield", "decltype", "default",
                "delete", "do", "double", "dynamic_cast", "else", "enum",
                "explicit", "export", "extern", "false", "float", "for", "friend",
                "goto", "if", "inline", "int", "long", "mutable", "namespace",
                "new", "noexcept", "not", "not_eq", "nullptr", "operator", "or",
                "or_eq", "private", "protected", "public", "register",
                "reinterpret_cast", "requires", "return", "short", "signed",
                "sizeof", "static", "static_assert", "static_cast", "struct",
                "switch", "template", "this", "thread_local", "throw", "true",
                "try", "typedef", "typeid", "typename", "union", "unsigned",
                "using", "virtual", "void", "volatile", "wchar_t", "while",
                "xor", "xor_eq",
            },
            function_patterns=[
                r"(?:virtual\s+)?(?:static\s+)?(?:inline\s+)?[\w*&<>,\s]+\s+(\w+)\s*\([^)]*\)\s*(?:const\s*)?(?:noexcept\s*)?(?:override\s*)?(?:final\s*)?\s*\{",
            ],
            class_patterns=[
                r"class\s+(\w+)(?:\s*:\s*(?:public|private|protected)\s+\w+)?",
                r"struct\s+(\w+)",
                r"enum\s+(?:class\s+)?(\w+)",
            ],
            import_patterns=[
                r"#include\s*[<\"]([^>\"]+)[>\"]",
                r"using\s+namespace\s+([\w:]+);",
            ],
            type_patterns=[
                r"template\s*<[^>]+>\s*(?:class|struct)\s+(\w+)",
            ],
            variable_patterns=[
                r"(?:static\s+)?(?:const\s+)?[\w*&<>,\s]+\s+(\w+)\s*[=;{]",
            ],
            async_patterns=[
                r"co_await\s+",
                r"co_return\s+",
                r"co_yield\s+",
            ],
            tree_sitter_grammar="cpp",
            supports_types=True,
            supports_async=True,  # C++20 coroutines
            supports_classes=True,
        ))

        # Ruby
        self.register_language(LanguageConfig(
            language_type=LanguageType.RUBY,
            extensions={".rb", ".rake", ".gemspec", "Rakefile", "Gemfile"},
            comment_single="#",
            comment_multi_start="=begin",
            comment_multi_end="=end",
            string_delimiters={'"', "'", "`"},
            shebangs={"ruby"},
            keywords={
                "BEGIN", "END", "alias", "and", "begin", "break", "case", "class",
                "def", "defined?", "do", "else", "elsif", "end", "ensure", "false",
                "for", "if", "in", "module", "next", "nil", "not", "or", "redo",
                "rescue", "retry", "return", "self", "super", "then", "true",
                "undef", "unless", "until", "when", "while", "yield", "__FILE__",
                "__LINE__", "__ENCODING__",
            },
            function_patterns=[
                r"def\s+(?:self\.)?(\w+[!?]?)",
            ],
            class_patterns=[
                r"class\s+(\w+)",
                r"module\s+(\w+)",
            ],
            import_patterns=[
                r"require\s+['\"]([^'\"]+)['\"]",
                r"require_relative\s+['\"]([^'\"]+)['\"]",
                r"load\s+['\"]([^'\"]+)['\"]",
            ],
            variable_patterns=[
                r"@(\w+)\s*=",
                r"@@(\w+)\s*=",
                r"\$(\w+)\s*=",
                r"(\w+)\s*=",
            ],
            tree_sitter_grammar="ruby",
            supports_types=False,
            supports_async=True,  # Fibers
            supports_classes=True,
            indentation_sensitive=False,
        ))

        # PHP
        self.register_language(LanguageConfig(
            language_type=LanguageType.PHP,
            extensions={".php", ".phtml", ".php3", ".php4", ".php5", ".phps"},
            comment_single="//",
            comment_multi_start="/*",
            comment_multi_end="*/",
            string_delimiters={'"', "'"},
            shebangs={"php"},
            keywords={
                "abstract", "and", "array", "as", "break", "callable", "case",
                "catch", "class", "clone", "const", "continue", "declare",
                "default", "die", "do", "echo", "else", "elseif", "empty",
                "enddeclare", "endfor", "endforeach", "endif", "endswitch",
                "endwhile", "eval", "exit", "extends", "final", "finally", "fn",
                "for", "foreach", "function", "global", "goto", "if", "implements",
                "include", "include_once", "instanceof", "insteadof", "interface",
                "isset", "list", "match", "namespace", "new", "or", "print",
                "private", "protected", "public", "readonly", "require",
                "require_once", "return", "static", "switch", "throw", "trait",
                "try", "unset", "use", "var", "while", "xor", "yield",
                "true", "false", "null",
            },
            function_patterns=[
                r"(?:public|private|protected|static|\s)*function\s+(\w+)\s*\(",
            ],
            class_patterns=[
                r"(?:abstract\s+)?class\s+(\w+)",
                r"interface\s+(\w+)",
                r"trait\s+(\w+)",
            ],
            import_patterns=[
                r"use\s+([\w\\]+)",
                r"require(?:_once)?\s*\(?['\"]([^'\"]+)['\"]\)?",
                r"include(?:_once)?\s*\(?['\"]([^'\"]+)['\"]\)?",
            ],
            type_patterns=[
                r":\s*\??([\w\\]+)",
            ],
            variable_patterns=[
                r"\$(\w+)\s*=",
            ],
            tree_sitter_grammar="php",
            supports_types=True,
            supports_async=True,  # Fibers in PHP 8.1+
            supports_classes=True,
        ))

        # Shell/Bash
        self.register_language(LanguageConfig(
            language_type=LanguageType.SHELL,
            extensions={".sh", ".bash", ".zsh", ".fish", ".ksh"},
            comment_single="#",
            string_delimiters={'"', "'"},
            shebangs={"sh", "bash", "zsh", "fish", "ksh", "dash"},
            keywords={
                "if", "then", "else", "elif", "fi", "case", "esac", "for", "while",
                "until", "do", "done", "in", "function", "select", "time", "coproc",
                "local", "export", "readonly", "declare", "typeset", "unset",
                "shift", "exit", "return", "break", "continue", "source", ".",
                "alias", "unalias", "trap", "eval", "exec", "set", "true", "false",
            },
            function_patterns=[
                r"(?:function\s+)?(\w+)\s*\(\s*\)",
            ],
            import_patterns=[
                r"source\s+([^\s;]+)",
                r"\.\s+([^\s;]+)",
            ],
            variable_patterns=[
                r"(\w+)=",
                r"export\s+(\w+)",
                r"local\s+(\w+)",
                r"declare\s+(?:-\w+\s+)*(\w+)",
            ],
            tree_sitter_grammar="bash",
            supports_types=False,
            supports_async=True,  # Background jobs
            supports_classes=False,
        ))

        # Swift
        self.register_language(LanguageConfig(
            language_type=LanguageType.SWIFT,
            extensions={".swift"},
            comment_single="//",
            comment_multi_start="/*",
            comment_multi_end="*/",
            string_delimiters={'"'},
            keywords={
                "actor", "any", "as", "associatedtype", "async", "await", "break",
                "case", "catch", "class", "continue", "convenience", "default",
                "defer", "deinit", "didSet", "do", "dynamic", "else", "enum",
                "extension", "fallthrough", "false", "fileprivate", "final", "for",
                "func", "get", "guard", "if", "import", "in", "indirect", "infix",
                "init", "inout", "internal", "is", "lazy", "let", "mutating",
                "nil", "nonisolated", "nonmutating", "open", "operator", "optional",
                "override", "postfix", "precedencegroup", "prefix", "private",
                "protocol", "public", "repeat", "required", "rethrows", "return",
                "self", "Self", "set", "some", "static", "struct", "subscript",
                "super", "switch", "throw", "throws", "true", "try", "typealias",
                "unowned", "var", "weak", "where", "while", "willSet",
            },
            function_patterns=[
                r"(?:public\s+|private\s+|internal\s+|fileprivate\s+|open\s+)?(?:static\s+)?func\s+(\w+)",
            ],
            class_patterns=[
                r"(?:public\s+|private\s+|internal\s+|fileprivate\s+|open\s+)?(?:final\s+)?class\s+(\w+)",
                r"(?:public\s+|private\s+|internal\s+|fileprivate\s+)?struct\s+(\w+)",
                r"(?:public\s+|private\s+|internal\s+|fileprivate\s+)?enum\s+(\w+)",
                r"(?:public\s+|private\s+|internal\s+|fileprivate\s+)?protocol\s+(\w+)",
                r"(?:public\s+|private\s+|internal\s+|fileprivate\s+)?actor\s+(\w+)",
            ],
            import_patterns=[
                r"import\s+(\w+)",
            ],
            type_patterns=[
                r":\s*(\w+(?:<[^>]+>)?)",
                r"->\s*(\w+(?:<[^>]+>)?)",
            ],
            variable_patterns=[
                r"(?:let|var)\s+(\w+)",
            ],
            async_patterns=[
                r"async\s+",
                r"await\s+",
            ],
            tree_sitter_grammar="swift",
            supports_types=True,
            supports_async=True,
            supports_classes=True,
        ))

        # Kotlin
        self.register_language(LanguageConfig(
            language_type=LanguageType.KOTLIN,
            extensions={".kt", ".kts"},
            comment_single="//",
            comment_multi_start="/*",
            comment_multi_end="*/",
            string_delimiters={'"'},
            keywords={
                "abstract", "actual", "annotation", "as", "break", "by", "catch",
                "class", "companion", "const", "constructor", "continue",
                "crossinline", "data", "delegate", "do", "dynamic", "else", "enum",
                "expect", "external", "false", "field", "file", "final", "finally",
                "for", "fun", "get", "if", "import", "in", "infix", "init",
                "inline", "inner", "interface", "internal", "is", "it", "lateinit",
                "noinline", "null", "object", "open", "operator", "out", "override",
                "package", "param", "private", "property", "protected", "public",
                "receiver", "reified", "return", "sealed", "set", "setparam",
                "super", "suspend", "tailrec", "this", "throw", "true", "try",
                "typealias", "typeof", "val", "value", "var", "vararg", "when",
                "where", "while",
            },
            function_patterns=[
                r"(?:suspend\s+)?fun\s+(?:<[^>]+>\s+)?(\w+)",
            ],
            class_patterns=[
                r"(?:data\s+|sealed\s+|open\s+|abstract\s+)?class\s+(\w+)",
                r"(?:fun\s+)?interface\s+(\w+)",
                r"object\s+(\w+)",
                r"enum\s+class\s+(\w+)",
            ],
            import_patterns=[
                r"import\s+([\w.]+)",
            ],
            type_patterns=[
                r":\s*(\w+(?:<[^>]+>)?)",
            ],
            variable_patterns=[
                r"(?:val|var)\s+(\w+)",
            ],
            async_patterns=[
                r"suspend\s+fun",
                r"launch\s*\{",
                r"async\s*\{",
            ],
            tree_sitter_grammar="kotlin",
            supports_types=True,
            supports_async=True,
            supports_classes=True,
        ))

        # YAML
        self.register_language(LanguageConfig(
            language_type=LanguageType.YAML,
            extensions={".yaml", ".yml"},
            comment_single="#",
            string_delimiters={'"', "'"},
            keywords=set(),
            tree_sitter_grammar="yaml",
            supports_types=False,
            supports_async=False,
            supports_classes=False,
            indentation_sensitive=True,
        ))

        # JSON
        self.register_language(LanguageConfig(
            language_type=LanguageType.JSON,
            extensions={".json", ".jsonc", ".json5"},
            comment_single="",  # JSON doesn't support comments (JSONC does with //)
            string_delimiters={'"'},
            keywords={"true", "false", "null"},
            tree_sitter_grammar="json",
            supports_types=False,
            supports_async=False,
            supports_classes=False,
        ))

        # HTML
        self.register_language(LanguageConfig(
            language_type=LanguageType.HTML,
            extensions={".html", ".htm", ".xhtml"},
            comment_single="",
            comment_multi_start="<!--",
            comment_multi_end="-->",
            string_delimiters={'"', "'"},
            keywords=set(),
            tree_sitter_grammar="html",
            supports_types=False,
            supports_async=False,
            supports_classes=False,
        ))

        # CSS
        self.register_language(LanguageConfig(
            language_type=LanguageType.CSS,
            extensions={".css", ".scss", ".sass", ".less"},
            comment_single="",
            comment_multi_start="/*",
            comment_multi_end="*/",
            string_delimiters={'"', "'"},
            keywords=set(),
            tree_sitter_grammar="css",
            supports_types=False,
            supports_async=False,
            supports_classes=False,
        ))

        self._initialized = True

    def register_language(self, config: LanguageConfig) -> None:
        """Register a language configuration."""
        self._languages[config.language_type] = config

        # Build extension map
        for ext in config.extensions:
            self._extension_map[ext.lower()] = config.language_type

        # Build shebang map
        for shebang in config.shebangs:
            self._shebang_map[shebang.lower()] = config.language_type

    def detect_language(self, file_path: Union[str, Path], content: Optional[str] = None) -> LanguageType:
        """
        Detect the programming language of a file.

        Uses multiple signals:
        1. File extension (primary)
        2. Shebang line (for scripts)
        3. Content analysis (fallback)
        """
        path = Path(file_path)

        # Check extension
        ext = path.suffix.lower()
        if ext in self._extension_map:
            return self._extension_map[ext]

        # Check full filename (for files like Rakefile, Gemfile)
        name = path.name.lower()
        if name in self._extension_map:
            return self._extension_map[name]

        # Check shebang if content provided
        if content:
            lines = content.split('\n')
            if lines and lines[0].startswith('#!'):
                shebang = lines[0].lower()
                for interpreter, lang in self._shebang_map.items():
                    if interpreter in shebang:
                        return lang

        return LanguageType.UNKNOWN

    def get_config(self, language: LanguageType) -> Optional[LanguageConfig]:
        """Get configuration for a language."""
        return self._languages.get(language)

    def get_supported_extensions(self) -> Set[str]:
        """Get all supported file extensions."""
        return set(self._extension_map.keys())

    def get_supported_languages(self) -> List[LanguageType]:
        """Get all supported languages."""
        return list(self._languages.keys())

    async def is_supported(self, file_path: Union[str, Path]) -> bool:
        """Check if a file's language is supported."""
        return self.detect_language(file_path) != LanguageType.UNKNOWN


class UniversalASTParser:
    """
    v9.0: Universal AST parser supporting multiple languages.

    Uses tree-sitter when available for accurate parsing,
    with regex-based fallback for unsupported languages.
    """

    def __init__(self, language_registry: LanguageRegistry):
        self._registry = language_registry
        self._tree_sitter_available = False
        self._parsers: Dict[LanguageType, Any] = {}
        self._lock = asyncio.Lock()

        # Try to initialize tree-sitter
        self._init_tree_sitter()

    def _init_tree_sitter(self) -> None:
        """Initialize tree-sitter if available."""
        try:
            import tree_sitter
            self._tree_sitter_available = True
            logger.info("tree-sitter available for AST parsing")
        except ImportError:
            self._tree_sitter_available = False
            logger.info("tree-sitter not available, using regex fallback")

    async def parse_file(self, file_path: Union[str, Path]) -> Optional[ASTNode]:
        """
        Parse a file and return its AST.

        Args:
            file_path: Path to the file to parse

        Returns:
            Root ASTNode of the parsed file, or None if parsing failed
        """
        path = Path(file_path)

        if not path.exists():
            return None

        try:
            content = path.read_text(encoding='utf-8', errors='replace')
        except Exception as e:
            logger.warning(f"Failed to read {file_path}: {e}")
            return None

        language = self._registry.detect_language(file_path, content)

        if language == LanguageType.UNKNOWN:
            return None

        return await self.parse_content(content, language, str(file_path))

    async def parse_content(
        self,
        content: str,
        language: LanguageType,
        file_path: Optional[str] = None,
    ) -> Optional[ASTNode]:
        """
        Parse content and return its AST.

        Args:
            content: Source code content
            language: Programming language
            file_path: Optional file path for error reporting

        Returns:
            Root ASTNode
        """
        if self._tree_sitter_available:
            try:
                return await self._parse_with_tree_sitter(content, language, file_path)
            except Exception as e:
                logger.debug(f"tree-sitter parsing failed, using regex: {e}")

        # Fallback to regex-based parsing
        return await self._parse_with_regex(content, language, file_path)

    async def _parse_with_tree_sitter(
        self,
        content: str,
        language: LanguageType,
        file_path: Optional[str],
    ) -> Optional[ASTNode]:
        """Parse using tree-sitter for accurate AST."""
        # Note: Full tree-sitter integration would require language-specific
        # grammar files. This is a placeholder for the architecture.
        config = self._registry.get_config(language)
        if not config or not config.tree_sitter_grammar:
            return await self._parse_with_regex(content, language, file_path)

        # In a full implementation, we'd use tree-sitter here
        # For now, fall back to regex
        return await self._parse_with_regex(content, language, file_path)

    async def _parse_with_regex(
        self,
        content: str,
        language: LanguageType,
        file_path: Optional[str],
    ) -> Optional[ASTNode]:
        """Parse using regex patterns for basic AST extraction."""
        config = self._registry.get_config(language)
        if not config:
            return None

        lines = content.split('\n')
        root = ASTNode(
            node_type="module",
            name=Path(file_path).stem if file_path else None,
            start_line=1,
            start_column=0,
            end_line=len(lines),
            end_column=len(lines[-1]) if lines else 0,
            language=language,
            properties={"file_path": file_path},
        )

        # Extract functions
        for pattern in config.function_patterns:
            for match in re.finditer(pattern, content, re.MULTILINE):
                name = match.group(1) if match.groups() else None
                if name:
                    line_num = content[:match.start()].count('\n') + 1
                    root.children.append(ASTNode(
                        node_type="function",
                        name=name,
                        start_line=line_num,
                        start_column=match.start() - content.rfind('\n', 0, match.start()) - 1,
                        end_line=line_num,  # Would need more analysis for actual end
                        end_column=match.end() - content.rfind('\n', 0, match.end()) - 1,
                        language=language,
                        raw_text=match.group(0),
                    ))

        # Extract classes
        for pattern in config.class_patterns:
            for match in re.finditer(pattern, content, re.MULTILINE):
                name = match.group(1) if match.groups() else None
                if name:
                    line_num = content[:match.start()].count('\n') + 1
                    root.children.append(ASTNode(
                        node_type="class",
                        name=name,
                        start_line=line_num,
                        start_column=match.start() - content.rfind('\n', 0, match.start()) - 1,
                        end_line=line_num,
                        end_column=match.end() - content.rfind('\n', 0, match.end()) - 1,
                        language=language,
                        raw_text=match.group(0),
                    ))

        # Extract imports
        for pattern in config.import_patterns:
            for match in re.finditer(pattern, content, re.MULTILINE):
                import_name = match.group(1) if match.groups() else match.group(0)
                line_num = content[:match.start()].count('\n') + 1
                root.children.append(ASTNode(
                    node_type="import",
                    name=import_name,
                    start_line=line_num,
                    start_column=0,
                    end_line=line_num,
                    end_column=len(match.group(0)),
                    language=language,
                    raw_text=match.group(0),
                ))

        # Extract variables/constants
        for pattern in config.variable_patterns:
            for match in re.finditer(pattern, content, re.MULTILINE):
                name = match.group(1) if match.groups() else None
                if name:
                    line_num = content[:match.start()].count('\n') + 1
                    root.children.append(ASTNode(
                        node_type="variable",
                        name=name,
                        start_line=line_num,
                        start_column=match.start() - content.rfind('\n', 0, match.start()) - 1,
                        end_line=line_num,
                        end_column=match.end() - content.rfind('\n', 0, match.end()) - 1,
                        language=language,
                        raw_text=match.group(0),
                    ))

        return root

    async def extract_symbols(
        self,
        file_path: Union[str, Path],
    ) -> List[CrossLanguageSymbol]:
        """
        Extract all symbols from a file.

        Returns a list of symbols with their locations and metadata.
        """
        ast = await self.parse_file(file_path)
        if not ast:
            return []

        symbols = []

        def extract_from_node(node: ASTNode, parent_path: str = "") -> None:
            if node.name:
                symbol_type = node.node_type
                full_name = f"{parent_path}.{node.name}" if parent_path else node.name

                symbols.append(CrossLanguageSymbol(
                    name=node.name,
                    symbol_type=symbol_type,
                    language=node.language,
                    file_path=str(file_path),
                    line=node.start_line,
                    column=node.start_column,
                    signature=node.raw_text,
                ))

                # Update parent path for nested symbols
                if symbol_type in ("class", "module"):
                    parent_path = full_name

            for child in node.children:
                extract_from_node(child, parent_path)

        extract_from_node(ast)
        return symbols


class CrossLanguageSymbolTracker:
    """
    v9.0: Track symbols across language boundaries.

    This component:
    - Builds a cross-language symbol index
    - Tracks API endpoints and their consumers
    - Identifies cross-language dependencies
    - Supports symbol resolution across file boundaries
    """

    def __init__(
        self,
        language_registry: LanguageRegistry,
        ast_parser: UniversalASTParser,
    ):
        self._registry = language_registry
        self._parser = ast_parser
        self._symbol_index: Dict[str, List[CrossLanguageSymbol]] = {}
        self._api_endpoints: Dict[str, CrossLanguageSymbol] = {}
        self._cross_refs: Dict[str, List[SymbolLocation]] = {}
        self._indexed_files: Set[str] = set()
        self._lock = asyncio.Lock()

    async def index_project(
        self,
        project_root: Path,
        include_patterns: Optional[List[str]] = None,
        exclude_patterns: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        Index all symbols in a project.

        Args:
            project_root: Root directory of the project
            include_patterns: Glob patterns to include (default: all supported)
            exclude_patterns: Glob patterns to exclude

        Returns:
            Statistics about the indexing operation
        """
        if exclude_patterns is None:
            exclude_patterns = [
                "**/node_modules/**",
                "**/.git/**",
                "**/venv/**",
                "**/__pycache__/**",
                "**/dist/**",
                "**/build/**",
                "**/.next/**",
                "**/target/**",  # Rust
                "**/vendor/**",  # Go, PHP
            ]

        stats = {
            "files_indexed": 0,
            "symbols_found": 0,
            "languages": defaultdict(int),
            "symbol_types": defaultdict(int),
            "errors": [],
        }

        # Collect all files to index
        files_to_index = []
        supported_extensions = self._registry.get_supported_extensions()

        for ext in supported_extensions:
            pattern = f"**/*{ext}"
            for file_path in project_root.glob(pattern):
                # Check exclusions
                should_exclude = False
                for exclude in exclude_patterns:
                    if file_path.match(exclude):
                        should_exclude = True
                        break

                if not should_exclude:
                    files_to_index.append(file_path)

        # Index files in parallel
        async def index_file(file_path: Path) -> Optional[List[CrossLanguageSymbol]]:
            try:
                symbols = await self._parser.extract_symbols(file_path)
                return symbols
            except Exception as e:
                stats["errors"].append(f"{file_path}: {e}")
                return None

        # Process in batches for memory efficiency
        batch_size = 50
        for i in range(0, len(files_to_index), batch_size):
            batch = files_to_index[i:i + batch_size]
            results = await asyncio.gather(
                *[index_file(f) for f in batch],
                return_exceptions=True,
            )

            for file_path, result in zip(batch, results):
                if isinstance(result, list):
                    async with self._lock:
                        for symbol in result:
                            # Add to symbol index
                            if symbol.name not in self._symbol_index:
                                self._symbol_index[symbol.name] = []
                            self._symbol_index[symbol.name].append(symbol)

                            # Track API endpoints
                            if symbol.api_endpoint:
                                self._api_endpoints[symbol.api_endpoint] = symbol

                            stats["symbols_found"] += 1
                            stats["languages"][symbol.language.value] += 1
                            stats["symbol_types"][symbol.symbol_type] += 1

                        self._indexed_files.add(str(file_path))
                        stats["files_indexed"] += 1

        return dict(stats)

    async def find_symbol(
        self,
        name: str,
        language: Optional[LanguageType] = None,
        symbol_type: Optional[str] = None,
    ) -> List[CrossLanguageSymbol]:
        """Find symbols by name, optionally filtered by language or type."""
        symbols = self._symbol_index.get(name, [])

        if language:
            symbols = [s for s in symbols if s.language == language]

        if symbol_type:
            symbols = [s for s in symbols if s.symbol_type == symbol_type]

        return symbols

    async def find_references(
        self,
        symbol: CrossLanguageSymbol,
        search_all_languages: bool = True,
    ) -> List[SymbolLocation]:
        """
        Find all references to a symbol across the codebase.

        Args:
            symbol: The symbol to find references for
            search_all_languages: If True, search in all languages

        Returns:
            List of locations where the symbol is referenced
        """
        references = []
        search_pattern = re.compile(rf'\b{re.escape(symbol.name)}\b')

        for file_path in self._indexed_files:
            path = Path(file_path)
            language = self._registry.detect_language(file_path)

            if not search_all_languages and language != symbol.language:
                continue

            try:
                content = path.read_text(encoding='utf-8', errors='replace')
                for match in search_pattern.finditer(content):
                    line_num = content[:match.start()].count('\n') + 1
                    col = match.start() - content.rfind('\n', 0, match.start()) - 1

                    # Skip the definition itself
                    if (file_path == symbol.file_path and
                        line_num == symbol.line and
                        col == symbol.column):
                        continue

                    references.append(SymbolLocation(
                        file_path=file_path,
                        line=line_num,
                        column=col,
                        language=language,
                    ))
            except Exception:
                continue

        return references

    async def find_cross_language_dependencies(
        self,
        file_path: Union[str, Path],
    ) -> Dict[str, List[CrossLanguageSymbol]]:
        """
        Find symbols from other languages that this file depends on.

        Useful for understanding cross-language API contracts.
        """
        path = Path(file_path)
        content = path.read_text(encoding='utf-8', errors='replace')
        file_language = self._registry.detect_language(file_path, content)

        dependencies: Dict[str, List[CrossLanguageSymbol]] = defaultdict(list)

        # Look for potential cross-language references
        # This includes API calls, shared constants, etc.
        for symbol_name, symbols in self._symbol_index.items():
            # Check if symbol is referenced in this file
            if re.search(rf'\b{re.escape(symbol_name)}\b', content):
                for symbol in symbols:
                    if symbol.language != file_language:
                        dependencies[symbol.language.value].append(symbol)

        return dict(dependencies)

    async def detect_api_consumers(
        self,
        endpoint: str,
    ) -> List[SymbolLocation]:
        """
        Find all code that consumes a specific API endpoint.

        Useful for tracking API usage across frontend/backend.
        """
        consumers = []
        endpoint_pattern = re.compile(re.escape(endpoint))

        for file_path in self._indexed_files:
            try:
                content = Path(file_path).read_text(encoding='utf-8', errors='replace')
                language = self._registry.detect_language(file_path, content)

                for match in endpoint_pattern.finditer(content):
                    line_num = content[:match.start()].count('\n') + 1
                    col = match.start() - content.rfind('\n', 0, match.start()) - 1

                    consumers.append(SymbolLocation(
                        file_path=file_path,
                        line=line_num,
                        column=col,
                        language=language,
                    ))
            except Exception:
                continue

        return consumers


class CrossLanguageRefactorer:
    """
    v9.0: Perform refactoring operations across language boundaries.

    Supports:
    - Rename symbols across all languages
    - Extract shared types/interfaces
    - Update API contracts
    - Move symbols between files
    """

    def __init__(
        self,
        language_registry: LanguageRegistry,
        ast_parser: UniversalASTParser,
        symbol_tracker: CrossLanguageSymbolTracker,
    ):
        self._registry = language_registry
        self._parser = ast_parser
        self._tracker = symbol_tracker
        self._pending_changes: List[RefactoringChange] = []
        self._lock = asyncio.Lock()

    async def rename_symbol(
        self,
        symbol: CrossLanguageSymbol,
        new_name: str,
        update_all_languages: bool = True,
        dry_run: bool = True,
    ) -> CrossLanguageRefactoringResult:
        """
        Rename a symbol across all languages.

        Args:
            symbol: The symbol to rename
            new_name: The new name for the symbol
            update_all_languages: If True, update references in all languages
            dry_run: If True, don't actually make changes

        Returns:
            Result with all changes that would be/were made
        """
        changes: List[RefactoringChange] = []
        affected_files: Dict[LanguageType, List[str]] = defaultdict(list)
        errors: List[str] = []

        # Validate new name
        if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', new_name):
            return CrossLanguageRefactoringResult(
                success=False,
                changes=[],
                affected_files={},
                symbols_updated=0,
                errors=[f"Invalid identifier: {new_name}"],
            )

        # Find all references
        references = await self._tracker.find_references(
            symbol,
            search_all_languages=update_all_languages,
        )

        # Add the definition itself
        all_locations = [
            SymbolLocation(
                file_path=symbol.file_path,
                line=symbol.line,
                column=symbol.column,
                language=symbol.language,
            )
        ] + references

        # Group by file for efficient processing
        by_file: Dict[str, List[SymbolLocation]] = defaultdict(list)
        for loc in all_locations:
            by_file[loc.file_path].append(loc)

        # Generate changes for each file
        for file_path, locations in by_file.items():
            try:
                content = Path(file_path).read_text(encoding='utf-8', errors='replace')
                language = self._registry.detect_language(file_path, content)

                # Sort locations in reverse order for safe replacement
                locations.sort(key=lambda l: (l.line, l.column), reverse=True)

                lines = content.split('\n')

                for loc in locations:
                    line_idx = loc.line - 1
                    if 0 <= line_idx < len(lines):
                        line = lines[line_idx]
                        # Find and replace the symbol
                        pattern = re.compile(rf'\b{re.escape(symbol.name)}\b')
                        new_line = pattern.sub(new_name, line, count=1)

                        if new_line != line:
                            changes.append(RefactoringChange(
                                file_path=file_path,
                                language=language,
                                old_text=line,
                                new_text=new_line,
                                start_line=loc.line,
                                end_line=loc.line,
                                change_type="rename",
                                description=f"Rename '{symbol.name}' to '{new_name}'",
                            ))

                            if file_path not in affected_files[language]:
                                affected_files[language].append(file_path)

            except Exception as e:
                errors.append(f"Error processing {file_path}: {e}")

        # Apply changes if not dry run
        if not dry_run and changes:
            await self._apply_changes(changes)

        return CrossLanguageRefactoringResult(
            success=len(errors) == 0,
            changes=changes,
            affected_files=dict(affected_files),
            symbols_updated=len(changes),
            errors=errors,
        )

    async def _apply_changes(self, changes: List[RefactoringChange]) -> None:
        """Apply refactoring changes to files."""
        # Group changes by file
        by_file: Dict[str, List[RefactoringChange]] = defaultdict(list)
        for change in changes:
            by_file[change.file_path].append(change)

        for file_path, file_changes in by_file.items():
            try:
                content = Path(file_path).read_text(encoding='utf-8', errors='replace')
                lines = content.split('\n')

                # Sort changes by line (reverse order for safe in-place modification)
                file_changes.sort(key=lambda c: c.start_line, reverse=True)

                for change in file_changes:
                    line_idx = change.start_line - 1
                    if 0 <= line_idx < len(lines):
                        lines[line_idx] = change.new_text

                # Write back
                Path(file_path).write_text('\n'.join(lines), encoding='utf-8')

            except Exception as e:
                logger.error(f"Failed to apply changes to {file_path}: {e}")

    async def extract_shared_interface(
        self,
        symbols: List[CrossLanguageSymbol],
        interface_name: str,
        target_file: str,
        target_language: LanguageType,
    ) -> CrossLanguageRefactoringResult:
        """
        Extract shared symbols into a common interface.

        Useful for creating shared type definitions between frontend and backend.
        """
        # This is a more complex refactoring that would need full type analysis
        # For now, we provide the structure for future implementation
        config = self._registry.get_config(target_language)
        if not config:
            return CrossLanguageRefactoringResult(
                success=False,
                changes=[],
                affected_files={},
                symbols_updated=0,
                errors=[f"Unsupported language: {target_language}"],
            )

        # Generate interface definition based on language
        interface_code = self._generate_interface(
            symbols, interface_name, target_language
        )

        changes = [
            RefactoringChange(
                file_path=target_file,
                language=target_language,
                old_text="",
                new_text=interface_code,
                start_line=1,
                end_line=1,
                change_type="extract",
                description=f"Extract interface '{interface_name}'",
            )
        ]

        return CrossLanguageRefactoringResult(
            success=True,
            changes=changes,
            affected_files={target_language: [target_file]},
            symbols_updated=len(symbols),
        )

    def _generate_interface(
        self,
        symbols: List[CrossLanguageSymbol],
        interface_name: str,
        language: LanguageType,
    ) -> str:
        """Generate interface code for a specific language."""
        if language == LanguageType.TYPESCRIPT:
            lines = [f"export interface {interface_name} {{"]
            for sym in symbols:
                type_str = sym.return_type or "unknown"
                lines.append(f"  {sym.name}: {type_str};")
            lines.append("}")
            return "\n".join(lines)

        elif language == LanguageType.PYTHON:
            lines = [
                "from typing import Protocol",
                "",
                f"class {interface_name}(Protocol):",
            ]
            for sym in symbols:
                type_str = sym.return_type or "Any"
                if sym.symbol_type == "function":
                    lines.append(f"    def {sym.name}(self) -> {type_str}: ...")
                else:
                    lines.append(f"    {sym.name}: {type_str}")
            return "\n".join(lines)

        elif language == LanguageType.GO:
            lines = [f"type {interface_name} interface {{"]
            for sym in symbols:
                if sym.symbol_type == "function":
                    type_str = sym.return_type or "interface{}"
                    lines.append(f"\t{sym.name.title()}() {type_str}")
            lines.append("}")
            return "\n".join(lines)

        return f"// Interface {interface_name} - manual implementation required"


class MultiLanguageFileSelector:
    """
    v9.0: Enhanced file selector with multi-language support.

    Extends IntelligentFileSelector to work with any supported language.
    """

    def __init__(
        self,
        language_registry: LanguageRegistry,
        ast_parser: UniversalASTParser,
    ):
        self._registry = language_registry
        self._parser = ast_parser
        self._complexity_cache: Dict[str, float] = {}
        self._lock = asyncio.Lock()

    async def select_files_to_improve(
        self,
        project_root: Optional[Path] = None,
        max_files: int = 5,
        languages: Optional[List[LanguageType]] = None,
        categories: Optional[List[ImprovementCategory]] = None,
        min_priority: ImprovementPriority = ImprovementPriority.LOW,
        exclude_patterns: Optional[List[str]] = None,
    ) -> List[FileImprovementCandidate]:
        """
        Select files to improve across all supported languages.

        Args:
            project_root: Root directory to search
            max_files: Maximum number of files to return
            languages: Languages to include (None = all)
            categories: Improvement categories to focus on
            min_priority: Minimum priority threshold
            exclude_patterns: Patterns to exclude

        Returns:
            List of file improvement candidates sorted by priority
        """
        if project_root is None:
            project_root = Path.cwd()

        if exclude_patterns is None:
            exclude_patterns = [
                "**/node_modules/**",
                "**/.git/**",
                "**/venv/**",
                "**/__pycache__/**",
                "**/dist/**",
                "**/build/**",
                "**/.next/**",
                "**/target/**",
                "**/vendor/**",
                "**/*.min.js",
                "**/*.bundle.js",
            ]

        candidates: List[FileImprovementCandidate] = []

        # Get supported extensions
        if languages:
            extensions = set()
            for lang in languages:
                config = self._registry.get_config(lang)
                if config:
                    extensions.update(config.extensions)
        else:
            extensions = self._registry.get_supported_extensions()

        # Find all matching files
        for ext in extensions:
            for file_path in project_root.rglob(f"*{ext}"):
                # Check exclusions
                should_exclude = False
                str_path = str(file_path)

                for pattern in exclude_patterns:
                    if file_path.match(pattern):
                        should_exclude = True
                        break

                if should_exclude:
                    continue

                # Analyze file
                try:
                    candidate = await self._analyze_file(file_path, categories)
                    if candidate and candidate.priority.value >= min_priority.value:
                        candidates.append(candidate)
                except Exception as e:
                    logger.debug(f"Failed to analyze {file_path}: {e}")

        # Sort by priority (highest first)
        candidates.sort(key=lambda c: c.priority.value, reverse=True)

        return candidates[:max_files]

    async def _analyze_file(
        self,
        file_path: Path,
        categories: Optional[List[ImprovementCategory]] = None,
    ) -> Optional[FileImprovementCandidate]:
        """Analyze a single file for improvement potential."""
        try:
            content = file_path.read_text(encoding='utf-8', errors='replace')
        except Exception:
            return None

        language = self._registry.detect_language(file_path, content)
        if language == LanguageType.UNKNOWN:
            return None

        config = self._registry.get_config(language)
        if not config:
            return None

        # Calculate metrics
        lines = content.split('\n')
        line_count = len(lines)

        # Skip very small or very large files
        if line_count < 10 or line_count > 5000:
            return None

        # Analyze for improvement opportunities
        issues: List[str] = []
        improvement_categories: List[ImprovementCategory] = []
        priority_score = 0

        # Check for security issues
        security_patterns = [
            (r"password\s*=\s*['\"][^'\"]+['\"]", "Hardcoded password"),
            (r"api[_-]?key\s*=\s*['\"][^'\"]+['\"]", "Hardcoded API key"),
            (r"eval\s*\(", "Use of eval()"),
            (r"exec\s*\(", "Use of exec()"),
            (r"\.innerHTML\s*=", "innerHTML assignment (XSS risk)"),
            (r"dangerouslySetInnerHTML", "dangerouslySetInnerHTML (XSS risk)"),
        ]

        for pattern, issue in security_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                issues.append(issue)
                improvement_categories.append(ImprovementCategory.SECURITY)
                priority_score += 25

        # Check for performance issues
        perf_patterns = [
            (r"SELECT\s+\*", "SELECT * (fetch only needed columns)"),
            (r"\.forEach\(async", "Async inside forEach"),
            (r"for\s*\([^)]+\)\s*{[^}]*await", "Sequential await in loop"),
        ]

        for pattern, issue in perf_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                issues.append(issue)
                improvement_categories.append(ImprovementCategory.PERFORMANCE)
                priority_score += 15

        # Check for error handling
        if config.supports_async:
            if re.search(r'async\s+', content) and not re.search(r'try\s*{', content):
                issues.append("Async code without try-catch")
                improvement_categories.append(ImprovementCategory.ERROR_HANDLING)
                priority_score += 10

        # Check complexity
        complexity = await self._estimate_complexity(content, config)
        if complexity > 20:
            issues.append(f"High complexity ({complexity:.0f})")
            improvement_categories.append(ImprovementCategory.MAINTAINABILITY)
            priority_score += 20

        # Check for TODO/FIXME comments
        todo_count = len(re.findall(r'(?:TODO|FIXME|HACK|XXX):', content, re.IGNORECASE))
        if todo_count > 0:
            issues.append(f"{todo_count} TODO/FIXME comments")
            priority_score += todo_count * 3

        # Determine priority
        if priority_score >= 50:
            priority = ImprovementPriority.CRITICAL
        elif priority_score >= 35:
            priority = ImprovementPriority.HIGH
        elif priority_score >= 20:
            priority = ImprovementPriority.MEDIUM
        elif priority_score >= 10:
            priority = ImprovementPriority.LOW
        else:
            priority = ImprovementPriority.COSMETIC

        # Filter by requested categories
        if categories:
            if not any(cat in categories for cat in improvement_categories):
                return None

        return FileImprovementCandidate(
            file_path=str(file_path),
            priority=priority,
            categories=list(set(improvement_categories)),
            issues=issues,
            line_count=line_count,
            complexity=complexity,
            last_modified=datetime.fromtimestamp(file_path.stat().st_mtime),
        )

    async def _estimate_complexity(
        self,
        content: str,
        config: LanguageConfig,
    ) -> float:
        """Estimate code complexity for any language."""
        complexity = 1.0

        # Control flow patterns that increase complexity
        control_patterns = [
            r'\bif\b', r'\belse\b', r'\belif\b', r'\belse\s+if\b',
            r'\bfor\b', r'\bwhile\b', r'\bdo\b',
            r'\bswitch\b', r'\bcase\b',
            r'\btry\b', r'\bcatch\b', r'\bexcept\b',
            r'\band\b', r'\bor\b', r'&&', r'\|\|',
            r'\?.*:', r'=>',  # Ternary and arrow functions
        ]

        for pattern in control_patterns:
            complexity += len(re.findall(pattern, content))

        # Nesting depth estimation
        max_indent = 0
        for line in content.split('\n'):
            stripped = line.lstrip()
            if stripped:
                indent = len(line) - len(stripped)
                max_indent = max(max_indent, indent)

        # Add complexity for deep nesting
        complexity += max_indent / 4

        return complexity


class LanguageSpecificAnalyzer:
    """
    v9.0: Language-specific code analysis.

    Provides detailed analysis considering language-specific patterns,
    best practices, and common issues.
    """

    def __init__(self, language_registry: LanguageRegistry):
        self._registry = language_registry
        self._best_practices: Dict[LanguageType, List[Tuple[str, str, str]]] = {}
        self._init_best_practices()

    def _init_best_practices(self) -> None:
        """Initialize language-specific best practices."""
        # Python best practices
        self._best_practices[LanguageType.PYTHON] = [
            (r"except\s*:", "Bare except clause", "Catch specific exceptions"),
            (r"from\s+\w+\s+import\s+\*", "Star import", "Import specific names"),
            (r"type\s*:\s*ignore", "Type ignore comment", "Fix type issues instead"),
            (r"# noqa", "Noqa comment", "Fix linting issues instead"),
            (r"global\s+\w+", "Global statement", "Avoid globals, use class/parameters"),
            (r"lambda\s+.*:\s*lambda", "Nested lambda", "Use regular functions"),
        ]

        # JavaScript/TypeScript best practices
        js_ts_practices = [
            (r"var\s+\w+", "var declaration", "Use const or let"),
            (r"==\s*(?!>)", "Loose equality", "Use strict equality (===)"),
            (r"!=\s*(?!>)", "Loose inequality", "Use strict inequality (!==)"),
            (r"console\.log\(", "Console log", "Remove or use proper logging"),
            (r"debugger;", "Debugger statement", "Remove before production"),
            (r"any\b", "Any type", "Use specific types"),
        ]
        self._best_practices[LanguageType.JAVASCRIPT] = js_ts_practices
        self._best_practices[LanguageType.TYPESCRIPT] = js_ts_practices

        # Go best practices
        self._best_practices[LanguageType.GO] = [
            (r"panic\(", "Panic usage", "Return errors instead"),
            (r"recover\(", "Recover usage", "Handle errors properly"),
            (r"interface\{\}", "Empty interface", "Use specific types"),
            (r"//\s*nolint", "Nolint comment", "Fix linting issues"),
        ]

        # Rust best practices
        self._best_practices[LanguageType.RUST] = [
            (r"unwrap\(\)", "Unwrap usage", "Handle errors with ? or match"),
            (r"expect\(", "Expect usage", "Use proper error handling"),
            (r"unsafe\s*\{", "Unsafe block", "Minimize unsafe usage"),
            (r"clone\(\)", "Clone usage", "Consider borrowing instead"),
        ]

        # Java best practices
        self._best_practices[LanguageType.JAVA] = [
            (r"catch\s*\(\s*Exception\s+", "Catch Exception", "Catch specific exceptions"),
            (r"e\.printStackTrace\(\)", "printStackTrace", "Use proper logging"),
            (r"System\.out\.print", "System.out", "Use proper logging"),
            (r"@SuppressWarnings", "SuppressWarnings", "Fix warnings instead"),
        ]

    async def analyze_file(
        self,
        file_path: Union[str, Path],
        content: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Perform language-specific analysis on a file.

        Returns detailed analysis including:
        - Best practice violations
        - Language-specific metrics
        - Improvement suggestions
        """
        path = Path(file_path)

        if content is None:
            try:
                content = path.read_text(encoding='utf-8', errors='replace')
            except Exception as e:
                return {"error": str(e)}

        language = self._registry.detect_language(file_path, content)
        config = self._registry.get_config(language)

        if not config:
            return {"language": "unknown", "supported": False}

        violations = []
        suggestions = []

        # Check best practices
        practices = self._best_practices.get(language, [])
        for pattern, issue, suggestion in practices:
            matches = list(re.finditer(pattern, content))
            if matches:
                for match in matches:
                    line_num = content[:match.start()].count('\n') + 1
                    violations.append({
                        "line": line_num,
                        "issue": issue,
                        "suggestion": suggestion,
                        "text": match.group(0)[:50],
                    })

        # Language-specific checks
        metrics = await self._compute_language_metrics(content, config, language)

        # Generate suggestions based on analysis
        if metrics.get("async_usage", 0) > 0 and metrics.get("error_handling", 0) == 0:
            suggestions.append("Add error handling for async code")

        if metrics.get("complexity", 0) > 15:
            suggestions.append("Consider breaking down complex functions")

        if metrics.get("line_count", 0) > 500:
            suggestions.append("Consider splitting into smaller modules")

        return {
            "file_path": str(file_path),
            "language": language.value,
            "violations": violations,
            "suggestions": suggestions,
            "metrics": metrics,
            "config": {
                "supports_types": config.supports_types,
                "supports_async": config.supports_async,
                "supports_classes": config.supports_classes,
            },
        }

    async def _compute_language_metrics(
        self,
        content: str,
        config: LanguageConfig,
        language: LanguageType,
    ) -> Dict[str, Any]:
        """Compute language-specific metrics."""
        lines = content.split('\n')

        metrics = {
            "line_count": len(lines),
            "code_lines": sum(1 for l in lines if l.strip() and not l.strip().startswith(config.comment_single)),
            "functions": 0,
            "classes": 0,
            "imports": 0,
            "async_usage": 0,
            "error_handling": 0,
            "complexity": 0,
        }

        # Count functions
        for pattern in config.function_patterns:
            metrics["functions"] += len(re.findall(pattern, content))

        # Count classes
        for pattern in config.class_patterns:
            metrics["classes"] += len(re.findall(pattern, content))

        # Count imports
        for pattern in config.import_patterns:
            metrics["imports"] += len(re.findall(pattern, content))

        # Check async usage
        if config.supports_async:
            for pattern in config.async_patterns:
                metrics["async_usage"] += len(re.findall(pattern, content))

        # Check error handling (basic)
        error_patterns = [r'\btry\b', r'\bcatch\b', r'\bexcept\b', r'\bresult\s*\?', r'Result<']
        for pattern in error_patterns:
            metrics["error_handling"] += len(re.findall(pattern, content))

        return metrics


# =============================================================================
# v9.0: GLOBAL INSTANCES
# =============================================================================

_language_registry: Optional[LanguageRegistry] = None
_ast_parser: Optional[UniversalASTParser] = None
_symbol_tracker: Optional[CrossLanguageSymbolTracker] = None
_refactorer: Optional[CrossLanguageRefactorer] = None
_multi_language_selector: Optional[MultiLanguageFileSelector] = None
_language_analyzer: Optional[LanguageSpecificAnalyzer] = None


# =============================================================================
# v9.0: FACTORY FUNCTIONS
# =============================================================================

def get_language_registry() -> LanguageRegistry:
    """Get or create the language registry."""
    global _language_registry
    if _language_registry is None:
        _language_registry = LanguageRegistry()
    return _language_registry


def get_ast_parser() -> UniversalASTParser:
    """Get or create the universal AST parser."""
    global _ast_parser
    if _ast_parser is None:
        _ast_parser = UniversalASTParser(get_language_registry())
    return _ast_parser


def get_symbol_tracker() -> CrossLanguageSymbolTracker:
    """Get or create the cross-language symbol tracker."""
    global _symbol_tracker
    if _symbol_tracker is None:
        _symbol_tracker = CrossLanguageSymbolTracker(
            get_language_registry(),
            get_ast_parser(),
        )
    return _symbol_tracker


def get_cross_language_refactorer() -> CrossLanguageRefactorer:
    """Get or create the cross-language refactorer."""
    global _refactorer
    if _refactorer is None:
        _refactorer = CrossLanguageRefactorer(
            get_language_registry(),
            get_ast_parser(),
            get_symbol_tracker(),
        )
    return _refactorer


def get_multi_language_selector() -> MultiLanguageFileSelector:
    """Get or create the multi-language file selector."""
    global _multi_language_selector
    if _multi_language_selector is None:
        _multi_language_selector = MultiLanguageFileSelector(
            get_language_registry(),
            get_ast_parser(),
        )
    return _multi_language_selector


def get_language_analyzer() -> LanguageSpecificAnalyzer:
    """Get or create the language-specific analyzer."""
    global _language_analyzer
    if _language_analyzer is None:
        _language_analyzer = LanguageSpecificAnalyzer(get_language_registry())
    return _language_analyzer


# =============================================================================
# v9.0: INITIALIZATION AND SHUTDOWN
# =============================================================================

async def initialize_multi_language_system(
    project_root: Optional[Path] = None,
    index_project: bool = True,
) -> Dict[str, Any]:
    """
    Initialize the multi-language support system.

    Args:
        project_root: Root directory of the project to index
        index_project: Whether to index the project on startup

    Returns:
        Dictionary with initialized components and stats
    """
    logger.info("🌐 Initializing Multi-Language Support System v9.0...")
    start_time = time.monotonic()

    components = {}

    try:
        # Initialize all components
        registry = get_language_registry()
        components["language_registry"] = registry

        parser = get_ast_parser()
        components["ast_parser"] = parser

        tracker = get_symbol_tracker()
        components["symbol_tracker"] = tracker

        refactorer = get_cross_language_refactorer()
        components["cross_language_refactorer"] = refactorer

        selector = get_multi_language_selector()
        components["multi_language_selector"] = selector

        analyzer = get_language_analyzer()
        components["language_analyzer"] = analyzer

        # Index project if requested
        if index_project and project_root:
            logger.info(f"  Indexing project: {project_root}")
            stats = await tracker.index_project(project_root)
            components["index_stats"] = stats
            logger.info(f"  Indexed {stats.get('files_indexed', 0)} files, "
                       f"{stats.get('symbols_found', 0)} symbols")

        elapsed = time.monotonic() - start_time
        logger.info(f"✅ Multi-Language Support System initialized in {elapsed:.2f}s")
        logger.info(f"   Supported languages: {len(registry.get_supported_languages())}")

        return components

    except Exception as e:
        logger.error(f"❌ Multi-Language Support System initialization failed: {e}")
        raise


async def shutdown_multi_language_system() -> None:
    """Shutdown the multi-language support system."""
    logger.info("Shutting down Multi-Language Support System...")

    global _language_registry, _ast_parser, _symbol_tracker
    global _refactorer, _multi_language_selector, _language_analyzer

    _language_registry = None
    _ast_parser = None
    _symbol_tracker = None
    _refactorer = None
    _multi_language_selector = None
    _language_analyzer = None

    logger.info("✅ Multi-Language Support System shutdown complete")


# =============================================================================
# v9.0: CONVENIENCE FUNCTIONS
# =============================================================================

async def detect_file_language(file_path: Union[str, Path]) -> str:
    """
    Detect the programming language of a file.

    Usage:
        language = await detect_file_language("src/components/App.tsx")
        # Returns: "typescript"
    """
    registry = get_language_registry()
    lang_type = registry.detect_language(file_path)
    return lang_type.value


async def analyze_file_cross_language(
    file_path: Union[str, Path],
) -> Dict[str, Any]:
    """
    Analyze a file with language-specific best practices.

    Usage:
        analysis = await analyze_file_cross_language("frontend/src/App.tsx")
    """
    analyzer = get_language_analyzer()
    return await analyzer.analyze_file(file_path)


async def find_symbol_across_languages(
    symbol_name: str,
    language: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    Find a symbol across all languages in the codebase.

    Usage:
        symbols = await find_symbol_across_languages("handleSubmit")
    """
    tracker = get_symbol_tracker()
    lang_type = LanguageType(language) if language else None

    symbols = await tracker.find_symbol(symbol_name, lang_type)

    return [
        {
            "name": s.name,
            "type": s.symbol_type,
            "language": s.language.value,
            "file": s.file_path,
            "line": s.line,
            "column": s.column,
        }
        for s in symbols
    ]


async def rename_symbol_across_languages(
    old_name: str,
    new_name: str,
    file_path: Optional[str] = None,
    dry_run: bool = True,
) -> Dict[str, Any]:
    """
    Rename a symbol across all languages.

    Usage:
        result = await rename_symbol_across_languages(
            "handleSubmit", "onFormSubmit", dry_run=True
        )
    """
    tracker = get_symbol_tracker()
    refactorer = get_cross_language_refactorer()

    # Find the symbol
    symbols = await tracker.find_symbol(old_name)

    if not symbols:
        return {"success": False, "error": f"Symbol '{old_name}' not found"}

    # If file_path specified, filter to that file
    if file_path:
        symbols = [s for s in symbols if s.file_path == file_path]
        if not symbols:
            return {"success": False, "error": f"Symbol '{old_name}' not found in {file_path}"}

    # Use first matching symbol
    result = await refactorer.rename_symbol(symbols[0], new_name, dry_run=dry_run)

    return {
        "success": result.success,
        "changes": len(result.changes),
        "affected_files": {k.value: v for k, v in result.affected_files.items()},
        "symbols_updated": result.symbols_updated,
        "errors": result.errors,
        "dry_run": dry_run,
    }


async def get_cross_language_dependencies(
    file_path: Union[str, Path],
) -> Dict[str, Any]:
    """
    Get symbols from other languages that this file depends on.

    Usage:
        deps = await get_cross_language_dependencies("backend/api/routes.py")
    """
    tracker = get_symbol_tracker()
    deps = await tracker.find_cross_language_dependencies(file_path)

    return {
        lang: [
            {"name": s.name, "type": s.symbol_type, "file": s.file_path, "line": s.line}
            for s in symbols
        ]
        for lang, symbols in deps.items()
    }


async def improve_any_file(
    file_path: str,
    goal: Optional[str] = None,
    dry_run: bool = False,
) -> Dict[str, Any]:
    """
    Improve any file, regardless of language.

    This is the multi-language equivalent of jarvis_improve_file().

    Usage:
        result = await improve_any_file(
            "frontend/src/components/App.tsx",
            goal="improve performance"
        )
    """
    path = Path(file_path)

    if not path.exists():
        return {"success": False, "error": f"File not found: {file_path}"}

    # Detect language
    registry = get_language_registry()
    language = registry.detect_language(file_path)

    if language == LanguageType.UNKNOWN:
        return {"success": False, "error": f"Unknown language for: {file_path}"}

    # Get language-specific analysis
    analyzer = get_language_analyzer()
    analysis = await analyzer.analyze_file(file_path)

    # Get improvement candidates
    selector = get_multi_language_selector()

    # For Python files, also use the v8.0 improvement engine
    if language == LanguageType.PYTHON:
        engine = get_improvement_engine()
        result = await engine.improve_file(file_path, goal, dry_run)
        return {
            "success": result.success,
            "language": language.value,
            "file": file_path,
            "changes_made": result.changes_made,
            "analysis": analysis,
        }

    # For other languages, provide analysis and suggestions
    return {
        "success": True,
        "language": language.value,
        "file": file_path,
        "analysis": analysis,
        "message": f"Analysis complete for {language.value} file. "
                  f"Found {len(analysis.get('violations', []))} issues.",
        "dry_run": dry_run,
    }


# =============================================================================
# v10.0: REAL-TIME CODE INTELLIGENCE SYSTEM
# =============================================================================
#
# This module provides real-time code intelligence features:
# - Live code completion (autocomplete while typing)
# - Inline code suggestions with explanations
# - Real-time error detection as you type
# - Line-by-line change explanation with reasoning
# - Inline comment generation for changes
# - Interactive code review with cherry-pick support
#
# Architecture:
#   ┌─────────────────────────────────────────────────────────────────────────┐
#   │                    REAL-TIME CODE INTELLIGENCE                          │
#   ├─────────────────────────────────────────────────────────────────────────┤
#   │                                                                          │
#   │  ┌──────────────────┐  ┌──────────────────┐  ┌──────────────────┐       │
#   │  │ Code Completion  │  │ Error Detection  │  │ Inline Suggest   │       │
#   │  │    Engine        │  │    (Real-time)   │  │   Provider       │       │
#   │  └────────┬─────────┘  └────────┬─────────┘  └────────┬─────────┘       │
#   │           │                      │                     │                 │
#   │           ▼                      ▼                     ▼                 │
#   │  ┌─────────────────────────────────────────────────────────────────┐    │
#   │  │                  UNIFIED INTELLIGENCE COORDINATOR               │    │
#   │  │                                                                 │    │
#   │  │  • Context Analysis    • Multi-language AST   • LLM Integration │    │
#   │  │  • Symbol Resolution   • Type Inference       • Caching         │    │
#   │  └─────────────────────────────────────────────────────────────────┘    │
#   │           │                      │                     │                 │
#   │           ▼                      ▼                     ▼                 │
#   │  ┌──────────────────┐  ┌──────────────────┐  ┌──────────────────┐       │
#   │  │ Change Explain   │  │ Comment Generate │  │ Interactive      │       │
#   │  │    Engine        │  │    Engine        │  │   Review         │       │
#   │  └──────────────────┘  └──────────────────┘  └──────────────────┘       │
#   │                                                                          │
#   └─────────────────────────────────────────────────────────────────────────┘
#
# =============================================================================


class CompletionType(Enum):
    """Types of code completions."""
    KEYWORD = "keyword"
    FUNCTION = "function"
    METHOD = "method"
    VARIABLE = "variable"
    CLASS = "class"
    MODULE = "module"
    PROPERTY = "property"
    PARAMETER = "parameter"
    SNIPPET = "snippet"
    FILE_PATH = "file_path"
    TYPE = "type"
    CONSTANT = "constant"


class ErrorSeverity(Enum):
    """Severity levels for detected errors."""
    ERROR = "error"
    WARNING = "warning"
    INFO = "info"
    HINT = "hint"


class ChangeAction(Enum):
    """Actions for interactive code review."""
    ACCEPT = "accept"
    REJECT = "reject"
    MODIFY = "modify"
    SKIP = "skip"
    ACCEPT_ALL = "accept_all"
    REJECT_ALL = "reject_all"


@dataclass
class CompletionItem:
    """A single code completion suggestion."""
    label: str
    kind: CompletionType
    detail: Optional[str] = None
    documentation: Optional[str] = None
    insert_text: Optional[str] = None
    sort_priority: int = 0
    filter_text: Optional[str] = None
    preselect: bool = False
    deprecated: bool = False
    commit_characters: List[str] = field(default_factory=list)
    additional_edits: List[Dict[str, Any]] = field(default_factory=list)
    command: Optional[str] = None
    data: Optional[Dict[str, Any]] = None
    source: str = "jarvis"
    confidence: float = 1.0


@dataclass
class CompletionContext:
    """Context for code completion request."""
    file_path: str
    line: int
    column: int
    trigger_character: Optional[str] = None
    trigger_kind: str = "invoked"  # invoked, trigger_character, incomplete
    prefix: str = ""
    line_content: str = ""
    language: LanguageType = LanguageType.UNKNOWN
    scope: Optional[str] = None
    imports: List[str] = field(default_factory=list)
    symbols_in_scope: List[str] = field(default_factory=list)


@dataclass
class DetectedError:
    """A detected error in the code."""
    file_path: str
    line: int
    column: int
    end_line: int
    end_column: int
    message: str
    severity: ErrorSeverity
    code: Optional[str] = None
    source: str = "jarvis"
    related_info: List[Dict[str, Any]] = field(default_factory=list)
    quick_fixes: List[Dict[str, Any]] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)


@dataclass
class InlineSuggestion:
    """An inline code suggestion."""
    file_path: str
    line: int
    column: int
    suggestion: str
    explanation: str
    category: str  # optimization, security, style, best_practice
    confidence: float = 1.0
    quick_fix: Optional[str] = None
    documentation_url: Optional[str] = None
    related_symbols: List[str] = field(default_factory=list)


@dataclass
class ChangeExplanation:
    """Explanation for a single code change."""
    line: int
    before: str
    after: str
    explanation: str
    reasoning: str
    category: str  # refactor, fix, optimize, style, security
    impact: str  # none, low, medium, high
    references: List[str] = field(default_factory=list)
    educational_notes: Optional[str] = None


@dataclass
class ReviewableChange:
    """A single change that can be reviewed."""
    id: str
    file_path: str
    line_start: int
    line_end: int
    before_text: str
    after_text: str
    explanation: ChangeExplanation
    status: ChangeAction = ChangeAction.SKIP
    user_modified_text: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class CodeReviewSession:
    """An interactive code review session."""
    session_id: str
    file_path: str
    changes: List[ReviewableChange]
    created_at: datetime = field(default_factory=datetime.now)
    status: str = "pending"  # pending, in_progress, completed, cancelled
    accepted_count: int = 0
    rejected_count: int = 0
    modified_count: int = 0


class LiveCodeCompletionEngine:
    """
    v10.0: Real-time code completion engine.

    Provides intelligent code completions as the user types:
    - Context-aware suggestions based on current scope
    - Multi-language support with language-specific completions
    - Symbol resolution from imports and local definitions
    - Type-aware completions (when type information available)
    - Snippet support for common patterns
    - Caching for sub-millisecond response times
    """

    def __init__(
        self,
        language_registry: Optional[LanguageRegistry] = None,
        ast_parser: Optional[UniversalASTParser] = None,
        llm_client: Optional[Any] = None,
    ):
        self._registry = language_registry or get_language_registry()
        self._parser = ast_parser or get_ast_parser()
        self._llm_client = llm_client
        self._completion_cache: Dict[str, List[CompletionItem]] = {}
        self._symbol_cache: Dict[str, List[str]] = {}
        self._snippet_cache: Dict[LanguageType, List[CompletionItem]] = {}
        self._lock = asyncio.Lock()
        self._max_cache_size = 1000
        self._cache_ttl = 300  # 5 minutes

        # Initialize built-in snippets
        self._init_snippets()

    def _init_snippets(self) -> None:
        """Initialize language-specific code snippets."""
        # Python snippets
        self._snippet_cache[LanguageType.PYTHON] = [
            CompletionItem(
                label="def",
                kind=CompletionType.SNIPPET,
                detail="Function definition",
                insert_text="def ${1:name}(${2:params}):\n    ${3:pass}",
                documentation="Define a new function",
            ),
            CompletionItem(
                label="async def",
                kind=CompletionType.SNIPPET,
                detail="Async function definition",
                insert_text="async def ${1:name}(${2:params}):\n    ${3:pass}",
                documentation="Define an async function",
            ),
            CompletionItem(
                label="class",
                kind=CompletionType.SNIPPET,
                detail="Class definition",
                insert_text="class ${1:Name}:\n    def __init__(self${2:, params}):\n        ${3:pass}",
                documentation="Define a new class",
            ),
            CompletionItem(
                label="try/except",
                kind=CompletionType.SNIPPET,
                detail="Try-except block",
                insert_text="try:\n    ${1:pass}\nexcept ${2:Exception} as e:\n    ${3:pass}",
                documentation="Error handling block",
            ),
            CompletionItem(
                label="with",
                kind=CompletionType.SNIPPET,
                detail="Context manager",
                insert_text="with ${1:expression} as ${2:var}:\n    ${3:pass}",
                documentation="Context manager statement",
            ),
            CompletionItem(
                label="if __name__",
                kind=CompletionType.SNIPPET,
                detail="Main entry point",
                insert_text='if __name__ == "__main__":\n    ${1:main()}',
                documentation="Python main entry point",
            ),
            CompletionItem(
                label="@dataclass",
                kind=CompletionType.SNIPPET,
                detail="Dataclass decorator",
                insert_text="@dataclass\nclass ${1:Name}:\n    ${2:field}: ${3:type}",
                documentation="Create a dataclass",
            ),
            CompletionItem(
                label="list comprehension",
                kind=CompletionType.SNIPPET,
                detail="List comprehension",
                insert_text="[${1:expr} for ${2:item} in ${3:iterable}${4: if ${5:condition}}]",
                documentation="List comprehension pattern",
            ),
            CompletionItem(
                label="dict comprehension",
                kind=CompletionType.SNIPPET,
                detail="Dict comprehension",
                insert_text="{${1:key}: ${2:value} for ${3:item} in ${4:iterable}}",
                documentation="Dictionary comprehension pattern",
            ),
        ]

        # JavaScript/TypeScript snippets
        js_snippets = [
            CompletionItem(
                label="function",
                kind=CompletionType.SNIPPET,
                detail="Function declaration",
                insert_text="function ${1:name}(${2:params}) {\n    ${3}\n}",
                documentation="Declare a function",
            ),
            CompletionItem(
                label="arrow",
                kind=CompletionType.SNIPPET,
                detail="Arrow function",
                insert_text="const ${1:name} = (${2:params}) => {\n    ${3}\n};",
                documentation="Arrow function expression",
            ),
            CompletionItem(
                label="async arrow",
                kind=CompletionType.SNIPPET,
                detail="Async arrow function",
                insert_text="const ${1:name} = async (${2:params}) => {\n    ${3}\n};",
                documentation="Async arrow function",
            ),
            CompletionItem(
                label="try/catch",
                kind=CompletionType.SNIPPET,
                detail="Try-catch block",
                insert_text="try {\n    ${1}\n} catch (${2:error}) {\n    ${3}\n}",
                documentation="Error handling block",
            ),
            CompletionItem(
                label="useState",
                kind=CompletionType.SNIPPET,
                detail="React useState hook",
                insert_text="const [${1:state}, set${1/(.*)/${1:/capitalize}/}] = useState(${2:initialValue});",
                documentation="React state hook",
            ),
            CompletionItem(
                label="useEffect",
                kind=CompletionType.SNIPPET,
                detail="React useEffect hook",
                insert_text="useEffect(() => {\n    ${1}\n    return () => {\n        ${2}\n    };\n}, [${3:deps}]);",
                documentation="React effect hook",
            ),
            CompletionItem(
                label="import",
                kind=CompletionType.SNIPPET,
                detail="ES6 import",
                insert_text="import { ${1:module} } from '${2:package}';",
                documentation="Import statement",
            ),
            CompletionItem(
                label="export default",
                kind=CompletionType.SNIPPET,
                detail="Default export",
                insert_text="export default ${1:expression};",
                documentation="Default export",
            ),
            CompletionItem(
                label="fetch",
                kind=CompletionType.SNIPPET,
                detail="Fetch API call",
                insert_text="const response = await fetch('${1:url}', {\n    method: '${2:GET}',\n    headers: {\n        'Content-Type': 'application/json',\n    },\n    ${3:body: JSON.stringify(data),}\n});",
                documentation="Fetch API request",
            ),
        ]
        self._snippet_cache[LanguageType.JAVASCRIPT] = js_snippets
        self._snippet_cache[LanguageType.TYPESCRIPT] = js_snippets

        # Go snippets
        self._snippet_cache[LanguageType.GO] = [
            CompletionItem(
                label="func",
                kind=CompletionType.SNIPPET,
                detail="Function declaration",
                insert_text="func ${1:name}(${2:params}) ${3:returnType} {\n    ${4}\n}",
                documentation="Declare a function",
            ),
            CompletionItem(
                label="struct",
                kind=CompletionType.SNIPPET,
                detail="Struct definition",
                insert_text="type ${1:Name} struct {\n    ${2:Field} ${3:Type}\n}",
                documentation="Define a struct",
            ),
            CompletionItem(
                label="interface",
                kind=CompletionType.SNIPPET,
                detail="Interface definition",
                insert_text="type ${1:Name} interface {\n    ${2:Method}(${3:params}) ${4:returnType}\n}",
                documentation="Define an interface",
            ),
            CompletionItem(
                label="if err",
                kind=CompletionType.SNIPPET,
                detail="Error check",
                insert_text="if err != nil {\n    return ${1:err}\n}",
                documentation="Error handling pattern",
            ),
            CompletionItem(
                label="goroutine",
                kind=CompletionType.SNIPPET,
                detail="Go routine",
                insert_text="go func() {\n    ${1}\n}()",
                documentation="Start a goroutine",
            ),
        ]

        # Rust snippets
        self._snippet_cache[LanguageType.RUST] = [
            CompletionItem(
                label="fn",
                kind=CompletionType.SNIPPET,
                detail="Function definition",
                insert_text="fn ${1:name}(${2:params}) -> ${3:ReturnType} {\n    ${4}\n}",
                documentation="Define a function",
            ),
            CompletionItem(
                label="struct",
                kind=CompletionType.SNIPPET,
                detail="Struct definition",
                insert_text="struct ${1:Name} {\n    ${2:field}: ${3:Type},\n}",
                documentation="Define a struct",
            ),
            CompletionItem(
                label="impl",
                kind=CompletionType.SNIPPET,
                detail="Implementation block",
                insert_text="impl ${1:Type} {\n    ${2}\n}",
                documentation="Implementation block",
            ),
            CompletionItem(
                label="match",
                kind=CompletionType.SNIPPET,
                detail="Match expression",
                insert_text="match ${1:expr} {\n    ${2:pattern} => ${3:result},\n    _ => ${4:default},\n}",
                documentation="Pattern matching",
            ),
            CompletionItem(
                label="?",
                kind=CompletionType.SNIPPET,
                detail="Error propagation",
                insert_text="${1:result}?",
                documentation="Propagate error with ?",
            ),
        ]

    async def get_completions(
        self,
        context: CompletionContext,
        max_items: int = 50,
        include_snippets: bool = True,
    ) -> List[CompletionItem]:
        """
        Get code completions for the current context.

        Args:
            context: The completion context (file, line, column, etc.)
            max_items: Maximum number of completions to return
            include_snippets: Whether to include code snippets

        Returns:
            List of completion items sorted by relevance
        """
        completions: List[CompletionItem] = []

        # Detect language if not provided
        if context.language == LanguageType.UNKNOWN:
            context.language = self._registry.detect_language(context.file_path)

        # Get language config
        config = self._registry.get_config(context.language)
        if not config:
            return completions

        # Generate cache key
        cache_key = f"{context.file_path}:{context.line}:{context.prefix[:10]}"

        # Check cache
        async with self._lock:
            if cache_key in self._completion_cache:
                return self._completion_cache[cache_key][:max_items]

        # Collect completions from various sources
        tasks = [
            self._get_keyword_completions(context, config),
            self._get_symbol_completions(context),
            self._get_import_completions(context, config),
        ]

        if include_snippets:
            tasks.append(self._get_snippet_completions(context))

        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, list):
                completions.extend(result)

        # Filter by prefix
        if context.prefix:
            prefix_lower = context.prefix.lower()
            completions = [
                c for c in completions
                if (c.filter_text or c.label).lower().startswith(prefix_lower)
                or prefix_lower in (c.filter_text or c.label).lower()
            ]

        # Sort by priority and relevance
        completions.sort(key=lambda c: (-c.sort_priority, -c.confidence, c.label))

        # Cache result
        async with self._lock:
            self._completion_cache[cache_key] = completions
            # Evict old entries if cache is too large
            if len(self._completion_cache) > self._max_cache_size:
                oldest_keys = list(self._completion_cache.keys())[:100]
                for key in oldest_keys:
                    del self._completion_cache[key]

        return completions[:max_items]

    async def _get_keyword_completions(
        self,
        context: CompletionContext,
        config: LanguageConfig,
    ) -> List[CompletionItem]:
        """Get language keyword completions."""
        completions = []

        for keyword in config.keywords:
            if not context.prefix or keyword.lower().startswith(context.prefix.lower()):
                completions.append(CompletionItem(
                    label=keyword,
                    kind=CompletionType.KEYWORD,
                    detail=f"{context.language.value} keyword",
                    sort_priority=50,
                    confidence=0.9,
                ))

        return completions

    async def _get_symbol_completions(
        self,
        context: CompletionContext,
    ) -> List[CompletionItem]:
        """Get symbol completions from the current file and imports."""
        completions = []

        try:
            # Parse current file for symbols
            ast = await self._parser.parse_file(context.file_path)
            if ast:
                for node in ast.children:
                    if node.name:
                        kind = CompletionType.VARIABLE
                        if node.node_type == "function":
                            kind = CompletionType.FUNCTION
                        elif node.node_type == "class":
                            kind = CompletionType.CLASS
                        elif node.node_type == "import":
                            kind = CompletionType.MODULE

                        completions.append(CompletionItem(
                            label=node.name,
                            kind=kind,
                            detail=f"{node.node_type} (line {node.start_line})",
                            sort_priority=80,
                            confidence=0.95,
                        ))
        except Exception:
            pass

        # Add symbols from context
        for symbol in context.symbols_in_scope:
            completions.append(CompletionItem(
                label=symbol,
                kind=CompletionType.VARIABLE,
                detail="In scope",
                sort_priority=90,
                confidence=1.0,
            ))

        return completions

    async def _get_import_completions(
        self,
        context: CompletionContext,
        config: LanguageConfig,
    ) -> List[CompletionItem]:
        """Get completions for import statements."""
        completions = []

        # Common imports by language
        common_imports = {
            LanguageType.PYTHON: [
                ("os", "Operating system interface"),
                ("sys", "System-specific parameters"),
                ("json", "JSON encoder and decoder"),
                ("asyncio", "Async I/O framework"),
                ("typing", "Type hints"),
                ("pathlib", "Object-oriented paths"),
                ("dataclasses", "Data classes"),
                ("collections", "Container datatypes"),
                ("functools", "Higher-order functions"),
                ("logging", "Logging facility"),
            ],
            LanguageType.JAVASCRIPT: [
                ("react", "React library"),
                ("useState", "React state hook"),
                ("useEffect", "React effect hook"),
                ("axios", "HTTP client"),
                ("lodash", "Utility library"),
            ],
            LanguageType.TYPESCRIPT: [
                ("react", "React library"),
                ("useState", "React state hook"),
                ("useEffect", "React effect hook"),
                ("axios", "HTTP client"),
            ],
            LanguageType.GO: [
                ("fmt", "Formatted I/O"),
                ("os", "Operating system"),
                ("io", "I/O primitives"),
                ("net/http", "HTTP client/server"),
                ("encoding/json", "JSON encoding"),
                ("context", "Context package"),
                ("sync", "Synchronization"),
            ],
        }

        if context.language in common_imports:
            for module, desc in common_imports[context.language]:
                completions.append(CompletionItem(
                    label=module,
                    kind=CompletionType.MODULE,
                    detail=desc,
                    sort_priority=60,
                    confidence=0.8,
                ))

        return completions

    async def _get_snippet_completions(
        self,
        context: CompletionContext,
    ) -> List[CompletionItem]:
        """Get snippet completions."""
        snippets = self._snippet_cache.get(context.language, [])

        # Filter by prefix
        if context.prefix:
            prefix_lower = context.prefix.lower()
            snippets = [
                s for s in snippets
                if s.label.lower().startswith(prefix_lower)
            ]

        return snippets

    async def invalidate_cache(self, file_path: Optional[str] = None) -> None:
        """Invalidate completion cache for a file or all files."""
        async with self._lock:
            if file_path:
                keys_to_remove = [k for k in self._completion_cache if k.startswith(file_path)]
                for key in keys_to_remove:
                    del self._completion_cache[key]
            else:
                self._completion_cache.clear()


class RealTimeErrorDetector:
    """
    v10.0: Real-time error detection engine.

    Detects errors as the user types:
    - Syntax errors with precise location
    - Semantic errors (undefined variables, type mismatches)
    - Style violations and best practice issues
    - Security vulnerabilities
    - Performance anti-patterns
    """

    def __init__(
        self,
        language_registry: Optional[LanguageRegistry] = None,
        language_analyzer: Optional[LanguageSpecificAnalyzer] = None,
    ):
        self._registry = language_registry or get_language_registry()
        self._analyzer = language_analyzer or get_language_analyzer()
        self._error_cache: Dict[str, List[DetectedError]] = {}
        self._lock = asyncio.Lock()

        # Common error patterns by language
        self._error_patterns: Dict[LanguageType, List[Tuple[str, str, ErrorSeverity]]] = {}
        self._init_error_patterns()

    def _init_error_patterns(self) -> None:
        """Initialize language-specific error patterns."""
        # Python error patterns
        self._error_patterns[LanguageType.PYTHON] = [
            (r"^\s*print\s+[^(]", "Missing parentheses in print (Python 3)", ErrorSeverity.ERROR),
            (r"except\s*:", "Bare except clause catches all exceptions", ErrorSeverity.WARNING),
            (r"==\s*None\b", "Use 'is None' instead of '== None'", ErrorSeverity.HINT),
            (r"!=\s*None\b", "Use 'is not None' instead of '!= None'", ErrorSeverity.HINT),
            (r"\beval\s*\(", "eval() is a security risk", ErrorSeverity.WARNING),
            (r"\bexec\s*\(", "exec() is a security risk", ErrorSeverity.WARNING),
            (r"from\s+\w+\s+import\s+\*", "Wildcard imports are discouraged", ErrorSeverity.WARNING),
            (r"global\s+\w+", "Avoid using global variables", ErrorSeverity.HINT),
            (r"^\s*#\s*type:\s*ignore", "Type ignore suppresses type checking", ErrorSeverity.INFO),
            (r"^\s*#\s*noqa", "Noqa suppresses linting", ErrorSeverity.INFO),
            (r"password\s*=\s*['\"][^'\"]+['\"]", "Hardcoded password detected", ErrorSeverity.ERROR),
            (r"api_?key\s*=\s*['\"][^'\"]+['\"]", "Hardcoded API key detected", ErrorSeverity.ERROR),
        ]

        # JavaScript/TypeScript error patterns
        js_patterns = [
            (r"\bvar\s+\w+", "Use 'const' or 'let' instead of 'var'", ErrorSeverity.WARNING),
            (r"==\s*(?![=])", "Use strict equality (===)", ErrorSeverity.WARNING),
            (r"!=\s*(?![=])", "Use strict inequality (!==)", ErrorSeverity.WARNING),
            (r"console\.log\s*\(", "Remove console.log before production", ErrorSeverity.INFO),
            (r"\bdebugger\b", "Remove debugger statement", ErrorSeverity.WARNING),
            (r"\.innerHTML\s*=", "innerHTML is vulnerable to XSS", ErrorSeverity.WARNING),
            (r"\beval\s*\(", "eval() is a security risk", ErrorSeverity.ERROR),
            (r"\bnew\s+Function\s*\(", "new Function() is similar to eval()", ErrorSeverity.WARNING),
            (r":\s*any\b", "Avoid using 'any' type", ErrorSeverity.HINT),
            (r"@ts-ignore", "ts-ignore suppresses type checking", ErrorSeverity.INFO),
        ]
        self._error_patterns[LanguageType.JAVASCRIPT] = js_patterns
        self._error_patterns[LanguageType.TYPESCRIPT] = js_patterns

        # Go error patterns
        self._error_patterns[LanguageType.GO] = [
            (r"\bpanic\s*\(", "Avoid panic, return errors instead", ErrorSeverity.WARNING),
            (r"interface\{\}", "Avoid empty interface when possible", ErrorSeverity.HINT),
            (r"//\s*nolint", "Nolint suppresses linting", ErrorSeverity.INFO),
            (r"fmt\.Print", "Consider using structured logging", ErrorSeverity.HINT),
        ]

        # Rust error patterns
        self._error_patterns[LanguageType.RUST] = [
            (r"\.unwrap\(\)", "Use proper error handling instead of unwrap()", ErrorSeverity.WARNING),
            (r"\.expect\(", "Consider using ? operator instead of expect()", ErrorSeverity.HINT),
            (r"\bunsafe\s*\{", "Unsafe block requires careful review", ErrorSeverity.INFO),
            (r"\.clone\(\)", "Consider borrowing instead of cloning", ErrorSeverity.HINT),
            (r"#\[allow\(", "Allow attribute suppresses warnings", ErrorSeverity.INFO),
        ]

    async def detect_errors(
        self,
        file_path: str,
        content: Optional[str] = None,
        include_hints: bool = True,
    ) -> List[DetectedError]:
        """
        Detect errors in a file.

        Args:
            file_path: Path to the file
            content: Optional content (reads from file if not provided)
            include_hints: Whether to include hint-level diagnostics

        Returns:
            List of detected errors
        """
        errors: List[DetectedError] = []

        # Get content
        if content is None:
            try:
                content = Path(file_path).read_text(encoding='utf-8', errors='replace')
            except Exception as e:
                return [DetectedError(
                    file_path=file_path,
                    line=1,
                    column=1,
                    end_line=1,
                    end_column=1,
                    message=f"Could not read file: {e}",
                    severity=ErrorSeverity.ERROR,
                    code="E001",
                )]

        # Detect language
        language = self._registry.detect_language(file_path, content)
        if language == LanguageType.UNKNOWN:
            return errors

        lines = content.split('\n')

        # Check syntax errors (basic)
        syntax_errors = await self._check_syntax(content, language, file_path)
        errors.extend(syntax_errors)

        # Check pattern-based errors
        patterns = self._error_patterns.get(language, [])
        for pattern, message, severity in patterns:
            if not include_hints and severity == ErrorSeverity.HINT:
                continue

            try:
                for match in re.finditer(pattern, content, re.MULTILINE):
                    line_num = content[:match.start()].count('\n') + 1
                    col = match.start() - content.rfind('\n', 0, match.start()) - 1

                    errors.append(DetectedError(
                        file_path=file_path,
                        line=line_num,
                        column=max(1, col),
                        end_line=line_num,
                        end_column=col + len(match.group(0)),
                        message=message,
                        severity=severity,
                        code=f"P{hash(pattern) % 1000:03d}",
                        source="jarvis-realtime",
                    ))
            except re.error:
                continue

        # Check for undefined variables (basic)
        undefined_errors = await self._check_undefined_variables(content, language, file_path)
        errors.extend(undefined_errors)

        # Cache errors
        async with self._lock:
            self._error_cache[file_path] = errors

        return errors

    async def _check_syntax(
        self,
        content: str,
        language: LanguageType,
        file_path: str,
    ) -> List[DetectedError]:
        """Check for syntax errors."""
        errors = []

        if language == LanguageType.PYTHON:
            try:
                import ast as python_ast
                python_ast.parse(content)
            except SyntaxError as e:
                errors.append(DetectedError(
                    file_path=file_path,
                    line=e.lineno or 1,
                    column=e.offset or 1,
                    end_line=e.lineno or 1,
                    end_column=(e.offset or 1) + 1,
                    message=str(e.msg),
                    severity=ErrorSeverity.ERROR,
                    code="E100",
                    source="python-syntax",
                ))

        elif language in (LanguageType.JAVASCRIPT, LanguageType.TYPESCRIPT):
            # Basic bracket matching
            brackets = {'(': ')', '[': ']', '{': '}'}
            stack = []
            for i, char in enumerate(content):
                if char in brackets:
                    stack.append((char, i))
                elif char in brackets.values():
                    if stack and brackets[stack[-1][0]] == char:
                        stack.pop()
                    else:
                        line_num = content[:i].count('\n') + 1
                        col = i - content.rfind('\n', 0, i)
                        errors.append(DetectedError(
                            file_path=file_path,
                            line=line_num,
                            column=col,
                            end_line=line_num,
                            end_column=col + 1,
                            message=f"Unmatched '{char}'",
                            severity=ErrorSeverity.ERROR,
                            code="E101",
                            source="bracket-check",
                        ))

            for bracket, pos in stack:
                line_num = content[:pos].count('\n') + 1
                col = pos - content.rfind('\n', 0, pos)
                errors.append(DetectedError(
                    file_path=file_path,
                    line=line_num,
                    column=col,
                    end_line=line_num,
                    end_column=col + 1,
                    message=f"Unclosed '{bracket}'",
                    severity=ErrorSeverity.ERROR,
                    code="E102",
                    source="bracket-check",
                ))

        return errors

    async def _check_undefined_variables(
        self,
        content: str,
        language: LanguageType,
        file_path: str,
    ) -> List[DetectedError]:
        """Check for potentially undefined variables."""
        errors = []

        if language == LanguageType.PYTHON:
            # Simple check for undefined variables
            # Extract all variable assignments and usages
            defined = set()
            used = set()

            # Add built-in names
            builtins = {
                'True', 'False', 'None', 'print', 'len', 'range', 'str', 'int',
                'float', 'list', 'dict', 'set', 'tuple', 'type', 'isinstance',
                'hasattr', 'getattr', 'setattr', 'open', 'input', 'any', 'all',
                'min', 'max', 'sum', 'abs', 'round', 'sorted', 'reversed',
                'enumerate', 'zip', 'map', 'filter', 'super', 'self', 'cls',
                'Exception', 'ValueError', 'TypeError', 'KeyError', 'IndexError',
            }
            defined.update(builtins)

            # Extract imports
            for match in re.finditer(r'import\s+(\w+)|from\s+\w+\s+import\s+(\w+)', content):
                name = match.group(1) or match.group(2)
                if name:
                    defined.add(name)

            # Extract function/class definitions and assignments
            for match in re.finditer(r'(?:def|class)\s+(\w+)|(\w+)\s*(?::\s*\w+)?\s*=', content):
                name = match.group(1) or match.group(2)
                if name:
                    defined.add(name)

        return errors

    async def get_quick_fixes(
        self,
        error: DetectedError,
    ) -> List[Dict[str, Any]]:
        """Get quick fix suggestions for an error."""
        fixes = []

        if "eval()" in error.message:
            fixes.append({
                "title": "Consider using ast.literal_eval() for safe evaluation",
                "edit": "ast.literal_eval",
            })

        if "var" in error.message.lower() and "const" in error.message.lower():
            fixes.append({
                "title": "Replace 'var' with 'const'",
                "edit": "const",
            })

        if "==" in error.message and "===" in error.message:
            fixes.append({
                "title": "Use strict equality (===)",
                "edit": "===",
            })

        if "console.log" in error.message:
            fixes.append({
                "title": "Remove console.log",
                "edit": "",
            })

        if "unwrap()" in error.message:
            fixes.append({
                "title": "Replace with ? operator",
                "edit": "?",
            })

        return fixes


class InlineSuggestionProvider:
    """
    v10.0: Inline code suggestion provider.

    Provides contextual suggestions for specific lines:
    - Hover documentation with explanations
    - Optimization suggestions
    - Best practice recommendations
    - Security improvement hints
    - Refactoring opportunities
    """

    def __init__(
        self,
        language_registry: Optional[LanguageRegistry] = None,
        llm_client: Optional[Any] = None,
    ):
        self._registry = language_registry or get_language_registry()
        self._llm_client = llm_client
        self._suggestion_cache: Dict[str, List[InlineSuggestion]] = {}
        self._lock = asyncio.Lock()

    async def get_suggestions_for_line(
        self,
        file_path: str,
        line: int,
        content: Optional[str] = None,
    ) -> List[InlineSuggestion]:
        """
        Get suggestions for a specific line.

        Args:
            file_path: Path to the file
            line: Line number (1-indexed)
            content: Optional content (reads from file if not provided)

        Returns:
            List of inline suggestions
        """
        suggestions: List[InlineSuggestion] = []

        # Get content
        if content is None:
            try:
                content = Path(file_path).read_text(encoding='utf-8', errors='replace')
            except Exception:
                return suggestions

        lines = content.split('\n')
        if line < 1 or line > len(lines):
            return suggestions

        line_content = lines[line - 1]
        language = self._registry.detect_language(file_path, content)

        # Check for various suggestion patterns
        suggestion_checks = [
            self._check_optimization_opportunities(file_path, line, line_content, language),
            self._check_best_practices(file_path, line, line_content, language),
            self._check_security_issues(file_path, line, line_content, language),
            self._check_style_issues(file_path, line, line_content, language),
        ]

        results = await asyncio.gather(*suggestion_checks, return_exceptions=True)

        for result in results:
            if isinstance(result, list):
                suggestions.extend(result)

        return suggestions

    async def _check_optimization_opportunities(
        self,
        file_path: str,
        line: int,
        line_content: str,
        language: LanguageType,
    ) -> List[InlineSuggestion]:
        """Check for optimization opportunities."""
        suggestions = []

        # Python optimizations
        if language == LanguageType.PYTHON:
            # List append in loop
            if re.search(r'\bfor\b.*:\s*\n.*\.append\(', line_content):
                suggestions.append(InlineSuggestion(
                    file_path=file_path,
                    line=line,
                    column=1,
                    suggestion="Consider using list comprehension instead of append in loop",
                    explanation="List comprehensions are generally faster than building lists with append()",
                    category="optimization",
                    confidence=0.8,
                    quick_fix="[expr for item in iterable]",
                ))

            # String concatenation in loop
            if re.search(r'(\+\s*=\s*["\'])|(["\'].*\+)', line_content):
                suggestions.append(InlineSuggestion(
                    file_path=file_path,
                    line=line,
                    column=1,
                    suggestion="Consider using join() for string concatenation",
                    explanation="String concatenation with + creates new string objects. join() is more efficient.",
                    category="optimization",
                    confidence=0.7,
                    quick_fix="''.join(strings)",
                ))

        # JavaScript optimizations
        if language in (LanguageType.JAVASCRIPT, LanguageType.TYPESCRIPT):
            # forEach with async
            if re.search(r'\.forEach\s*\(\s*async', line_content):
                suggestions.append(InlineSuggestion(
                    file_path=file_path,
                    line=line,
                    column=1,
                    suggestion="async callback in forEach won't await properly",
                    explanation="forEach doesn't await async callbacks. Use for...of or Promise.all() instead.",
                    category="optimization",
                    confidence=0.95,
                    quick_fix="await Promise.all(array.map(async (item) => { ... }))",
                ))

            # Nested ternary
            if line_content.count('?') > 1 and line_content.count(':') > 1:
                suggestions.append(InlineSuggestion(
                    file_path=file_path,
                    line=line,
                    column=1,
                    suggestion="Nested ternary operators reduce readability",
                    explanation="Consider using if-else statements or a lookup object for complex conditions.",
                    category="optimization",
                    confidence=0.7,
                ))

        return suggestions

    async def _check_best_practices(
        self,
        file_path: str,
        line: int,
        line_content: str,
        language: LanguageType,
    ) -> List[InlineSuggestion]:
        """Check for best practice violations."""
        suggestions = []

        # Universal best practices
        if len(line_content) > 120:
            suggestions.append(InlineSuggestion(
                file_path=file_path,
                line=line,
                column=1,
                suggestion="Line exceeds 120 characters",
                explanation="Long lines reduce readability. Consider breaking into multiple lines.",
                category="best_practice",
                confidence=0.6,
            ))

        # Python best practices
        if language == LanguageType.PYTHON:
            # Mutable default argument
            if re.search(r'def\s+\w+\([^)]*=\s*(\[\]|\{\})\)', line_content):
                suggestions.append(InlineSuggestion(
                    file_path=file_path,
                    line=line,
                    column=1,
                    suggestion="Mutable default argument is a common Python gotcha",
                    explanation="Mutable defaults are shared between calls. Use None and create inside function.",
                    category="best_practice",
                    confidence=0.95,
                    quick_fix="def func(arg=None):\n    if arg is None:\n        arg = []",
                ))

            # Using == True/False
            if re.search(r'==\s*True\b|==\s*False\b', line_content):
                suggestions.append(InlineSuggestion(
                    file_path=file_path,
                    line=line,
                    column=1,
                    suggestion="Explicit comparison to True/False is unnecessary",
                    explanation="Use 'if x:' instead of 'if x == True:'",
                    category="best_practice",
                    confidence=0.9,
                ))

        return suggestions

    async def _check_security_issues(
        self,
        file_path: str,
        line: int,
        line_content: str,
        language: LanguageType,
    ) -> List[InlineSuggestion]:
        """Check for security issues."""
        suggestions = []

        # SQL injection risk
        if re.search(r'(execute|query)\s*\([^)]*\%s.*\%|f["\'].*SELECT|f["\'].*INSERT', line_content, re.IGNORECASE):
            suggestions.append(InlineSuggestion(
                file_path=file_path,
                line=line,
                column=1,
                suggestion="Potential SQL injection vulnerability",
                explanation="Use parameterized queries instead of string formatting for SQL.",
                category="security",
                confidence=0.85,
                quick_fix="cursor.execute('SELECT * FROM table WHERE id = %s', (id,))",
            ))

        # Command injection risk
        if re.search(r'subprocess\.call\s*\([^)]*shell\s*=\s*True', line_content):
            suggestions.append(InlineSuggestion(
                file_path=file_path,
                line=line,
                column=1,
                suggestion="shell=True can lead to command injection",
                explanation="Avoid shell=True with untrusted input. Pass command as list instead.",
                category="security",
                confidence=0.9,
                quick_fix="subprocess.call(['cmd', 'arg1', 'arg2'])",
            ))

        # XSS risk in JavaScript
        if language in (LanguageType.JAVASCRIPT, LanguageType.TYPESCRIPT):
            if re.search(r'\.innerHTML\s*=|dangerouslySetInnerHTML', line_content):
                suggestions.append(InlineSuggestion(
                    file_path=file_path,
                    line=line,
                    column=1,
                    suggestion="Potential XSS vulnerability",
                    explanation="innerHTML and dangerouslySetInnerHTML can execute malicious scripts. Sanitize input first.",
                    category="security",
                    confidence=0.85,
                ))

        return suggestions

    async def _check_style_issues(
        self,
        file_path: str,
        line: int,
        line_content: str,
        language: LanguageType,
    ) -> List[InlineSuggestion]:
        """Check for style issues."""
        suggestions = []

        # Trailing whitespace
        if line_content.endswith(' ') or line_content.endswith('\t'):
            suggestions.append(InlineSuggestion(
                file_path=file_path,
                line=line,
                column=len(line_content.rstrip()) + 1,
                suggestion="Trailing whitespace",
                explanation="Remove trailing spaces for cleaner code.",
                category="style",
                confidence=0.5,
            ))

        # Magic numbers
        if re.search(r'\b(?<!\.)\d{2,}\b(?!\.\d)', line_content):
            # Exclude obvious cases like port numbers, common sizes
            if not re.search(r'(port|size|width|height|count|length)\s*[=:]\s*\d+', line_content, re.IGNORECASE):
                suggestions.append(InlineSuggestion(
                    file_path=file_path,
                    line=line,
                    column=1,
                    suggestion="Consider using a named constant instead of magic number",
                    explanation="Named constants make code more readable and maintainable.",
                    category="style",
                    confidence=0.5,
                ))

        return suggestions


class ChangeExplanationEngine:
    """
    v10.0: Explains code changes with detailed reasoning.

    Provides:
    - Line-by-line explanation of changes
    - Reasoning for why each change was made
    - Educational notes for learning
    - Impact assessment
    """

    def __init__(
        self,
        llm_client: Optional[Any] = None,
    ):
        self._llm_client = llm_client

    async def explain_changes(
        self,
        before: str,
        after: str,
        file_path: Optional[str] = None,
        context: Optional[str] = None,
    ) -> List[ChangeExplanation]:
        """
        Explain the differences between before and after versions.

        Args:
            before: Original code
            after: Modified code
            file_path: Optional file path for context
            context: Optional additional context

        Returns:
            List of change explanations
        """
        explanations: List[ChangeExplanation] = []

        before_lines = before.split('\n')
        after_lines = after.split('\n')

        # Use simple diff algorithm
        import difflib
        differ = difflib.Differ()
        diff = list(differ.compare(before_lines, after_lines))

        before_idx = 0
        after_idx = 0

        for line in diff:
            if line.startswith('  '):  # Unchanged
                before_idx += 1
                after_idx += 1
            elif line.startswith('- '):  # Removed
                before_line = line[2:]
                # Look for corresponding addition
                explanation = await self._generate_explanation(
                    before_line,
                    "",
                    before_idx + 1,
                    "remove",
                )
                explanations.append(explanation)
                before_idx += 1
            elif line.startswith('+ '):  # Added
                after_line = line[2:]
                explanation = await self._generate_explanation(
                    "",
                    after_line,
                    after_idx + 1,
                    "add",
                )
                explanations.append(explanation)
                after_idx += 1
            elif line.startswith('? '):  # Info line, skip
                pass

        return explanations

    async def _generate_explanation(
        self,
        before: str,
        after: str,
        line: int,
        change_type: str,
    ) -> ChangeExplanation:
        """Generate explanation for a single change."""
        # Determine category and generate explanation
        category = self._categorize_change(before, after)
        explanation = self._generate_explanation_text(before, after, category)
        reasoning = self._generate_reasoning(before, after, category)
        impact = self._assess_impact(before, after, category)
        educational = self._generate_educational_notes(before, after, category)

        return ChangeExplanation(
            line=line,
            before=before,
            after=after,
            explanation=explanation,
            reasoning=reasoning,
            category=category,
            impact=impact,
            educational_notes=educational,
        )

    def _categorize_change(self, before: str, after: str) -> str:
        """Categorize the type of change."""
        if not before and after:
            return "addition"
        if before and not after:
            return "deletion"

        # Check for specific patterns
        if "def " in after and "def " not in before:
            return "extract"
        if re.search(r'\bclass\b', after) and not re.search(r'\bclass\b', before):
            return "refactor"
        if "try:" in after and "try:" not in before:
            return "error_handling"
        if any(kw in after for kw in ["password", "secret", "key"]) and \
           any(kw in before for kw in ["password", "secret", "key"]):
            return "security"
        if re.search(r'\basync\b|\bawait\b', after) and not re.search(r'\basync\b|\bawait\b', before):
            return "async_conversion"

        return "modify"

    def _generate_explanation_text(self, before: str, after: str, category: str) -> str:
        """Generate human-readable explanation."""
        explanations = {
            "addition": f"Added new code: {after.strip()[:50]}...",
            "deletion": f"Removed code: {before.strip()[:50]}...",
            "extract": "Extracted code into a function for reusability",
            "refactor": "Refactored for better structure and maintainability",
            "error_handling": "Added error handling for robustness",
            "security": "Updated for improved security",
            "async_conversion": "Converted to async for non-blocking operation",
            "modify": f"Modified: '{before.strip()[:30]}' → '{after.strip()[:30]}'",
        }
        return explanations.get(category, "Code modification")

    def _generate_reasoning(self, before: str, after: str, category: str) -> str:
        """Generate reasoning for the change."""
        reasons = {
            "addition": "New functionality was needed to fulfill the requirement",
            "deletion": "This code was no longer needed or was redundant",
            "extract": "Extracting into a function improves reusability and testability",
            "refactor": "The new structure is cleaner and more maintainable",
            "error_handling": "Without error handling, failures could crash the application",
            "security": "Security improvements protect against vulnerabilities",
            "async_conversion": "Async operations prevent blocking the main thread",
            "modify": "The modification improves code quality or fixes an issue",
        }
        return reasons.get(category, "This change improves the code")

    def _assess_impact(self, before: str, after: str, category: str) -> str:
        """Assess the impact of the change."""
        high_impact = {"security", "error_handling", "async_conversion"}
        medium_impact = {"refactor", "extract"}
        low_impact = {"addition", "modify"}

        if category in high_impact:
            return "high"
        if category in medium_impact:
            return "medium"
        if category in low_impact:
            return "low"
        return "none"

    def _generate_educational_notes(self, before: str, after: str, category: str) -> Optional[str]:
        """Generate educational notes for learning."""
        notes = {
            "error_handling": "Always wrap I/O operations and external calls in try-except blocks to handle failures gracefully.",
            "security": "Never hardcode secrets. Use environment variables or a secrets manager.",
            "async_conversion": "Async functions use 'await' to pause execution without blocking. This allows other tasks to run.",
            "extract": "The Single Responsibility Principle suggests each function should do one thing well.",
        }
        return notes.get(category)


class InlineCommentGenerator:
    """
    v10.0: Generates explanatory comments for code changes.

    Creates:
    - Comments explaining why code changed
    - Documentation for complex logic
    - TODO comments for follow-up work
    - Style-appropriate comments per language
    """

    def __init__(
        self,
        language_registry: Optional[LanguageRegistry] = None,
    ):
        self._registry = language_registry or get_language_registry()

    async def generate_comments(
        self,
        file_path: str,
        changes: List[ChangeExplanation],
        comment_style: str = "inline",  # inline, block, docstring
    ) -> Dict[int, str]:
        """
        Generate comments for changes.

        Args:
            file_path: Path to the file
            changes: List of change explanations
            comment_style: Style of comments to generate

        Returns:
            Dictionary mapping line numbers to comments
        """
        language = self._registry.detect_language(file_path)
        config = self._registry.get_config(language)

        if not config:
            return {}

        comments: Dict[int, str] = {}

        for change in changes:
            comment = await self._generate_comment(
                change,
                config,
                comment_style,
            )
            if comment:
                comments[change.line] = comment

        return comments

    async def _generate_comment(
        self,
        change: ChangeExplanation,
        config: LanguageConfig,
        style: str,
    ) -> Optional[str]:
        """Generate a single comment."""
        # Skip trivial changes
        if change.impact == "none":
            return None

        # Build comment content
        content = []

        # Add category tag
        category_tags = {
            "security": "SECURITY",
            "error_handling": "ERROR HANDLING",
            "async_conversion": "ASYNC",
            "refactor": "REFACTOR",
            "extract": "EXTRACT",
        }

        if change.category in category_tags:
            content.append(f"[{category_tags[change.category]}]")

        # Add explanation
        content.append(change.explanation)

        # Add reasoning for high-impact changes
        if change.impact == "high":
            content.append(f"Reason: {change.reasoning}")

        # Format as comment
        comment_text = " ".join(content)

        if style == "inline":
            return f"{config.comment_single} {comment_text}"
        elif style == "block" and config.comment_multi_start:
            return f"{config.comment_multi_start} {comment_text} {config.comment_multi_end}"
        else:
            return f"{config.comment_single} {comment_text}"

    async def generate_docstring(
        self,
        function_code: str,
        language: LanguageType,
    ) -> Optional[str]:
        """Generate a docstring for a function."""
        config = self._registry.get_config(language)
        if not config:
            return None

        # Extract function signature
        func_match = None
        for pattern in config.function_patterns:
            match = re.search(pattern, function_code)
            if match:
                func_match = match
                break

        if not func_match:
            return None

        func_name = func_match.group(1) if func_match.groups() else "function"

        # Generate docstring based on language
        if language == LanguageType.PYTHON:
            return f'''"""
    {self._humanize_name(func_name)}.

    Args:
        TODO: Add argument descriptions

    Returns:
        TODO: Add return description
    """'''
        elif language in (LanguageType.JAVASCRIPT, LanguageType.TYPESCRIPT):
            return f'''/**
 * {self._humanize_name(func_name)}.
 *
 * @param {{TODO}} param - Description
 * @returns {{TODO}} Description
 */'''
        elif language == LanguageType.GO:
            return f'// {self._humanize_name(func_name)}.'

        return None

    def _humanize_name(self, name: str) -> str:
        """Convert function name to human-readable string."""
        # Convert camelCase or snake_case to words
        name = re.sub(r'([a-z])([A-Z])', r'\1 \2', name)
        name = name.replace('_', ' ')
        return name.strip().capitalize()


class InteractiveCodeReviewer:
    """
    v10.0: Interactive code review with cherry-pick support.

    Allows:
    - Accept/reject individual changes
    - Modify suggested changes
    - Preview changes before applying
    - Undo/redo support
    - Session management
    """

    def __init__(
        self,
        explanation_engine: Optional[ChangeExplanationEngine] = None,
        comment_generator: Optional[InlineCommentGenerator] = None,
    ):
        self._explanation_engine = explanation_engine or ChangeExplanationEngine()
        self._comment_generator = comment_generator or InlineCommentGenerator()
        self._sessions: Dict[str, CodeReviewSession] = {}
        self._undo_stack: Dict[str, List[Dict[str, Any]]] = {}
        self._redo_stack: Dict[str, List[Dict[str, Any]]] = {}
        self._lock = asyncio.Lock()

    async def create_review_session(
        self,
        file_path: str,
        before: str,
        after: str,
    ) -> CodeReviewSession:
        """
        Create a new interactive review session.

        Args:
            file_path: Path to the file being reviewed
            before: Original code
            after: Modified code

        Returns:
            A new review session with individual changes
        """
        session_id = str(uuid.uuid4())

        # Get change explanations
        explanations = await self._explanation_engine.explain_changes(before, after, file_path)

        # Create reviewable changes
        changes: List[ReviewableChange] = []
        before_lines = before.split('\n')
        after_lines = after.split('\n')

        import difflib
        matcher = difflib.SequenceMatcher(None, before_lines, after_lines)

        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'equal':
                continue

            change_id = str(uuid.uuid4())[:8]
            before_text = '\n'.join(before_lines[i1:i2])
            after_text = '\n'.join(after_lines[j1:j2])

            # Find matching explanation
            explanation = None
            for exp in explanations:
                if i1 < exp.line <= i2 or j1 < exp.line <= j2:
                    explanation = exp
                    break

            if not explanation:
                explanation = await self._explanation_engine._generate_explanation(
                    before_text, after_text, i1 + 1, tag
                )

            changes.append(ReviewableChange(
                id=change_id,
                file_path=file_path,
                line_start=i1 + 1,
                line_end=i2,
                before_text=before_text,
                after_text=after_text,
                explanation=explanation,
                status=ChangeAction.SKIP,
            ))

        session = CodeReviewSession(
            session_id=session_id,
            file_path=file_path,
            changes=changes,
            status="pending",
        )

        async with self._lock:
            self._sessions[session_id] = session
            self._undo_stack[session_id] = []
            self._redo_stack[session_id] = []

        return session

    async def get_session(self, session_id: str) -> Optional[CodeReviewSession]:
        """Get a review session by ID."""
        return self._sessions.get(session_id)

    async def apply_action(
        self,
        session_id: str,
        change_id: str,
        action: ChangeAction,
        modified_text: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Apply an action to a specific change.

        Args:
            session_id: The review session ID
            change_id: The change ID
            action: The action to apply (accept, reject, modify, skip)
            modified_text: If action is MODIFY, the new text

        Returns:
            Result of the action
        """
        session = self._sessions.get(session_id)
        if not session:
            return {"success": False, "error": "Session not found"}

        # Find the change
        change = None
        for c in session.changes:
            if c.id == change_id:
                change = c
                break

        if not change:
            return {"success": False, "error": "Change not found"}

        # Save to undo stack
        async with self._lock:
            self._undo_stack[session_id].append({
                "change_id": change_id,
                "previous_status": change.status,
                "previous_modified_text": change.user_modified_text,
            })
            self._redo_stack[session_id].clear()

            # Apply action
            old_status = change.status
            change.status = action

            if action == ChangeAction.MODIFY:
                change.user_modified_text = modified_text

            # Update counts
            if old_status == ChangeAction.ACCEPT:
                session.accepted_count -= 1
            elif old_status == ChangeAction.REJECT:
                session.rejected_count -= 1
            elif old_status == ChangeAction.MODIFY:
                session.modified_count -= 1

            if action == ChangeAction.ACCEPT:
                session.accepted_count += 1
            elif action == ChangeAction.REJECT:
                session.rejected_count += 1
            elif action == ChangeAction.MODIFY:
                session.modified_count += 1

        return {
            "success": True,
            "change_id": change_id,
            "action": action.value,
            "session_status": {
                "accepted": session.accepted_count,
                "rejected": session.rejected_count,
                "modified": session.modified_count,
                "pending": len(session.changes) - session.accepted_count - session.rejected_count - session.modified_count,
            },
        }

    async def accept_all(self, session_id: str) -> Dict[str, Any]:
        """Accept all pending changes in a session."""
        session = self._sessions.get(session_id)
        if not session:
            return {"success": False, "error": "Session not found"}

        for change in session.changes:
            if change.status == ChangeAction.SKIP:
                change.status = ChangeAction.ACCEPT
                session.accepted_count += 1

        return {"success": True, "accepted": session.accepted_count}

    async def reject_all(self, session_id: str) -> Dict[str, Any]:
        """Reject all pending changes in a session."""
        session = self._sessions.get(session_id)
        if not session:
            return {"success": False, "error": "Session not found"}

        for change in session.changes:
            if change.status == ChangeAction.SKIP:
                change.status = ChangeAction.REJECT
                session.rejected_count += 1

        return {"success": True, "rejected": session.rejected_count}

    async def undo(self, session_id: str) -> Dict[str, Any]:
        """Undo the last action."""
        async with self._lock:
            stack = self._undo_stack.get(session_id, [])
            if not stack:
                return {"success": False, "error": "Nothing to undo"}

            action = stack.pop()
            self._redo_stack[session_id].append(action)

            session = self._sessions.get(session_id)
            if session:
                for change in session.changes:
                    if change.id == action["change_id"]:
                        change.status = action["previous_status"]
                        change.user_modified_text = action["previous_modified_text"]
                        break

        return {"success": True}

    async def redo(self, session_id: str) -> Dict[str, Any]:
        """Redo the last undone action."""
        async with self._lock:
            stack = self._redo_stack.get(session_id, [])
            if not stack:
                return {"success": False, "error": "Nothing to redo"}

            action = stack.pop()
            self._undo_stack[session_id].append(action)

        return {"success": True}

    async def apply_changes(
        self,
        session_id: str,
        include_comments: bool = True,
    ) -> Dict[str, Any]:
        """
        Apply all accepted/modified changes to the file.

        Args:
            session_id: The review session ID
            include_comments: Whether to add explanatory comments

        Returns:
            Result with the final content
        """
        session = self._sessions.get(session_id)
        if not session:
            return {"success": False, "error": "Session not found"}

        # Read original file
        try:
            original_content = Path(session.file_path).read_text(encoding='utf-8')
        except Exception as e:
            return {"success": False, "error": f"Could not read file: {e}"}

        lines = original_content.split('\n')

        # Apply changes in reverse order (to preserve line numbers)
        sorted_changes = sorted(
            [c for c in session.changes if c.status in (ChangeAction.ACCEPT, ChangeAction.MODIFY)],
            key=lambda c: c.line_start,
            reverse=True,
        )

        # Generate comments if requested
        comments = {}
        if include_comments:
            comments = await self._comment_generator.generate_comments(
                session.file_path,
                [c.explanation for c in sorted_changes],
            )

        for change in sorted_changes:
            # Determine the text to use
            if change.status == ChangeAction.MODIFY and change.user_modified_text:
                new_text = change.user_modified_text
            else:
                new_text = change.after_text

            # Get comment if any
            comment = comments.get(change.line_start)
            if comment:
                new_text = f"{comment}\n{new_text}"

            # Apply change
            new_lines = new_text.split('\n')
            start_idx = change.line_start - 1
            end_idx = change.line_end

            lines[start_idx:end_idx] = new_lines

        # Join and write
        final_content = '\n'.join(lines)

        try:
            Path(session.file_path).write_text(final_content, encoding='utf-8')
        except Exception as e:
            return {"success": False, "error": f"Could not write file: {e}"}

        session.status = "completed"

        return {
            "success": True,
            "file_path": session.file_path,
            "changes_applied": len(sorted_changes),
            "final_content": final_content,
        }

    async def preview_changes(
        self,
        session_id: str,
    ) -> Dict[str, Any]:
        """Preview what the file would look like after applying changes."""
        session = self._sessions.get(session_id)
        if not session:
            return {"success": False, "error": "Session not found"}

        # Read original file
        try:
            original_content = Path(session.file_path).read_text(encoding='utf-8')
        except Exception as e:
            return {"success": False, "error": f"Could not read file: {e}"}

        lines = original_content.split('\n')

        # Apply changes in reverse order
        sorted_changes = sorted(
            [c for c in session.changes if c.status in (ChangeAction.ACCEPT, ChangeAction.MODIFY)],
            key=lambda c: c.line_start,
            reverse=True,
        )

        for change in sorted_changes:
            if change.status == ChangeAction.MODIFY and change.user_modified_text:
                new_text = change.user_modified_text
            else:
                new_text = change.after_text

            new_lines = new_text.split('\n')
            start_idx = change.line_start - 1
            end_idx = change.line_end
            lines[start_idx:end_idx] = new_lines

        return {
            "success": True,
            "preview": '\n'.join(lines),
            "changes_count": len(sorted_changes),
        }

    async def close_session(self, session_id: str) -> bool:
        """Close and cleanup a review session."""
        async with self._lock:
            if session_id in self._sessions:
                del self._sessions[session_id]
            if session_id in self._undo_stack:
                del self._undo_stack[session_id]
            if session_id in self._redo_stack:
                del self._redo_stack[session_id]
            return True
        return False


# =============================================================================
# v10.0: GLOBAL INSTANCES
# =============================================================================

_completion_engine: Optional[LiveCodeCompletionEngine] = None
_error_detector: Optional[RealTimeErrorDetector] = None
_suggestion_provider: Optional[InlineSuggestionProvider] = None
_explanation_engine: Optional[ChangeExplanationEngine] = None
_comment_generator: Optional[InlineCommentGenerator] = None
_interactive_reviewer: Optional[InteractiveCodeReviewer] = None


# =============================================================================
# v10.0: FACTORY FUNCTIONS
# =============================================================================

def get_completion_engine() -> LiveCodeCompletionEngine:
    """Get or create the live code completion engine."""
    global _completion_engine
    if _completion_engine is None:
        _completion_engine = LiveCodeCompletionEngine()
    return _completion_engine


def get_error_detector() -> RealTimeErrorDetector:
    """Get or create the real-time error detector."""
    global _error_detector
    if _error_detector is None:
        _error_detector = RealTimeErrorDetector()
    return _error_detector


def get_suggestion_provider() -> InlineSuggestionProvider:
    """Get or create the inline suggestion provider."""
    global _suggestion_provider
    if _suggestion_provider is None:
        _suggestion_provider = InlineSuggestionProvider()
    return _suggestion_provider


def get_explanation_engine() -> ChangeExplanationEngine:
    """Get or create the change explanation engine."""
    global _explanation_engine
    if _explanation_engine is None:
        _explanation_engine = ChangeExplanationEngine()
    return _explanation_engine


def get_comment_generator() -> InlineCommentGenerator:
    """Get or create the inline comment generator."""
    global _comment_generator
    if _comment_generator is None:
        _comment_generator = InlineCommentGenerator()
    return _comment_generator


def get_interactive_reviewer() -> InteractiveCodeReviewer:
    """Get or create the interactive code reviewer."""
    global _interactive_reviewer
    if _interactive_reviewer is None:
        _interactive_reviewer = InteractiveCodeReviewer()
    return _interactive_reviewer


# =============================================================================
# v10.0: INITIALIZATION AND SHUTDOWN
# =============================================================================

async def initialize_realtime_intelligence_system(
    llm_client: Optional[Any] = None,
) -> Dict[str, Any]:
    """
    Initialize the real-time code intelligence system.

    Args:
        llm_client: Optional LLM client for enhanced suggestions

    Returns:
        Dictionary with initialized components
    """
    logger.info("🧠 Initializing Real-Time Code Intelligence System v10.0...")
    start_time = time.monotonic()

    components = {}

    try:
        # Initialize all components
        components["completion_engine"] = get_completion_engine()
        components["error_detector"] = get_error_detector()
        components["suggestion_provider"] = get_suggestion_provider()
        components["explanation_engine"] = get_explanation_engine()
        components["comment_generator"] = get_comment_generator()
        components["interactive_reviewer"] = get_interactive_reviewer()

        elapsed = time.monotonic() - start_time
        logger.info(f"✅ Real-Time Code Intelligence System initialized in {elapsed:.2f}s")
        logger.info(f"   Components: {len(components)}")

        return components

    except Exception as e:
        logger.error(f"❌ Real-Time Code Intelligence System initialization failed: {e}")
        raise


async def shutdown_realtime_intelligence_system() -> None:
    """Shutdown the real-time code intelligence system."""
    logger.info("Shutting down Real-Time Code Intelligence System...")

    global _completion_engine, _error_detector, _suggestion_provider
    global _explanation_engine, _comment_generator, _interactive_reviewer

    _completion_engine = None
    _error_detector = None
    _suggestion_provider = None
    _explanation_engine = None
    _comment_generator = None
    _interactive_reviewer = None

    logger.info("✅ Real-Time Code Intelligence System shutdown complete")


# =============================================================================
# v10.0: CONVENIENCE FUNCTIONS
# =============================================================================

async def get_completions(
    file_path: str,
    line: int,
    column: int,
    trigger_character: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    Get code completions at a specific position.

    Usage:
        completions = await get_completions("main.py", 10, 5)
    """
    engine = get_completion_engine()

    context = CompletionContext(
        file_path=file_path,
        line=line,
        column=column,
        trigger_character=trigger_character,
    )

    items = await engine.get_completions(context)

    return [
        {
            "label": item.label,
            "kind": item.kind.value,
            "detail": item.detail,
            "documentation": item.documentation,
            "insert_text": item.insert_text,
            "sort_priority": item.sort_priority,
        }
        for item in items
    ]


async def detect_errors_realtime(
    file_path: str,
    content: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    Detect errors in a file in real-time.

    Usage:
        errors = await detect_errors_realtime("main.py")
    """
    detector = get_error_detector()
    errors = await detector.detect_errors(file_path, content)

    return [
        {
            "line": error.line,
            "column": error.column,
            "message": error.message,
            "severity": error.severity.value,
            "code": error.code,
            "source": error.source,
        }
        for error in errors
    ]


async def get_line_suggestions(
    file_path: str,
    line: int,
) -> List[Dict[str, Any]]:
    """
    Get inline suggestions for a specific line.

    Usage:
        suggestions = await get_line_suggestions("main.py", 42)
    """
    provider = get_suggestion_provider()
    suggestions = await provider.get_suggestions_for_line(file_path, line)

    return [
        {
            "line": s.line,
            "suggestion": s.suggestion,
            "explanation": s.explanation,
            "category": s.category,
            "confidence": s.confidence,
            "quick_fix": s.quick_fix,
        }
        for s in suggestions
    ]


async def explain_code_changes(
    before: str,
    after: str,
) -> List[Dict[str, Any]]:
    """
    Explain the differences between two versions of code.

    Usage:
        explanations = await explain_code_changes(old_code, new_code)
    """
    engine = get_explanation_engine()
    explanations = await engine.explain_changes(before, after)

    return [
        {
            "line": exp.line,
            "before": exp.before,
            "after": exp.after,
            "explanation": exp.explanation,
            "reasoning": exp.reasoning,
            "category": exp.category,
            "impact": exp.impact,
            "educational_notes": exp.educational_notes,
        }
        for exp in explanations
    ]


async def create_interactive_review(
    file_path: str,
    before: str,
    after: str,
) -> Dict[str, Any]:
    """
    Create an interactive code review session.

    Usage:
        session = await create_interactive_review("main.py", old_code, new_code)
    """
    reviewer = get_interactive_reviewer()
    session = await reviewer.create_review_session(file_path, before, after)

    return {
        "session_id": session.session_id,
        "file_path": session.file_path,
        "changes_count": len(session.changes),
        "changes": [
            {
                "id": c.id,
                "line_start": c.line_start,
                "line_end": c.line_end,
                "before": c.before_text[:100],
                "after": c.after_text[:100],
                "explanation": c.explanation.explanation,
                "status": c.status.value,
            }
            for c in session.changes
        ],
    }


async def review_change(
    session_id: str,
    change_id: str,
    action: str,
    modified_text: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Apply an action to a change in a review session.

    Usage:
        result = await review_change(session_id, change_id, "accept")
        result = await review_change(session_id, change_id, "reject")
        result = await review_change(session_id, change_id, "modify", new_code)
    """
    reviewer = get_interactive_reviewer()

    action_map = {
        "accept": ChangeAction.ACCEPT,
        "reject": ChangeAction.REJECT,
        "modify": ChangeAction.MODIFY,
        "skip": ChangeAction.SKIP,
    }

    action_enum = action_map.get(action.lower(), ChangeAction.SKIP)

    return await reviewer.apply_action(
        session_id,
        change_id,
        action_enum,
        modified_text,
    )


async def apply_reviewed_changes(
    session_id: str,
    include_comments: bool = True,
) -> Dict[str, Any]:
    """
    Apply all accepted changes from a review session.

    Usage:
        result = await apply_reviewed_changes(session_id)
    """
    reviewer = get_interactive_reviewer()
    return await reviewer.apply_changes(session_id, include_comments)


# =============================================================================
# v11.0: RESILIENT SERVICE MESH - Self-Healing Cross-Repo Infrastructure
# =============================================================================
# Fixes:
# - Race conditions between service startup and health checks
# - Missing startup handshake protocol
# - No retry logic in health checks
# - No auto-recovery from stale services
# - Cascading failures across services
# - Circuit breaker timeout too long
# =============================================================================


class ServiceState(Enum):
    """Service lifecycle states."""
    UNKNOWN = "unknown"
    STARTING = "starting"
    REGISTERING = "registering"
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    RECOVERING = "recovering"
    STALE = "stale"
    DEAD = "dead"
    CIRCUIT_OPEN = "circuit_open"


class HealthCheckResult(Enum):
    """Result of a health check."""
    SUCCESS = "success"
    TIMEOUT = "timeout"
    CONNECTION_REFUSED = "connection_refused"
    CONNECTION_ERROR = "connection_error"
    HTTP_ERROR = "http_error"
    INVALID_RESPONSE = "invalid_response"
    NOT_REGISTERED = "not_registered"
    SKIPPED = "skipped"


@dataclass
class CircuitBreakerConfig:
    """
    Configuration for intelligent circuit breaker.

    v125.1: Tuned for startup resilience:
    - Higher failure_threshold (10) prevents premature opening during startup
    - Longer timeout (60s) allows CloudSQL proxy time to initialize
    - Startup grace period prevents circuit from opening during initial warmup
    - Environment variable overrides for runtime tuning
    """
    # v125.1: Increased from 3 to 10 - prevents premature opening during startup
    # Connection refused errors during proxy startup shouldn't open circuit immediately
    failure_threshold: int = int(os.environ.get("CIRCUIT_FAILURE_THRESHOLD", "10"))

    # Successes needed to close circuit after recovery
    success_threshold: int = int(os.environ.get("CIRCUIT_SUCCESS_THRESHOLD", "2"))

    # v125.1: Increased from 30s to 60s - gives proxy more time to recover
    timeout_seconds: float = float(os.environ.get("CIRCUIT_TIMEOUT_SECONDS", "60.0"))

    # Max requests allowed in half-open state
    half_open_max_requests: int = int(os.environ.get("CIRCUIT_HALF_OPEN_MAX", "3"))

    # Window for failure rate calculation
    sliding_window_size: int = int(os.environ.get("CIRCUIT_SLIDING_WINDOW", "20"))

    # v125.1: Increased from 0.5 to 0.7 - more tolerant of transient failures
    failure_rate_threshold: float = float(os.environ.get("CIRCUIT_FAILURE_RATE", "0.7"))

    # v125.1: NEW - Startup grace period before circuit breaker activates
    # During this period, failures don't count toward opening the circuit
    startup_grace_seconds: float = float(os.environ.get("CIRCUIT_STARTUP_GRACE", "120.0"))


@dataclass
class HealthCheckConfig:
    """Configuration for adaptive health monitoring."""
    initial_delay_seconds: float = 5.0  # Grace period before first check
    interval_seconds: float = 5.0  # Normal check interval
    timeout_seconds: float = 3.0  # Timeout per check
    retry_count: int = 3  # Retries before marking failed
    retry_delay_seconds: float = 0.5  # Delay between retries
    backoff_multiplier: float = 1.5  # Exponential backoff multiplier
    max_backoff_seconds: float = 30.0  # Max backoff delay
    jitter_factor: float = 0.2  # Random jitter (0-20%)
    stale_threshold_seconds: float = 60.0  # Seconds until stale
    dead_threshold_seconds: float = 180.0  # Seconds until dead


@dataclass
class ServiceEndpoint:
    """Service endpoint information."""
    service_name: str
    host: str = "localhost"
    port: Optional[int] = None
    health_path: str = "/health"
    pid: Optional[int] = None
    registered_at: Optional[float] = None
    last_heartbeat: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class HealthCheckSnapshot:
    """Snapshot of a health check result."""
    timestamp: float
    result: HealthCheckResult
    latency_ms: float
    status_code: Optional[int] = None
    error_message: Optional[str] = None


@dataclass
class ServiceHealthState:
    """Complete health state for a service."""
    service_name: str
    state: ServiceState
    endpoint: Optional[ServiceEndpoint] = None
    circuit_state: str = "closed"  # closed, open, half_open
    consecutive_failures: int = 0
    consecutive_successes: int = 0
    total_checks: int = 0
    total_failures: int = 0
    recent_checks: List[HealthCheckSnapshot] = field(default_factory=list)
    last_state_change: float = field(default_factory=time.time)
    recovery_attempts: int = 0
    last_recovery_attempt: Optional[float] = None


class StartupHandshakeProtocol:
    """
    Ensures services are properly registered before health checks begin.

    Fixes the race condition where health checks start before service
    has registered its port with the service registry.
    """

    def __init__(
        self,
        handshake_timeout: float = 60.0,
        poll_interval: float = 0.5,
        registry_path: Optional[Path] = None,
    ):
        self.handshake_timeout = handshake_timeout
        self.poll_interval = poll_interval
        self.registry_path = registry_path or Path.home() / ".jarvis" / "registry"
        self._pending_handshakes: Dict[str, asyncio.Future] = {}
        self._registered_services: Dict[str, ServiceEndpoint] = {}
        self._lock = asyncio.Lock()
        self._watcher_task: Optional[asyncio.Task] = None
        self._running = False

    async def start(self) -> None:
        """Start the handshake protocol watcher."""
        if self._running:
            return
        self._running = True
        self._watcher_task = asyncio.create_task(self._watch_registrations())
        logger.info("StartupHandshakeProtocol started")

    async def stop(self) -> None:
        """Stop the handshake protocol watcher."""
        self._running = False
        if self._watcher_task:
            self._watcher_task.cancel()
            try:
                await self._watcher_task
            except asyncio.CancelledError:
                pass
        # Cancel pending handshakes
        for future in self._pending_handshakes.values():
            if not future.done():
                future.cancel()
        self._pending_handshakes.clear()
        logger.info("StartupHandshakeProtocol stopped")

    async def await_registration(
        self,
        service_name: str,
        timeout: Optional[float] = None,
    ) -> Optional[ServiceEndpoint]:
        """
        Wait for a service to register itself.

        Returns the endpoint once registered, or None on timeout.
        """
        timeout = timeout or self.handshake_timeout

        async with self._lock:
            # Already registered?
            if service_name in self._registered_services:
                return self._registered_services[service_name]

            # Create future if not exists
            if service_name not in self._pending_handshakes:
                self._pending_handshakes[service_name] = asyncio.get_running_loop().create_future()

        try:
            endpoint = await asyncio.wait_for(
                self._pending_handshakes[service_name],
                timeout=timeout,
            )
            return endpoint
        except asyncio.TimeoutError:
            logger.warning(f"Handshake timeout waiting for {service_name} registration")
            return None
        except asyncio.CancelledError:
            return None

    async def notify_registration(
        self,
        service_name: str,
        endpoint: ServiceEndpoint,
    ) -> None:
        """Notify that a service has registered."""
        async with self._lock:
            self._registered_services[service_name] = endpoint

            # Complete pending handshake
            if service_name in self._pending_handshakes:
                future = self._pending_handshakes[service_name]
                if not future.done():
                    future.set_result(endpoint)

        logger.info(f"Service {service_name} registered: {endpoint.host}:{endpoint.port}")

    async def _watch_registrations(self) -> None:
        """Watch registry file for new registrations."""
        registry_file = self.registry_path / "services.json"
        last_mtime = 0.0

        while self._running:
            try:
                if registry_file.exists():
                    mtime = registry_file.stat().st_mtime
                    if mtime > last_mtime:
                        last_mtime = mtime
                        await self._process_registry_update(registry_file)

                await asyncio.sleep(self.poll_interval)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.debug(f"Registry watch error: {e}")
                await asyncio.sleep(self.poll_interval)

    async def _process_registry_update(self, registry_file: Path) -> None:
        """Process registry file update."""
        try:
            content = registry_file.read_text()
            data = json.loads(content)

            for service_name, info in data.get("services", {}).items():
                if service_name not in self._registered_services:
                    endpoint = ServiceEndpoint(
                        service_name=service_name,
                        host=info.get("host", "localhost"),
                        port=info.get("port"),
                        health_path=info.get("health_endpoint", "/health"),
                        pid=info.get("pid"),
                        registered_at=info.get("registered_at", time.time()),
                        last_heartbeat=info.get("last_heartbeat"),
                    )
                    await self.notify_registration(service_name, endpoint)
        except Exception as e:
            logger.debug(f"Registry parse error: {e}")


class IntelligentCircuitBreaker:
    """
    Advanced circuit breaker with:
    - Sliding window failure rate calculation
    - Half-open state with limited requests
    - Adaptive timeout based on failure patterns
    - Fast recovery for transient failures
    """

    def __init__(
        self,
        service_name: str,
        config: Optional[CircuitBreakerConfig] = None,
    ):
        self.service_name = service_name
        self.config = config or CircuitBreakerConfig()
        self._state = "closed"  # closed, open, half_open
        self._failure_count = 0
        self._success_count = 0
        self._half_open_requests = 0
        self._last_failure_time: Optional[float] = None
        self._opened_at: Optional[float] = None
        self._window: deque = deque(maxlen=self.config.sliding_window_size)
        self._lock = asyncio.Lock()
        self._state_change_callbacks: List[Callable] = []

    @property
    def state(self) -> str:
        return self._state

    @property
    def is_open(self) -> bool:
        return self._state == "open"

    @property
    def is_half_open(self) -> bool:
        return self._state == "half_open"

    @property
    def is_closed(self) -> bool:
        return self._state == "closed"

    def on_state_change(self, callback: Callable[[str, str], None]) -> None:
        """Register callback for state changes."""
        self._state_change_callbacks.append(callback)

    async def can_execute(self) -> bool:
        """Check if request can be executed."""
        async with self._lock:
            if self._state == "closed":
                return True

            if self._state == "open":
                # Check if timeout expired
                if self._opened_at:
                    elapsed = time.time() - self._opened_at
                    if elapsed >= self.config.timeout_seconds:
                        await self._transition_to("half_open")
                        return True
                return False

            if self._state == "half_open":
                # Allow limited requests
                if self._half_open_requests < self.config.half_open_max_requests:
                    self._half_open_requests += 1
                    return True
                return False

            return False

    async def record_success(self) -> None:
        """Record a successful execution."""
        async with self._lock:
            self._window.append(True)
            self._failure_count = 0
            self._success_count += 1

            if self._state == "half_open":
                if self._success_count >= self.config.success_threshold:
                    await self._transition_to("closed")

    async def record_failure(self) -> None:
        """Record a failed execution."""
        async with self._lock:
            self._window.append(False)
            self._failure_count += 1
            self._success_count = 0
            self._last_failure_time = time.time()

            if self._state == "half_open":
                # Immediately open on failure in half_open
                await self._transition_to("open")
            elif self._state == "closed":
                # Check if should open
                failure_rate = self._calculate_failure_rate()
                if (self._failure_count >= self.config.failure_threshold or
                    failure_rate >= self.config.failure_rate_threshold):
                    await self._transition_to("open")

    def _calculate_failure_rate(self) -> float:
        """Calculate failure rate from sliding window."""
        if not self._window:
            return 0.0
        failures = sum(1 for x in self._window if not x)
        return failures / len(self._window)

    async def _transition_to(self, new_state: str) -> None:
        """Transition to a new state."""
        old_state = self._state
        self._state = new_state

        if new_state == "open":
            self._opened_at = time.time()
            self._half_open_requests = 0
            logger.warning(f"Circuit breaker OPEN for {self.service_name}")
        elif new_state == "half_open":
            self._half_open_requests = 0
            self._success_count = 0
            logger.info(f"Circuit breaker HALF-OPEN for {self.service_name}")
        elif new_state == "closed":
            self._opened_at = None
            self._failure_count = 0
            self._half_open_requests = 0
            logger.info(f"Circuit breaker CLOSED for {self.service_name}")

        # Notify callbacks
        for callback in self._state_change_callbacks:
            try:
                callback(old_state, new_state)
            except Exception:
                pass

    async def force_open(self) -> None:
        """Force circuit to open state."""
        async with self._lock:
            await self._transition_to("open")

    async def force_close(self) -> None:
        """Force circuit to closed state."""
        async with self._lock:
            await self._transition_to("closed")

    def get_status(self) -> Dict[str, Any]:
        """Get circuit breaker status."""
        return {
            "service": self.service_name,
            "state": self._state,
            "failure_count": self._failure_count,
            "success_count": self._success_count,
            "failure_rate": self._calculate_failure_rate(),
            "window_size": len(self._window),
            "opened_at": self._opened_at,
            "last_failure": self._last_failure_time,
        }


class AdaptiveHealthMonitor:
    """
    Smart health monitoring with:
    - Retry logic with exponential backoff
    - Jitter to prevent thundering herd
    - Sliding window for failure rate
    - Startup grace period
    - Adaptive check intervals
    """

    def __init__(
        self,
        service_name: str,
        endpoint: ServiceEndpoint,
        config: Optional[HealthCheckConfig] = None,
        circuit_breaker: Optional[IntelligentCircuitBreaker] = None,
    ):
        self.service_name = service_name
        self.endpoint = endpoint
        self.config = config or HealthCheckConfig()
        self.circuit_breaker = circuit_breaker or IntelligentCircuitBreaker(service_name)

        self._state = ServiceState.STARTING
        self._health_history: deque = deque(maxlen=20)
        self._consecutive_failures = 0
        self._consecutive_successes = 0
        self._current_backoff = self.config.interval_seconds
        self._last_check: Optional[float] = None
        self._started_at = time.time()
        self._lock = asyncio.Lock()
        self._session: Optional[aiohttp.ClientSession] = None
        self._callbacks: List[Callable] = []

    def on_state_change(self, callback: Callable[[ServiceState, ServiceState], None]) -> None:
        """Register callback for state changes."""
        self._callbacks.append(callback)

    async def check_health(self) -> HealthCheckSnapshot:
        """
        Perform health check with retries and backoff.
        """
        # Respect startup grace period
        if time.time() - self._started_at < self.config.initial_delay_seconds:
            return HealthCheckSnapshot(
                timestamp=time.time(),
                result=HealthCheckResult.SKIPPED,
                latency_ms=0,
                error_message="In startup grace period",
            )

        # Check circuit breaker
        if not await self.circuit_breaker.can_execute():
            return HealthCheckSnapshot(
                timestamp=time.time(),
                result=HealthCheckResult.SKIPPED,
                latency_ms=0,
                error_message="Circuit breaker open",
            )

        # Check if port is known
        if not self.endpoint.port:
            return HealthCheckSnapshot(
                timestamp=time.time(),
                result=HealthCheckResult.NOT_REGISTERED,
                latency_ms=0,
                error_message="Port not registered",
            )

        # Perform check with retries
        last_snapshot = None
        retry_delay = self.config.retry_delay_seconds

        for attempt in range(self.config.retry_count):
            snapshot = await self._do_health_check()
            last_snapshot = snapshot

            if snapshot.result == HealthCheckResult.SUCCESS:
                await self._handle_success(snapshot)
                return snapshot

            # Add jitter to retry delay
            if attempt < self.config.retry_count - 1:
                jitter = random.uniform(0, self.config.jitter_factor * retry_delay)
                await asyncio.sleep(retry_delay + jitter)
                retry_delay *= self.config.backoff_multiplier

        # All retries failed
        await self._handle_failure(last_snapshot)
        return last_snapshot

    async def _do_health_check(self) -> HealthCheckSnapshot:
        """Perform a single health check."""
        url = f"http://{self.endpoint.host}:{self.endpoint.port}{self.endpoint.health_path}"
        start_time = time.time()

        try:
            if not self._session:
                self._session = aiohttp.ClientSession()

            async with self._session.get(
                url,
                timeout=aiohttp.ClientTimeout(total=self.config.timeout_seconds),
            ) as response:
                latency_ms = (time.time() - start_time) * 1000

                if response.status == 200:
                    return HealthCheckSnapshot(
                        timestamp=time.time(),
                        result=HealthCheckResult.SUCCESS,
                        latency_ms=latency_ms,
                        status_code=response.status,
                    )
                else:
                    return HealthCheckSnapshot(
                        timestamp=time.time(),
                        result=HealthCheckResult.HTTP_ERROR,
                        latency_ms=latency_ms,
                        status_code=response.status,
                        error_message=f"HTTP {response.status}",
                    )

        except asyncio.TimeoutError:
            return HealthCheckSnapshot(
                timestamp=time.time(),
                result=HealthCheckResult.TIMEOUT,
                latency_ms=(time.time() - start_time) * 1000,
                error_message="Request timed out",
            )

        except aiohttp.ClientConnectorError as e:
            error_msg = str(e)
            if "Connection refused" in error_msg or "Errno 61" in error_msg:
                result = HealthCheckResult.CONNECTION_REFUSED
            else:
                result = HealthCheckResult.CONNECTION_ERROR

            return HealthCheckSnapshot(
                timestamp=time.time(),
                result=result,
                latency_ms=(time.time() - start_time) * 1000,
                error_message=error_msg[:200],
            )

        except Exception as e:
            return HealthCheckSnapshot(
                timestamp=time.time(),
                result=HealthCheckResult.CONNECTION_ERROR,
                latency_ms=(time.time() - start_time) * 1000,
                error_message=str(e)[:200],
            )

    async def _handle_success(self, snapshot: HealthCheckSnapshot) -> None:
        """Handle successful health check."""
        async with self._lock:
            self._health_history.append(snapshot)
            self._consecutive_failures = 0
            self._consecutive_successes += 1
            self._current_backoff = self.config.interval_seconds
            self._last_check = time.time()

            await self.circuit_breaker.record_success()

            old_state = self._state
            if self._state != ServiceState.HEALTHY:
                self._state = ServiceState.HEALTHY
                await self._notify_state_change(old_state, self._state)

    async def _handle_failure(self, snapshot: HealthCheckSnapshot) -> None:
        """Handle failed health check."""
        async with self._lock:
            self._health_history.append(snapshot)
            self._consecutive_failures += 1
            self._consecutive_successes = 0
            self._last_check = time.time()

            await self.circuit_breaker.record_failure()

            # Increase backoff
            self._current_backoff = min(
                self._current_backoff * self.config.backoff_multiplier,
                self.config.max_backoff_seconds,
            )

            old_state = self._state

            # Determine new state based on failure pattern
            if self._consecutive_failures >= 3:
                elapsed_since_heartbeat = time.time() - (self.endpoint.last_heartbeat or self._started_at)

                if elapsed_since_heartbeat > self.config.dead_threshold_seconds:
                    self._state = ServiceState.DEAD
                elif elapsed_since_heartbeat > self.config.stale_threshold_seconds:
                    self._state = ServiceState.STALE
                else:
                    self._state = ServiceState.UNHEALTHY
            else:
                self._state = ServiceState.DEGRADED

            if old_state != self._state:
                await self._notify_state_change(old_state, self._state)

    async def _notify_state_change(
        self,
        old_state: ServiceState,
        new_state: ServiceState,
    ) -> None:
        """Notify callbacks of state change."""
        logger.info(f"Service {self.service_name}: {old_state.value} -> {new_state.value}")
        for callback in self._callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(old_state, new_state)
                else:
                    callback(old_state, new_state)
            except Exception as e:
                logger.debug(f"State change callback error: {e}")

    def get_next_check_delay(self) -> float:
        """Get delay until next health check with jitter."""
        jitter = random.uniform(0, self.config.jitter_factor * self._current_backoff)
        return self._current_backoff + jitter

    def get_status(self) -> Dict[str, Any]:
        """Get health monitor status."""
        recent_success_rate = 0.0
        if self._health_history:
            successes = sum(
                1 for h in self._health_history
                if h.result == HealthCheckResult.SUCCESS
            )
            recent_success_rate = successes / len(self._health_history)

        return {
            "service": self.service_name,
            "state": self._state.value,
            "consecutive_failures": self._consecutive_failures,
            "consecutive_successes": self._consecutive_successes,
            "current_backoff": self._current_backoff,
            "recent_success_rate": recent_success_rate,
            "history_size": len(self._health_history),
            "circuit_breaker": self.circuit_breaker.get_status(),
        }

    async def close(self) -> None:
        """Close resources."""
        if self._session:
            await self._session.close()
            self._session = None


class SelfHealingServiceManager:
    """
    Manages service recovery with:
    - Automatic restart of stale/dead services
    - Exponential backoff for restart attempts
    - Process monitoring and cleanup
    - Graceful degradation during recovery
    """

    def __init__(
        self,
        max_restart_attempts: int = 5,
        restart_backoff_base: float = 2.0,
        restart_backoff_max: float = 120.0,
        cleanup_orphans: bool = True,
    ):
        self.max_restart_attempts = max_restart_attempts
        self.restart_backoff_base = restart_backoff_base
        self.restart_backoff_max = restart_backoff_max
        self.cleanup_orphans = cleanup_orphans

        self._restart_attempts: Dict[str, int] = defaultdict(int)
        self._last_restart: Dict[str, float] = {}
        self._recovery_in_progress: Set[str] = set()
        self._restart_commands: Dict[str, Callable] = {}
        self._lock = asyncio.Lock()
        self._callbacks: List[Callable] = []

    def register_restart_command(
        self,
        service_name: str,
        restart_fn: Callable[[], Coroutine],
    ) -> None:
        """Register restart function for a service."""
        self._restart_commands[service_name] = restart_fn

    def on_recovery(self, callback: Callable[[str, bool, str], None]) -> None:
        """Register callback for recovery events."""
        self._callbacks.append(callback)

    async def attempt_recovery(
        self,
        service_name: str,
        current_state: ServiceState,
        endpoint: Optional[ServiceEndpoint] = None,
    ) -> bool:
        """
        Attempt to recover a failed service.

        Returns True if recovery initiated successfully.
        """
        async with self._lock:
            # Already recovering?
            if service_name in self._recovery_in_progress:
                logger.debug(f"Recovery already in progress for {service_name}")
                return False

            # Max attempts exceeded?
            if self._restart_attempts[service_name] >= self.max_restart_attempts:
                logger.error(f"Max restart attempts exceeded for {service_name}")
                await self._notify_recovery(service_name, False, "Max attempts exceeded")
                return False

            # Calculate backoff
            attempts = self._restart_attempts[service_name]
            backoff = min(
                self.restart_backoff_base ** attempts,
                self.restart_backoff_max,
            )

            # Check if enough time passed since last restart
            last = self._last_restart.get(service_name, 0)
            if time.time() - last < backoff:
                remaining = backoff - (time.time() - last)
                logger.debug(f"Backoff for {service_name}: {remaining:.1f}s remaining")
                return False

            self._recovery_in_progress.add(service_name)

        try:
            logger.info(f"Attempting recovery for {service_name} (attempt {attempts + 1})")

            # Cleanup orphan processes if enabled
            if self.cleanup_orphans and endpoint and endpoint.pid:
                await self._cleanup_orphan(endpoint.pid)

            # Execute restart command
            if service_name in self._restart_commands:
                restart_fn = self._restart_commands[service_name]
                await restart_fn()

                async with self._lock:
                    self._restart_attempts[service_name] += 1
                    self._last_restart[service_name] = time.time()

                await self._notify_recovery(service_name, True, "Restart initiated")
                return True
            else:
                logger.warning(f"No restart command registered for {service_name}")
                await self._notify_recovery(service_name, False, "No restart command")
                return False

        except Exception as e:
            logger.error(f"Recovery failed for {service_name}: {e}")
            async with self._lock:
                self._restart_attempts[service_name] += 1
                self._last_restart[service_name] = time.time()
            await self._notify_recovery(service_name, False, str(e))
            return False

        finally:
            async with self._lock:
                self._recovery_in_progress.discard(service_name)

    async def _cleanup_orphan(self, pid: int) -> None:
        """Kill orphan process if still running."""
        try:
            import psutil
            if psutil.pid_exists(pid):
                proc = psutil.Process(pid)
                proc.terminate()
                await asyncio.sleep(1)
                if proc.is_running():
                    proc.kill()
                logger.info(f"Cleaned up orphan process {pid}")
        except Exception as e:
            logger.debug(f"Orphan cleanup error: {e}")

    async def _notify_recovery(
        self,
        service_name: str,
        success: bool,
        message: str,
    ) -> None:
        """Notify callbacks of recovery event."""
        for callback in self._callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(service_name, success, message)
                else:
                    callback(service_name, success, message)
            except Exception:
                pass

    def reset_attempts(self, service_name: str) -> None:
        """Reset restart attempts for a service (call on successful recovery)."""
        self._restart_attempts[service_name] = 0

    def get_status(self) -> Dict[str, Any]:
        """Get recovery manager status."""
        return {
            "restart_attempts": dict(self._restart_attempts),
            "last_restart": {k: time.time() - v for k, v in self._last_restart.items()},
            "recovery_in_progress": list(self._recovery_in_progress),
            "registered_services": list(self._restart_commands.keys()),
        }


class CascadeFailurePreventor:
    """
    Prevents cascading failures across services with:
    - Dependency tracking
    - Bulkhead pattern (resource isolation)
    - Load shedding during pressure
    - Graceful degradation routing
    """

    def __init__(
        self,
        max_concurrent_failures: int = 2,
        failure_cascade_window: float = 10.0,
        load_shedding_threshold: float = 0.8,
    ):
        self.max_concurrent_failures = max_concurrent_failures
        self.failure_cascade_window = failure_cascade_window
        self.load_shedding_threshold = load_shedding_threshold

        self._dependencies: Dict[str, Set[str]] = defaultdict(set)
        self._failure_times: Dict[str, List[float]] = defaultdict(list)
        self._in_cascade_mode = False
        self._shedding_services: Set[str] = set()
        self._lock = asyncio.Lock()

    def register_dependency(self, service: str, depends_on: str) -> None:
        """Register that service depends on another."""
        self._dependencies[service].add(depends_on)

    async def record_failure(self, service_name: str) -> None:
        """Record a service failure."""
        async with self._lock:
            now = time.time()
            self._failure_times[service_name].append(now)

            # Clean old failures
            cutoff = now - self.failure_cascade_window
            self._failure_times[service_name] = [
                t for t in self._failure_times[service_name] if t > cutoff
            ]

            # Check for cascade
            concurrent_failures = sum(
                1 for times in self._failure_times.values()
                if any(t > cutoff for t in times)
            )

            if concurrent_failures >= self.max_concurrent_failures:
                if not self._in_cascade_mode:
                    self._in_cascade_mode = True
                    logger.warning("CASCADE FAILURE DETECTED - Entering protection mode")
                    await self._enter_cascade_protection()

    async def _enter_cascade_protection(self) -> None:
        """Enter cascade protection mode."""
        # Identify services to shed load from
        for service, deps in self._dependencies.items():
            failed_deps = [
                d for d in deps
                if self._failure_times.get(d) and len(self._failure_times[d]) > 0
            ]
            if len(failed_deps) > 0:
                self._shedding_services.add(service)
                logger.info(f"Load shedding enabled for {service}")

    async def exit_cascade_protection(self) -> None:
        """Exit cascade protection mode."""
        async with self._lock:
            self._in_cascade_mode = False
            self._shedding_services.clear()
            logger.info("Exiting cascade protection mode")

    def should_shed_load(self, service_name: str) -> bool:
        """Check if load should be shed for service."""
        return service_name in self._shedding_services

    def is_cascade_mode(self) -> bool:
        """Check if in cascade protection mode."""
        return self._in_cascade_mode

    def get_healthy_alternatives(self, failed_service: str) -> List[str]:
        """Get healthy alternatives for a failed service."""
        alternatives = []
        for service, deps in self._dependencies.items():
            if failed_service not in deps:
                # This service doesn't depend on failed one
                if service not in self._shedding_services:
                    alternatives.append(service)
        return alternatives

    def get_status(self) -> Dict[str, Any]:
        """Get cascade prevention status."""
        return {
            "cascade_mode": self._in_cascade_mode,
            "shedding_services": list(self._shedding_services),
            "dependencies": {k: list(v) for k, v in self._dependencies.items()},
            "recent_failures": {
                k: len(v) for k, v in self._failure_times.items() if v
            },
        }


class HeartbeatWatchdog:
    """
    Monitors service heartbeats and triggers recovery for stale services.

    Features:
    - Adaptive stale detection
    - Automatic recovery triggering
    - Heartbeat pattern analysis
    - Proactive failure prediction
    """

    def __init__(
        self,
        stale_threshold: float = 60.0,
        dead_threshold: float = 180.0,
        check_interval: float = 10.0,
        recovery_manager: Optional[SelfHealingServiceManager] = None,
    ):
        self.stale_threshold = stale_threshold
        self.dead_threshold = dead_threshold
        self.check_interval = check_interval
        self.recovery_manager = recovery_manager

        self._services: Dict[str, ServiceEndpoint] = {}
        self._heartbeat_history: Dict[str, List[float]] = defaultdict(list)
        self._expected_intervals: Dict[str, float] = {}
        self._running = False
        self._task: Optional[asyncio.Task] = None
        self._lock = asyncio.Lock()
        self._callbacks: List[Callable] = []

    async def start(self) -> None:
        """Start the heartbeat watchdog."""
        if self._running:
            return
        self._running = True
        self._task = asyncio.create_task(self._monitor_loop())
        logger.info("HeartbeatWatchdog started")

    async def stop(self) -> None:
        """Stop the heartbeat watchdog."""
        self._running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("HeartbeatWatchdog stopped")

    def on_stale(self, callback: Callable[[str, float], None]) -> None:
        """Register callback for stale detection."""
        self._callbacks.append(callback)

    async def register_service(
        self,
        service_name: str,
        endpoint: ServiceEndpoint,
        expected_interval: float = 30.0,
    ) -> None:
        """Register a service for heartbeat monitoring."""
        async with self._lock:
            self._services[service_name] = endpoint
            self._expected_intervals[service_name] = expected_interval
            self._heartbeat_history[service_name].append(time.time())

    async def record_heartbeat(self, service_name: str) -> None:
        """Record a heartbeat from a service."""
        async with self._lock:
            now = time.time()
            if service_name in self._services:
                self._services[service_name].last_heartbeat = now
                self._heartbeat_history[service_name].append(now)

                # Keep only recent history
                cutoff = now - 300  # 5 minutes
                self._heartbeat_history[service_name] = [
                    t for t in self._heartbeat_history[service_name] if t > cutoff
                ]

    async def _monitor_loop(self) -> None:
        """Main monitoring loop."""
        while self._running:
            try:
                await self._check_heartbeats()
                await asyncio.sleep(self.check_interval)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Heartbeat monitor error: {e}")
                await asyncio.sleep(self.check_interval)

    async def _check_heartbeats(self) -> None:
        """
        v93.0: Check all registered services for stale heartbeats.

        Fixed timestamp validation to prevent "56 year" bugs when timestamps
        are None or 0 (which would be interpreted as Unix epoch 1970).
        """
        now = time.time()

        async with self._lock:
            services = list(self._services.items())

        for service_name, endpoint in services:
            # v93.0: Robust timestamp handling - prevent epoch fallback bug
            last_heartbeat = endpoint.last_heartbeat
            registered_at = endpoint.registered_at

            # Determine the reference timestamp
            if last_heartbeat and last_heartbeat > 0:
                # Use actual heartbeat if available
                reference_time = last_heartbeat
            elif registered_at and registered_at > 0:
                # Fall back to registration time for new services
                reference_time = registered_at
            else:
                # v93.0: Skip services with no valid timestamp
                # This prevents the "56 year" bug (now - 0 = epoch age)
                logger.debug(
                    f"[v93.0] Skipping {service_name}: no valid timestamp "
                    f"(heartbeat={last_heartbeat}, registered={registered_at})"
                )
                continue

            # v93.0: Sanity check - timestamp should be within reasonable bounds
            # Reject timestamps more than 24 hours in the future or more than 1 year old
            if reference_time > now + 86400:  # More than 1 day in future
                logger.warning(
                    f"[v93.0] Skipping {service_name}: timestamp in future "
                    f"(reference={reference_time:.0f}, now={now:.0f})"
                )
                continue
            if reference_time < now - 31536000:  # More than 1 year old
                logger.warning(
                    f"[v93.0] Skipping {service_name}: timestamp too old "
                    f"(reference={reference_time:.0f}, now={now:.0f}, age={(now - reference_time):.0f}s)"
                )
                # Remove invalid entry
                async with self._lock:
                    if service_name in self._services:
                        del self._services[service_name]
                continue

            age = now - reference_time

            if age > self.dead_threshold:
                logger.error(f"Service {service_name} is DEAD (no heartbeat for {age:.0f}s)")
                await self._trigger_recovery(service_name, endpoint, ServiceState.DEAD)
            elif age > self.stale_threshold:
                logger.warning(f"Service {service_name} is STALE (last heartbeat {age:.0f}s ago)")
                await self._trigger_recovery(service_name, endpoint, ServiceState.STALE)
                await self._notify_stale(service_name, age)

    async def _trigger_recovery(
        self,
        service_name: str,
        endpoint: ServiceEndpoint,
        state: ServiceState,
    ) -> None:
        """Trigger recovery for a stale/dead service."""
        if self.recovery_manager:
            await self.recovery_manager.attempt_recovery(
                service_name,
                state,
                endpoint,
            )

    async def _notify_stale(self, service_name: str, age: float) -> None:
        """Notify callbacks of stale service."""
        for callback in self._callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(service_name, age)
                else:
                    callback(service_name, age)
            except Exception:
                pass

    def predict_next_stale(self) -> Dict[str, float]:
        """Predict when services might go stale based on patterns."""
        predictions = {}
        now = time.time()

        for service_name, history in self._heartbeat_history.items():
            if len(history) < 2:
                continue

            # Calculate average interval
            intervals = [
                history[i] - history[i-1]
                for i in range(1, len(history))
            ]
            avg_interval = sum(intervals) / len(intervals)

            # Predict next heartbeat
            last = history[-1]
            next_expected = last + avg_interval
            time_until_stale = (last + self.stale_threshold) - now

            predictions[service_name] = {
                "avg_interval": avg_interval,
                "last_heartbeat_age": now - last,
                "next_expected_in": max(0, next_expected - now),
                "stale_in": max(0, time_until_stale),
            }

        return predictions

    def get_status(self) -> Dict[str, Any]:
        """Get watchdog status."""
        now = time.time()
        return {
            "running": self._running,
            "services": {
                name: {
                    "last_heartbeat_age": now - (ep.last_heartbeat or 0),
                    "expected_interval": self._expected_intervals.get(name, 30),
                    "heartbeat_count": len(self._heartbeat_history.get(name, [])),
                }
                for name, ep in self._services.items()
            },
            "predictions": self.predict_next_stale(),
        }


class GracefulDegradationRouter:
    """
    Routes requests around failed services with:
    - Fallback chain support
    - Health-aware routing
    - Load balancing across healthy instances
    - Timeout-based failover
    """

    def __init__(
        self,
        default_timeout: float = 5.0,
        fallback_timeout: float = 10.0,
    ):
        self.default_timeout = default_timeout
        self.fallback_timeout = fallback_timeout

        self._routes: Dict[str, List[ServiceEndpoint]] = defaultdict(list)
        self._health_states: Dict[str, ServiceState] = {}
        self._fallback_chains: Dict[str, List[str]] = {}
        self._lock = asyncio.Lock()

    def register_route(
        self,
        route_name: str,
        endpoints: List[ServiceEndpoint],
        fallback_chain: Optional[List[str]] = None,
    ) -> None:
        """Register a route with multiple endpoints."""
        self._routes[route_name] = endpoints
        if fallback_chain:
            self._fallback_chains[route_name] = fallback_chain

    async def update_health(self, service_name: str, state: ServiceState) -> None:
        """Update health state for a service."""
        async with self._lock:
            self._health_states[service_name] = state

    async def get_healthy_endpoint(
        self,
        route_name: str,
    ) -> Optional[ServiceEndpoint]:
        """Get a healthy endpoint for a route."""
        async with self._lock:
            endpoints = self._routes.get(route_name, [])

            # Try primary endpoints
            for endpoint in endpoints:
                state = self._health_states.get(endpoint.service_name, ServiceState.UNKNOWN)
                if state in (ServiceState.HEALTHY, ServiceState.UNKNOWN):
                    return endpoint

            # Try degraded endpoints
            for endpoint in endpoints:
                state = self._health_states.get(endpoint.service_name, ServiceState.UNKNOWN)
                if state == ServiceState.DEGRADED:
                    logger.warning(f"Using degraded endpoint for {route_name}")
                    return endpoint

            # Try fallback chain
            fallback_chain = self._fallback_chains.get(route_name, [])
            for fallback_route in fallback_chain:
                fallback_endpoints = self._routes.get(fallback_route, [])
                for endpoint in fallback_endpoints:
                    state = self._health_states.get(endpoint.service_name, ServiceState.UNKNOWN)
                    if state in (ServiceState.HEALTHY, ServiceState.DEGRADED, ServiceState.UNKNOWN):
                        logger.info(f"Using fallback {fallback_route} for {route_name}")
                        return endpoint

            return None

    async def execute_with_fallback(
        self,
        route_name: str,
        request_fn: Callable[[ServiceEndpoint], Coroutine],
        timeout: Optional[float] = None,
    ) -> Any:
        """Execute request with automatic fallback."""
        timeout = timeout or self.default_timeout

        # Try primary
        endpoint = await self.get_healthy_endpoint(route_name)
        if endpoint:
            try:
                return await asyncio.wait_for(
                    request_fn(endpoint),
                    timeout=timeout,
                )
            except asyncio.TimeoutError:
                logger.warning(f"Timeout on {route_name} primary, trying fallback")
                await self.update_health(endpoint.service_name, ServiceState.DEGRADED)
            except Exception as e:
                logger.warning(f"Error on {route_name} primary: {e}")
                await self.update_health(endpoint.service_name, ServiceState.UNHEALTHY)

        # Try fallbacks
        fallback_chain = self._fallback_chains.get(route_name, [])
        for fallback_route in fallback_chain:
            endpoint = await self.get_healthy_endpoint(fallback_route)
            if endpoint:
                try:
                    return await asyncio.wait_for(
                        request_fn(endpoint),
                        timeout=self.fallback_timeout,
                    )
                except Exception:
                    continue

        raise RuntimeError(f"All endpoints exhausted for {route_name}")

    def get_status(self) -> Dict[str, Any]:
        """Get router status."""
        return {
            "routes": {
                name: [
                    {
                        "service": ep.service_name,
                        "port": ep.port,
                        "health": self._health_states.get(ep.service_name, ServiceState.UNKNOWN).value,
                    }
                    for ep in endpoints
                ]
                for name, endpoints in self._routes.items()
            },
            "fallback_chains": self._fallback_chains,
        }


class ResilientServiceMesh:
    """
    Master orchestrator for resilient service mesh.

    Combines all v11.0 components:
    - StartupHandshakeProtocol
    - IntelligentCircuitBreaker
    - AdaptiveHealthMonitor
    - SelfHealingServiceManager
    - CascadeFailurePreventor
    - HeartbeatWatchdog
    - GracefulDegradationRouter
    """

    def __init__(
        self,
        health_config: Optional[HealthCheckConfig] = None,
        circuit_config: Optional[CircuitBreakerConfig] = None,
    ):
        self.health_config = health_config or HealthCheckConfig()
        self.circuit_config = circuit_config or CircuitBreakerConfig()

        # Core components
        self.handshake = StartupHandshakeProtocol()
        self.recovery_manager = SelfHealingServiceManager()
        self.cascade_preventor = CascadeFailurePreventor()
        self.heartbeat_watchdog = HeartbeatWatchdog(
            recovery_manager=self.recovery_manager,
        )
        self.router = GracefulDegradationRouter()

        # Per-service components
        self._circuit_breakers: Dict[str, IntelligentCircuitBreaker] = {}
        self._health_monitors: Dict[str, AdaptiveHealthMonitor] = {}
        self._service_states: Dict[str, ServiceHealthState] = {}

        self._running = False
        self._monitor_task: Optional[asyncio.Task] = None
        self._lock = asyncio.Lock()

    async def start(self) -> None:
        """Start the resilient service mesh."""
        if self._running:
            return

        self._running = True

        # Start sub-components
        await self.handshake.start()
        await self.heartbeat_watchdog.start()

        # Start main monitoring loop
        self._monitor_task = asyncio.create_task(self._monitor_loop())

        logger.info("🛡️ ResilientServiceMesh v11.0 started")

    async def stop(self) -> None:
        """Stop the resilient service mesh."""
        self._running = False

        if self._monitor_task:
            self._monitor_task.cancel()
            try:
                await self._monitor_task
            except asyncio.CancelledError:
                pass

        # Stop sub-components
        await self.handshake.stop()
        await self.heartbeat_watchdog.stop()

        # Close health monitors
        for monitor in self._health_monitors.values():
            await monitor.close()

        logger.info("ResilientServiceMesh stopped")

    async def register_service(
        self,
        service_name: str,
        endpoint: ServiceEndpoint,
        dependencies: Optional[List[str]] = None,
        restart_fn: Optional[Callable[[], Coroutine]] = None,
    ) -> None:
        """Register a service with the mesh."""
        async with self._lock:
            # Create circuit breaker
            circuit_breaker = IntelligentCircuitBreaker(
                service_name,
                self.circuit_config,
            )
            self._circuit_breakers[service_name] = circuit_breaker

            # Create health monitor
            monitor = AdaptiveHealthMonitor(
                service_name,
                endpoint,
                self.health_config,
                circuit_breaker,
            )
            self._health_monitors[service_name] = monitor

            # Register with watchdog
            await self.heartbeat_watchdog.register_service(
                service_name,
                endpoint,
            )

            # Register dependencies
            if dependencies:
                for dep in dependencies:
                    self.cascade_preventor.register_dependency(service_name, dep)

            # Register restart function
            if restart_fn:
                self.recovery_manager.register_restart_command(service_name, restart_fn)

            # Create health state
            self._service_states[service_name] = ServiceHealthState(
                service_name=service_name,
                state=ServiceState.STARTING,
                endpoint=endpoint,
            )

            # Set up state change callbacks
            def on_state_change(old: ServiceState, new: ServiceState) -> None:
                self._handle_state_change(service_name, old, new)

            monitor.on_state_change(on_state_change)

            logger.info(f"Registered service: {service_name}")

    async def await_service_ready(
        self,
        service_name: str,
        timeout: float = 60.0,
    ) -> bool:
        """Wait for a service to be ready."""
        endpoint = await self.handshake.await_registration(service_name, timeout)
        if not endpoint:
            return False

        # Update endpoint
        async with self._lock:
            if service_name in self._health_monitors:
                self._health_monitors[service_name].endpoint = endpoint
            if service_name in self._service_states:
                self._service_states[service_name].endpoint = endpoint

        return True

    async def record_heartbeat(self, service_name: str) -> None:
        """Record a heartbeat from a service."""
        await self.heartbeat_watchdog.record_heartbeat(service_name)

        # Reset recovery attempts on successful heartbeat
        self.recovery_manager.reset_attempts(service_name)

    async def get_service_health(self, service_name: str) -> Optional[ServiceHealthState]:
        """Get health state for a service."""
        async with self._lock:
            return self._service_states.get(service_name)

    async def get_healthy_endpoint(
        self,
        service_name: str,
    ) -> Optional[ServiceEndpoint]:
        """Get a healthy endpoint for a service."""
        state = await self.get_service_health(service_name)
        if state and state.state in (ServiceState.HEALTHY, ServiceState.DEGRADED):
            return state.endpoint
        return None

    def _handle_state_change(
        self,
        service_name: str,
        old_state: ServiceState,
        new_state: ServiceState,
    ) -> None:
        """Handle service state change."""
        if service_name in self._service_states:
            self._service_states[service_name].state = new_state
            self._service_states[service_name].last_state_change = time.time()

        # Update router
        asyncio.create_task(self.router.update_health(service_name, new_state))

        # Record failure for cascade prevention
        if new_state in (ServiceState.UNHEALTHY, ServiceState.DEAD, ServiceState.STALE):
            asyncio.create_task(self.cascade_preventor.record_failure(service_name))

    async def _monitor_loop(self) -> None:
        """Main health monitoring loop."""
        while self._running:
            try:
                async with self._lock:
                    monitors = list(self._health_monitors.items())

                # Run health checks in parallel
                tasks = []
                for service_name, monitor in monitors:
                    tasks.append(self._check_service_health(service_name, monitor))

                if tasks:
                    await asyncio.gather(*tasks, return_exceptions=True)

                # Get minimum delay for next check
                min_delay = min(
                    (m.get_next_check_delay() for m in self._health_monitors.values()),
                    default=5.0,
                )

                await asyncio.sleep(min_delay)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Monitor loop error: {e}")
                await asyncio.sleep(5.0)

    async def _check_service_health(
        self,
        service_name: str,
        monitor: AdaptiveHealthMonitor,
    ) -> None:
        """Check health of a single service."""
        try:
            snapshot = await monitor.check_health()

            # Update health state
            async with self._lock:
                if service_name in self._service_states:
                    state = self._service_states[service_name]
                    state.total_checks += 1
                    if snapshot.result != HealthCheckResult.SUCCESS:
                        state.total_failures += 1
                    state.recent_checks.append(snapshot)
                    if len(state.recent_checks) > 20:
                        state.recent_checks.pop(0)

            # Trigger recovery if needed
            current_state = monitor._state
            if current_state in (ServiceState.STALE, ServiceState.DEAD, ServiceState.UNHEALTHY):
                await self.recovery_manager.attempt_recovery(
                    service_name,
                    current_state,
                    monitor.endpoint,
                )

        except Exception as e:
            logger.debug(f"Health check error for {service_name}: {e}")

    def get_status(self) -> Dict[str, Any]:
        """Get comprehensive mesh status."""
        return {
            "running": self._running,
            "services": {
                name: {
                    "state": state.state.value,
                    "circuit": self._circuit_breakers.get(name, {}).get_status() if name in self._circuit_breakers else None,
                    "health": self._health_monitors.get(name, {}).get_status() if name in self._health_monitors else None,
                    "total_checks": state.total_checks,
                    "total_failures": state.total_failures,
                    "recovery_attempts": state.recovery_attempts,
                }
                for name, state in self._service_states.items()
            },
            "recovery_manager": self.recovery_manager.get_status(),
            "cascade_preventor": self.cascade_preventor.get_status(),
            "heartbeat_watchdog": self.heartbeat_watchdog.get_status(),
            "router": self.router.get_status(),
        }


# =============================================================================
# v11.0: SINGLETON INSTANCES AND FACTORY FUNCTIONS
# =============================================================================

_resilient_mesh: Optional[ResilientServiceMesh] = None
_handshake_protocol: Optional[StartupHandshakeProtocol] = None
_heartbeat_watchdog: Optional[HeartbeatWatchdog] = None
_recovery_manager: Optional[SelfHealingServiceManager] = None
_cascade_preventor: Optional[CascadeFailurePreventor] = None
_degradation_router: Optional[GracefulDegradationRouter] = None


def get_resilient_mesh() -> ResilientServiceMesh:
    """Get or create the resilient service mesh singleton."""
    global _resilient_mesh
    if _resilient_mesh is None:
        _resilient_mesh = ResilientServiceMesh()
    return _resilient_mesh


def get_handshake_protocol() -> StartupHandshakeProtocol:
    """Get or create the startup handshake protocol singleton."""
    global _handshake_protocol
    if _handshake_protocol is None:
        _handshake_protocol = StartupHandshakeProtocol()
    return _handshake_protocol


def get_heartbeat_watchdog() -> HeartbeatWatchdog:
    """Get or create the heartbeat watchdog singleton."""
    global _heartbeat_watchdog
    if _heartbeat_watchdog is None:
        _heartbeat_watchdog = HeartbeatWatchdog()
    return _heartbeat_watchdog


def get_recovery_manager() -> SelfHealingServiceManager:
    """Get or create the self-healing service manager singleton."""
    global _recovery_manager
    if _recovery_manager is None:
        _recovery_manager = SelfHealingServiceManager()
    return _recovery_manager


def get_cascade_preventor() -> CascadeFailurePreventor:
    """Get or create the cascade failure preventor singleton."""
    global _cascade_preventor
    if _cascade_preventor is None:
        _cascade_preventor = CascadeFailurePreventor()
    return _cascade_preventor


def get_degradation_router() -> GracefulDegradationRouter:
    """Get or create the graceful degradation router singleton."""
    global _degradation_router
    if _degradation_router is None:
        _degradation_router = GracefulDegradationRouter()
    return _degradation_router


# =============================================================================
# v11.0: INITIALIZATION AND SHUTDOWN
# =============================================================================

async def initialize_resilient_mesh(
    services: Optional[List[Dict[str, Any]]] = None,
) -> Dict[str, Any]:
    """
    Initialize the resilient service mesh.

    Args:
        services: Optional list of service configurations

    Returns:
        Dictionary with initialized components
    """
    logger.info("🛡️ Initializing Resilient Service Mesh v11.0...")
    start_time = time.monotonic()

    components = {}

    try:
        mesh = get_resilient_mesh()
        await mesh.start()
        components["resilient_mesh"] = mesh

        # Register services if provided
        if services:
            for svc in services:
                endpoint = ServiceEndpoint(
                    service_name=svc["name"],
                    host=svc.get("host", "localhost"),
                    port=svc.get("port"),
                    health_path=svc.get("health_path", "/health"),
                )
                await mesh.register_service(
                    svc["name"],
                    endpoint,
                    dependencies=svc.get("dependencies"),
                )

        # Also expose individual components
        components["handshake_protocol"] = mesh.handshake
        components["heartbeat_watchdog"] = mesh.heartbeat_watchdog
        components["recovery_manager"] = mesh.recovery_manager
        components["cascade_preventor"] = mesh.cascade_preventor
        components["degradation_router"] = mesh.router

        elapsed = time.monotonic() - start_time
        logger.info(f"✅ Resilient Service Mesh initialized in {elapsed:.2f}s")
        logger.info(f"   Components: {len(components)}")

        return components

    except Exception as e:
        logger.error(f"❌ Resilient Service Mesh initialization failed: {e}")
        raise


async def shutdown_resilient_mesh() -> None:
    """Shutdown the resilient service mesh."""
    logger.info("Shutting down Resilient Service Mesh...")

    global _resilient_mesh, _handshake_protocol, _heartbeat_watchdog
    global _recovery_manager, _cascade_preventor, _degradation_router

    if _resilient_mesh:
        await _resilient_mesh.stop()
        _resilient_mesh = None

    if _handshake_protocol:
        await _handshake_protocol.stop()
        _handshake_protocol = None

    if _heartbeat_watchdog:
        await _heartbeat_watchdog.stop()
        _heartbeat_watchdog = None

    _recovery_manager = None
    _cascade_preventor = None
    _degradation_router = None

    logger.info("✅ Resilient Service Mesh shutdown complete")


# =============================================================================
# v11.0: CONVENIENCE FUNCTIONS
# =============================================================================

async def register_service_with_mesh(
    service_name: str,
    host: str = "localhost",
    port: Optional[int] = None,
    health_path: str = "/health",
    dependencies: Optional[List[str]] = None,
    restart_fn: Optional[Callable[[], Coroutine]] = None,
) -> None:
    """
    Register a service with the resilient mesh.

    Usage:
        await register_service_with_mesh(
            "jarvis-prime",
            port=8001,
            dependencies=["jarvis-core"],
            restart_fn=restart_prime,
        )
    """
    mesh = get_resilient_mesh()
    endpoint = ServiceEndpoint(
        service_name=service_name,
        host=host,
        port=port,
        health_path=health_path,
    )
    await mesh.register_service(
        service_name,
        endpoint,
        dependencies=dependencies,
        restart_fn=restart_fn,
    )


async def await_service_registration(
    service_name: str,
    timeout: float = 60.0,
) -> bool:
    """
    Wait for a service to register before checking health.

    Usage:
        if await await_service_registration("reactor-core"):
            print("Reactor Core is registered!")
    """
    mesh = get_resilient_mesh()
    return await mesh.await_service_ready(service_name, timeout)


async def send_heartbeat(service_name: str) -> None:
    """
    Send a heartbeat from a service.

    Usage:
        await send_heartbeat("jarvis-core")
    """
    mesh = get_resilient_mesh()
    await mesh.record_heartbeat(service_name)


async def get_healthy_service(
    service_name: str,
) -> Optional[Dict[str, Any]]:
    """
    Get a healthy endpoint for a service.

    Usage:
        endpoint = await get_healthy_service("jarvis-prime")
        if endpoint:
            url = f"http://{endpoint['host']}:{endpoint['port']}"
    """
    mesh = get_resilient_mesh()
    endpoint = await mesh.get_healthy_endpoint(service_name)
    if endpoint:
        return {
            "service_name": endpoint.service_name,
            "host": endpoint.host,
            "port": endpoint.port,
            "health_path": endpoint.health_path,
        }
    return None


async def get_mesh_status() -> Dict[str, Any]:
    """
    Get comprehensive mesh status.

    Usage:
        status = await get_mesh_status()
        print(f"Services: {list(status['services'].keys())}")
    """
    mesh = get_resilient_mesh()
    return mesh.get_status()


# =============================================================================
# v12.0: RESILIENT EXPERIENCE MESH - Multi-Backend Experience Forwarding
# =============================================================================
# Fixes:
# - Hard Redis dependency blocking initialization
# - No degraded-mode initialization path
# - Multiple uncoordinated experience systems
# - Event Bus not monitored continuously
# - No in-memory context store fallback
# =============================================================================


class BackendType(Enum):
    """Available storage backends."""
    REDIS = "redis"
    SQLITE = "sqlite"
    MEMORY = "memory"
    FILE = "file"


class BackendHealth(Enum):
    """Backend health states."""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"
    UNAVAILABLE = "unavailable"
    RECOVERING = "recovering"


class ExperienceType(Enum):
    """Types of experiences to forward."""
    INTERACTION = "interaction"
    TOOL_USAGE = "tool_usage"
    REASONING = "reasoning"
    OUTCOME = "outcome"
    ERROR = "error"
    METRIC = "metric"
    LEARNING = "learning"


class ForwardingStatus(Enum):
    """Status of experience forwarding."""
    PENDING = "pending"
    QUEUED = "queued"
    FORWARDING = "forwarding"
    FORWARDED = "forwarded"
    FAILED = "failed"
    RETRYING = "retrying"
    EXPIRED = "expired"


@dataclass
class BackendConfig:
    """Configuration for a storage backend."""
    backend_type: BackendType
    enabled: bool = True
    priority: int = 0  # Lower = higher priority
    connection_string: str = ""
    timeout_seconds: float = 5.0
    max_retries: int = 3
    health_check_interval: float = 30.0
    max_queue_size: int = 10000
    batch_size: int = 100
    flush_interval: float = 5.0


@dataclass
class ExperiencePacket:
    """Unified experience data packet."""
    packet_id: str
    experience_type: ExperienceType
    timestamp: float
    source: str  # jarvis, jarvis-prime, reactor-core
    destination: str
    payload: Dict[str, Any]
    priority: int = 5  # 1-10, lower = higher priority
    ttl_seconds: float = 3600.0  # 1 hour default
    retry_count: int = 0
    max_retries: int = 3
    status: ForwardingStatus = ForwardingStatus.PENDING
    created_at: float = field(default_factory=time.time)
    forwarded_at: Optional[float] = None
    error_message: Optional[str] = None
    content_hash: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

    def is_expired(self) -> bool:
        """Check if packet has expired."""
        return time.time() > (self.created_at + self.ttl_seconds)

    def can_retry(self) -> bool:
        """Check if packet can be retried."""
        return self.retry_count < self.max_retries and not self.is_expired()


@dataclass
class BackendState:
    """Runtime state of a backend."""
    backend_type: BackendType
    health: BackendHealth
    last_health_check: float
    consecutive_failures: int
    consecutive_successes: int
    total_operations: int
    total_failures: int
    avg_latency_ms: float
    queue_size: int
    last_error: Optional[str] = None
    connected_at: Optional[float] = None


class InMemoryExperienceStore:
    """
    In-memory experience store with LRU eviction.
    Used as fallback when all other backends are unavailable.
    """

    def __init__(
        self,
        max_size: int = 10000,
        eviction_batch_size: int = 100,
    ):
        self.max_size = max_size
        self.eviction_batch_size = eviction_batch_size
        self._store: Dict[str, ExperiencePacket] = {}
        self._order: deque = deque(maxlen=max_size)
        self._lock = asyncio.Lock()
        self._stats = {
            "writes": 0,
            "reads": 0,
            "evictions": 0,
            "hits": 0,
            "misses": 0,
        }

    async def store(self, packet: ExperiencePacket) -> bool:
        """Store an experience packet."""
        async with self._lock:
            # Evict if at capacity
            while len(self._store) >= self.max_size:
                if self._order:
                    old_id = self._order.popleft()
                    self._store.pop(old_id, None)
                    self._stats["evictions"] += 1
                else:
                    break

            self._store[packet.packet_id] = packet
            self._order.append(packet.packet_id)
            self._stats["writes"] += 1
            return True

    async def retrieve(self, packet_id: str) -> Optional[ExperiencePacket]:
        """Retrieve an experience packet."""
        async with self._lock:
            self._stats["reads"] += 1
            packet = self._store.get(packet_id)
            if packet:
                self._stats["hits"] += 1
            else:
                self._stats["misses"] += 1
            return packet

    async def retrieve_batch(
        self,
        limit: int = 100,
        status: Optional[ForwardingStatus] = None,
    ) -> List[ExperiencePacket]:
        """Retrieve a batch of packets."""
        async with self._lock:
            packets = []
            for packet in self._store.values():
                if status and packet.status != status:
                    continue
                packets.append(packet)
                if len(packets) >= limit:
                    break
            return packets

    async def update_status(
        self,
        packet_id: str,
        status: ForwardingStatus,
        error: Optional[str] = None,
    ) -> bool:
        """Update packet status."""
        async with self._lock:
            if packet_id in self._store:
                self._store[packet_id].status = status
                if error:
                    self._store[packet_id].error_message = error
                if status == ForwardingStatus.FORWARDED:
                    self._store[packet_id].forwarded_at = time.time()
                return True
            return False

    async def remove(self, packet_id: str) -> bool:
        """Remove a packet."""
        async with self._lock:
            if packet_id in self._store:
                del self._store[packet_id]
                return True
            return False

    async def get_pending_count(self) -> int:
        """Get count of pending packets."""
        async with self._lock:
            return sum(
                1 for p in self._store.values()
                if p.status in (ForwardingStatus.PENDING, ForwardingStatus.QUEUED)
            )

    def get_stats(self) -> Dict[str, Any]:
        """Get store statistics."""
        return {
            "size": len(self._store),
            "max_size": self.max_size,
            **self._stats,
        }


class SQLiteExperienceStore:
    """
    SQLite-backed experience store with WAL mode for performance.
    Used as primary fallback when Redis is unavailable.
    """

    def __init__(
        self,
        db_path: Optional[Path] = None,
        max_queue_size: int = 100000,
    ):
        self.db_path = db_path or Path.home() / ".jarvis" / "experience_mesh.db"
        self.max_queue_size = max_queue_size
        self._conn: Optional[Any] = None  # aiosqlite.Connection when available
        self._lock = asyncio.Lock()
        self._initialized = False

    async def connect(self) -> bool:
        """Connect to SQLite database."""
        if aiosqlite is None:
            logger.warning("aiosqlite not installed - SQLite backend unavailable")
            return False

        try:
            self.db_path.parent.mkdir(parents=True, exist_ok=True)
            self._conn = await aiosqlite.connect(str(self.db_path))

            # v113.0: Enable WAL mode + busy timeout for better concurrency
            # This prevents "database is locked" errors during cleanup
            await self._conn.execute("PRAGMA journal_mode=WAL")
            await self._conn.execute("PRAGMA synchronous=NORMAL")
            await self._conn.execute("PRAGMA cache_size=-64000")  # 64MB cache
            await self._conn.execute("PRAGMA busy_timeout=30000")  # v113.0: 30s busy timeout
            await self._conn.execute("PRAGMA wal_autocheckpoint=1000")  # v113.0: Auto-checkpoint

            # Create tables
            await self._conn.execute("""
                CREATE TABLE IF NOT EXISTS experiences (
                    packet_id TEXT PRIMARY KEY,
                    experience_type TEXT NOT NULL,
                    timestamp REAL NOT NULL,
                    source TEXT NOT NULL,
                    destination TEXT NOT NULL,
                    payload TEXT NOT NULL,
                    priority INTEGER DEFAULT 5,
                    ttl_seconds REAL DEFAULT 3600,
                    retry_count INTEGER DEFAULT 0,
                    max_retries INTEGER DEFAULT 3,
                    status TEXT DEFAULT 'pending',
                    created_at REAL NOT NULL,
                    forwarded_at REAL,
                    error_message TEXT,
                    content_hash TEXT,
                    metadata TEXT
                )
            """)

            # Create indexes
            await self._conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_status ON experiences(status)
            """)
            await self._conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_priority ON experiences(priority, created_at)
            """)
            await self._conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_destination ON experiences(destination)
            """)

            await self._conn.commit()
            self._initialized = True
            logger.info(f"SQLiteExperienceStore connected: {self.db_path}")
            return True

        except Exception as e:
            logger.error(f"SQLiteExperienceStore connection failed: {e}")
            return False

    async def store(self, packet: ExperiencePacket) -> bool:
        """Store an experience packet."""
        if not self._initialized:
            return False

        try:
            async with self._lock:
                await self._conn.execute("""
                    INSERT OR REPLACE INTO experiences (
                        packet_id, experience_type, timestamp, source, destination,
                        payload, priority, ttl_seconds, retry_count, max_retries,
                        status, created_at, forwarded_at, error_message, content_hash, metadata
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    packet.packet_id,
                    packet.experience_type.value,
                    packet.timestamp,
                    packet.source,
                    packet.destination,
                    json.dumps(packet.payload),
                    packet.priority,
                    packet.ttl_seconds,
                    packet.retry_count,
                    packet.max_retries,
                    packet.status.value,
                    packet.created_at,
                    packet.forwarded_at,
                    packet.error_message,
                    packet.content_hash,
                    json.dumps(packet.metadata),
                ))
                await self._conn.commit()
                return True

        except Exception as e:
            logger.error(f"SQLiteExperienceStore store failed: {e}")
            return False

    async def retrieve(self, packet_id: str) -> Optional[ExperiencePacket]:
        """Retrieve an experience packet."""
        if not self._initialized:
            return None

        try:
            async with self._lock:
                cursor = await self._conn.execute("""
                    SELECT * FROM experiences WHERE packet_id = ?
                """, (packet_id,))
                row = await cursor.fetchone()

                if row:
                    return self._row_to_packet(row)
                return None

        except Exception as e:
            logger.error(f"SQLiteExperienceStore retrieve failed: {e}")
            return None

    async def retrieve_batch(
        self,
        limit: int = 100,
        status: Optional[ForwardingStatus] = None,
        destination: Optional[str] = None,
    ) -> List[ExperiencePacket]:
        """Retrieve a batch of packets."""
        if not self._initialized:
            return []

        try:
            async with self._lock:
                query = "SELECT * FROM experiences WHERE 1=1"
                params = []

                if status:
                    query += " AND status = ?"
                    params.append(status.value)

                if destination:
                    query += " AND destination = ?"
                    params.append(destination)

                query += " ORDER BY priority ASC, created_at ASC LIMIT ?"
                params.append(limit)

                cursor = await self._conn.execute(query, params)
                rows = await cursor.fetchall()

                return [self._row_to_packet(row) for row in rows]

        except Exception as e:
            logger.error(f"SQLiteExperienceStore retrieve_batch failed: {e}")
            return []

    async def update_status(
        self,
        packet_id: str,
        status: ForwardingStatus,
        error: Optional[str] = None,
    ) -> bool:
        """Update packet status."""
        if not self._initialized:
            return False

        try:
            async with self._lock:
                forwarded_at = time.time() if status == ForwardingStatus.FORWARDED else None
                await self._conn.execute("""
                    UPDATE experiences
                    SET status = ?, error_message = ?, forwarded_at = ?
                    WHERE packet_id = ?
                """, (status.value, error, forwarded_at, packet_id))
                await self._conn.commit()
                return True

        except Exception as e:
            logger.error(f"SQLiteExperienceStore update_status failed: {e}")
            return False

    async def remove(self, packet_id: str) -> bool:
        """Remove a packet."""
        if not self._initialized:
            return False

        try:
            async with self._lock:
                await self._conn.execute("""
                    DELETE FROM experiences WHERE packet_id = ?
                """, (packet_id,))
                await self._conn.commit()
                return True

        except Exception as e:
            logger.error(f"SQLiteExperienceStore remove failed: {e}")
            return False

    async def cleanup_expired(self) -> int:
        """
        Remove expired packets.
        
        v113.0: Added timeout protection to prevent indefinite blocking
        on database locks during concurrent operations.
        """
        if not self._initialized:
            return 0

        async def _do_cleanup() -> int:
            async with self._lock:
                now = time.time()
                cursor = await self._conn.execute("""
                    DELETE FROM experiences
                    WHERE created_at + ttl_seconds < ?
                    RETURNING packet_id
                """, (now,))
                rows = await cursor.fetchall()
                await self._conn.commit()
                return len(rows)

        try:
            # v113.0: Use timeout to prevent indefinite blocking on locks
            return await asyncio.wait_for(_do_cleanup(), timeout=5.0)
        except asyncio.TimeoutError:
            logger.warning("SQLiteExperienceStore cleanup_expired timed out (5s) - will retry later")
            return 0
        except Exception as e:
            # v113.0: Enhanced error message with recovery hint
            logger.error(f"SQLiteExperienceStore cleanup_expired failed: {e}")
            logger.debug("Hint: 'database is locked' usually resolves after busy_timeout (30s)")
            return 0

    async def get_pending_count(self) -> int:
        """Get count of pending packets."""
        if not self._initialized:
            return 0

        try:
            async with self._lock:
                cursor = await self._conn.execute("""
                    SELECT COUNT(*) FROM experiences
                    WHERE status IN ('pending', 'queued')
                """)
                row = await cursor.fetchone()
                return row[0] if row else 0

        except Exception as e:
            logger.error(f"SQLiteExperienceStore get_pending_count failed: {e}")
            return 0

    async def close(self) -> None:
        """Close database connection."""
        if self._conn:
            await self._conn.close()
            self._conn = None
            self._initialized = False

    def _row_to_packet(self, row) -> ExperiencePacket:
        """Convert database row to ExperiencePacket."""
        return ExperiencePacket(
            packet_id=row[0],
            experience_type=ExperienceType(row[1]),
            timestamp=row[2],
            source=row[3],
            destination=row[4],
            payload=json.loads(row[5]),
            priority=row[6],
            ttl_seconds=row[7],
            retry_count=row[8],
            max_retries=row[9],
            status=ForwardingStatus(row[10]),
            created_at=row[11],
            forwarded_at=row[12],
            error_message=row[13],
            content_hash=row[14],
            metadata=json.loads(row[15]) if row[15] else {},
        )


class FileExperienceStore:
    """
    File-based experience store for ultimate fallback.
    Uses JSONL format with atomic writes for reliability.
    """

    def __init__(
        self,
        base_path: Optional[Path] = None,
        max_file_size_mb: int = 10,
        retention_days: int = 7,
    ):
        self.base_path = base_path or Path.home() / ".jarvis" / "experience_mesh" / "fallback"
        self.max_file_size_bytes = max_file_size_mb * 1024 * 1024
        self.retention_days = retention_days
        self._current_file: Optional[Path] = None
        self._current_size = 0
        self._lock = asyncio.Lock()

    async def initialize(self) -> bool:
        """Initialize file store."""
        try:
            self.base_path.mkdir(parents=True, exist_ok=True)
            self._rotate_file()
            return True
        except Exception as e:
            logger.error(f"FileExperienceStore initialization failed: {e}")
            return False

    def _rotate_file(self) -> None:
        """Rotate to a new file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self._current_file = self.base_path / f"experiences_{timestamp}.jsonl"
        self._current_size = 0

    async def store(self, packet: ExperiencePacket) -> bool:
        """Store an experience packet."""
        try:
            async with self._lock:
                # Check if rotation needed
                if self._current_size >= self.max_file_size_bytes:
                    self._rotate_file()

                # Serialize packet
                data = {
                    "packet_id": packet.packet_id,
                    "experience_type": packet.experience_type.value,
                    "timestamp": packet.timestamp,
                    "source": packet.source,
                    "destination": packet.destination,
                    "payload": packet.payload,
                    "priority": packet.priority,
                    "status": packet.status.value,
                    "created_at": packet.created_at,
                    "metadata": packet.metadata,
                }
                line = json.dumps(data) + "\n"

                # Atomic write
                with open(self._current_file, "a") as f:
                    f.write(line)
                    f.flush()
                    os.fsync(f.fileno())

                self._current_size += len(line)
                return True

        except Exception as e:
            logger.error(f"FileExperienceStore store failed: {e}")
            return False

    async def retrieve_all(self) -> List[ExperiencePacket]:
        """Retrieve all packets from all files."""
        packets = []

        try:
            for file_path in sorted(self.base_path.glob("experiences_*.jsonl")):
                with open(file_path, "r") as f:
                    for line in f:
                        try:
                            data = json.loads(line.strip())
                            packet = ExperiencePacket(
                                packet_id=data["packet_id"],
                                experience_type=ExperienceType(data["experience_type"]),
                                timestamp=data["timestamp"],
                                source=data["source"],
                                destination=data["destination"],
                                payload=data["payload"],
                                priority=data.get("priority", 5),
                                status=ForwardingStatus(data.get("status", "pending")),
                                created_at=data.get("created_at", time.time()),
                                metadata=data.get("metadata", {}),
                            )
                            packets.append(packet)
                        except Exception:
                            continue

        except Exception as e:
            logger.error(f"FileExperienceStore retrieve_all failed: {e}")

        return packets

    async def cleanup_old(self) -> int:
        """Remove old files beyond retention period."""
        removed = 0
        cutoff = datetime.now() - timedelta(days=self.retention_days)

        try:
            for file_path in self.base_path.glob("experiences_*.jsonl"):
                try:
                    # Extract timestamp from filename
                    timestamp_str = file_path.stem.replace("experiences_", "")
                    file_date = datetime.strptime(timestamp_str[:8], "%Y%m%d")
                    if file_date < cutoff:
                        file_path.unlink()
                        removed += 1
                except Exception:
                    continue

        except Exception as e:
            logger.error(f"FileExperienceStore cleanup_old failed: {e}")

        return removed

    def get_stats(self) -> Dict[str, Any]:
        """Get store statistics."""
        total_files = len(list(self.base_path.glob("experiences_*.jsonl")))
        total_size = sum(
            f.stat().st_size
            for f in self.base_path.glob("experiences_*.jsonl")
        )
        return {
            "base_path": str(self.base_path),
            "total_files": total_files,
            "total_size_mb": total_size / (1024 * 1024),
            "current_file": str(self._current_file) if self._current_file else None,
            "current_size_bytes": self._current_size,
        }


class AdaptiveBackendSelector:
    """
    Intelligently selects the best backend based on:
    - Current health status
    - Priority ordering
    - Recent performance metrics
    - Queue depths
    """

    def __init__(
        self,
        backends: List[BackendConfig],
        health_weight: float = 0.4,
        latency_weight: float = 0.3,
        queue_weight: float = 0.3,
    ):
        self.backends = sorted(backends, key=lambda b: b.priority)
        self.health_weight = health_weight
        self.latency_weight = latency_weight
        self.queue_weight = queue_weight

        self._states: Dict[BackendType, BackendState] = {}
        self._lock = asyncio.Lock()

        # Initialize states
        for config in backends:
            self._states[config.backend_type] = BackendState(
                backend_type=config.backend_type,
                health=BackendHealth.UNAVAILABLE,
                last_health_check=0,
                consecutive_failures=0,
                consecutive_successes=0,
                total_operations=0,
                total_failures=0,
                avg_latency_ms=0,
                queue_size=0,
            )

    async def get_best_backend(self) -> Optional[BackendType]:
        """Get the best available backend."""
        async with self._lock:
            # First, try to find healthy backend in priority order
            for config in self.backends:
                if not config.enabled:
                    continue
                state = self._states.get(config.backend_type)
                if state and state.health == BackendHealth.HEALTHY:
                    return config.backend_type

            # Fall back to degraded backends
            for config in self.backends:
                if not config.enabled:
                    continue
                state = self._states.get(config.backend_type)
                if state and state.health == BackendHealth.DEGRADED:
                    return config.backend_type

            # Last resort: any recovering backend
            for config in self.backends:
                if not config.enabled:
                    continue
                state = self._states.get(config.backend_type)
                if state and state.health == BackendHealth.RECOVERING:
                    return config.backend_type

            return None

    async def get_fallback_chain(self) -> List[BackendType]:
        """Get ordered list of fallback backends."""
        async with self._lock:
            chain = []
            for config in self.backends:
                if not config.enabled:
                    continue
                state = self._states.get(config.backend_type)
                if state and state.health not in (
                    BackendHealth.UNAVAILABLE,
                    BackendHealth.UNHEALTHY,
                ):
                    chain.append(config.backend_type)
            return chain

    async def record_success(
        self,
        backend_type: BackendType,
        latency_ms: float,
    ) -> None:
        """Record successful operation."""
        async with self._lock:
            state = self._states.get(backend_type)
            if state:
                state.consecutive_successes += 1
                state.consecutive_failures = 0
                state.total_operations += 1

                # Update average latency with exponential moving average
                alpha = 0.3
                state.avg_latency_ms = (
                    alpha * latency_ms + (1 - alpha) * state.avg_latency_ms
                )

                # Promote to healthy if recovering
                if state.health == BackendHealth.RECOVERING:
                    if state.consecutive_successes >= 3:
                        state.health = BackendHealth.HEALTHY

                elif state.health == BackendHealth.DEGRADED:
                    if state.consecutive_successes >= 5:
                        state.health = BackendHealth.HEALTHY

    async def record_failure(
        self,
        backend_type: BackendType,
        error: str,
    ) -> None:
        """Record failed operation."""
        async with self._lock:
            state = self._states.get(backend_type)
            if state:
                state.consecutive_failures += 1
                state.consecutive_successes = 0
                state.total_operations += 1
                state.total_failures += 1
                state.last_error = error

                # Degrade health based on failure count
                if state.consecutive_failures >= 5:
                    state.health = BackendHealth.UNHEALTHY
                elif state.consecutive_failures >= 3:
                    state.health = BackendHealth.DEGRADED

    async def update_health(
        self,
        backend_type: BackendType,
        health: BackendHealth,
        queue_size: int = 0,
    ) -> None:
        """Update backend health status."""
        async with self._lock:
            state = self._states.get(backend_type)
            if state:
                old_health = state.health
                state.health = health
                state.last_health_check = time.time()
                state.queue_size = queue_size

                if health == BackendHealth.HEALTHY and old_health != BackendHealth.HEALTHY:
                    state.connected_at = time.time()
                    logger.info(f"Backend {backend_type.value} is now HEALTHY")
                elif health == BackendHealth.UNHEALTHY and old_health == BackendHealth.HEALTHY:
                    logger.warning(f"Backend {backend_type.value} is now UNHEALTHY")

    def get_states(self) -> Dict[str, Dict[str, Any]]:
        """Get all backend states."""
        return {
            bt.value: {
                "health": state.health.value,
                "consecutive_failures": state.consecutive_failures,
                "consecutive_successes": state.consecutive_successes,
                "total_operations": state.total_operations,
                "total_failures": state.total_failures,
                "avg_latency_ms": round(state.avg_latency_ms, 2),
                "queue_size": state.queue_size,
                "last_error": state.last_error,
                "connected_at": state.connected_at,
            }
            for bt, state in self._states.items()
        }


class EventBusHealthMonitor:
    """
    Continuously monitors Trinity Event Bus health.
    Triggers fallback when bus becomes unavailable.
    """

    def __init__(
        self,
        check_interval: float = 10.0,
        failure_threshold: int = 3,
        recovery_threshold: int = 2,
    ):
        self.check_interval = check_interval
        self.failure_threshold = failure_threshold
        self.recovery_threshold = recovery_threshold

        self._healthy = False
        self._consecutive_failures = 0
        self._consecutive_successes = 0
        self._last_check: Optional[float] = None
        self._running = False
        self._task: Optional[asyncio.Task] = None
        self._callbacks: List[Callable] = []
        self._lock = asyncio.Lock()

    @property
    def is_healthy(self) -> bool:
        return self._healthy

    def on_health_change(self, callback: Callable[[bool], None]) -> None:
        """Register callback for health changes."""
        self._callbacks.append(callback)

    async def start(self) -> None:
        """Start health monitoring."""
        if self._running:
            return
        self._running = True
        self._task = asyncio.create_task(self._monitor_loop())
        logger.info("EventBusHealthMonitor started")

    async def stop(self) -> None:
        """Stop health monitoring."""
        self._running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("EventBusHealthMonitor stopped")

    async def _monitor_loop(self) -> None:
        """Main monitoring loop."""
        while self._running:
            try:
                healthy = await self._check_health()
                await self._update_health(healthy)
                await asyncio.sleep(self.check_interval)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"EventBusHealthMonitor error: {e}")
                await asyncio.sleep(self.check_interval)

    async def _check_health(self) -> bool:
        """Check if event bus is healthy."""
        try:
            # Try to import and check Trinity Event Bus
            try:
                from backend.core.trinity.event_bus import get_trinity_event_bus
                bus = await get_trinity_event_bus()
                if bus and hasattr(bus, 'is_connected'):
                    return bus.is_connected()
                return bus is not None
            except ImportError:
                # Event bus module not available
                return False
            except Exception:
                return False
        except Exception:
            return False

    async def _update_health(self, healthy: bool) -> None:
        """Update health status and notify if changed."""
        async with self._lock:
            self._last_check = time.time()

            if healthy:
                self._consecutive_successes += 1
                self._consecutive_failures = 0

                if not self._healthy:
                    if self._consecutive_successes >= self.recovery_threshold:
                        self._healthy = True
                        await self._notify_health_change(True)
            else:
                self._consecutive_failures += 1
                self._consecutive_successes = 0

                if self._healthy:
                    if self._consecutive_failures >= self.failure_threshold:
                        self._healthy = False
                        await self._notify_health_change(False)

    async def _notify_health_change(self, healthy: bool) -> None:
        """Notify callbacks of health change."""
        status = "HEALTHY" if healthy else "UNHEALTHY"
        logger.info(f"Trinity Event Bus is now {status}")

        for callback in self._callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(healthy)
                else:
                    callback(healthy)
            except Exception:
                pass

    def get_status(self) -> Dict[str, Any]:
        """Get monitor status."""
        return {
            "healthy": self._healthy,
            "consecutive_failures": self._consecutive_failures,
            "consecutive_successes": self._consecutive_successes,
            "last_check": self._last_check,
            "running": self._running,
        }


class DegradedModeManager:
    """
    Manages graceful degradation when backends fail.
    Ensures experiences are never lost.
    """

    def __init__(
        self,
        min_backends_required: int = 1,
        degraded_mode_timeout: float = 300.0,  # 5 minutes
    ):
        self.min_backends_required = min_backends_required
        self.degraded_mode_timeout = degraded_mode_timeout

        self._in_degraded_mode = False
        self._degraded_since: Optional[float] = None
        self._available_backends: Set[BackendType] = set()
        self._callbacks: List[Callable] = []
        self._lock = asyncio.Lock()

    @property
    def is_degraded(self) -> bool:
        return self._in_degraded_mode

    def on_mode_change(self, callback: Callable[[bool], None]) -> None:
        """Register callback for mode changes."""
        self._callbacks.append(callback)

    async def update_backend_availability(
        self,
        backend_type: BackendType,
        available: bool,
    ) -> None:
        """Update backend availability."""
        async with self._lock:
            if available:
                self._available_backends.add(backend_type)
            else:
                self._available_backends.discard(backend_type)

            # Check if should enter/exit degraded mode
            was_degraded = self._in_degraded_mode

            if len(self._available_backends) < self.min_backends_required:
                if not self._in_degraded_mode:
                    self._in_degraded_mode = True
                    self._degraded_since = time.time()
                    logger.warning("Entering DEGRADED MODE - insufficient backends")
                    await self._notify_mode_change(True)
            else:
                if self._in_degraded_mode:
                    self._in_degraded_mode = False
                    self._degraded_since = None
                    logger.info("Exiting DEGRADED MODE - backends recovered")
                    await self._notify_mode_change(False)

    async def _notify_mode_change(self, degraded: bool) -> None:
        """Notify callbacks of mode change."""
        for callback in self._callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(degraded)
                else:
                    callback(degraded)
            except Exception:
                pass

    def get_degradation_duration(self) -> Optional[float]:
        """Get how long system has been degraded."""
        if self._in_degraded_mode and self._degraded_since:
            return time.time() - self._degraded_since
        return None

    def get_status(self) -> Dict[str, Any]:
        """Get degradation status."""
        return {
            "degraded": self._in_degraded_mode,
            "degraded_since": self._degraded_since,
            "duration_seconds": self.get_degradation_duration(),
            "available_backends": [bt.value for bt in self._available_backends],
            "required_backends": self.min_backends_required,
        }


class ResilientExperienceMesh:
    """
    Master orchestrator for resilient experience forwarding.

    Features:
    - Multi-backend support (Redis, SQLite, Memory, File)
    - Automatic failover and recovery
    - Continuous health monitoring
    - Graceful degradation
    - Experience deduplication
    - Priority-based forwarding
    """

    def __init__(
        self,
        backends: Optional[List[BackendConfig]] = None,
    ):
        # Default backend configuration
        if backends is None:
            backends = [
                BackendConfig(
                    backend_type=BackendType.REDIS,
                    priority=0,
                    connection_string=os.getenv("REDIS_URL", "redis://localhost:6379"),
                ),
                BackendConfig(
                    backend_type=BackendType.SQLITE,
                    priority=1,
                ),
                BackendConfig(
                    backend_type=BackendType.MEMORY,
                    priority=2,
                ),
                BackendConfig(
                    backend_type=BackendType.FILE,
                    priority=3,
                ),
            ]

        self.backends = backends

        # Core components
        self.backend_selector = AdaptiveBackendSelector(backends)
        self.event_bus_monitor = EventBusHealthMonitor()
        self.degraded_manager = DegradedModeManager()

        # Storage backends
        self._memory_store = InMemoryExperienceStore()
        self._sqlite_store = SQLiteExperienceStore()
        self._file_store = FileExperienceStore()
        self._redis_client: Optional[Any] = None

        # State
        self._running = False
        self._forwarding_task: Optional[asyncio.Task] = None
        self._cleanup_task: Optional[asyncio.Task] = None
        self._health_task: Optional[asyncio.Task] = None
        self._session: Optional[aiohttp.ClientSession] = None
        self._lock = asyncio.Lock()

        # Metrics
        self._metrics = {
            "experiences_received": 0,
            "experiences_forwarded": 0,
            "experiences_failed": 0,
            "experiences_expired": 0,
            "backend_switches": 0,
        }

    async def start(self) -> None:
        """Start the experience mesh."""
        if self._running:
            return

        self._running = True
        logger.info("🔗 Starting Resilient Experience Mesh v12.0...")

        # Initialize backends in order
        await self._initialize_backends()

        # Start monitoring
        await self.event_bus_monitor.start()

        # Register health change callbacks
        self.event_bus_monitor.on_health_change(self._on_event_bus_health_change)

        # Start background tasks
        self._forwarding_task = asyncio.create_task(self._forwarding_loop())
        self._cleanup_task = asyncio.create_task(self._cleanup_loop())
        self._health_task = asyncio.create_task(self._health_check_loop())

        logger.info("✅ Resilient Experience Mesh started")

    async def stop(self) -> None:
        """Stop the experience mesh."""
        self._running = False

        # Cancel tasks
        for task in [self._forwarding_task, self._cleanup_task, self._health_task]:
            if task:
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    pass

        # Stop monitoring
        await self.event_bus_monitor.stop()

        # Close backends
        await self._sqlite_store.close()

        if self._session:
            await self._session.close()

        logger.info("Resilient Experience Mesh stopped")

    async def _initialize_backends(self) -> None:
        """Initialize all storage backends."""
        # Always initialize memory store (never fails)
        await self.backend_selector.update_health(
            BackendType.MEMORY,
            BackendHealth.HEALTHY,
        )
        await self.degraded_manager.update_backend_availability(
            BackendType.MEMORY,
            True,
        )
        logger.info("  ✅ Memory backend initialized")

        # Initialize SQLite
        if await self._sqlite_store.connect():
            await self.backend_selector.update_health(
                BackendType.SQLITE,
                BackendHealth.HEALTHY,
            )
            await self.degraded_manager.update_backend_availability(
                BackendType.SQLITE,
                True,
            )
            logger.info("  ✅ SQLite backend initialized")
        else:
            await self.backend_selector.update_health(
                BackendType.SQLITE,
                BackendHealth.UNAVAILABLE,
            )
            logger.warning("  ⚠️ SQLite backend unavailable")

        # Initialize file store
        if await self._file_store.initialize():
            await self.backend_selector.update_health(
                BackendType.FILE,
                BackendHealth.HEALTHY,
            )
            await self.degraded_manager.update_backend_availability(
                BackendType.FILE,
                True,
            )
            logger.info("  ✅ File backend initialized")
        else:
            logger.warning("  ⚠️ File backend unavailable")

        # Try Redis (optional)
        try:
            redis_available = await self._try_connect_redis()
            if redis_available:
                await self.backend_selector.update_health(
                    BackendType.REDIS,
                    BackendHealth.HEALTHY,
                )
                await self.degraded_manager.update_backend_availability(
                    BackendType.REDIS,
                    True,
                )
                logger.info("  ✅ Redis backend initialized")
            else:
                await self.backend_selector.update_health(
                    BackendType.REDIS,
                    BackendHealth.UNAVAILABLE,
                )
                logger.info("  ℹ️ Redis unavailable - using fallback backends")
        except Exception as e:
            logger.info(f"  ℹ️ Redis not available: {e}")

    async def _try_connect_redis(self) -> bool:
        """Try to connect to Redis."""
        try:
            import redis.asyncio as aioredis
            redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
            self._redis_client = aioredis.from_url(
                redis_url,
                socket_timeout=5.0,
                socket_connect_timeout=5.0,
            )
            await self._redis_client.ping()
            return True
        except ImportError:
            return False
        except Exception:
            return False

    async def ingest(self, packet: ExperiencePacket) -> bool:
        """
        Ingest an experience packet.
        Routes to best available backend.
        """
        self._metrics["experiences_received"] += 1

        # Generate content hash for deduplication
        if not packet.content_hash:
            content = json.dumps(packet.payload, sort_keys=True)
            packet.content_hash = hashlib.md5(content.encode()).hexdigest()

        # Get best backend
        backend = await self.backend_selector.get_best_backend()

        if not backend:
            # Emergency: use memory store
            backend = BackendType.MEMORY
            logger.warning("All backends unhealthy, using memory fallback")

        # Store in selected backend
        start = time.time()
        success = await self._store_in_backend(backend, packet)
        latency = (time.time() - start) * 1000

        if success:
            await self.backend_selector.record_success(backend, latency)
        else:
            await self.backend_selector.record_failure(backend, "Store failed")
            # Try fallback
            for fallback in await self.backend_selector.get_fallback_chain():
                if fallback != backend:
                    if await self._store_in_backend(fallback, packet):
                        self._metrics["backend_switches"] += 1
                        break

        return success

    async def _store_in_backend(
        self,
        backend: BackendType,
        packet: ExperiencePacket,
    ) -> bool:
        """Store packet in specific backend."""
        try:
            if backend == BackendType.MEMORY:
                return await self._memory_store.store(packet)
            elif backend == BackendType.SQLITE:
                return await self._sqlite_store.store(packet)
            elif backend == BackendType.FILE:
                return await self._file_store.store(packet)
            elif backend == BackendType.REDIS:
                if self._redis_client:
                    data = json.dumps({
                        "packet_id": packet.packet_id,
                        "experience_type": packet.experience_type.value,
                        "timestamp": packet.timestamp,
                        "source": packet.source,
                        "destination": packet.destination,
                        "payload": packet.payload,
                        "priority": packet.priority,
                        "status": packet.status.value,
                        "created_at": packet.created_at,
                    })
                    await self._redis_client.hset(
                        "experience_mesh:packets",
                        packet.packet_id,
                        data,
                    )
                    return True
            return False
        except Exception as e:
            logger.error(f"Store in {backend.value} failed: {e}")
            return False

    async def _forwarding_loop(self) -> None:
        """Background loop for forwarding experiences."""
        while self._running:
            try:
                # Check if event bus is healthy
                if self.event_bus_monitor.is_healthy:
                    await self._forward_pending()

                await asyncio.sleep(5.0)  # Forward every 5 seconds
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Forwarding loop error: {e}")
                await asyncio.sleep(5.0)

    async def _forward_pending(self) -> None:
        """Forward pending experiences to event bus."""
        # Get pending from SQLite first (most reliable)
        packets = await self._sqlite_store.retrieve_batch(
            limit=100,
            status=ForwardingStatus.PENDING,
        )

        # Also check memory store
        memory_packets = await self._memory_store.retrieve_batch(
            limit=50,
            status=ForwardingStatus.PENDING,
        )
        packets.extend(memory_packets)

        for packet in packets:
            try:
                # Forward to event bus
                success = await self._forward_to_event_bus(packet)

                if success:
                    packet.status = ForwardingStatus.FORWARDED
                    self._metrics["experiences_forwarded"] += 1
                else:
                    packet.retry_count += 1
                    if not packet.can_retry():
                        packet.status = ForwardingStatus.FAILED
                        self._metrics["experiences_failed"] += 1
                    else:
                        packet.status = ForwardingStatus.RETRYING

                # Update status in stores
                await self._sqlite_store.update_status(
                    packet.packet_id,
                    packet.status,
                    packet.error_message,
                )
                await self._memory_store.update_status(
                    packet.packet_id,
                    packet.status,
                    packet.error_message,
                )

            except Exception as e:
                logger.error(f"Forward packet {packet.packet_id} failed: {e}")

    async def _forward_to_event_bus(self, packet: ExperiencePacket) -> bool:
        """Forward a single packet to Trinity Event Bus."""
        try:
            from backend.core.trinity.event_bus import get_trinity_event_bus
            bus = await get_trinity_event_bus()

            if bus:
                await bus.publish(
                    topic=f"experience.{packet.experience_type.value}",
                    data={
                        "packet_id": packet.packet_id,
                        "source": packet.source,
                        "destination": packet.destination,
                        "payload": packet.payload,
                        "timestamp": packet.timestamp,
                    },
                )
                return True
            return False

        except ImportError:
            return False
        except Exception as e:
            packet.error_message = str(e)[:200]
            return False

    async def _cleanup_loop(self) -> None:
        """Background loop for cleaning up expired packets."""
        while self._running:
            try:
                # Clean SQLite
                expired = await self._sqlite_store.cleanup_expired()
                if expired > 0:
                    self._metrics["experiences_expired"] += expired
                    logger.info(f"Cleaned up {expired} expired experiences")

                # Clean old files
                await self._file_store.cleanup_old()

                await asyncio.sleep(60.0)  # Cleanup every minute
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Cleanup loop error: {e}")
                await asyncio.sleep(60.0)

    async def _health_check_loop(self) -> None:
        """Background loop for health checking backends."""
        while self._running:
            try:
                # Check Redis
                if self._redis_client:
                    try:
                        await self._redis_client.ping()
                        await self.backend_selector.update_health(
                            BackendType.REDIS,
                            BackendHealth.HEALTHY,
                        )
                    except Exception:
                        await self.backend_selector.update_health(
                            BackendType.REDIS,
                            BackendHealth.UNHEALTHY,
                        )

                # Update queue sizes
                sqlite_pending = await self._sqlite_store.get_pending_count()
                memory_pending = await self._memory_store.get_pending_count()

                await self.backend_selector.update_health(
                    BackendType.SQLITE,
                    BackendHealth.HEALTHY,
                    sqlite_pending,
                )
                await self.backend_selector.update_health(
                    BackendType.MEMORY,
                    BackendHealth.HEALTHY,
                    memory_pending,
                )

                await asyncio.sleep(30.0)  # Check every 30 seconds
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Health check loop error: {e}")
                await asyncio.sleep(30.0)

    async def _on_event_bus_health_change(self, healthy: bool) -> None:
        """Handle event bus health change."""
        if healthy:
            # Event bus recovered - flush pending experiences
            logger.info("Event bus recovered - flushing pending experiences")
            await self._forward_pending()

    def get_status(self) -> Dict[str, Any]:
        """Get comprehensive mesh status."""
        return {
            "running": self._running,
            "metrics": self._metrics.copy(),
            "backends": self.backend_selector.get_states(),
            "event_bus": self.event_bus_monitor.get_status(),
            "degraded_mode": self.degraded_manager.get_status(),
            "memory_store": self._memory_store.get_stats(),
            "file_store": self._file_store.get_stats(),
        }


# =============================================================================
# v12.0: SINGLETON INSTANCES AND FACTORY FUNCTIONS
# =============================================================================

_experience_mesh: Optional[ResilientExperienceMesh] = None
_memory_store: Optional[InMemoryExperienceStore] = None
_sqlite_store: Optional[SQLiteExperienceStore] = None
_file_store: Optional[FileExperienceStore] = None
_backend_selector: Optional[AdaptiveBackendSelector] = None
_event_bus_monitor: Optional[EventBusHealthMonitor] = None
_degraded_manager: Optional[DegradedModeManager] = None


def get_experience_mesh() -> ResilientExperienceMesh:
    """Get or create the experience mesh singleton."""
    global _experience_mesh
    if _experience_mesh is None:
        _experience_mesh = ResilientExperienceMesh()
    return _experience_mesh


def get_memory_store() -> InMemoryExperienceStore:
    """Get or create the memory store singleton."""
    global _memory_store
    if _memory_store is None:
        _memory_store = InMemoryExperienceStore()
    return _memory_store


def get_sqlite_store() -> SQLiteExperienceStore:
    """Get or create the SQLite store singleton."""
    global _sqlite_store
    if _sqlite_store is None:
        _sqlite_store = SQLiteExperienceStore()
    return _sqlite_store


def get_file_store() -> FileExperienceStore:
    """Get or create the file store singleton."""
    global _file_store
    if _file_store is None:
        _file_store = FileExperienceStore()
    return _file_store


def get_event_bus_monitor() -> EventBusHealthMonitor:
    """Get or create the event bus monitor singleton."""
    global _event_bus_monitor
    if _event_bus_monitor is None:
        _event_bus_monitor = EventBusHealthMonitor()
    return _event_bus_monitor


def get_degraded_manager() -> DegradedModeManager:
    """Get or create the degraded mode manager singleton."""
    global _degraded_manager
    if _degraded_manager is None:
        _degraded_manager = DegradedModeManager()
    return _degraded_manager


# =============================================================================
# v12.0: INITIALIZATION AND SHUTDOWN
# =============================================================================

async def initialize_experience_mesh(
    backends: Optional[List[Dict[str, Any]]] = None,
) -> Dict[str, Any]:
    """
    Initialize the resilient experience mesh.

    Args:
        backends: Optional list of backend configurations

    Returns:
        Dictionary with initialized components
    """
    logger.info("🔗 Initializing Resilient Experience Mesh v12.0...")
    start_time = time.monotonic()

    components = {}

    try:
        # Parse backend configs if provided
        backend_configs = None
        if backends:
            backend_configs = [
                BackendConfig(
                    backend_type=BackendType(b["type"]),
                    enabled=b.get("enabled", True),
                    priority=b.get("priority", 0),
                    connection_string=b.get("connection_string", ""),
                )
                for b in backends
            ]

        mesh = get_experience_mesh()
        if backend_configs:
            mesh.backends = backend_configs
            mesh.backend_selector = AdaptiveBackendSelector(backend_configs)

        await mesh.start()

        components["experience_mesh"] = mesh
        components["memory_store"] = mesh._memory_store
        components["sqlite_store"] = mesh._sqlite_store
        components["file_store"] = mesh._file_store
        components["backend_selector"] = mesh.backend_selector
        components["event_bus_monitor"] = mesh.event_bus_monitor
        components["degraded_manager"] = mesh.degraded_manager

        elapsed = time.monotonic() - start_time
        logger.info(f"✅ Resilient Experience Mesh initialized in {elapsed:.2f}s")
        logger.info(f"   Components: {len(components)}")
        logger.info(f"   Backends: {[b.backend_type.value for b in mesh.backends]}")

        return components

    except Exception as e:
        logger.error(f"❌ Resilient Experience Mesh initialization failed: {e}")
        raise


async def shutdown_experience_mesh() -> None:
    """Shutdown the resilient experience mesh."""
    logger.info("Shutting down Resilient Experience Mesh...")

    global _experience_mesh, _memory_store, _sqlite_store, _file_store
    global _backend_selector, _event_bus_monitor, _degraded_manager

    if _experience_mesh:
        await _experience_mesh.stop()
        _experience_mesh = None

    if _sqlite_store:
        await _sqlite_store.close()
        _sqlite_store = None

    _memory_store = None
    _file_store = None
    _backend_selector = None
    _event_bus_monitor = None
    _degraded_manager = None

    logger.info("✅ Resilient Experience Mesh shutdown complete")


# =============================================================================
# v12.0: CONVENIENCE FUNCTIONS
# =============================================================================

async def ingest_experience(
    experience_type: str,
    payload: Dict[str, Any],
    source: str = "jarvis",
    destination: str = "reactor-core",
    priority: int = 5,
    ttl_seconds: float = 3600.0,
) -> str:
    """
    Ingest an experience into the mesh.

    Usage:
        packet_id = await ingest_experience(
            experience_type="interaction",
            payload={"user_input": "hello", "response": "hi"},
            source="jarvis",
            destination="reactor-core",
        )
    """
    mesh = get_experience_mesh()

    packet = ExperiencePacket(
        packet_id=str(uuid.uuid4()),
        experience_type=ExperienceType(experience_type),
        timestamp=time.time(),
        source=source,
        destination=destination,
        payload=payload,
        priority=priority,
        ttl_seconds=ttl_seconds,
    )

    await mesh.ingest(packet)
    return packet.packet_id


async def get_experience_mesh_status() -> Dict[str, Any]:
    """
    Get comprehensive mesh status.

    Usage:
        status = await get_experience_mesh_status()
        print(f"Running: {status['running']}")
    """
    mesh = get_experience_mesh()
    return mesh.get_status()


async def get_pending_experiences_count() -> int:
    """
    Get count of pending experiences.

    Usage:
        count = await get_pending_experiences_count()
    """
    mesh = get_experience_mesh()
    return await mesh._sqlite_store.get_pending_count()


# =============================================================================
# v13.0: BULLETPROOF ORCHESTRATION LAYER
# =============================================================================
#
# Fixes 20 identified critical/high/medium issues:
#
# CRITICAL:
# 1. Flush task not monitored (silent crash)
# 2. Non-atomic file truncation (event loss)
# 3. Event bus started too late
#
# HIGH:
# 1. Unordered task cancellation (shutdown hangs)
# 2. Queue lock nesting (deadlock possible)
# 3. Silent exception swallowing
# 4. Inconsistent lock ordering
# 5. Mixed thread/async locks
# 6. Failed experiences dropped silently
# 7. No health check for Reactor Core
# 8. Forwarder startup ordering
#
# MEDIUM:
# Various configuration and minor issues
# =============================================================================


class ShutdownPhase(Enum):
    """Shutdown phases for ordered shutdown choreography."""
    NOT_STARTED = "not_started"
    PREPARING = "preparing"
    DRAINING = "draining"
    CANCELLING = "cancelling"
    WAITING = "waiting"
    CLEANUP = "cleanup"
    COMPLETED = "completed"
    FAILED = "failed"


class TaskHealth(Enum):
    """Health status for supervised tasks."""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    CRASHED = "crashed"
    CANCELLED = "cancelled"
    UNKNOWN = "unknown"


class LockPriority(Enum):
    """Lock acquisition priority (lower = acquired first)."""
    CRITICAL = 1       # System-critical locks
    HIGH = 2           # Resource management locks
    NORMAL = 3         # Standard operation locks
    LOW = 4            # Metrics and monitoring locks
    BACKGROUND = 5     # Background task locks


@dataclass
class SupervisedTask:
    """Metadata for a supervised background task."""
    task: asyncio.Task
    name: str
    created_at: float = field(default_factory=time.time)
    health: TaskHealth = TaskHealth.HEALTHY
    restart_count: int = 0
    max_restarts: int = 3
    restart_delay: float = 1.0
    critical: bool = False
    factory: Optional[Callable[[], Awaitable[None]]] = None
    last_heartbeat: float = field(default_factory=time.time)
    heartbeat_timeout: float = 60.0


@dataclass
class LockAcquisition:
    """Track a lock acquisition for deadlock detection."""
    lock_id: str
    priority: LockPriority
    acquired_at: float
    task_id: int
    stack_trace: str


@dataclass
class AtomicWriteContext:
    """Context for atomic file write operations."""
    target_path: Path
    temp_path: Path
    backup_path: Optional[Path]
    content_hash: str
    started_at: float


# =============================================================================
# VALIDATED TIMEOUT CONFIGURATION
# =============================================================================

class ValidatedTimeouts:
    """
    Centralized timeout configuration with validation.
    Prevents invalid timeout combinations that cause failures.
    """

    # Minimum and maximum bounds for all timeouts
    BOUNDS = {
        "operation": (1.0, 300.0),       # 1s - 5min
        "shutdown": (30.0, 600.0),       # 30s - 10min
        "health_check": (1.0, 30.0),     # 1s - 30s
        "lock_acquire": (0.1, 60.0),     # 100ms - 1min
        "task_cancel": (1.0, 30.0),      # 1s - 30s
        "heartbeat": (5.0, 300.0),       # 5s - 5min
        "startup": (5.0, 120.0),         # 5s - 2min
        "batch_flush": (5.0, 60.0),      # 5s - 1min
        "connection": (1.0, 30.0),       # 1s - 30s
    }

    @classmethod
    def get(cls, key: str, default: float) -> float:
        """Get a validated timeout value."""
        env_key = f"Ironcliw_TIMEOUT_{key.upper()}"
        try:
            value = float(os.environ.get(env_key, default))
        except (ValueError, TypeError):
            logger.warning(f"Invalid timeout value for {env_key}, using default {default}")
            value = default

        # Apply bounds
        min_val, max_val = cls.BOUNDS.get(key, (0.1, 3600.0))
        bounded = max(min_val, min(max_val, value))

        if bounded != value:
            logger.warning(
                f"Timeout {key}={value} out of bounds [{min_val}, {max_val}], "
                f"clamped to {bounded}"
            )

        return bounded

    @classmethod
    def validate_relationships(cls) -> List[str]:
        """Validate timeout relationships are logical."""
        issues = []

        shutdown = cls.get("shutdown", 60.0)
        operation = cls.get("operation", 10.0)
        health_check = cls.get("health_check", 5.0)
        batch_flush = cls.get("batch_flush", 10.0)

        if health_check >= operation:
            issues.append(
                f"health_check ({health_check}s) >= operation ({operation}s): "
                "health checks may always timeout"
            )

        if batch_flush >= shutdown / 2:
            issues.append(
                f"batch_flush ({batch_flush}s) >= shutdown/2 ({shutdown/2}s): "
                "final flush may not complete during shutdown"
            )

        if operation >= shutdown:
            issues.append(
                f"operation ({operation}s) >= shutdown ({shutdown}s): "
                "operations may prevent clean shutdown"
            )

        return issues


# =============================================================================
# UNIFIED ASYNC LOCK GUARD
# =============================================================================

class AsyncLockGuard:
    """
    Unified async lock management with:
    - Deadlock detection via lock ordering
    - Timeout protection on all acquisitions
    - Lock priority enforcement
    - Circular wait prevention
    """

    # Canonical lock ordering (lower number = acquired first)
    LOCK_ORDER = {
        "system": 1,
        "resource": 2,
        "queue": 3,
        "pending": 4,
        "dedup": 5,
        "metrics": 6,
        "cache": 7,
    }

    def __init__(self):
        self._locks: Dict[str, asyncio.Lock] = {}
        self._acquisitions: Dict[int, List[LockAcquisition]] = defaultdict(list)
        self._lock = asyncio.Lock()
        self._stats = {
            "acquisitions": 0,
            "releases": 0,
            "timeouts": 0,
            "order_violations": 0,
        }

    def register_lock(self, name: str, priority: LockPriority = LockPriority.NORMAL) -> asyncio.Lock:
        """Register a new lock with ordering."""
        if name not in self._locks:
            self._locks[name] = asyncio.Lock()
        return self._locks[name]

    def get_lock(self, name: str) -> asyncio.Lock:
        """Get a registered lock."""
        if name not in self._locks:
            self._locks[name] = asyncio.Lock()
        return self._locks[name]

    @asynccontextmanager
    async def acquire(
        self,
        name: str,
        timeout: Optional[float] = None,
        priority: LockPriority = LockPriority.NORMAL,
    ):
        """
        Acquire a lock with timeout and ordering enforcement.

        Usage:
            async with lock_guard.acquire("queue", timeout=5.0):
                # Critical section
        """
        if timeout is None:
            timeout = ValidatedTimeouts.get("lock_acquire", 10.0)

        task_id = id(asyncio.current_task())
        lock = self.get_lock(name)
        lock_order = self.LOCK_ORDER.get(name, 100)

        # Check for lock ordering violation
        current_acquisitions = self._acquisitions.get(task_id, [])
        for acq in current_acquisitions:
            acq_order = self.LOCK_ORDER.get(acq.lock_id, 100)
            if lock_order < acq_order:
                self._stats["order_violations"] += 1
                logger.warning(
                    f"Lock ordering violation: acquiring '{name}' (order={lock_order}) "
                    f"while holding '{acq.lock_id}' (order={acq_order}). "
                    f"Risk of deadlock!"
                )

        # Acquire with timeout
        acquired = False
        try:
            acquired = await asyncio.wait_for(lock.acquire(), timeout=timeout)
            self._stats["acquisitions"] += 1

            # Track acquisition
            acquisition = LockAcquisition(
                lock_id=name,
                priority=priority,
                acquired_at=time.time(),
                task_id=task_id,
                stack_trace="",  # Could add traceback.format_stack() for debugging
            )
            self._acquisitions[task_id].append(acquisition)

            yield

        except asyncio.TimeoutError:
            self._stats["timeouts"] += 1
            logger.error(f"Lock acquisition timeout: {name} after {timeout}s")
            raise

        finally:
            if acquired:
                lock.release()
                self._stats["releases"] += 1

                # Remove from acquisitions
                if task_id in self._acquisitions:
                    self._acquisitions[task_id] = [
                        a for a in self._acquisitions[task_id] if a.lock_id != name
                    ]
                    if not self._acquisitions[task_id]:
                        del self._acquisitions[task_id]

    def get_stats(self) -> Dict[str, Any]:
        """Get lock statistics."""
        return {
            **self._stats,
            "active_locks": sum(
                1 for lock in self._locks.values() if lock.locked()
            ),
            "tasks_holding_locks": len(self._acquisitions),
        }


# =============================================================================
# TASK SUPERVISOR
# =============================================================================

class TaskSupervisor:
    """
    Supervises background tasks with:
    - Crash detection and automatic restart
    - Health monitoring via heartbeats
    - Graceful shutdown coordination
    - Exception propagation to supervisor
    """

    def __init__(
        self,
        on_task_crash: Optional[Callable[[str, Exception], Awaitable[None]]] = None,
        on_task_restart: Optional[Callable[[str, int], Awaitable[None]]] = None,
    ):
        self._tasks: Dict[str, SupervisedTask] = {}
        self._lock = asyncio.Lock()
        self._running = False
        self._monitor_task: Optional[asyncio.Task] = None
        self._on_task_crash = on_task_crash
        self._on_task_restart = on_task_restart
        self._crash_count = 0
        self._restart_count = 0

    async def start(self) -> None:
        """Start the task supervisor."""
        if self._running:
            return

        self._running = True
        self._monitor_task = asyncio.create_task(
            self._monitor_loop(),
            name="task_supervisor_monitor"
        )
        logger.info("TaskSupervisor started")

    async def stop(self) -> None:
        """Stop all supervised tasks gracefully."""
        if not self._running:
            return

        self._running = False

        # Cancel monitor task
        if self._monitor_task:
            self._monitor_task.cancel()
            try:
                await asyncio.wait_for(self._monitor_task, timeout=5.0)
            except (asyncio.CancelledError, asyncio.TimeoutError):
                pass

        # Stop all supervised tasks
        await self._shutdown_all_tasks()

        logger.info(
            f"TaskSupervisor stopped: {self._crash_count} crashes, "
            f"{self._restart_count} restarts"
        )

    async def supervise(
        self,
        name: str,
        coro: Awaitable[None],
        *,
        critical: bool = False,
        max_restarts: int = 3,
        restart_delay: float = 1.0,
        heartbeat_timeout: float = 60.0,
        factory: Optional[Callable[[], Awaitable[None]]] = None,
    ) -> asyncio.Task:
        """
        Create and supervise a background task.

        Args:
            name: Unique task name
            coro: The coroutine to run
            critical: If True, crash propagates to supervisor
            max_restarts: Maximum automatic restart attempts
            restart_delay: Delay between restarts (with exponential backoff)
            heartbeat_timeout: Task must heartbeat within this interval
            factory: Optional factory to recreate the coroutine for restarts
        """
        task = asyncio.create_task(coro, name=name)

        supervised = SupervisedTask(
            task=task,
            name=name,
            critical=critical,
            max_restarts=max_restarts,
            restart_delay=restart_delay,
            heartbeat_timeout=heartbeat_timeout,
            factory=factory,
        )

        # Add done callback for crash detection
        task.add_done_callback(
            lambda t: asyncio.create_task(self._handle_task_done(name, t))
        )

        async with self._lock:
            self._tasks[name] = supervised

        logger.debug(f"Supervising task: {name} (critical={critical})")
        return task

    async def heartbeat(self, name: str) -> None:
        """Record a heartbeat for a supervised task."""
        async with self._lock:
            if name in self._tasks:
                self._tasks[name].last_heartbeat = time.time()

    async def _handle_task_done(self, name: str, task: asyncio.Task) -> None:
        """Handle task completion or crash."""
        async with self._lock:
            if name not in self._tasks:
                return

            supervised = self._tasks[name]

            if task.cancelled():
                supervised.health = TaskHealth.CANCELLED
                logger.debug(f"Task {name} was cancelled")
                return

            exception = task.exception()
            if exception:
                supervised.health = TaskHealth.CRASHED
                self._crash_count += 1

                logger.error(
                    f"Supervised task {name} crashed: {exception}",
                    exc_info=exception
                )

                # Notify crash callback
                if self._on_task_crash:
                    try:
                        await self._on_task_crash(name, exception)
                    except Exception as e:
                        logger.error(f"Crash callback failed: {e}")

                # Attempt restart if allowed
                if supervised.restart_count < supervised.max_restarts and supervised.factory:
                    await self._restart_task(name, supervised)
                elif supervised.critical:
                    logger.critical(
                        f"Critical task {name} exhausted restarts. "
                        "System may be in degraded state."
                    )
            else:
                logger.debug(f"Task {name} completed normally")

    async def _restart_task(self, name: str, supervised: SupervisedTask) -> None:
        """Restart a crashed task."""
        if not supervised.factory:
            return

        supervised.restart_count += 1
        self._restart_count += 1

        # Exponential backoff
        delay = supervised.restart_delay * (2 ** (supervised.restart_count - 1))
        delay = min(delay, 60.0)  # Cap at 60s

        logger.warning(
            f"Restarting task {name} (attempt {supervised.restart_count}/"
            f"{supervised.max_restarts}) after {delay:.1f}s delay"
        )

        await asyncio.sleep(delay)

        try:
            new_coro = supervised.factory()
            new_task = asyncio.create_task(new_coro, name=name)
            new_task.add_done_callback(
                lambda t: asyncio.create_task(self._handle_task_done(name, t))
            )

            supervised.task = new_task
            supervised.health = TaskHealth.HEALTHY
            supervised.last_heartbeat = time.time()

            if self._on_task_restart:
                await self._on_task_restart(name, supervised.restart_count)

        except Exception as e:
            logger.error(f"Failed to restart task {name}: {e}")
            supervised.health = TaskHealth.CRASHED

    async def _monitor_loop(self) -> None:
        """Monitor task health via heartbeats."""
        while self._running:
            try:
                await asyncio.sleep(10.0)

                now = time.time()
                async with self._lock:
                    for name, supervised in self._tasks.items():
                        if supervised.health != TaskHealth.HEALTHY:
                            continue

                        time_since_heartbeat = now - supervised.last_heartbeat
                        if time_since_heartbeat > supervised.heartbeat_timeout:
                            logger.warning(
                                f"Task {name} missed heartbeat "
                                f"({time_since_heartbeat:.1f}s > "
                                f"{supervised.heartbeat_timeout}s)"
                            )
                            supervised.health = TaskHealth.DEGRADED

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Task monitor error: {e}")

    async def _shutdown_all_tasks(self) -> None:
        """Shutdown all tasks in order."""
        timeout = ValidatedTimeouts.get("task_cancel", 10.0)

        async with self._lock:
            tasks_to_cancel = list(self._tasks.values())

        # Cancel in reverse order of creation
        tasks_to_cancel.sort(key=lambda t: t.created_at, reverse=True)

        for supervised in tasks_to_cancel:
            if supervised.task.done():
                continue

            supervised.task.cancel()
            try:
                await asyncio.wait_for(
                    supervised.task,
                    timeout=timeout
                )
            except (asyncio.CancelledError, asyncio.TimeoutError):
                pass
            except Exception as e:
                logger.warning(f"Task {supervised.name} shutdown error: {e}")

    def get_status(self) -> Dict[str, Any]:
        """Get supervisor status."""
        return {
            "running": self._running,
            "task_count": len(self._tasks),
            "crash_count": self._crash_count,
            "restart_count": self._restart_count,
            "tasks": {
                name: {
                    "health": t.health.value,
                    "restart_count": t.restart_count,
                    "critical": t.critical,
                    "age_seconds": time.time() - t.created_at,
                }
                for name, t in self._tasks.items()
            }
        }


# =============================================================================
# ATOMIC FILE MANAGER
# =============================================================================

class AtomicFileManager:
    """
    Atomic file operations with:
    - Write-then-rename pattern for atomicity
    - Automatic backup before overwrite
    - Crash recovery from incomplete writes
    - Content hash verification
    """

    def __init__(
        self,
        backup_dir: Optional[Path] = None,
        max_backups: int = 5,
    ):
        self.backup_dir = backup_dir or Path.home() / ".jarvis" / "atomic_backups"
        self.max_backups = max_backups
        self._pending_writes: Dict[str, AtomicWriteContext] = {}
        self._lock = asyncio.Lock()

    async def write_atomic(
        self,
        path: Path,
        content: Union[str, bytes],
        *,
        create_backup: bool = True,
        verify: bool = True,
    ) -> bool:
        """
        Write file atomically using temp-then-rename pattern.

        Args:
            path: Target file path
            content: Content to write
            create_backup: If True, backup existing file first
            verify: If True, verify written content matches

        Returns:
            True if write succeeded
        """
        path = Path(path)
        is_bytes = isinstance(content, bytes)
        content_bytes = content if is_bytes else content.encode("utf-8")
        content_hash = hashlib.sha256(content_bytes).hexdigest()[:16]

        # Create temp file path
        temp_path = path.with_suffix(f".{content_hash}.tmp")
        backup_path = None

        async with self._lock:
            try:
                # Ensure parent directory exists
                path.parent.mkdir(parents=True, exist_ok=True)

                # Backup existing file if requested
                if create_backup and path.exists():
                    backup_path = await self._create_backup(path)

                # Track pending write
                context = AtomicWriteContext(
                    target_path=path,
                    temp_path=temp_path,
                    backup_path=backup_path,
                    content_hash=content_hash,
                    started_at=time.time(),
                )
                self._pending_writes[str(path)] = context

                # Write to temp file
                if aiofiles:
                    mode = "wb" if is_bytes else "w"
                    async with aiofiles.open(temp_path, mode) as f:
                        await f.write(content)
                else:
                    # Fallback to sync write in executor
                    def sync_write():
                        mode = "wb" if is_bytes else "w"
                        with open(temp_path, mode) as f:
                            f.write(content)
                    await asyncio.get_running_loop().run_in_executor(None, sync_write)

                # Sync to disk
                await self._sync_file(temp_path)

                # Verify if requested
                if verify:
                    if not await self._verify_content(temp_path, content_hash, is_bytes):
                        raise IOError(f"Content verification failed for {path}")

                # Atomic rename
                temp_path.rename(path)

                # Remove from pending
                del self._pending_writes[str(path)]

                logger.debug(f"Atomic write completed: {path}")
                return True

            except Exception as e:
                logger.error(f"Atomic write failed for {path}: {e}")

                # Cleanup temp file
                if temp_path.exists():
                    try:
                        temp_path.unlink()
                    except Exception:
                        pass

                # Restore backup if write failed
                if backup_path and backup_path.exists():
                    try:
                        backup_path.rename(path)
                        logger.info(f"Restored backup for {path}")
                    except Exception as restore_err:
                        logger.error(f"Failed to restore backup: {restore_err}")

                return False

    async def _create_backup(self, path: Path) -> Path:
        """Create a backup of an existing file."""
        self.backup_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{path.stem}_{timestamp}{path.suffix}"
        backup_path = self.backup_dir / backup_name

        # Copy file
        if aiofiles:
            async with aiofiles.open(path, "rb") as src:
                content = await src.read()
            async with aiofiles.open(backup_path, "wb") as dst:
                await dst.write(content)
        else:
            shutil.copy2(path, backup_path)

        # Cleanup old backups
        await self._cleanup_old_backups(path.stem)

        return backup_path

    async def _cleanup_old_backups(self, stem: str) -> None:
        """Remove old backups keeping only max_backups."""
        try:
            backups = sorted(
                [f for f in self.backup_dir.iterdir() if f.stem.startswith(stem)],
                key=lambda f: f.stat().st_mtime,
                reverse=True
            )

            for old_backup in backups[self.max_backups:]:
                old_backup.unlink()

        except Exception as e:
            logger.debug(f"Backup cleanup error: {e}")

    async def _sync_file(self, path: Path) -> None:
        """Sync file to disk. v109.3: Uses safe_close to prevent EXC_GUARD."""
        try:
            # v109.3: Use async_safe_sync_file if available
            if async_safe_sync_file is not None:
                await async_safe_sync_file(path)
                return

            # Fallback: Use safe_close manually
            def sync():
                fd = safe_open(str(path), os.O_RDONLY)
                try:
                    os.fsync(fd)
                finally:
                    safe_close(fd)

            await asyncio.get_running_loop().run_in_executor(None, sync)
        except Exception as e:
            logger.debug(f"File sync warning: {e}")

    async def _verify_content(
        self,
        path: Path,
        expected_hash: str,
        is_bytes: bool,
    ) -> bool:
        """Verify file content matches expected hash."""
        try:
            if aiofiles:
                async with aiofiles.open(path, "rb") as f:
                    content = await f.read()
            else:
                with open(path, "rb") as f:
                    content = f.read()

            actual_hash = hashlib.sha256(content).hexdigest()[:16]
            return actual_hash == expected_hash

        except Exception as e:
            logger.error(f"Content verification error: {e}")
            return False

    async def recover_pending(self) -> int:
        """
        Recover from incomplete writes on startup.
        Returns count of recovered files.
        """
        recovered = 0

        try:
            # Find temp files
            for temp_file in Path("/tmp").glob("*.tmp"):
                if temp_file.stem.count(".") >= 1:
                    # Extract target path from temp name
                    parts = temp_file.stem.rsplit(".", 1)
                    if len(parts) == 2:
                        target_name = parts[0] + temp_file.suffix.replace(".tmp", "")
                        target_path = temp_file.parent / target_name

                        # If temp is newer than target (or target missing), recover
                        if not target_path.exists() or \
                           temp_file.stat().st_mtime > target_path.stat().st_mtime:
                            temp_file.rename(target_path)
                            recovered += 1
                            logger.info(f"Recovered incomplete write: {target_path}")

        except Exception as e:
            logger.warning(f"Recovery scan error: {e}")

        return recovered


# =============================================================================
# GRACEFUL SHUTDOWN ORCHESTRATOR
# =============================================================================

class GracefulShutdownOrchestrator:
    """
    Choreographed shutdown with:
    - Ordered phase execution
    - Timeout enforcement per phase
    - Error collection without early exit
    - Final state reporting
    """

    def __init__(self):
        self._phase = ShutdownPhase.NOT_STARTED
        self._errors: List[Tuple[str, Exception]] = []
        self._start_time: Optional[float] = None
        self._callbacks: Dict[ShutdownPhase, List[Callable[[], Awaitable[None]]]] = {
            phase: [] for phase in ShutdownPhase
        }

    def register_callback(
        self,
        phase: ShutdownPhase,
        callback: Callable[[], Awaitable[None]],
    ) -> None:
        """Register a callback for a shutdown phase."""
        self._callbacks[phase].append(callback)

    async def execute_shutdown(
        self,
        components: List[Tuple[str, Callable[[], Awaitable[None]]]],
        tasks: List[asyncio.Task],
    ) -> Dict[str, Any]:
        """
        Execute graceful shutdown sequence.

        Args:
            components: List of (name, stop_coroutine) tuples
            tasks: List of background tasks to cancel

        Returns:
            Shutdown report
        """
        self._start_time = time.time()
        self._phase = ShutdownPhase.PREPARING
        self._errors.clear()

        try:
            # Phase 1: Preparing
            await self._run_phase_callbacks(ShutdownPhase.PREPARING)

            # Phase 2: Draining - stop accepting new work
            self._phase = ShutdownPhase.DRAINING
            drain_timeout = ValidatedTimeouts.get("shutdown", 60.0) / 3
            await self._drain_components(components, drain_timeout)

            # Phase 3: Cancelling - cancel background tasks
            self._phase = ShutdownPhase.CANCELLING
            cancel_timeout = ValidatedTimeouts.get("task_cancel", 10.0)
            await self._cancel_tasks(tasks, cancel_timeout)

            # Phase 4: Waiting - wait for tasks to complete
            self._phase = ShutdownPhase.WAITING
            wait_timeout = ValidatedTimeouts.get("shutdown", 60.0) / 3
            await self._wait_for_tasks(tasks, wait_timeout)

            # Phase 5: Cleanup - final cleanup
            self._phase = ShutdownPhase.CLEANUP
            await self._run_phase_callbacks(ShutdownPhase.CLEANUP)

            self._phase = ShutdownPhase.COMPLETED

        except Exception as e:
            self._errors.append(("orchestrator", e))
            self._phase = ShutdownPhase.FAILED

        return self._generate_report()

    async def _drain_components(
        self,
        components: List[Tuple[str, Callable[[], Awaitable[None]]]],
        timeout: float,
    ) -> None:
        """Drain components by calling their stop methods."""
        for name, stop_coro in components:
            try:
                await asyncio.wait_for(stop_coro(), timeout=timeout / len(components))
                logger.debug(f"Drained component: {name}")
            except asyncio.TimeoutError:
                self._errors.append((name, TimeoutError(f"Drain timeout for {name}")))
                logger.warning(f"Drain timeout for {name}")
            except Exception as e:
                self._errors.append((name, e))
                logger.warning(f"Drain error for {name}: {e}")

    async def _cancel_tasks(
        self,
        tasks: List[asyncio.Task],
        timeout: float,
    ) -> None:
        """Cancel all tasks."""
        for task in tasks:
            if not task.done():
                task.cancel()

    async def _wait_for_tasks(
        self,
        tasks: List[asyncio.Task],
        timeout: float,
    ) -> None:
        """Wait for tasks to complete after cancellation."""
        if not tasks:
            return

        try:
            done, pending = await asyncio.wait(
                tasks,
                timeout=timeout,
                return_when=asyncio.ALL_COMPLETED,
            )

            for task in pending:
                self._errors.append((
                    task.get_name(),
                    TimeoutError(f"Task did not complete within {timeout}s")
                ))

        except Exception as e:
            self._errors.append(("wait_for_tasks", e))

    async def _run_phase_callbacks(self, phase: ShutdownPhase) -> None:
        """Run callbacks for a shutdown phase."""
        for callback in self._callbacks[phase]:
            try:
                await callback()
            except Exception as e:
                self._errors.append((f"callback_{phase.value}", e))

    def _generate_report(self) -> Dict[str, Any]:
        """Generate shutdown report."""
        return {
            "phase": self._phase.value,
            "duration_seconds": time.time() - self._start_time if self._start_time else 0,
            "errors": [
                {"component": name, "error": str(e)}
                for name, e in self._errors
            ],
            "success": self._phase == ShutdownPhase.COMPLETED and not self._errors,
        }


# =============================================================================
# CROSS-REPO HEALTH COORDINATOR
# =============================================================================

class CrossRepoHealthCoordinator:
    """
    Coordinates health checking across repos:
    - Ironcliw Core
    - Ironcliw Prime
    - Reactor Core

    Uses active health probes, not just path existence.
    """

    def __init__(self):
        self._health_cache: Dict[str, Tuple[bool, float]] = {}
        self._cache_ttl = 30.0  # seconds
        self._endpoints = {
            "jarvis_core": {
                "path": Path(os.environ.get(
                    "Ironcliw_PROJECT_ROOT",
                    Path(__file__).parent.parent.parent.parent
                )),
                "health_file": ".jarvis_health",
                "port": int(os.environ.get("Ironcliw_CORE_PORT", 8000)),
            },
            "jarvis_prime": {
                "path": Path(os.environ.get(
                    "Ironcliw_PRIME_ROOT",
                    Path.home() / "Documents/repos/Ironcliw-Prime"
                )),
                "health_file": ".jarvis_prime_health",
                "port": int(os.environ.get("Ironcliw_PRIME_PORT", 8001)),
            },
            "reactor_core": {
                "path": Path(os.environ.get(
                    "REACTOR_CORE_ROOT",
                    Path.home() / "Documents/repos/reactor-core"
                )),
                "health_file": ".reactor_health",
                "port": int(os.environ.get("REACTOR_CORE_PORT", 8090)),
            },
        }
        self._lock = asyncio.Lock()

    async def check_health(
        self,
        repo: str,
        use_cache: bool = True,
    ) -> Dict[str, Any]:
        """
        Check health of a repo.

        Returns:
            {
                "healthy": bool,
                "checks": {
                    "path_exists": bool,
                    "health_file_fresh": bool,
                    "port_responsive": bool,
                    "supervisor_running": bool,
                },
                "last_check": float,
            }
        """
        if repo not in self._endpoints:
            return {"healthy": False, "error": f"Unknown repo: {repo}"}

        # Check cache
        if use_cache and repo in self._health_cache:
            cached_health, cached_time = self._health_cache[repo]
            if time.time() - cached_time < self._cache_ttl:
                return {
                    "healthy": cached_health,
                    "cached": True,
                    "last_check": cached_time,
                }

        endpoint = self._endpoints[repo]
        checks = {}

        # Check 1: Path exists
        checks["path_exists"] = endpoint["path"].exists()

        # Check 2: Health file is fresh (written within last 60s)
        health_file = endpoint["path"] / endpoint["health_file"]
        checks["health_file_fresh"] = False
        if health_file.exists():
            try:
                mtime = health_file.stat().st_mtime
                checks["health_file_fresh"] = (time.time() - mtime) < 60.0
            except Exception:
                pass

        # Check 3: Port is responsive
        checks["port_responsive"] = await self._check_port(
            "localhost",
            endpoint["port"]
        )

        # Check 4: Supervisor running (check PID file)
        pid_file = endpoint["path"] / ".supervisor.pid"
        checks["supervisor_running"] = await self._check_pid_file(pid_file)

        # Overall health: at least 2 of 4 checks must pass
        passing = sum(1 for v in checks.values() if v)
        healthy = passing >= 2

        # Cache result
        async with self._lock:
            self._health_cache[repo] = (healthy, time.time())

        return {
            "healthy": healthy,
            "checks": checks,
            "passing_checks": passing,
            "last_check": time.time(),
        }

    async def check_all(self) -> Dict[str, Dict[str, Any]]:
        """Check health of all repos."""
        results = {}
        for repo in self._endpoints:
            results[repo] = await self.check_health(repo, use_cache=False)
        return results

    async def _check_port(self, host: str, port: int) -> bool:
        """Check if a port is responsive."""
        try:
            timeout = ValidatedTimeouts.get("health_check", 5.0)

            async def probe():
                reader, writer = await asyncio.open_connection(host, port)
                writer.close()
                await writer.wait_closed()
                return True

            return await asyncio.wait_for(probe(), timeout=timeout)

        except Exception:
            return False

    async def _check_pid_file(self, pid_file: Path) -> bool:
        """Check if process from PID file is running."""
        try:
            if not pid_file.exists():
                return False

            pid = int(pid_file.read_text().strip())

            # Check if process exists
            os.kill(pid, 0)  # Doesn't actually kill, just checks
            return True

        except (ValueError, OSError, PermissionError):
            return False

    async def write_health_beacon(self, repo: str) -> bool:
        """Write health beacon for this repo."""
        if repo not in self._endpoints:
            return False

        endpoint = self._endpoints[repo]
        health_file = endpoint["path"] / endpoint["health_file"]

        try:
            health_data = json.dumps({
                "timestamp": time.time(),
                "pid": os.getpid(),
                "status": "healthy",
            })

            health_file.write_text(health_data)
            return True

        except Exception as e:
            logger.warning(f"Failed to write health beacon: {e}")
            return False


# =============================================================================
# EVENT LOSS PREVENTOR (Dead Letter Queue)
# =============================================================================

class EventLossPreventor:
    """
    Prevents event loss with:
    - Dead Letter Queue for failed events
    - Automatic retry scheduling
    - Manual recovery interface
    - Loss metrics tracking
    """

    def __init__(
        self,
        dlq_path: Optional[Path] = None,
        max_dlq_size: int = 10000,
        retention_days: int = 7,
    ):
        self.dlq_path = dlq_path or Path.home() / ".jarvis" / "dlq"
        self.max_dlq_size = max_dlq_size
        self.retention_days = retention_days
        self._lock = asyncio.Lock()
        self._metrics = {
            "events_saved": 0,
            "events_recovered": 0,
            "events_expired": 0,
            "save_failures": 0,
        }

    async def start(self) -> None:
        """Initialize the DLQ."""
        self.dlq_path.mkdir(parents=True, exist_ok=True)

        # Cleanup expired events
        await self._cleanup_expired()

    async def save_failed_event(
        self,
        event_id: str,
        event_data: Dict[str, Any],
        failure_reason: str,
        retry_count: int = 0,
    ) -> bool:
        """Save a failed event to the DLQ."""
        async with self._lock:
            try:
                # Check DLQ size
                current_size = len(list(self.dlq_path.glob("*.json")))
                if current_size >= self.max_dlq_size:
                    logger.warning(f"DLQ full ({current_size} events), dropping oldest")
                    await self._drop_oldest()

                # Create DLQ entry
                entry = {
                    "event_id": event_id,
                    "event_data": event_data,
                    "failure_reason": failure_reason,
                    "retry_count": retry_count,
                    "saved_at": time.time(),
                    "expires_at": time.time() + (self.retention_days * 86400),
                }

                # Write to file
                filename = f"{event_id}_{int(time.time())}.json"
                filepath = self.dlq_path / filename

                if aiofiles:
                    async with aiofiles.open(filepath, "w") as f:
                        await f.write(json.dumps(entry, indent=2))
                else:
                    with open(filepath, "w") as f:
                        json.dump(entry, f, indent=2)

                self._metrics["events_saved"] += 1
                logger.debug(f"Saved failed event to DLQ: {event_id}")
                return True

            except Exception as e:
                self._metrics["save_failures"] += 1
                logger.error(f"Failed to save event to DLQ: {e}")
                return False

    async def get_recoverable_events(
        self,
        limit: int = 100,
    ) -> List[Dict[str, Any]]:
        """Get events that can be retried."""
        events = []

        try:
            files = sorted(
                self.dlq_path.glob("*.json"),
                key=lambda f: f.stat().st_mtime
            )

            for filepath in files[:limit]:
                try:
                    if aiofiles:
                        async with aiofiles.open(filepath, "r") as f:
                            content = await f.read()
                            entry = json.loads(content)
                    else:
                        with open(filepath, "r") as f:
                            entry = json.load(f)

                    # Check if not expired
                    if entry.get("expires_at", 0) > time.time():
                        entry["_filepath"] = str(filepath)
                        events.append(entry)

                except Exception as e:
                    logger.debug(f"Error reading DLQ file {filepath}: {e}")

        except Exception as e:
            logger.error(f"Error scanning DLQ: {e}")

        return events

    async def mark_recovered(self, event_id: str) -> bool:
        """Mark an event as recovered (remove from DLQ)."""
        async with self._lock:
            try:
                for filepath in self.dlq_path.glob(f"{event_id}_*.json"):
                    filepath.unlink()
                    self._metrics["events_recovered"] += 1
                    return True
            except Exception as e:
                logger.error(f"Error marking event recovered: {e}")

        return False

    async def _cleanup_expired(self) -> int:
        """Remove expired events."""
        removed = 0
        now = time.time()

        try:
            for filepath in self.dlq_path.glob("*.json"):
                try:
                    if aiofiles:
                        async with aiofiles.open(filepath, "r") as f:
                            content = await f.read()
                            entry = json.loads(content)
                    else:
                        with open(filepath, "r") as f:
                            entry = json.load(f)

                    if entry.get("expires_at", 0) <= now:
                        filepath.unlink()
                        removed += 1
                        self._metrics["events_expired"] += 1

                except Exception:
                    # If we can't read it, check file age
                    if (now - filepath.stat().st_mtime) > (self.retention_days * 86400):
                        filepath.unlink()
                        removed += 1

        except Exception as e:
            logger.error(f"DLQ cleanup error: {e}")

        if removed:
            logger.info(f"Cleaned up {removed} expired DLQ events")

        return removed

    async def _drop_oldest(self) -> None:
        """Drop oldest events when DLQ is full."""
        try:
            files = sorted(
                self.dlq_path.glob("*.json"),
                key=lambda f: f.stat().st_mtime
            )

            # Drop oldest 10%
            to_drop = max(1, len(files) // 10)
            for filepath in files[:to_drop]:
                filepath.unlink()
                self._metrics["events_expired"] += 1

        except Exception as e:
            logger.error(f"Error dropping oldest DLQ events: {e}")

    def get_metrics(self) -> Dict[str, Any]:
        """Get DLQ metrics."""
        try:
            current_size = len(list(self.dlq_path.glob("*.json")))
        except Exception:
            current_size = 0

        return {
            **self._metrics,
            "current_size": current_size,
            "max_size": self.max_dlq_size,
        }


# =============================================================================
# STARTUP SEQUENCER
# =============================================================================

class StartupPhase(Enum):
    """Startup phases for ordered initialization."""
    NOT_STARTED = "not_started"
    ENVIRONMENT = "environment"
    CORE_SERVICES = "core_services"
    EVENT_BUS = "event_bus"
    CROSS_REPO = "cross_repo"
    COMPONENTS = "components"
    BACKGROUND_TASKS = "background_tasks"
    HEALTH_CHECK = "health_check"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class StartupComponent:
    """A component to initialize during startup."""
    name: str
    phase: StartupPhase
    init_func: Callable[[], Awaitable[Any]]
    dependencies: List[str] = field(default_factory=list)
    timeout: float = 30.0
    critical: bool = True
    result: Optional[Any] = None
    error: Optional[Exception] = None
    duration: float = 0.0


class StartupSequencer:
    """
    Ordered startup with:
    - Dependency resolution
    - Phase-based initialization
    - Timeout enforcement
    - Rollback on failure
    """

    def __init__(self):
        self._components: Dict[str, StartupComponent] = {}
        self._phase = StartupPhase.NOT_STARTED
        self._initialized: Set[str] = set()
        self._failed: Set[str] = set()
        self._start_time: Optional[float] = None

    def register(
        self,
        name: str,
        phase: StartupPhase,
        init_func: Callable[[], Awaitable[Any]],
        dependencies: Optional[List[str]] = None,
        timeout: float = 30.0,
        critical: bool = True,
    ) -> None:
        """Register a component for startup."""
        self._components[name] = StartupComponent(
            name=name,
            phase=phase,
            init_func=init_func,
            dependencies=dependencies or [],
            timeout=timeout,
            critical=critical,
        )

    async def execute_startup(self) -> Dict[str, Any]:
        """
        Execute startup sequence in phase order.

        Returns:
            Startup report with status of all components
        """
        self._start_time = time.time()
        self._phase = StartupPhase.ENVIRONMENT

        # Group components by phase
        phases = defaultdict(list)
        for comp in self._components.values():
            phases[comp.phase].append(comp)

        # Execute phases in order
        phase_order = [
            StartupPhase.ENVIRONMENT,
            StartupPhase.CORE_SERVICES,
            StartupPhase.EVENT_BUS,
            StartupPhase.CROSS_REPO,
            StartupPhase.COMPONENTS,
            StartupPhase.BACKGROUND_TASKS,
            StartupPhase.HEALTH_CHECK,
        ]

        for phase in phase_order:
            self._phase = phase
            components = phases.get(phase, [])

            if not components:
                continue

            # Sort by dependencies
            sorted_components = self._topological_sort(components)

            for comp in sorted_components:
                # Check dependencies
                unmet = [d for d in comp.dependencies if d not in self._initialized]
                if unmet:
                    comp.error = RuntimeError(f"Unmet dependencies: {unmet}")
                    self._failed.add(comp.name)
                    if comp.critical:
                        self._phase = StartupPhase.FAILED
                        return self._generate_report()
                    continue

                # Initialize component
                success = await self._init_component(comp)

                if not success and comp.critical:
                    self._phase = StartupPhase.FAILED
                    return self._generate_report()

        self._phase = StartupPhase.COMPLETED
        return self._generate_report()

    async def _init_component(self, comp: StartupComponent) -> bool:
        """Initialize a single component."""
        start = time.time()

        try:
            comp.result = await asyncio.wait_for(
                comp.init_func(),
                timeout=comp.timeout
            )
            comp.duration = time.time() - start
            self._initialized.add(comp.name)
            logger.debug(f"Initialized {comp.name} in {comp.duration:.2f}s")
            return True

        except asyncio.TimeoutError:
            comp.error = TimeoutError(f"Initialization timeout after {comp.timeout}s")
            comp.duration = time.time() - start
            self._failed.add(comp.name)
            logger.error(f"Timeout initializing {comp.name}")
            return False

        except Exception as e:
            comp.error = e
            comp.duration = time.time() - start
            self._failed.add(comp.name)
            logger.error(f"Error initializing {comp.name}: {e}")
            return False

    def _topological_sort(
        self,
        components: List[StartupComponent]
    ) -> List[StartupComponent]:
        """Sort components by dependencies."""
        # Simple topological sort
        sorted_list = []
        visited = set()

        def visit(comp):
            if comp.name in visited:
                return
            visited.add(comp.name)

            for dep_name in comp.dependencies:
                dep_comp = next((c for c in components if c.name == dep_name), None)
                if dep_comp:
                    visit(dep_comp)

            sorted_list.append(comp)

        for comp in components:
            visit(comp)

        return sorted_list

    def _generate_report(self) -> Dict[str, Any]:
        """Generate startup report."""
        return {
            "phase": self._phase.value,
            "duration_seconds": time.time() - self._start_time if self._start_time else 0,
            "initialized": list(self._initialized),
            "failed": list(self._failed),
            "components": {
                name: {
                    "phase": comp.phase.value,
                    "status": "initialized" if name in self._initialized else (
                        "failed" if name in self._failed else "pending"
                    ),
                    "duration": comp.duration,
                    "error": str(comp.error) if comp.error else None,
                    "critical": comp.critical,
                }
                for name, comp in self._components.items()
            },
            "success": self._phase == StartupPhase.COMPLETED,
        }


# =============================================================================
# v13.0 BULLETPROOF ORCHESTRATION MESH (Master Orchestrator)
# =============================================================================

class BulletproofOrchestrationMesh:
    """
    Master orchestrator combining all v13.0 components:
    - ValidatedTimeouts
    - AsyncLockGuard
    - TaskSupervisor
    - AtomicFileManager
    - GracefulShutdownOrchestrator
    - CrossRepoHealthCoordinator
    - EventLossPreventor
    - StartupSequencer
    """

    _instance: Optional["BulletproofOrchestrationMesh"] = None
    _lock = asyncio.Lock()

    def __init__(self):
        self.lock_guard = AsyncLockGuard()
        self.task_supervisor = TaskSupervisor(
            on_task_crash=self._handle_task_crash,
            on_task_restart=self._handle_task_restart,
        )
        self.atomic_file_manager = AtomicFileManager()
        self.shutdown_orchestrator = GracefulShutdownOrchestrator()
        self.health_coordinator = CrossRepoHealthCoordinator()
        self.event_loss_preventor = EventLossPreventor()
        self.startup_sequencer = StartupSequencer()

        self._running = False
        self._health_beacon_task: Optional[asyncio.Task] = None

    @classmethod
    async def get_instance(cls) -> "BulletproofOrchestrationMesh":
        """Get or create singleton instance."""
        async with cls._lock:
            if cls._instance is None:
                cls._instance = cls()
            return cls._instance

    async def start(self) -> Dict[str, Any]:
        """Start the orchestration mesh."""
        if self._running:
            return {"status": "already_running"}

        logger.info("🛡️ Starting Bulletproof Orchestration Mesh v13.0...")
        start_time = time.monotonic()

        # Validate timeout configuration
        timeout_issues = ValidatedTimeouts.validate_relationships()
        if timeout_issues:
            for issue in timeout_issues:
                logger.warning(f"Timeout config issue: {issue}")

        # Start components
        await self.task_supervisor.start()
        await self.event_loss_preventor.start()

        # Recover any incomplete file writes
        recovered = await self.atomic_file_manager.recover_pending()
        if recovered:
            logger.info(f"Recovered {recovered} incomplete file writes")

        # Start health beacon task
        self._health_beacon_task = await self.task_supervisor.supervise(
            "health_beacon",
            self._health_beacon_loop(),
            critical=False,
            max_restarts=5,
            factory=lambda: self._health_beacon_loop(),
        )

        self._running = True
        elapsed = time.monotonic() - start_time

        logger.info(f"✅ Bulletproof Orchestration Mesh started in {elapsed:.2f}s")

        return {
            "status": "running",
            "elapsed": elapsed,
            "timeout_issues": timeout_issues,
            "recovered_files": recovered,
        }

    async def stop(self) -> Dict[str, Any]:
        """Stop the orchestration mesh gracefully."""
        if not self._running:
            return {"status": "not_running"}

        logger.info("🛡️ Stopping Bulletproof Orchestration Mesh...")

        # Execute choreographed shutdown
        components = [
            ("task_supervisor", self.task_supervisor.stop),
        ]

        tasks = []
        if self._health_beacon_task:
            tasks.append(self._health_beacon_task)

        report = await self.shutdown_orchestrator.execute_shutdown(
            components=components,
            tasks=tasks,
        )

        self._running = False

        logger.info(f"✅ Orchestration Mesh shutdown: {report['phase']}")

        return report

    async def _health_beacon_loop(self) -> None:
        """Periodically write health beacon."""
        while self._running:
            try:
                await self.task_supervisor.heartbeat("health_beacon")
                await self.health_coordinator.write_health_beacon("jarvis_core")
                await asyncio.sleep(30.0)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.warning(f"Health beacon error: {e}")
                await asyncio.sleep(5.0)

    async def _handle_task_crash(self, name: str, error: Exception) -> None:
        """Handle task crash notification."""
        logger.error(f"Task {name} crashed: {error}")

        # Save crash info to DLQ for analysis
        await self.event_loss_preventor.save_failed_event(
            event_id=f"crash_{name}_{int(time.time())}",
            event_data={
                "task_name": name,
                "error_type": type(error).__name__,
                "error_message": str(error),
            },
            failure_reason="task_crash",
        )

    async def _handle_task_restart(self, name: str, count: int) -> None:
        """Handle task restart notification."""
        logger.warning(f"Task {name} restarted (attempt {count})")

    def get_status(self) -> Dict[str, Any]:
        """Get comprehensive mesh status."""
        return {
            "running": self._running,
            "task_supervisor": self.task_supervisor.get_status(),
            "lock_guard": self.lock_guard.get_stats(),
            "dlq": self.event_loss_preventor.get_metrics(),
        }


# =============================================================================
# v13.0 CONVENIENCE FUNCTIONS
# =============================================================================

_bulletproof_mesh: Optional[BulletproofOrchestrationMesh] = None


def get_bulletproof_mesh() -> BulletproofOrchestrationMesh:
    """Get the global bulletproof mesh instance (sync)."""
    global _bulletproof_mesh
    if _bulletproof_mesh is None:
        _bulletproof_mesh = BulletproofOrchestrationMesh()
    return _bulletproof_mesh


async def initialize_bulletproof_mesh() -> Dict[str, Any]:
    """
    Initialize the bulletproof orchestration mesh.

    Returns:
        Dictionary with initialization status and components
    """
    logger.info("🔗 Initializing Bulletproof Orchestration Mesh v13.0...")
    start_time = time.monotonic()

    components = {}

    try:
        mesh = get_bulletproof_mesh()
        start_result = await mesh.start()

        components["bulletproof_mesh"] = mesh
        components["lock_guard"] = mesh.lock_guard
        components["task_supervisor"] = mesh.task_supervisor
        components["atomic_file_manager"] = mesh.atomic_file_manager
        components["shutdown_orchestrator"] = mesh.shutdown_orchestrator
        components["health_coordinator"] = mesh.health_coordinator
        components["event_loss_preventor"] = mesh.event_loss_preventor
        components["startup_sequencer"] = mesh.startup_sequencer

        elapsed = time.monotonic() - start_time
        logger.info(f"✅ Bulletproof Orchestration Mesh initialized in {elapsed:.2f}s")
        logger.info(f"   Components: {len(components)}")

        return components

    except Exception as e:
        logger.error(f"❌ Bulletproof Mesh initialization failed: {e}")
        raise


async def shutdown_bulletproof_mesh() -> None:
    """Shutdown the bulletproof orchestration mesh."""
    global _bulletproof_mesh
    if _bulletproof_mesh:
        await _bulletproof_mesh.stop()
        _bulletproof_mesh = None
        logger.info("✅ Bulletproof Orchestration Mesh shutdown complete")


async def get_bulletproof_mesh_status() -> Dict[str, Any]:
    """Get comprehensive mesh status."""
    mesh = get_bulletproof_mesh()
    return mesh.get_status()


# Convenience accessors for common operations
async def supervise_task(
    name: str,
    coro: Awaitable[None],
    critical: bool = False,
    max_restarts: int = 3,
) -> asyncio.Task:
    """Supervise a background task with automatic crash recovery."""
    mesh = get_bulletproof_mesh()
    return await mesh.task_supervisor.supervise(
        name=name,
        coro=coro,
        critical=critical,
        max_restarts=max_restarts,
    )


async def write_file_atomic(
    path: Path,
    content: Union[str, bytes],
    create_backup: bool = True,
) -> bool:
    """Write a file atomically with backup."""
    mesh = get_bulletproof_mesh()
    return await mesh.atomic_file_manager.write_atomic(
        path=path,
        content=content,
        create_backup=create_backup,
    )


async def check_repo_health(repo: str) -> Dict[str, Any]:
    """Check health of a repo."""
    mesh = get_bulletproof_mesh()
    return await mesh.health_coordinator.check_health(repo)


async def save_to_dlq(
    event_id: str,
    event_data: Dict[str, Any],
    failure_reason: str,
) -> bool:
    """Save a failed event to the Dead Letter Queue."""
    mesh = get_bulletproof_mesh()
    return await mesh.event_loss_preventor.save_failed_event(
        event_id=event_id,
        event_data=event_data,
        failure_reason=failure_reason,
    )


# =============================================================================
# v14.0: RESILIENT BOOTSTRAP LAYER
# =============================================================================
#
# This layer provides bulletproof initialization with:
# 1. Pre-flight directory validation
# 2. Async method detection (prevents missing await bugs)
# 3. Dependency-aware startup ordering (topological sort)
# 4. Health-gated initialization
# 5. Graceful degradation registry
# 6. Bootstrap transaction with rollback
# 7. Initialization timeout management
#
# Architecture:
#     ┌─────────────────────────────────────────────────────────────────┐
#     │  ResilientBootstrapLayer                                        │
#     │  ┌───────────────┐  ┌────────────────┐  ┌──────────────────┐  │
#     │  │ PreflightChk  │  │ AsyncValidator │  │ DependencyGraph  │  │
#     │  └───────────────┘  └────────────────┘  └──────────────────┘  │
#     │  ┌───────────────┐  ┌────────────────┐  ┌──────────────────┐  │
#     │  │ HealthGate    │  │ DegradedReg    │  │ BootstrapTxn     │  │
#     │  └───────────────┘  └────────────────┘  └──────────────────┘  │
#     │  ┌───────────────────────────────────────────────────────────┐│
#     │  │               InitializationOrchestrator                  ││
#     │  └───────────────────────────────────────────────────────────┘│
#     └─────────────────────────────────────────────────────────────────┘
# =============================================================================


class PreflightDirectoryValidator:
    """
    v14.0: Validates and creates all required directories before startup.

    Prevents "No such file or directory" errors during initialization by
    ensuring all paths exist with proper permissions.
    """

    # Required directory specifications
    REQUIRED_DIRS: Dict[str, Dict[str, Any]] = {
        "cross_repo": {
            "path": Path.home() / ".jarvis" / "cross_repo",
            "critical": True,
            "subdirs": ["locks", "events", "state", "cache"],
        },
        "experience_mesh": {
            "path": Path.home() / ".jarvis" / "experience_mesh",
            "critical": False,
            "subdirs": ["sqlite", "file_store", "metrics"],
        },
        "bulletproof": {
            "path": Path.home() / ".jarvis" / "bulletproof",
            "critical": False,
            "subdirs": ["dlq", "atomic_backups", "health_probes"],
        },
        "logs": {
            "path": Path.home() / ".jarvis" / "logs",
            "critical": False,
            "subdirs": ["supervisor", "components", "errors"],
        },
        "cache": {
            "path": Path.home() / ".jarvis" / "cache",
            "critical": False,
            "subdirs": ["models", "embeddings", "responses"],
        },
    }

    def __init__(self):
        self._validated: Set[str] = set()
        self._failed: Dict[str, str] = {}
        self._lock = asyncio.Lock()
        self.logger = logging.getLogger("v14.PreflightValidator")

    async def validate_all(self) -> Tuple[bool, Dict[str, Any]]:
        """
        Validate and create all required directories.

        Returns:
            Tuple of (all_critical_ok, detailed_results)
        """
        async with self._lock:
            results = {
                "validated": [],
                "created": [],
                "failed": [],
                "warnings": [],
            }
            all_critical_ok = True

            for name, spec in self.REQUIRED_DIRS.items():
                try:
                    base_path = spec["path"]
                    is_critical = spec.get("critical", False)
                    subdirs = spec.get("subdirs", [])

                    # Create base directory
                    if not base_path.exists():
                        base_path.mkdir(parents=True, exist_ok=True)
                        results["created"].append(str(base_path))
                        self.logger.info(f"Created directory: {base_path}")

                    # Create subdirectories
                    for subdir in subdirs:
                        sub_path = base_path / subdir
                        if not sub_path.exists():
                            sub_path.mkdir(parents=True, exist_ok=True)
                            results["created"].append(str(sub_path))

                    # Verify write permissions
                    test_file = base_path / ".preflight_test"
                    try:
                        test_file.write_text("preflight_check")
                        test_file.unlink()
                    except PermissionError:
                        raise PermissionError(f"No write permission for {base_path}")

                    self._validated.add(name)
                    results["validated"].append(name)

                except Exception as e:
                    error_msg = f"{name}: {e}"
                    self._failed[name] = str(e)

                    if is_critical:
                        results["failed"].append(error_msg)
                        all_critical_ok = False
                        self.logger.error(f"Critical directory validation failed: {error_msg}")
                    else:
                        results["warnings"].append(error_msg)
                        self.logger.warning(f"Non-critical directory validation failed: {error_msg}")

            return all_critical_ok, results

    async def ensure_directory(self, path: Path, create: bool = True) -> bool:
        """
        Ensure a specific directory exists.

        Args:
            path: Directory path to ensure
            create: Whether to create if missing

        Returns:
            True if directory exists/created, False otherwise
        """
        try:
            if path.exists():
                return True

            if create:
                path.mkdir(parents=True, exist_ok=True)
                self.logger.debug(f"Created directory on-demand: {path}")
                return True

            return False

        except Exception as e:
            self.logger.error(f"Failed to ensure directory {path}: {e}")
            return False

    def get_status(self) -> Dict[str, Any]:
        """Get validation status."""
        return {
            "validated_count": len(self._validated),
            "failed_count": len(self._failed),
            "validated": list(self._validated),
            "failed": self._failed.copy(),
        }


class AsyncMethodValidator:
    """
    v14.0: Runtime validator that detects async methods and ensures proper await.

    Prevents the "missing await" bug pattern by:
    1. Inspecting method signatures at registration time
    2. Wrapping calls to ensure coroutines are awaited
    3. Logging warnings for potential issues
    """

    def __init__(self):
        self._async_methods: Dict[str, Set[str]] = {}  # class -> set of async method names
        self._sync_methods: Dict[str, Set[str]] = {}
        self._warnings: List[Dict[str, Any]] = []
        self._lock = asyncio.Lock()
        self.logger = logging.getLogger("v14.AsyncValidator")

    def register_class(self, cls: type) -> None:
        """
        Register a class and catalog its async/sync methods.

        Args:
            cls: Class to analyze
        """
        class_name = cls.__name__
        self._async_methods[class_name] = set()
        self._sync_methods[class_name] = set()

        for name, method in inspect.getmembers(cls, predicate=inspect.isfunction):
            if name.startswith("_"):
                continue

            if asyncio.iscoroutinefunction(method):
                self._async_methods[class_name].add(name)
            else:
                self._sync_methods[class_name].add(name)

        self.logger.debug(
            f"Registered {class_name}: "
            f"{len(self._async_methods[class_name])} async, "
            f"{len(self._sync_methods[class_name])} sync methods"
        )

    def is_async_method(self, class_name: str, method_name: str) -> Optional[bool]:
        """
        Check if a method is async.

        Returns:
            True if async, False if sync, None if unknown
        """
        if class_name in self._async_methods:
            if method_name in self._async_methods[class_name]:
                return True
            if method_name in self._sync_methods[class_name]:
                return False
        return None

    def validate_call(
        self,
        obj: Any,
        method_name: str,
        result: Any,
    ) -> Any:
        """
        Validate a method call result and warn if coroutine not awaited.

        Args:
            obj: Object the method was called on
            method_name: Name of the method
            result: Result of the call

        Returns:
            The result (unchanged)
        """
        if asyncio.iscoroutine(result):
            class_name = obj.__class__.__name__
            warning = {
                "class": class_name,
                "method": method_name,
                "timestamp": time.time(),
                "message": f"Coroutine returned but likely not awaited: {class_name}.{method_name}()",
            }
            self._warnings.append(warning)
            self.logger.warning(
                f"⚠️ POTENTIAL BUG: {class_name}.{method_name}() returned coroutine - "
                f"ensure it is awaited!"
            )

        return result

    def create_safe_wrapper(self, obj: Any, method_name: str) -> Callable:
        """
        Create a wrapper that ensures async methods are properly called.

        Args:
            obj: Object to wrap method for
            method_name: Method name to wrap

        Returns:
            Wrapped callable
        """
        original = getattr(obj, method_name)
        class_name = obj.__class__.__name__
        is_async = self.is_async_method(class_name, method_name)

        if is_async:
            @functools.wraps(original)
            async def async_wrapper(*args, **kwargs):
                return await original(*args, **kwargs)
            return async_wrapper
        else:
            return original

    def get_warnings(self) -> List[Dict[str, Any]]:
        """Get accumulated warnings."""
        return self._warnings.copy()

    def clear_warnings(self) -> None:
        """Clear accumulated warnings."""
        self._warnings.clear()

    def get_status(self) -> Dict[str, Any]:
        """Get validator status."""
        return {
            "registered_classes": len(self._async_methods),
            "total_async_methods": sum(len(v) for v in self._async_methods.values()),
            "total_sync_methods": sum(len(v) for v in self._sync_methods.values()),
            "warning_count": len(self._warnings),
            "recent_warnings": self._warnings[-5:] if self._warnings else [],
        }


class ComponentDependencyGraph:
    """
    v14.0: Manages component dependencies for proper startup ordering.

    Uses topological sort to determine initialization order, ensuring
    dependencies are ready before dependents start.
    """

    def __init__(self):
        self._nodes: Dict[str, Dict[str, Any]] = {}  # component -> metadata
        self._edges: Dict[str, Set[str]] = {}  # component -> set of dependencies
        self._initialized: Set[str] = set()
        self._failed: Set[str] = set()
        self._lock = asyncio.Lock()
        self.logger = logging.getLogger("v14.DependencyGraph")

    def register_component(
        self,
        name: str,
        dependencies: List[str] = None,
        optional_deps: List[str] = None,
        critical: bool = False,
        init_timeout: float = 30.0,
    ) -> None:
        """
        Register a component with its dependencies.

        Args:
            name: Component name
            dependencies: Required dependencies (must succeed)
            optional_deps: Optional dependencies (can degrade)
            critical: Whether failure should abort startup
            init_timeout: Initialization timeout in seconds
        """
        self._nodes[name] = {
            "dependencies": dependencies or [],
            "optional_deps": optional_deps or [],
            "critical": critical,
            "init_timeout": init_timeout,
            "registered_at": time.time(),
        }
        self._edges[name] = set(dependencies or [])

        self.logger.debug(
            f"Registered component: {name} "
            f"(deps: {dependencies}, optional: {optional_deps}, critical: {critical})"
        )

    def get_initialization_order(self) -> List[str]:
        """
        Get components in topological order for initialization.

        Returns:
            List of component names in initialization order

        Raises:
            ValueError: If circular dependency detected
        """
        # Kahn's algorithm for topological sort
        in_degree = {node: 0 for node in self._nodes}

        for node, deps in self._edges.items():
            for dep in deps:
                if dep in in_degree:
                    in_degree[node] += 1

        # Start with nodes that have no dependencies
        queue = [node for node, degree in in_degree.items() if degree == 0]
        result = []

        while queue:
            # Sort for deterministic ordering
            queue.sort()
            node = queue.pop(0)
            result.append(node)

            # Reduce in-degree for dependents
            for other_node, deps in self._edges.items():
                if node in deps:
                    in_degree[other_node] -= 1
                    if in_degree[other_node] == 0:
                        queue.append(other_node)

        if len(result) != len(self._nodes):
            # Circular dependency detected
            remaining = set(self._nodes.keys()) - set(result)
            raise ValueError(f"Circular dependency detected involving: {remaining}")

        return result

    def mark_initialized(self, name: str) -> None:
        """Mark a component as successfully initialized."""
        self._initialized.add(name)
        self._failed.discard(name)

    def mark_failed(self, name: str) -> None:
        """Mark a component as failed to initialize."""
        self._failed.add(name)
        self._initialized.discard(name)

    def can_initialize(self, name: str) -> Tuple[bool, List[str]]:
        """
        Check if a component can be initialized (all deps ready).

        Returns:
            Tuple of (can_init, list of missing deps)
        """
        if name not in self._nodes:
            return False, [f"Unknown component: {name}"]

        node = self._nodes[name]
        missing = []

        for dep in node["dependencies"]:
            if dep not in self._initialized:
                if dep in self._failed:
                    missing.append(f"{dep} (FAILED)")
                else:
                    missing.append(f"{dep} (not initialized)")

        return len(missing) == 0, missing

    def get_component_info(self, name: str) -> Optional[Dict[str, Any]]:
        """Get information about a component."""
        if name not in self._nodes:
            return None

        node = self._nodes[name].copy()
        node["initialized"] = name in self._initialized
        node["failed"] = name in self._failed
        can_init, missing = self.can_initialize(name)
        node["can_initialize"] = can_init
        node["missing_deps"] = missing

        return node

    def get_status(self) -> Dict[str, Any]:
        """Get graph status."""
        return {
            "total_components": len(self._nodes),
            "initialized": len(self._initialized),
            "failed": len(self._failed),
            "pending": len(self._nodes) - len(self._initialized) - len(self._failed),
            "initialization_order": self.get_initialization_order(),
        }


class HealthGatedInitializer:
    """
    v14.0: Ensures components only start after their dependencies are healthy.

    Provides health checks and gates initialization on health status.
    """

    def __init__(
        self,
        dependency_graph: ComponentDependencyGraph,
        health_check_timeout: float = 5.0,
        health_retry_count: int = 3,
        health_retry_delay: float = 1.0,
    ):
        self._graph = dependency_graph
        self._health_timeout = health_check_timeout
        self._retry_count = health_retry_count
        self._retry_delay = health_retry_delay
        self._health_status: Dict[str, Dict[str, Any]] = {}
        self._health_checks: Dict[str, Callable[[], Awaitable[bool]]] = {}
        self._lock = asyncio.Lock()
        self.logger = logging.getLogger("v14.HealthGate")

    def register_health_check(
        self,
        component: str,
        check_fn: Callable[[], Awaitable[bool]],
    ) -> None:
        """
        Register a health check function for a component.

        Args:
            component: Component name
            check_fn: Async function that returns True if healthy
        """
        self._health_checks[component] = check_fn
        self.logger.debug(f"Registered health check for: {component}")

    async def check_health(self, component: str) -> bool:
        """
        Check health of a component with retry.

        Args:
            component: Component name

        Returns:
            True if healthy, False otherwise
        """
        if component not in self._health_checks:
            # No health check registered - assume healthy
            return True

        check_fn = self._health_checks[component]

        for attempt in range(self._retry_count):
            try:
                # v211.0: Use asyncio.wait_for for Python 3.9 compatibility
                is_healthy = await asyncio.wait_for(check_fn(), timeout=self._health_timeout)

                self._health_status[component] = {
                    "healthy": is_healthy,
                    "last_check": time.time(),
                    "attempts": attempt + 1,
                }

                if is_healthy:
                    return True

            except asyncio.TimeoutError:
                self.logger.warning(
                    f"Health check timeout for {component} (attempt {attempt + 1})"
                )
            except Exception as e:
                self.logger.warning(
                    f"Health check error for {component}: {e} (attempt {attempt + 1})"
                )

            if attempt < self._retry_count - 1:
                await asyncio.sleep(self._retry_delay)

        self._health_status[component] = {
            "healthy": False,
            "last_check": time.time(),
            "attempts": self._retry_count,
        }

        return False

    async def wait_for_healthy(
        self,
        component: str,
        timeout: float = 30.0,
    ) -> bool:
        """
        Wait for a component to become healthy.

        Args:
            component: Component name
            timeout: Maximum wait time

        Returns:
            True if became healthy, False if timeout
        """
        start = time.time()

        while time.time() - start < timeout:
            if await self.check_health(component):
                return True
            await asyncio.sleep(self._retry_delay)

        return False

    async def gate_initialization(
        self,
        component: str,
    ) -> Tuple[bool, List[str]]:
        """
        Check if a component can be initialized (deps healthy).

        Args:
            component: Component to check

        Returns:
            Tuple of (can_proceed, list of unhealthy deps)
        """
        can_init, missing_deps = self._graph.can_initialize(component)

        if not can_init:
            return False, missing_deps

        # Check health of all dependencies
        node = self._graph.get_component_info(component)
        if not node:
            return False, [f"Unknown component: {component}"]

        unhealthy = []

        for dep in node["dependencies"]:
            if not await self.check_health(dep):
                unhealthy.append(dep)

        return len(unhealthy) == 0, unhealthy

    def get_status(self) -> Dict[str, Any]:
        """Get health gate status."""
        return {
            "registered_checks": len(self._health_checks),
            "health_status": self._health_status.copy(),
            "config": {
                "timeout": self._health_timeout,
                "retry_count": self._retry_count,
                "retry_delay": self._retry_delay,
            },
        }


class GracefulDegradationRegistry:
    """
    v14.0: Centralized registry tracking degraded components and their fallbacks.

    Provides visibility into what's degraded and why, enabling intelligent
    fallback behavior and status reporting.
    """

    @dataclass
    class DegradedComponent:
        """Information about a degraded component."""
        name: str
        reason: str
        degraded_at: float
        fallback_mode: str
        original_error: Optional[str] = None
        recovery_attempts: int = 0
        last_recovery_attempt: Optional[float] = None

    def __init__(self):
        self._degraded: Dict[str, GracefulDegradationRegistry.DegradedComponent] = {}
        self._recovery_handlers: Dict[str, Callable[[], Awaitable[bool]]] = {}
        self._lock = asyncio.Lock()
        self.logger = logging.getLogger("v14.DegradationRegistry")

    async def register_degraded(
        self,
        component: str,
        reason: str,
        fallback_mode: str,
        original_error: Optional[str] = None,
    ) -> None:
        """
        Register a component as degraded.

        Args:
            component: Component name
            reason: Why it's degraded
            fallback_mode: What fallback is being used
            original_error: Optional original error message
        """
        async with self._lock:
            self._degraded[component] = self.DegradedComponent(
                name=component,
                reason=reason,
                degraded_at=time.time(),
                fallback_mode=fallback_mode,
                original_error=original_error,
            )

            self.logger.warning(
                f"Component degraded: {component} - {reason} "
                f"(fallback: {fallback_mode})"
            )

    async def mark_recovered(self, component: str) -> None:
        """Mark a component as recovered from degraded state."""
        async with self._lock:
            if component in self._degraded:
                del self._degraded[component]
                self.logger.info(f"Component recovered: {component}")

    def register_recovery_handler(
        self,
        component: str,
        handler: Callable[[], Awaitable[bool]],
    ) -> None:
        """
        Register a recovery handler for a degraded component.

        Args:
            component: Component name
            handler: Async function that attempts recovery, returns True if successful
        """
        self._recovery_handlers[component] = handler

    async def attempt_recovery(self, component: str) -> bool:
        """
        Attempt to recover a degraded component.

        Returns:
            True if recovery successful, False otherwise
        """
        if component not in self._degraded:
            return True  # Not degraded

        if component not in self._recovery_handlers:
            self.logger.debug(f"No recovery handler for: {component}")
            return False

        async with self._lock:
            degraded = self._degraded[component]
            degraded.recovery_attempts += 1
            degraded.last_recovery_attempt = time.time()

        try:
            handler = self._recovery_handlers[component]
            if await handler():
                await self.mark_recovered(component)
                return True
        except Exception as e:
            self.logger.warning(f"Recovery attempt failed for {component}: {e}")

        return False

    async def attempt_all_recoveries(self) -> Dict[str, bool]:
        """
        Attempt recovery for all degraded components.

        Returns:
            Dict mapping component name to recovery success
        """
        results = {}

        for component in list(self._degraded.keys()):
            results[component] = await self.attempt_recovery(component)

        return results

    def is_degraded(self, component: str) -> bool:
        """Check if a component is degraded."""
        return component in self._degraded

    def get_degraded_info(self, component: str) -> Optional[Dict[str, Any]]:
        """Get information about a degraded component."""
        if component not in self._degraded:
            return None

        d = self._degraded[component]
        return {
            "name": d.name,
            "reason": d.reason,
            "degraded_at": d.degraded_at,
            "fallback_mode": d.fallback_mode,
            "original_error": d.original_error,
            "recovery_attempts": d.recovery_attempts,
            "degraded_duration": time.time() - d.degraded_at,
        }

    def get_status(self) -> Dict[str, Any]:
        """Get registry status."""
        return {
            "degraded_count": len(self._degraded),
            "degraded_components": [
                self.get_degraded_info(c) for c in self._degraded
            ],
            "recovery_handlers": len(self._recovery_handlers),
        }


class BootstrapTransaction:
    """
    v14.0: Provides transaction-like initialization with rollback on failure.

    If any critical component fails, previously initialized components
    are properly shut down to prevent partial system state.
    """

    @dataclass
    class InitializedComponent:
        """Tracking for an initialized component."""
        name: str
        instance: Any
        shutdown_fn: Optional[Callable[[], Awaitable[None]]]
        initialized_at: float

    def __init__(self):
        self._initialized: List[BootstrapTransaction.InitializedComponent] = []
        self._in_transaction = False
        self._committed = False
        self._lock = asyncio.Lock()
        self.logger = logging.getLogger("v14.BootstrapTransaction")

    async def begin(self) -> None:
        """Begin a bootstrap transaction."""
        async with self._lock:
            if self._in_transaction:
                raise RuntimeError("Transaction already in progress")

            self._in_transaction = True
            self._committed = False
            self._initialized.clear()
            self.logger.info("Bootstrap transaction started")

    async def register_initialized(
        self,
        name: str,
        instance: Any,
        shutdown_fn: Optional[Callable[[], Awaitable[None]]] = None,
    ) -> None:
        """
        Register a successfully initialized component.

        Args:
            name: Component name
            instance: Component instance
            shutdown_fn: Optional async shutdown function
        """
        async with self._lock:
            if not self._in_transaction:
                raise RuntimeError("No transaction in progress")

            self._initialized.append(self.InitializedComponent(
                name=name,
                instance=instance,
                shutdown_fn=shutdown_fn,
                initialized_at=time.time(),
            ))
            self.logger.debug(f"Registered initialized component: {name}")

    async def commit(self) -> None:
        """Commit the transaction (all components successfully initialized)."""
        async with self._lock:
            if not self._in_transaction:
                raise RuntimeError("No transaction in progress")

            self._committed = True
            self._in_transaction = False
            self.logger.info(
                f"Bootstrap transaction committed "
                f"({len(self._initialized)} components)"
            )

    async def rollback(self, reason: str = "unknown") -> None:
        """
        Rollback the transaction (shutdown all initialized components).

        Args:
            reason: Reason for rollback
        """
        async with self._lock:
            if not self._in_transaction:
                self.logger.warning("Rollback called but no transaction in progress")
                return

            self.logger.warning(
                f"Rolling back bootstrap transaction: {reason} "
                f"({len(self._initialized)} components to shutdown)"
            )

            # Shutdown in reverse order
            for component in reversed(self._initialized):
                try:
                    if component.shutdown_fn:
                        await asyncio.wait_for(
                            component.shutdown_fn(),
                            timeout=10.0,
                        )
                        self.logger.debug(f"Shutdown: {component.name}")
                except asyncio.TimeoutError:
                    self.logger.warning(f"Shutdown timeout: {component.name}")
                except Exception as e:
                    self.logger.error(f"Shutdown error for {component.name}: {e}")

            self._initialized.clear()
            self._in_transaction = False
            self._committed = False
            self.logger.info("Bootstrap transaction rolled back")

    def get_initialized_components(self) -> List[str]:
        """Get list of initialized component names."""
        return [c.name for c in self._initialized]

    def get_status(self) -> Dict[str, Any]:
        """Get transaction status."""
        return {
            "in_transaction": self._in_transaction,
            "committed": self._committed,
            "initialized_count": len(self._initialized),
            "components": [c.name for c in self._initialized],
        }


class InitializationTimeoutManager:
    """
    v14.0: Manages initialization timeouts with circuit breaker pattern.

    Prevents hung initializations and provides fallback when components
    consistently timeout.
    """

    @dataclass
    class TimeoutConfig:
        """Timeout configuration for a component."""
        default_timeout: float = 30.0
        max_timeout: float = 120.0
        min_timeout: float = 5.0
        backoff_factor: float = 1.5
        max_failures: int = 3

    def __init__(self, default_config: TimeoutConfig = None):
        self._default_config = default_config or self.TimeoutConfig()
        self._component_configs: Dict[str, InitializationTimeoutManager.TimeoutConfig] = {}
        self._failure_counts: Dict[str, int] = {}
        self._current_timeouts: Dict[str, float] = {}
        self._circuit_open: Set[str] = set()
        self._lock = asyncio.Lock()
        self.logger = logging.getLogger("v14.TimeoutManager")

    def configure_component(
        self,
        component: str,
        config: TimeoutConfig,
    ) -> None:
        """
        Configure timeout settings for a component.

        Args:
            component: Component name
            config: Timeout configuration
        """
        self._component_configs[component] = config
        self._current_timeouts[component] = config.default_timeout

    def get_timeout(self, component: str) -> float:
        """
        Get current timeout for a component.

        Args:
            component: Component name

        Returns:
            Timeout in seconds
        """
        if component in self._circuit_open:
            # Circuit is open - use max timeout
            config = self._component_configs.get(component, self._default_config)
            return config.max_timeout

        return self._current_timeouts.get(
            component,
            self._default_config.default_timeout,
        )

    async def record_success(self, component: str) -> None:
        """Record successful initialization."""
        async with self._lock:
            # Reset failure count
            self._failure_counts[component] = 0

            # Close circuit if open
            if component in self._circuit_open:
                self._circuit_open.discard(component)
                self.logger.info(f"Circuit closed for: {component}")

            # Reset timeout to default
            config = self._component_configs.get(component, self._default_config)
            self._current_timeouts[component] = config.default_timeout

    async def record_timeout(self, component: str) -> bool:
        """
        Record a timeout failure.

        Returns:
            True if circuit is now open (should skip future attempts)
        """
        async with self._lock:
            self._failure_counts[component] = self._failure_counts.get(component, 0) + 1
            config = self._component_configs.get(component, self._default_config)

            # Increase timeout with backoff
            current = self._current_timeouts.get(component, config.default_timeout)
            new_timeout = min(current * config.backoff_factor, config.max_timeout)
            self._current_timeouts[component] = new_timeout

            # Check if circuit should open
            if self._failure_counts[component] >= config.max_failures:
                self._circuit_open.add(component)
                self.logger.warning(
                    f"Circuit opened for {component} after {config.max_failures} failures"
                )
                return True

            self.logger.debug(
                f"Timeout recorded for {component} "
                f"(failures: {self._failure_counts[component]}, new timeout: {new_timeout:.1f}s)"
            )
            return False

    def is_circuit_open(self, component: str) -> bool:
        """Check if circuit is open for a component."""
        return component in self._circuit_open

    def reset_circuit(self, component: str) -> None:
        """Manually reset circuit for a component."""
        self._circuit_open.discard(component)
        self._failure_counts[component] = 0
        config = self._component_configs.get(component, self._default_config)
        self._current_timeouts[component] = config.default_timeout

    def get_status(self) -> Dict[str, Any]:
        """Get manager status."""
        return {
            "components_configured": len(self._component_configs),
            "open_circuits": list(self._circuit_open),
            "failure_counts": self._failure_counts.copy(),
            "current_timeouts": self._current_timeouts.copy(),
        }


class ResilientBootstrapLayer:
    """
    v14.0: Master orchestrator for resilient system initialization.

    Combines all v14.0 components to provide:
    - Pre-flight validation
    - Async method safety
    - Dependency-ordered initialization
    - Health-gated startup
    - Graceful degradation
    - Transaction rollback
    - Timeout management
    """

    def __init__(self):
        # Core components
        self.preflight_validator = PreflightDirectoryValidator()
        self.async_validator = AsyncMethodValidator()
        self.dependency_graph = ComponentDependencyGraph()
        self.health_gate = HealthGatedInitializer(self.dependency_graph)
        self.degradation_registry = GracefulDegradationRegistry()
        self.transaction = BootstrapTransaction()
        self.timeout_manager = InitializationTimeoutManager()

        # State
        self._initialized = False
        self._lock = asyncio.Lock()
        self.logger = logging.getLogger("v14.ResilientBootstrap")

        # Metrics
        self._metrics = {
            "initialization_start": None,
            "initialization_end": None,
            "total_components": 0,
            "successful_components": 0,
            "degraded_components": 0,
            "failed_components": 0,
        }

    async def initialize(self) -> Tuple[bool, Dict[str, Any]]:
        """
        Run the full initialization sequence.

        Returns:
            Tuple of (success, detailed_results)
        """
        async with self._lock:
            if self._initialized:
                return True, {"status": "already_initialized"}

            self._metrics["initialization_start"] = time.time()
            results = {
                "preflight": None,
                "components": {},
                "degraded": [],
                "failed": [],
                "warnings": [],
            }

            try:
                # Phase 1: Pre-flight validation
                self.logger.info("═" * 60)
                self.logger.info("v14.0: Starting Resilient Bootstrap")
                self.logger.info("═" * 60)

                self.logger.info("Phase 1: Pre-flight validation...")
                preflight_ok, preflight_results = await self.preflight_validator.validate_all()
                results["preflight"] = preflight_results

                if not preflight_ok:
                    self.logger.error("Pre-flight validation failed!")
                    return False, results

                self.logger.info(f"  ✅ Pre-flight: {len(preflight_results['validated'])} directories validated")

                # Phase 2: Begin transaction
                self.logger.info("Phase 2: Beginning bootstrap transaction...")
                await self.transaction.begin()

                # Phase 3: Get initialization order
                self.logger.info("Phase 3: Computing initialization order...")
                try:
                    init_order = self.dependency_graph.get_initialization_order()
                    self._metrics["total_components"] = len(init_order)
                    self.logger.info(f"  ✅ {len(init_order)} components to initialize")
                except ValueError as e:
                    self.logger.error(f"Dependency resolution failed: {e}")
                    await self.transaction.rollback(str(e))
                    return False, results

                # Phase 4: Initialize components in order
                self.logger.info("Phase 4: Initializing components...")

                for component in init_order:
                    component_result = await self._initialize_component(component)
                    results["components"][component] = component_result

                    if component_result["status"] == "success":
                        self._metrics["successful_components"] += 1
                    elif component_result["status"] == "degraded":
                        self._metrics["degraded_components"] += 1
                        results["degraded"].append(component)
                    else:
                        self._metrics["failed_components"] += 1
                        results["failed"].append(component)

                        # Check if critical
                        info = self.dependency_graph.get_component_info(component)
                        if info and info.get("critical", False):
                            self.logger.error(f"Critical component failed: {component}")
                            await self.transaction.rollback(f"Critical failure: {component}")
                            return False, results

                # Phase 5: Commit transaction
                self.logger.info("Phase 5: Committing transaction...")
                await self.transaction.commit()

                self._initialized = True
                self._metrics["initialization_end"] = time.time()

                duration = self._metrics["initialization_end"] - self._metrics["initialization_start"]

                self.logger.info("═" * 60)
                self.logger.info(f"v14.0: Bootstrap Complete ({duration:.2f}s)")
                self.logger.info(f"  ✅ Successful: {self._metrics['successful_components']}")
                self.logger.info(f"  ⚠️ Degraded: {self._metrics['degraded_components']}")
                self.logger.info(f"  ❌ Failed: {self._metrics['failed_components']}")
                self.logger.info("═" * 60)

                return True, results

            except Exception as e:
                self.logger.error(f"Bootstrap failed with exception: {e}", exc_info=True)
                await self.transaction.rollback(str(e))
                return False, results

    async def _initialize_component(
        self,
        component: str,
    ) -> Dict[str, Any]:
        """
        Initialize a single component with full safety checks.

        Returns:
            Dict with initialization result
        """
        result = {
            "component": component,
            "status": "pending",
            "start_time": time.time(),
            "end_time": None,
            "error": None,
        }

        try:
            # Check if circuit is open
            if self.timeout_manager.is_circuit_open(component):
                self.logger.warning(f"  ⏭️ {component}: Circuit open, skipping")
                result["status"] = "skipped"
                result["error"] = "Circuit breaker open"
                return result

            # Check health gate
            can_proceed, unhealthy = await self.health_gate.gate_initialization(component)
            if not can_proceed:
                self.logger.warning(f"  ⏸️ {component}: Waiting for deps: {unhealthy}")

                # Try waiting for deps
                for dep in unhealthy:
                    await self.health_gate.wait_for_healthy(dep, timeout=10.0)

                # Re-check
                can_proceed, unhealthy = await self.health_gate.gate_initialization(component)
                if not can_proceed:
                    result["status"] = "blocked"
                    result["error"] = f"Dependencies not healthy: {unhealthy}"
                    return result

            # Get timeout
            timeout = self.timeout_manager.get_timeout(component)

            self.logger.info(f"  🚀 {component}: Initializing (timeout: {timeout:.1f}s)...")

            # Component initialization would happen here
            # For now, we just mark as success
            # In actual usage, this would call the component's init function

            result["status"] = "success"
            result["end_time"] = time.time()

            self.dependency_graph.mark_initialized(component)
            await self.timeout_manager.record_success(component)

            duration = result["end_time"] - result["start_time"]
            self.logger.info(f"  ✅ {component}: Initialized ({duration:.2f}s)")

            return result

        except asyncio.TimeoutError:
            result["status"] = "timeout"
            result["error"] = "Initialization timeout"
            result["end_time"] = time.time()

            circuit_opened = await self.timeout_manager.record_timeout(component)
            if circuit_opened:
                self.logger.error(f"  ❌ {component}: Timeout (circuit opened)")
            else:
                self.logger.warning(f"  ⚠️ {component}: Timeout")

            return result

        except Exception as e:
            result["status"] = "error"
            result["error"] = str(e)
            result["end_time"] = time.time()

            self.dependency_graph.mark_failed(component)
            self.logger.error(f"  ❌ {component}: {e}")

            return result

    def register_component(
        self,
        name: str,
        dependencies: List[str] = None,
        optional_deps: List[str] = None,
        critical: bool = False,
        init_timeout: float = 30.0,
        health_check: Callable[[], Awaitable[bool]] = None,
    ) -> None:
        """
        Register a component for managed initialization.

        Args:
            name: Component name
            dependencies: Required dependencies
            optional_deps: Optional dependencies
            critical: Whether failure should abort startup
            init_timeout: Initialization timeout
            health_check: Optional health check function
        """
        self.dependency_graph.register_component(
            name=name,
            dependencies=dependencies,
            optional_deps=optional_deps,
            critical=critical,
            init_timeout=init_timeout,
        )

        if health_check:
            self.health_gate.register_health_check(name, health_check)

        self.timeout_manager.configure_component(
            name,
            InitializationTimeoutManager.TimeoutConfig(
                default_timeout=init_timeout,
            ),
        )

    async def shutdown(self) -> None:
        """Shutdown the bootstrap layer."""
        self.logger.info("v14.0: Shutting down bootstrap layer...")

        # Attempt recovery of degraded components before shutdown
        await self.degradation_registry.attempt_all_recoveries()

        self._initialized = False
        self.logger.info("v14.0: Bootstrap layer shutdown complete")

    def get_status(self) -> Dict[str, Any]:
        """Get comprehensive status."""
        return {
            "initialized": self._initialized,
            "metrics": self._metrics.copy(),
            "preflight": self.preflight_validator.get_status(),
            "async_validator": self.async_validator.get_status(),
            "dependency_graph": self.dependency_graph.get_status(),
            "health_gate": self.health_gate.get_status(),
            "degradation": self.degradation_registry.get_status(),
            "transaction": self.transaction.get_status(),
            "timeout_manager": self.timeout_manager.get_status(),
        }


# =============================================================================
# v14.0 Global Instance and Accessors
# =============================================================================

_resilient_bootstrap: Optional[ResilientBootstrapLayer] = None


def get_resilient_bootstrap() -> ResilientBootstrapLayer:
    """Get or create the global resilient bootstrap layer."""
    global _resilient_bootstrap

    if _resilient_bootstrap is None:
        _resilient_bootstrap = ResilientBootstrapLayer()

    return _resilient_bootstrap


async def initialize_resilient_bootstrap() -> Tuple[bool, Dict[str, Any]]:
    """Initialize the resilient bootstrap layer."""
    bootstrap = get_resilient_bootstrap()
    return await bootstrap.initialize()


async def shutdown_resilient_bootstrap() -> None:
    """Shutdown the resilient bootstrap layer."""
    global _resilient_bootstrap

    if _resilient_bootstrap:
        await _resilient_bootstrap.shutdown()
        _resilient_bootstrap = None


def get_resilient_bootstrap_status() -> Dict[str, Any]:
    """Get comprehensive bootstrap status."""
    bootstrap = get_resilient_bootstrap()
    return bootstrap.get_status()


# Convenience function for component registration
def register_managed_component(
    name: str,
    dependencies: List[str] = None,
    optional_deps: List[str] = None,
    critical: bool = False,
    init_timeout: float = 30.0,
    health_check: Callable[[], Awaitable[bool]] = None,
) -> None:
    """
    Register a component for managed initialization.

    Example:
        register_managed_component(
            name="experience_forwarder",
            dependencies=["lock_manager", "event_bus"],
            critical=False,
            init_timeout=15.0,
        )
    """
    bootstrap = get_resilient_bootstrap()
    bootstrap.register_component(
        name=name,
        dependencies=dependencies,
        optional_deps=optional_deps,
        critical=critical,
        init_timeout=init_timeout,
        health_check=health_check,
    )
