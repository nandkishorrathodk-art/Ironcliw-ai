#!/usr/bin/env python3
"""
ðŸ”¬ ADVANCED BIOMETRIC FEATURE EXTRACTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Extracts comprehensive voice biometric features:
- Deep learning embeddings (ECAPA-TDNN)
- Acoustic features (pitch, formants, spectral)
- Voice quality metrics (jitter, shimmer, HNR)
- Temporal characteristics (speaking rate, rhythm)
- Energy contours

All async with proper thread pool execution for CPU-intensive work.
Zero hardcoding, fully dynamic.

Author: Claude Code + Derek J. Russell
Version: 2.0.0 - Async Optimized
"""

import asyncio
import logging
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from functools import partial
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch

# Import managed executor for clean shutdown
try:
    from core.thread_manager import ManagedThreadPoolExecutor
    _HAS_MANAGED_EXECUTOR = True
except ImportError:
    _HAS_MANAGED_EXECUTOR = False

logger = logging.getLogger(__name__)

# Shared thread pool for CPU-intensive feature extraction
# Using a dedicated pool prevents blocking the main event loop
_feature_executor: Optional[ThreadPoolExecutor] = None


def get_feature_executor() -> ThreadPoolExecutor:
    """Get or create the shared thread pool for feature extraction."""
    global _feature_executor
    if _feature_executor is None:
        # Use 4 workers for parallel feature extraction
        if _HAS_MANAGED_EXECUTOR:
            _feature_executor = ManagedThreadPoolExecutor(
                max_workers=4,
                thread_name_prefix="feature_extract",
                name="feature_extract"
            )
        else:
            _feature_executor = ThreadPoolExecutor(
                max_workers=4,
                thread_name_prefix="feature_extract"
            )
        logger.info("ðŸ”§ Created feature extraction thread pool (4 workers)")
    return _feature_executor


class AdvancedFeatureExtractor:
    """
    ðŸ”¬ Advanced biometric feature extraction with proper async execution.

    All CPU-intensive numpy/scipy operations run in a thread pool
    to prevent blocking the async event loop.
    """

    def __init__(self, sample_rate: int = 16000):
        self.sample_rate = sample_rate
        self._executor = get_feature_executor()

    async def _run_in_executor(self, func, *args, **kwargs) -> Any:
        """Run a CPU-intensive function in the thread pool."""
        loop = asyncio.get_running_loop()
        if kwargs:
            func = partial(func, **kwargs)
        return await loop.run_in_executor(self._executor, func, *args)

    async def extract_features(
        self,
        audio_tensor: torch.Tensor,
        embedding: np.ndarray,
        transcription: str = ""
    ) -> "VoiceBiometricFeatures":
        """
        Extract comprehensive biometric features.

        All CPU-intensive work runs in thread pool to avoid blocking.

        Args:
            audio_tensor: Audio as torch tensor
            embedding: Pre-computed ECAPA-TDNN embedding
            transcription: Optional transcription

        Returns:
            VoiceBiometricFeatures object
        """
        from voice.advanced_biometric_verification import VoiceBiometricFeatures

        # Convert to numpy (quick operation)
        # CRITICAL: Use .copy() to avoid memory corruption when tensor is GC'd
        audio_np = audio_tensor.cpu().numpy().copy() if torch.is_tensor(audio_tensor) else np.asarray(audio_tensor)

        # Extract all features in parallel using thread pool
        # Each extraction runs in its own thread, truly parallel
        results = await asyncio.gather(
            self._extract_pitch_features_async(audio_np),
            self._extract_formants_async(audio_np),
            self._extract_spectral_features_async(audio_np),
            self._extract_temporal_features_async(audio_np, transcription),
            self._extract_voice_quality_async(audio_np),
            return_exceptions=True
        )

        pitch_features, formants, spectral_features, temporal_features, quality_features = results

        # Handle any exceptions with safe defaults
        if isinstance(pitch_features, Exception):
            logger.warning(f"Pitch extraction failed: {pitch_features}")
            pitch_features = {'mean': 150.0, 'std': 20.0, 'range': 50.0}

        if isinstance(formants, Exception):
            logger.warning(f"Formant extraction failed: {formants}")
            formants = [500.0, 1500.0, 2500.0, 3500.0]

        if isinstance(spectral_features, Exception):
            logger.warning(f"Spectral extraction failed: {spectral_features}")
            spectral_features = {'centroid': 1000.0, 'rolloff': 2000.0, 'flux': 0.1, 'entropy': 0.5}

        if isinstance(temporal_features, Exception):
            logger.warning(f"Temporal extraction failed: {temporal_features}")
            temporal_features = {'rate': 0.0, 'pause_ratio': 0.3, 'energy': np.zeros(100)}

        if isinstance(quality_features, Exception):
            logger.warning(f"Quality extraction failed: {quality_features}")
            quality_features = {'jitter': 0.01, 'shimmer': 0.05, 'hnr': 15.0}

        # Create feature object
        features = VoiceBiometricFeatures(
            embedding=embedding,
            embedding_confidence=0.9,
            pitch_mean=pitch_features['mean'],
            pitch_std=pitch_features['std'],
            pitch_range=pitch_features['range'],
            formant_f1=formants[0],
            formant_f2=formants[1],
            formant_f3=formants[2],
            formant_f4=formants[3],
            spectral_centroid=spectral_features['centroid'],
            spectral_rolloff=spectral_features['rolloff'],
            spectral_flux=spectral_features['flux'],
            spectral_entropy=spectral_features['entropy'],
            speaking_rate=temporal_features['rate'],
            pause_ratio=temporal_features['pause_ratio'],
            energy_contour=temporal_features['energy'],
            jitter=quality_features['jitter'],
            shimmer=quality_features['shimmer'],
            harmonic_to_noise_ratio=quality_features['hnr'],
            duration_seconds=len(audio_np) / self.sample_rate,
            sample_rate=self.sample_rate
        )

        return features

    # =========================================================================
    # ASYNC WRAPPERS - Run CPU-intensive work in thread pool
    # =========================================================================

    async def _extract_pitch_features_async(self, audio: np.ndarray) -> dict:
        """Async wrapper for pitch extraction."""
        return await self._run_in_executor(self._extract_pitch_features_sync, audio)

    async def _extract_formants_async(self, audio: np.ndarray) -> list:
        """Async wrapper for formant extraction."""
        return await self._run_in_executor(self._extract_formants_sync, audio)

    async def _extract_spectral_features_async(self, audio: np.ndarray) -> dict:
        """Async wrapper for spectral feature extraction."""
        return await self._run_in_executor(self._extract_spectral_features_sync, audio)

    async def _extract_temporal_features_async(self, audio: np.ndarray, transcription: str) -> dict:
        """Async wrapper for temporal feature extraction."""
        return await self._run_in_executor(self._extract_temporal_features_sync, audio, transcription)

    async def _extract_voice_quality_async(self, audio: np.ndarray) -> dict:
        """Async wrapper for voice quality extraction."""
        return await self._run_in_executor(self._extract_voice_quality_sync, audio)

    # =========================================================================
    # SYNC IMPLEMENTATIONS - Run in thread pool
    # =========================================================================

    def _extract_pitch_features_sync(self, audio: np.ndarray) -> dict:
        """Extract pitch features using autocorrelation (CPU-intensive)."""
        frame_size = 2048
        hop_size = 512
        pitches = []

        for i in range(0, len(audio) - frame_size, hop_size):
            frame = audio[i:i + frame_size]

            # Autocorrelation
            correlation = np.correlate(frame, frame, mode='full')
            correlation = correlation[len(correlation) // 2:]

            # Peak detection
            min_lag = int(self.sample_rate / 500)
            max_lag = int(self.sample_rate / 50)

            if max_lag < len(correlation):
                search_region = correlation[min_lag:max_lag]
                if len(search_region) > 0 and correlation[0] > 0:
                    peak_lag = min_lag + np.argmax(search_region)
                    if correlation[peak_lag] > 0.3 * correlation[0]:
                        pitch = self.sample_rate / peak_lag
                        if 50 <= pitch <= 500:
                            pitches.append(pitch)

        if pitches:
            return {
                'mean': float(np.mean(pitches)),
                'std': float(np.std(pitches)),
                'range': float(np.max(pitches) - np.min(pitches))
            }
        else:
            return {'mean': 150.0, 'std': 20.0, 'range': 50.0}

    def _extract_formants_sync(self, audio: np.ndarray) -> list:
        """Extract formant frequencies using LPC (CPU-intensive)."""
        try:
            # Pre-emphasis
            emphasized = np.append(audio[0], audio[1:] - 0.97 * audio[:-1])

            # FFT
            fft = np.fft.rfft(emphasized)
            magnitude = np.abs(fft)
            freqs = np.fft.rfftfreq(len(audio), 1 / self.sample_rate)

            # Find peaks
            from scipy.signal import find_peaks
            peaks, _ = find_peaks(magnitude, height=np.max(magnitude) * 0.1, distance=20)

            if len(peaks) >= 4:
                formants = [float(freqs[p]) for p in peaks[:4]]
            else:
                formants = [500.0, 1500.0, 2500.0, 3500.0]

            return formants

        except Exception as e:
            logger.debug(f"Formant extraction failed: {e}")
            return [500.0, 1500.0, 2500.0, 3500.0]

    def _extract_spectral_features_sync(self, audio: np.ndarray) -> dict:
        """Extract spectral features (CPU-intensive FFT operations)."""
        fft = np.fft.rfft(audio)
        magnitude = np.abs(fft)
        power = magnitude ** 2
        freqs = np.fft.rfftfreq(len(audio), 1 / self.sample_rate)

        # Spectral centroid
        centroid = float(np.sum(freqs * magnitude) / (np.sum(magnitude) + 1e-10))

        # Spectral rolloff
        cumsum = np.cumsum(power)
        rolloff_threshold = 0.85 * cumsum[-1]
        rolloff_idx = np.where(cumsum >= rolloff_threshold)[0]
        rolloff = float(freqs[rolloff_idx[0]] if len(rolloff_idx) > 0 else freqs[-1])

        # Spectral flux
        flux = float(np.std(magnitude))

        # Spectral entropy
        power_norm = power / (np.sum(power) + 1e-10)
        entropy = float(-np.sum(power_norm * np.log2(power_norm + 1e-10)))

        return {
            'centroid': centroid,
            'rolloff': rolloff,
            'flux': flux,
            'entropy': entropy
        }

    def _extract_temporal_features_sync(self, audio: np.ndarray, transcription: str) -> dict:
        """Extract temporal features (CPU-intensive)."""
        duration = len(audio) / self.sample_rate

        # Speaking rate (words per minute)
        word_count = len(transcription.split()) if transcription else 0
        speaking_rate = (word_count / duration) * 60 if duration > 0 else 0.0

        # Energy contour (frame-based)
        frame_size = self.sample_rate // 20
        num_frames = len(audio) // frame_size

        energy_contour = []
        pauses = 0
        mean_energy = np.mean(audio ** 2)

        for i in range(num_frames):
            frame = audio[i * frame_size:(i + 1) * frame_size]
            energy = np.sum(frame ** 2)
            energy_contour.append(energy)

            if energy < mean_energy * 0.05:
                pauses += 1

        pause_ratio = pauses / max(num_frames, 1)

        return {
            'rate': float(speaking_rate),
            'pause_ratio': float(pause_ratio),
            'energy': np.array(energy_contour)
        }

    def _extract_voice_quality_sync(self, audio: np.ndarray) -> dict:
        """Extract voice quality metrics (CPU-intensive).

        IMPORTANT: For long audio, we sample a representative chunk to avoid
        O(nÂ²) autocorrelation complexity that would hang for 30+ seconds.
        """
        # Limit audio length for voice quality analysis (max 3 seconds)
        # Voice quality metrics don't need the entire audio
        max_samples = int(self.sample_rate * 3)  # 3 seconds max
        if len(audio) > max_samples:
            # Take middle portion for best representation
            start = (len(audio) - max_samples) // 2
            audio_chunk = audio[start:start + max_samples]
        else:
            audio_chunk = audio

        jitter = self._compute_jitter_sync(audio_chunk)
        shimmer = self._compute_shimmer_sync(audio_chunk)
        hnr = self._compute_hnr_sync(audio_chunk)

        return {
            'jitter': float(jitter),
            'shimmer': float(shimmer),
            'hnr': float(hnr)
        }

    def _compute_jitter_sync(self, audio: np.ndarray) -> float:
        """Compute jitter (pitch period variation)."""
        try:
            frame_size = 2048
            hop_size = 512
            periods = []

            for i in range(0, len(audio) - frame_size, hop_size):
                frame = audio[i:i + frame_size]
                correlation = np.correlate(frame, frame, mode='full')
                correlation = correlation[len(correlation) // 2:]

                min_lag = int(self.sample_rate / 500)
                max_lag = int(self.sample_rate / 50)

                if max_lag < len(correlation) and correlation[0] > 0:
                    search_region = correlation[min_lag:max_lag]
                    if len(search_region) > 0:
                        peak_lag = min_lag + np.argmax(search_region)
                        if correlation[peak_lag] > 0.3 * correlation[0]:
                            period = peak_lag / self.sample_rate
                            periods.append(period)

            if len(periods) > 1:
                diffs = np.abs(np.diff(periods))
                jitter = np.mean(diffs) / np.mean(periods)
                return min(jitter, 0.1)

            return 0.01

        except Exception:
            return 0.01

    def _compute_shimmer_sync(self, audio: np.ndarray) -> float:
        """Compute shimmer (amplitude variation)."""
        try:
            frame_size = int(self.sample_rate * 0.01)  # 10ms frames
            num_frames = len(audio) // frame_size

            peaks = []
            for i in range(num_frames):
                frame = audio[i * frame_size:(i + 1) * frame_size]
                peak = np.max(np.abs(frame))
                peaks.append(peak)

            if len(peaks) > 1:
                diffs = np.abs(np.diff(peaks))
                shimmer = np.mean(diffs) / (np.mean(peaks) + 1e-10)
                return min(shimmer, 0.5)

            return 0.05

        except Exception:
            return 0.05

    def _compute_hnr_sync(self, audio: np.ndarray) -> float:
        """Compute Harmonic-to-Noise Ratio.

        Uses a single representative frame instead of full-length autocorrelation
        to avoid O(nÂ²) complexity that would hang on long audio.
        """
        try:
            # Use a single frame for HNR (much faster than full-length autocorrelation)
            # Take a frame from the middle of the audio for best representation
            frame_size = min(4096, len(audio))
            start = max(0, (len(audio) - frame_size) // 2)
            frame = audio[start:start + frame_size]

            correlation = np.correlate(frame, frame, mode='full')
            correlation = correlation[len(correlation) // 2:]

            min_lag = int(self.sample_rate / 500)
            max_lag = int(self.sample_rate / 50)

            if max_lag < len(correlation) and correlation[0] > 0:
                search_region = correlation[min_lag:max_lag]
                if len(search_region) > 0:
                    peak_idx = min_lag + np.argmax(search_region)
                    peak_value = correlation[peak_idx]

                    noise_floor = np.median(correlation[min_lag:max_lag])
                    hnr_linear = peak_value / (noise_floor + 1e-10)
                    hnr_db = 10 * np.log10(hnr_linear + 1e-10)

                    # Handle NaN (can occur with noise-only audio)
                    if np.isnan(hnr_db):
                        return 15.0

                    return float(np.clip(hnr_db, 0.0, 40.0))

            return 15.0

        except Exception:
            return 15.0


def shutdown_feature_executor():
    """Shutdown the feature extraction thread pool gracefully."""
    global _feature_executor
    if _feature_executor is not None:
        logger.info("ðŸ”§ Shutting down feature extraction thread pool...")
        _feature_executor.shutdown(wait=True)
        _feature_executor = None
