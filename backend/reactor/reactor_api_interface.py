"""
Reactor Core FastAPI Interface v3.0 - Drop-in API Router for Training Services
===============================================================================

This module provides a FastAPI router that Reactor Core can import to expose
training APIs compatible with JARVIS's Advanced Training Coordinator v3.0.

Features:
- ðŸš€ Drop-in FastAPI router for Reactor Core
- ðŸ“¦ Drop-Box Protocol support for large dataset transfer
- ðŸ“¡ Server-Sent Events (SSE) for streaming training status
- ðŸ”’ Service Registry integration for dynamic discovery
- ðŸ§¹ Automatic dataset cleanup after training
- ðŸ“Š Comprehensive training metrics and monitoring
- âš¡ Async training execution (non-blocking)
- ðŸ”„ Training checkpoint support

Architecture:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              Reactor Core API Interface v3.0                     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                                                                   â”‚
    â”‚  POST /api/training/start                                        â”‚
    â”‚  â”œâ”€ Accepts inline experiences OR drop-box path                  â”‚
    â”‚  â”œâ”€ Validates request and starts training                        â”‚
    â”‚  â””â”€ Returns job_id immediately (async training)                  â”‚
    â”‚                                                                   â”‚
    â”‚  GET /api/training/stream/{job_id}                               â”‚
    â”‚  â”œâ”€ Server-Sent Events (SSE) stream                              â”‚
    â”‚  â”œâ”€ Real-time epoch progress, loss, metrics                      â”‚
    â”‚  â””â”€ Automatic cleanup on completion                              â”‚
    â”‚                                                                   â”‚
    â”‚  GET /api/training/status/{job_id}                               â”‚
    â”‚  â”œâ”€ Non-streaming status check                                   â”‚
    â”‚  â””â”€ Returns current training state                               â”‚
    â”‚                                                                   â”‚
    â”‚  POST /api/training/cancel/{job_id}                              â”‚
    â”‚  â””â”€ Graceful training cancellation                               â”‚
    â”‚                                                                   â”‚
    â”‚  GET /api/health                                                 â”‚
    â”‚  â””â”€ Health check for service registry                            â”‚
    â”‚                                                                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Usage in Reactor Core:
    from backend.reactor.reactor_api_interface import create_training_router

    app = FastAPI(title="Reactor Core", version="3.0.0")

    # Create router with your training engine
    router = create_training_router(
        training_engine=your_training_engine,
        service_name="reactor-core",
        port=8090
    )

    app.include_router(router)

Author: JARVIS AI System
Version: 3.0.0
"""

from __future__ import annotations

import asyncio
import gzip
import json
import logging
import os
import time
from dataclasses import dataclass, field, asdict
from enum import Enum
from pathlib import Path
from typing import Any, AsyncIterator, Callable, Dict, List, Optional, Protocol, runtime_checkable

from fastapi import APIRouter, BackgroundTasks, HTTPException, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)


# =============================================================================
# Configuration
# =============================================================================

@dataclass
class ReactorAPIConfig:
    """Configuration for Reactor API interface."""

    # Service identity
    service_name: str = field(
        default_factory=lambda: os.getenv("REACTOR_SERVICE_NAME", "reactor-core")
    )
    port: int = field(
        default_factory=lambda: int(os.getenv("REACTOR_CORE_PORT", "8090"))
    )
    # v117.0: Fixed to match actual Reactor-Core endpoint (was /api/health)
    health_endpoint: str = "/health"

    # Drop-box protocol
    dropbox_enabled: bool = field(
        default_factory=lambda: os.getenv("DROPBOX_ENABLED", "true").lower() == "true"
    )
    dropbox_dir: Path = field(
        default_factory=lambda: Path(os.getenv(
            "TRAINING_DROPBOX_DIR",
            str(Path.home() / ".jarvis" / "bridge" / "training_staging")
        ))
    )
    dropbox_cleanup_enabled: bool = field(
        default_factory=lambda: os.getenv("DROPBOX_CLEANUP_ENABLED", "true").lower() == "true"
    )

    # Training settings
    max_concurrent_jobs: int = field(
        default_factory=lambda: int(os.getenv("MAX_CONCURRENT_TRAINING_JOBS", "1"))
    )
    job_timeout_seconds: float = field(
        default_factory=lambda: float(os.getenv("TRAINING_JOB_TIMEOUT", "7200"))  # 2 hours
    )

    # Checkpointing
    checkpoint_enabled: bool = field(
        default_factory=lambda: os.getenv("CHECKPOINT_ENABLED", "true").lower() == "true"
    )
    checkpoint_dir: Path = field(
        default_factory=lambda: Path(os.getenv(
            "TRAINING_CHECKPOINT_DIR",
            str(Path.home() / ".jarvis" / "training_checkpoints")
        ))
    )


# =============================================================================
# Request/Response Models
# =============================================================================

class TrainingRequest(BaseModel):
    """Request model for starting training."""
    job_id: str = Field(..., description="Unique job identifier")
    model_type: str = Field(..., description="Type of model to train")

    # Either experiences OR dataset_path (drop-box protocol)
    experiences: Optional[List[Dict[str, Any]]] = Field(
        None, description="Inline training experiences (for small datasets)"
    )
    dataset_path: Optional[str] = Field(
        None, description="Path to dataset file (drop-box protocol)"
    )
    use_dropbox: bool = Field(
        False, description="Whether dataset_path uses drop-box protocol"
    )

    # Training configuration
    config: Dict[str, Any] = Field(
        default_factory=dict, description="Training configuration"
    )
    epochs: int = Field(10, ge=1, le=1000, description="Number of training epochs")
    batch_size: int = Field(32, ge=1, le=4096, description="Training batch size")
    learning_rate: float = Field(0.001, gt=0, lt=1, description="Learning rate")

    # Checkpointing
    checkpoint_enabled: bool = Field(True, description="Enable checkpointing")
    checkpoint_interval: int = Field(10, ge=1, description="Epochs between checkpoints")


class TrainingResponse(BaseModel):
    """Response model for training start."""
    job_id: str
    status: str
    message: str
    started_at: float
    estimated_duration_seconds: Optional[float] = None


class TrainingStatusResponse(BaseModel):
    """Response model for training status."""
    job_id: str
    status: str
    epoch: int
    total_epochs: int
    loss: float
    accuracy: Optional[float] = None
    metrics: Dict[str, Any] = Field(default_factory=dict)
    started_at: float
    updated_at: float
    error: Optional[str] = None


class HealthResponse(BaseModel):
    """Response model for health check."""
    status: str
    service: str
    version: str
    uptime_seconds: float
    active_jobs: int
    memory_usage_mb: float


# =============================================================================
# Training Engine Protocol
# =============================================================================

@runtime_checkable
class TrainingEngineProtocol(Protocol):
    """Protocol defining what a training engine must implement."""

    async def start_training(
        self,
        job_id: str,
        model_type: str,
        experiences: List[Dict[str, Any]],
        config: Dict[str, Any],
        epochs: int
    ) -> bool:
        """Start training job."""
        ...

    async def get_status(self, job_id: str) -> Dict[str, Any]:
        """Get current training status."""
        ...

    async def stream_status(self, job_id: str) -> AsyncIterator[Dict[str, Any]]:
        """Stream training status updates."""
        ...

    async def cancel(self, job_id: str) -> bool:
        """Cancel training job."""
        ...


# =============================================================================
# Training Job Manager
# =============================================================================

class TrainingJobStatus(str, Enum):
    """Training job status."""
    PENDING = "pending"
    RUNNING = "running"
    TRAINING = "training"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class TrainingJob:
    """Internal representation of a training job."""
    job_id: str
    model_type: str
    status: TrainingJobStatus
    epochs: int
    current_epoch: int = 0
    loss: float = 0.0
    accuracy: float = 0.0
    metrics: Dict[str, Any] = field(default_factory=dict)
    started_at: float = field(default_factory=time.time)
    updated_at: float = field(default_factory=time.time)
    error: Optional[str] = None
    task: Optional[asyncio.Task] = None


class TrainingJobManager:
    """
    Manages training jobs and their lifecycle.

    This is a default implementation that can be replaced with a custom
    training engine that implements TrainingEngineProtocol.
    """

    def __init__(self, config: ReactorAPIConfig):
        self.config = config
        self._jobs: Dict[str, TrainingJob] = {}
        self._startup_time = time.time()

    async def start_training(
        self,
        job_id: str,
        model_type: str,
        experiences: List[Dict[str, Any]],
        config: Dict[str, Any],
        epochs: int
    ) -> bool:
        """Start a training job."""
        if job_id in self._jobs:
            logger.warning(f"Job {job_id} already exists")
            return False

        if len(self._jobs) >= self.config.max_concurrent_jobs:
            logger.warning("Max concurrent jobs reached")
            return False

        job = TrainingJob(
            job_id=job_id,
            model_type=model_type,
            status=TrainingJobStatus.RUNNING,
            epochs=epochs
        )

        self._jobs[job_id] = job

        # Start training in background task
        job.task = asyncio.create_task(
            self._run_training(job, experiences, config)
        )

        logger.info(f"Started training job: {job_id}")
        return True

    async def _run_training(
        self,
        job: TrainingJob,
        experiences: List[Dict[str, Any]],
        config: Dict[str, Any]
    ) -> None:
        """Run training simulation (replace with actual training logic)."""
        try:
            job.status = TrainingJobStatus.TRAINING

            for epoch in range(job.epochs):
                if job.status == TrainingJobStatus.CANCELLED:
                    logger.info(f"Training cancelled: {job.job_id}")
                    return

                # Simulate training epoch
                await asyncio.sleep(0.5)  # Replace with actual training

                job.current_epoch = epoch + 1
                job.loss = max(0.01, 1.0 - (epoch / job.epochs) * 0.9 + 0.1 * (0.5 - asyncio.get_event_loop().time() % 1))
                job.accuracy = min(0.99, (epoch / job.epochs) * 0.95)
                job.updated_at = time.time()

                logger.debug(f"Epoch {job.current_epoch}/{job.epochs}: loss={job.loss:.4f}")

            job.status = TrainingJobStatus.COMPLETED
            job.metrics["final_loss"] = job.loss
            job.metrics["final_accuracy"] = job.accuracy

            logger.info(f"Training completed: {job.job_id}")

        except Exception as e:
            job.status = TrainingJobStatus.FAILED
            job.error = str(e)
            logger.error(f"Training failed: {job.job_id} - {e}")

    async def get_status(self, job_id: str) -> Dict[str, Any]:
        """Get current training status."""
        job = self._jobs.get(job_id)
        if not job:
            raise KeyError(f"Job {job_id} not found")

        return {
            "job_id": job.job_id,
            "status": job.status.value,
            "epoch": job.current_epoch,
            "total_epochs": job.epochs,
            "loss": job.loss,
            "accuracy": job.accuracy,
            "metrics": job.metrics,
            "started_at": job.started_at,
            "updated_at": job.updated_at,
            "error": job.error
        }

    async def stream_status(self, job_id: str) -> AsyncIterator[Dict[str, Any]]:
        """Stream training status updates."""
        job = self._jobs.get(job_id)
        if not job:
            raise KeyError(f"Job {job_id} not found")

        last_epoch = -1

        while job.status in (TrainingJobStatus.RUNNING, TrainingJobStatus.TRAINING):
            if job.current_epoch != last_epoch:
                last_epoch = job.current_epoch
                yield await self.get_status(job_id)

            await asyncio.sleep(0.1)

        # Final status
        yield await self.get_status(job_id)

    async def cancel(self, job_id: str) -> bool:
        """Cancel a training job."""
        job = self._jobs.get(job_id)
        if not job:
            return False

        if job.status in (TrainingJobStatus.RUNNING, TrainingJobStatus.TRAINING):
            job.status = TrainingJobStatus.CANCELLED
            if job.task and not job.task.done():
                job.task.cancel()
            logger.info(f"Cancelled training job: {job_id}")
            return True

        return False

    def get_active_job_count(self) -> int:
        """Get number of active training jobs."""
        return sum(
            1 for job in self._jobs.values()
            if job.status in (TrainingJobStatus.RUNNING, TrainingJobStatus.TRAINING)
        )

    def get_uptime(self) -> float:
        """Get manager uptime in seconds."""
        return time.time() - self._startup_time


# =============================================================================
# Drop-Box Protocol Handler
# =============================================================================

class DropBoxHandler:
    """Handles drop-box protocol for large dataset transfer."""

    def __init__(self, config: ReactorAPIConfig):
        self.config = config
        config.dropbox_dir.mkdir(parents=True, exist_ok=True)

    async def load_dataset(self, dataset_path: str) -> List[Dict[str, Any]]:
        """
        Load dataset from drop-box file.

        Supports both compressed (.json.gz) and uncompressed (.json) files.
        """
        path = Path(dataset_path)

        if not path.exists():
            raise FileNotFoundError(f"Dataset file not found: {dataset_path}")

        # Read file
        data = await asyncio.to_thread(path.read_bytes)

        # Decompress if needed
        if path.suffix == '.gz' or dataset_path.endswith('.json.gz'):
            try:
                data = gzip.decompress(data)
            except gzip.BadGzipFile:
                pass  # Not compressed

        # Parse JSON
        experiences = json.loads(data.decode('utf-8'))

        logger.info(f"Loaded {len(experiences)} experiences from drop-box: {path.name}")
        return experiences

    async def cleanup(self, dataset_path: str) -> bool:
        """Clean up drop-box file after training."""
        if not self.config.dropbox_cleanup_enabled:
            return False

        path = Path(dataset_path)

        if path.exists():
            await asyncio.to_thread(path.unlink)
            logger.debug(f"Cleaned up drop-box file: {path.name}")
            return True

        return False


# =============================================================================
# Service Registry Integration
# =============================================================================

async def register_with_service_registry(
    service_name: str,
    port: int,
    health_endpoint: str
) -> bool:
    """
    Register this service with the JARVIS service registry.

    This enables dynamic discovery by the Training Coordinator.
    """
    try:
        # Try to import service registry (optional dependency)
        from backend.core.service_registry import register_current_service

        await register_current_service(
            service_name=service_name,
            port=port,
            health_endpoint=health_endpoint,
            metadata={
                "version": "3.0.0",
                "capabilities": ["training", "dropbox", "streaming"]
            }
        )

        logger.info(f"Registered with service registry: {service_name}:{port}")
        return True

    except ImportError:
        logger.warning("Service registry not available - running standalone")
        return False
    except Exception as e:
        logger.error(f"Failed to register with service registry: {e}")
        return False


# =============================================================================
# Router Factory
# =============================================================================

def create_training_router(
    training_engine: Optional[TrainingEngineProtocol] = None,
    config: Optional[ReactorAPIConfig] = None,
    service_name: str = "reactor-core",
    port: int = 8090
) -> APIRouter:
    """
    Create FastAPI router for training API.

    Args:
        training_engine: Custom training engine implementing TrainingEngineProtocol.
                        If None, uses default TrainingJobManager.
        config: API configuration. If None, uses defaults from environment.
        service_name: Service name for registry registration.
        port: Port number for registry registration.

    Returns:
        FastAPI APIRouter ready to be included in the app.
    """
    config = config or ReactorAPIConfig(service_name=service_name, port=port)

    # Use provided engine or create default manager
    if training_engine is None:
        manager = TrainingJobManager(config)
    else:
        manager = training_engine

    dropbox_handler = DropBoxHandler(config)
    router = APIRouter(prefix="/api", tags=["training"])
    startup_time = time.time()

    # ==========================================================================
    # Lifecycle Events
    # ==========================================================================

    @router.on_event("startup")
    async def on_startup():
        """Register with service registry on startup."""
        await register_with_service_registry(
            service_name=config.service_name,
            port=config.port,
            health_endpoint=config.health_endpoint
        )

    # ==========================================================================
    # Endpoints
    # ==========================================================================

    @router.post("/training/start", response_model=TrainingResponse)
    async def start_training(
        request: TrainingRequest,
        background_tasks: BackgroundTasks
    ) -> TrainingResponse:
        """
        Start a training job.

        Supports two modes:
        1. Inline experiences: Send experiences directly in request body
        2. Drop-box protocol: Send path to pre-prepared dataset file
        """
        experiences = request.experiences or []

        # Load from drop-box if using drop-box protocol
        if request.use_dropbox and request.dataset_path:
            try:
                experiences = await dropbox_handler.load_dataset(request.dataset_path)

                # Schedule cleanup after training
                if config.dropbox_cleanup_enabled:
                    background_tasks.add_task(
                        dropbox_handler.cleanup,
                        request.dataset_path
                    )
            except FileNotFoundError:
                raise HTTPException(
                    status_code=404,
                    detail=f"Dataset file not found: {request.dataset_path}"
                )
            except Exception as e:
                raise HTTPException(
                    status_code=500,
                    detail=f"Failed to load dataset: {str(e)}"
                )

        if not experiences:
            raise HTTPException(
                status_code=400,
                detail="No training experiences provided"
            )

        # Start training
        success = await manager.start_training(
            job_id=request.job_id,
            model_type=request.model_type,
            experiences=experiences,
            config=request.config,
            epochs=request.epochs
        )

        if not success:
            raise HTTPException(
                status_code=503,
                detail="Failed to start training - max concurrent jobs reached"
            )

        return TrainingResponse(
            job_id=request.job_id,
            status="started",
            message=f"Training started with {len(experiences)} experiences",
            started_at=time.time(),
            estimated_duration_seconds=request.epochs * 0.5  # Rough estimate
        )

    @router.get("/training/stream/{job_id}")
    async def stream_training_status(job_id: str) -> StreamingResponse:
        """
        Stream training status updates using Server-Sent Events.

        Returns real-time updates on epoch progress, loss, and metrics.
        """
        async def event_generator() -> AsyncIterator[str]:
            try:
                async for status in manager.stream_status(job_id):
                    # Format as SSE
                    data = json.dumps(status)
                    yield f"event: status\ndata: {data}\n\n"

            except KeyError:
                error = {"error": f"Job {job_id} not found"}
                yield f"event: error\ndata: {json.dumps(error)}\n\n"
            except Exception as e:
                error = {"error": str(e)}
                yield f"event: error\ndata: {json.dumps(error)}\n\n"

        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive"
            }
        )

    @router.get("/training/status/{job_id}", response_model=TrainingStatusResponse)
    async def get_training_status(job_id: str) -> TrainingStatusResponse:
        """Get current training status (non-streaming)."""
        try:
            status = await manager.get_status(job_id)
            return TrainingStatusResponse(**status)
        except KeyError:
            raise HTTPException(
                status_code=404,
                detail=f"Job {job_id} not found"
            )

    @router.post("/training/cancel/{job_id}")
    async def cancel_training(job_id: str) -> Dict[str, Any]:
        """Cancel a running training job."""
        success = await manager.cancel(job_id)

        if success:
            return {"status": "cancelled", "job_id": job_id}
        else:
            raise HTTPException(
                status_code=404,
                detail=f"Job {job_id} not found or already completed"
            )

    @router.get("/health", response_model=HealthResponse)
    async def health_check() -> HealthResponse:
        """Health check endpoint for service registry."""
        import psutil

        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024

        return HealthResponse(
            status="healthy",
            service=config.service_name,
            version="3.0.0",
            uptime_seconds=time.time() - startup_time,
            active_jobs=manager.get_active_job_count() if hasattr(manager, 'get_active_job_count') else 0,
            memory_usage_mb=round(memory_mb, 2)
        )

    @router.get("/metrics")
    async def get_metrics() -> Dict[str, Any]:
        """Get training metrics and statistics."""
        return {
            "uptime_seconds": time.time() - startup_time,
            "active_jobs": manager.get_active_job_count() if hasattr(manager, 'get_active_job_count') else 0,
            "config": {
                "max_concurrent_jobs": config.max_concurrent_jobs,
                "dropbox_enabled": config.dropbox_enabled,
                "checkpoint_enabled": config.checkpoint_enabled
            }
        }

    return router


# =============================================================================
# Standalone App Factory (for testing)
# =============================================================================

def create_standalone_app():
    """
    Create standalone FastAPI app for testing or standalone deployment.

    Usage:
        uvicorn backend.reactor.reactor_api_interface:app --port 8090
    """
    from fastapi import FastAPI

    app = FastAPI(
        title="Reactor Core Training API",
        description="Enterprise-grade training API for JARVIS ecosystem",
        version="3.0.0"
    )

    router = create_training_router()
    app.include_router(router)

    @app.get("/")
    async def root():
        return {
            "service": "reactor-core",
            "version": "3.0.0",
            "status": "operational"
        }

    return app


# Create default app instance for uvicorn
app = create_standalone_app()


# =============================================================================
# Module Exports
# =============================================================================

__all__ = [
    # Router Factory
    "create_training_router",
    "create_standalone_app",

    # Configuration
    "ReactorAPIConfig",

    # Request/Response Models
    "TrainingRequest",
    "TrainingResponse",
    "TrainingStatusResponse",
    "HealthResponse",

    # Training Components
    "TrainingJobManager",
    "TrainingEngineProtocol",
    "DropBoxHandler",

    # Utilities
    "register_with_service_registry",
]
