"""
Intelligent Voice Unlock Service
=================================

Advanced voice-authenticated screen unlock with:
- Hybrid STT integration (Wav2Vec, Vosk, Whisper)
- Dynamic speaker recognition and learning
- Database-driven intelligence
- CAI (Context-Aware Intelligence) integration
- SAI (Scenario-Aware Intelligence) integration
- Owner profile detection and password management

JARVIS learns the owner's voice over time and automatically rejects
non-owner unlock attempts without hardcoding.
"""

import asyncio
import hashlib
import logging
from datetime import datetime
from typing import Any, Dict, Optional, Tuple
from collections import defaultdict
from dataclasses import dataclass, field

logger = logging.getLogger(__name__)


@dataclass
class UnlockDiagnostics:
    """Comprehensive diagnostics for unlock attempts"""
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    audio_size_bytes: int = 0
    audio_duration_seconds: float = 0.0
    transcription_text: str = ""
    transcription_confidence: float = 0.0
    speaker_identified: Optional[str] = None
    speaker_confidence: float = 0.0
    is_owner: bool = False
    verification_passed: bool = False
    failure_reason: Optional[str] = None
    processing_time_ms: float = 0.0
    stt_engine_used: Optional[str] = None
    cai_analysis: Optional[Dict] = None
    sai_analysis: Optional[Dict] = None
    retry_count: int = 0
    error_messages: list = field(default_factory=list)


class IntelligentVoiceUnlockService:
    """
    Ultra-intelligent voice unlock service that learns and adapts.

    Features:
    - Dynamic speaker learning (no hardcoding)
    - Automatic rejection of non-owner voices
    - Hybrid STT for accurate transcription
    - Database recording for continuous learning
    - CAI integration for context awareness
    - SAI integration for scenario detection
    - Owner profile with password management
    """

    def __init__(self):
        self.initialized = False

        # Hybrid STT Router
        self.stt_router = None

        # Speaker Recognition Engine
        self.speaker_engine = None

        # Learning Database
        self.learning_db = None

        # Advanced error handling and retry logic
        self.max_retries = 3
        self.retry_delay_seconds = 0.5
        self.circuit_breaker_threshold = 5  # failures before circuit opens
        self.circuit_breaker_timeout = 60  # seconds
        self._circuit_breaker_failures = defaultdict(int)
        self._circuit_breaker_last_failure = defaultdict(float)

        # Performance tracking
        self._diagnostics_history = []
        self._max_diagnostics_history = 100

        # Context-Aware Intelligence
        self.cai_handler = None

        # Scenario-Aware Intelligence
        self.sai_analyzer = None

        # Owner profile cache
        self.owner_profile = None
        self.owner_password_hash = None

        # ü§ñ CONTINUOUS LEARNING: ML engine for voice biometrics and password typing
        self.ml_engine = None

        # Statistics
        self.stats = {
            "total_unlock_attempts": 0,
            "owner_unlock_attempts": 0,
            "rejected_attempts": 0,
            "successful_unlocks": 0,
            "failed_authentications": 0,
            "learning_updates": 0,
            "ml_voice_updates": 0,
            "ml_typing_updates": 0,
            "last_unlock_time": None,
        }

    async def initialize(self):
        """Initialize all components"""
        if self.initialized:
            return

        logger.info("üöÄ Initializing Intelligent Voice Unlock Service...")

        # Initialize Hybrid STT Router
        await self._initialize_stt()

        # Initialize Speaker Recognition
        await self._initialize_speaker_recognition()

        # Initialize Learning Database
        await self._initialize_learning_db()

        # Initialize CAI Handler
        await self._initialize_cai()

        # Initialize SAI Analyzer
        await self._initialize_sai()

        # ü§ñ Initialize ML Continuous Learning Engine
        await self._initialize_ml_engine()

        # Load owner profile
        await self._load_owner_profile()

        self.initialized = True
        logger.info("‚úÖ Intelligent Voice Unlock Service initialized")

    async def _initialize_stt(self):
        """Initialize Hybrid STT Router"""
        try:
            from voice.hybrid_stt_router import get_hybrid_router

            self.stt_router = get_hybrid_router()
            logger.info("‚úÖ Hybrid STT Router connected")
        except Exception as e:
            logger.error(f"Failed to initialize Hybrid STT: {e}")
            self.stt_router = None

    async def _initialize_speaker_recognition(self):
        """Initialize Speaker Recognition Engine"""
        try:
            # Try new SpeakerVerificationService first
            try:
                from voice.speaker_verification_service import get_speaker_verification_service

                self.speaker_engine = await get_speaker_verification_service()
                logger.info("‚úÖ Speaker Verification Service connected (new)")
                return
            except ImportError:
                logger.debug("New speaker verification service not available, trying legacy")

            # Fallback to legacy speaker recognition
            from voice.speaker_recognition import get_speaker_recognition_engine

            self.speaker_engine = get_speaker_recognition_engine()
            await self.speaker_engine.initialize()
            logger.info("‚úÖ Speaker Recognition Engine connected (legacy)")
        except Exception as e:
            logger.error(f"Failed to initialize Speaker Recognition: {e}")
            self.speaker_engine = None

    async def _initialize_learning_db(self):
        """Initialize Learning Database"""
        try:
            from intelligence.learning_database import JARVISLearningDatabase

            self.learning_db = JARVISLearningDatabase()
            await self.learning_db.initialize()
            logger.info("‚úÖ Learning Database connected")
        except Exception as e:
            logger.error(f"Failed to initialize Learning Database: {e}")
            self.learning_db = None

    async def _initialize_cai(self):
        """Initialize Context-Aware Intelligence"""
        try:
            from context_intelligence.handlers.context_aware_handler import ContextAwareHandler

            self.cai_handler = ContextAwareHandler()
            logger.info("‚úÖ Context-Aware Intelligence connected")
        except Exception as e:
            logger.warning(f"CAI not available: {e}")
            self.cai_handler = None

    async def _initialize_sai(self):
        """Initialize Scenario-Aware Intelligence"""
        try:
            from intelligence.scenario_intelligence import ScenarioIntelligence

            self.sai_analyzer = ScenarioIntelligence()
            await self.sai_analyzer.initialize()
            logger.info("‚úÖ Scenario-Aware Intelligence connected")
        except Exception as e:
            logger.warning(f"SAI not available: {e}")
            self.sai_analyzer = None

    async def _initialize_ml_engine(self):
        """ü§ñ Initialize ML Continuous Learning Engine"""
        try:
            from voice_unlock.continuous_learning_engine import get_learning_engine

            self.ml_engine = get_learning_engine()
            await self.ml_engine.initialize()
            logger.info("‚úÖ ü§ñ ML Continuous Learning Engine connected")
        except Exception as e:
            logger.warning(f"ML Learning Engine not available: {e}")
            self.ml_engine = None

    async def _load_owner_profile(self):
        """Load or create owner profile"""
        if not self.learning_db or not self.speaker_engine:
            logger.warning("Cannot load owner profile - dependencies not available")
            return

        try:
            # Get all speaker profiles
            profiles = await self.learning_db.get_all_speaker_profiles()

            # Find owner (is_primary_user = True)
            for profile in profiles:
                if profile.get("is_primary_user"):
                    self.owner_profile = profile
                    logger.info(f"üëë Owner profile loaded: {profile['speaker_name']}")

                    # Also set in speaker engine
                    self.speaker_engine.owner_profile = self.speaker_engine.profiles.get(
                        profile["speaker_name"]
                    )
                    break

            if not self.owner_profile:
                logger.warning(
                    "‚ö†Ô∏è  No owner profile found - first speaker will be enrolled as owner"
                )

            # Load password hash from keychain
            await self._load_owner_password()

        except Exception as e:
            logger.error(f"Failed to load owner profile: {e}")

    async def _load_owner_password(self):
        """Load owner password from keychain"""
        try:
            import subprocess

            result = subprocess.run(
                [
                    "security",
                    "find-generic-password",
                    "-s",
                    "JARVIS_Screen_Unlock",
                    "-a",
                    "jarvis_user",
                    "-w",
                ],
                capture_output=True,
                text=True,
            )

            if result.returncode == 0:
                password = result.stdout.strip()
                # Store hash for verification (not the actual password)
                self.owner_password_hash = hashlib.sha256(password.encode()).hexdigest()
                logger.info("üîê Owner password loaded from keychain")
            else:
                logger.warning("‚ö†Ô∏è  No password found in keychain")

        except Exception as e:
            logger.error(f"Failed to load owner password: {e}")

    async def process_voice_unlock_command(
        self, audio_data, context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Process voice unlock command with full intelligence stack.

        Features:
        - Retry logic with exponential backoff
        - Circuit breaker pattern for fault tolerance
        - Comprehensive diagnostics tracking
        - Async/await throughout for non-blocking operation

        Args:
            audio_data: Audio data in any format (bytes, string, base64, etc.)
            context: Optional context (screen state, time, location, etc.)

        Returns:
            Result dict with success, speaker, reason, and diagnostics
        """
        # CRITICAL: Start caffeinate IMMEDIATELY to prevent screen sleep during processing
        caffeinate_process = await asyncio.create_subprocess_exec(
            "caffeinate", "-d", "-u",
            stdout=asyncio.subprocess.DEVNULL,
            stderr=asyncio.subprocess.DEVNULL
        )
        logger.info("üîã Caffeinate started to keep screen awake")

        if not self.initialized:
            await self.initialize()

        start_time = datetime.now()
        self.stats["total_unlock_attempts"] += 1

        # Initialize diagnostics
        diagnostics = UnlockDiagnostics()

        # Initialize advanced metrics logger with stage tracking
        from voice_unlock.unlock_metrics_logger import get_metrics_logger, StageMetrics
        metrics_logger = get_metrics_logger()
        stages: List[StageMetrics] = []

        logger.info("üé§ Processing voice unlock command...")

        # Stage 1: Audio Preparation
        stage_audio_prep = metrics_logger.create_stage(
            "audio_preparation",
            input_type=type(audio_data).__name__,
            input_size_raw=len(audio_data) if isinstance(audio_data, bytes) else 0
        )

        # Convert audio to proper format with error handling
        try:
            from voice.audio_format_converter import prepare_audio_for_stt
            audio_data = prepare_audio_for_stt(audio_data)
            diagnostics.audio_size_bytes = len(audio_data) if audio_data else 0
            logger.info(f"üìä Audio prepared: {diagnostics.audio_size_bytes} bytes")

            stage_audio_prep.complete(
                success=True,
                algorithm_used="prepare_audio_for_stt",
                input_size_bytes=stage_audio_prep.metadata.get('input_size_raw', 0),
                output_size_bytes=diagnostics.audio_size_bytes
            )
            stages.append(stage_audio_prep)
        except Exception as e:
            logger.error(f"‚ùå Audio preparation failed: {e}")
            diagnostics.error_messages.append(f"Audio preparation failed: {str(e)}")
            stage_audio_prep.complete(success=False, error_message=str(e))
            stages.append(stage_audio_prep)

            # Log failed attempt
            self._log_failed_unlock_attempt(
                metrics_logger, stages, "audio_preparation_failed",
                "Failed to prepare audio data", str(e)
            )

            return await self._create_failure_response(
                "audio_preparation_failed",
                "Failed to prepare audio data",
                diagnostics=diagnostics.__dict__
            )

        # Extract sample_rate from context if provided by frontend
        sample_rate = None
        if context:
            sample_rate = context.get("audio_sample_rate")
            if sample_rate:
                logger.info(f"üéµ Using frontend-provided sample rate: {sample_rate}Hz")

        # Stage 2: Transcription using Hybrid STT (with timeout protection)
        stage_transcription = metrics_logger.create_stage(
            "transcription",
            sample_rate=sample_rate,
            audio_size=diagnostics.audio_size_bytes
        )

        try:
            transcription_result = await asyncio.wait_for(
                self._transcribe_audio_with_retry(
                    audio_data, diagnostics, sample_rate=sample_rate
                ),
                timeout=20.0  # 20 second timeout for transcription
            )
        except asyncio.TimeoutError:
            logger.error("‚è±Ô∏è Transcription timed out after 20 seconds")
            stage_transcription.complete(success=False, error_message="Transcription timeout")
            stages.append(stage_transcription)
            return await self._create_failure_response(
                "transcription_timeout",
                "Speech recognition took too long. Please try again.",
                diagnostics=diagnostics.__dict__
            )

        if not transcription_result:
            diagnostics.failure_reason = "transcription_failed"
            diagnostics.processing_time_ms = (datetime.now() - start_time).total_seconds() * 1000
            self._store_diagnostics(diagnostics)

            stage_transcription.complete(success=False, error_message="Transcription failed")
            stages.append(stage_transcription)

            return await self._create_failure_response(
                "transcription_failed",
                "Could not transcribe audio",
                diagnostics=diagnostics.__dict__
            )

        transcribed_text = transcription_result.text
        stt_confidence = transcription_result.confidence
        speaker_identified = transcription_result.speaker_identified

        # Get STT engine used
        stt_engine = getattr(transcription_result, 'engine_used', 'unknown')
        if hasattr(diagnostics, 'stt_engine_used') and diagnostics.stt_engine_used:
            stt_engine = diagnostics.stt_engine_used

        stage_transcription.complete(
            success=True,
            algorithm_used=stt_engine,
            confidence_score=stt_confidence,
            output_size_bytes=len(transcribed_text.encode('utf-8')),
            metadata={
                'transcribed_text': transcribed_text,
                'speaker_identified': speaker_identified
            }
        )
        stages.append(stage_transcription)

        logger.info(f"üìù Transcribed: '{transcribed_text}' (confidence: {stt_confidence:.2f})")
        logger.info(f"üë§ Speaker: {speaker_identified or 'Unknown'}")

        # üß† HALLUCINATION GUARD: Check and correct STT hallucinations
        try:
            from voice.stt_hallucination_guard import verify_stt_transcription

            original_text = transcribed_text
            transcribed_text, was_corrected, hallucination_detection = await verify_stt_transcription(
                text=transcribed_text,
                confidence=stt_confidence,
                audio_data=audio_data,
                context="unlock_command"
            )

            if was_corrected:
                logger.info(
                    f"üß† [HALLUCINATION-GUARD] Corrected: '{original_text}' ‚Üí '{transcribed_text}'"
                )
                diagnostics.error_messages.append(
                    f"STT hallucination corrected: '{original_text}' ‚Üí '{transcribed_text}'"
                )
        except ImportError:
            logger.debug("Hallucination guard not available, skipping")
        except Exception as e:
            logger.warning(f"Hallucination guard error (continuing): {e}")

        # Stage 3: Intent Verification
        stage_intent = metrics_logger.create_stage(
            "intent_verification",
            text_to_verify=transcribed_text
        )

        is_unlock_command = await self._verify_unlock_intent(transcribed_text, context)

        stage_intent.complete(
            success=is_unlock_command,
            algorithm_used="NLP pattern matching",
            metadata={'is_unlock_command': is_unlock_command}
        )
        stages.append(stage_intent)

        if not is_unlock_command:
            return await self._create_failure_response(
                "not_unlock_command", f"Command '{transcribed_text}' is not an unlock request"
            )

        # Stage 4: Speaker Identification
        stage_speaker_id = metrics_logger.create_stage(
            "speaker_identification",
            already_identified=speaker_identified is not None
        )

        if not speaker_identified:
            speaker_identified, speaker_confidence = await self._identify_speaker(audio_data)
        else:
            # Verify speaker confidence
            speaker_confidence = await self._get_speaker_confidence(audio_data, speaker_identified)

        stage_speaker_id.complete(
            success=speaker_identified is not None,
            algorithm_used="SpeechBrain speaker recognition",
            confidence_score=speaker_confidence if speaker_identified else 0,
            metadata={'speaker_name': speaker_identified}
        )
        stages.append(stage_speaker_id)

        logger.info(
            f"üîê Speaker identified: {speaker_identified} (confidence: {speaker_confidence:.2f})"
        )

        # Stage 5: Owner Verification
        stage_owner_check = metrics_logger.create_stage(
            "owner_verification",
            speaker_name=speaker_identified
        )

        is_owner = await self._verify_owner(speaker_identified)

        stage_owner_check.complete(
            success=is_owner,
            algorithm_used="Database owner lookup",
            metadata={'is_owner': is_owner}
        )
        stages.append(stage_owner_check)

        if not is_owner:
            self.stats["rejected_attempts"] += 1
            logger.warning(f"üö´ Non-owner '{speaker_identified}' attempted unlock - REJECTED")

            # Analyze security event with SAI
            security_analysis = await self._analyze_security_event(
                speaker_name=speaker_identified,
                transcribed_text=transcribed_text,
                context=context,
                speaker_confidence=speaker_confidence,
            )

            # Record rejection to database with full analysis
            await self._record_unlock_attempt(
                speaker_name=speaker_identified,
                transcribed_text=transcribed_text,
                success=False,
                rejection_reason="not_owner",
                audio_data=audio_data,
                stt_confidence=stt_confidence,
                speaker_confidence=speaker_confidence,
                security_analysis=security_analysis,
            )

            # Generate intelligent, dynamic security response
            security_message = await self._generate_security_response(
                speaker_name=speaker_identified,
                reason="not_owner",
                analysis=security_analysis,
                context=context,
            )

            return await self._create_failure_response(
                "not_owner",
                security_message,
                speaker_name=speaker_identified,
                security_analysis=security_analysis,
            )

        # Stage 6: Biometric Verification (anti-spoofing) - with timeout protection
        stage_biometric = metrics_logger.create_stage(
            "biometric_verification",
            speaker_name=speaker_identified,
            audio_size=diagnostics.audio_size_bytes
        )

        try:
            verification_passed, verification_confidence = await asyncio.wait_for(
                self._verify_speaker_identity(audio_data, speaker_identified),
                timeout=30.0  # 30 second timeout for speaker verification
            )
        except asyncio.TimeoutError:
            logger.error("‚è±Ô∏è Biometric verification timed out after 30 seconds")
            stage_biometric.complete(success=False, error_message="Verification timeout")
            stages.append(stage_biometric)
            return await self._create_failure_response(
                "biometric_timeout",
                "Voice verification took too long. Please try again.",
                diagnostics=diagnostics.__dict__
            )

        # Get threshold dynamically
        threshold = getattr(self.speaker_engine, 'threshold', 0.35) if self.speaker_engine else 0.35

        stage_biometric.complete(
            success=verification_passed,
            algorithm_used="SpeechBrain ECAPA-TDNN",
            confidence_score=verification_confidence,
            threshold=threshold,
            above_threshold=verification_confidence >= threshold,
            metadata={
                'verification_method': 'cosine_similarity',
                'embedding_dimension': 192
            }
        )
        stages.append(stage_biometric)

        # ü§ñ ML LEARNING: Update voice biometric model (learn from this attempt)
        if self.ml_engine:
            try:
                await self.ml_engine.voice_learner.update_from_attempt(
                    confidence=verification_confidence,
                    success=verification_passed,
                    is_owner=True,  # We verified this is the owner (passed owner check)
                    audio_quality=stt_confidence
                )
                self.stats["ml_voice_updates"] += 1
                logger.debug(f"ü§ñ ML: Voice biometric model updated (confidence: {verification_confidence:.2%})")
            except Exception as e:
                logger.error(f"ML voice learning update failed: {e}")

        if not verification_passed:
            self.stats["failed_authentications"] += 1

            # üîç DETAILED DIAGNOSTICS: Analyze why verification failed
            failure_diagnostics = await self._analyze_verification_failure(
                audio_data=audio_data,
                speaker_name=speaker_identified,
                confidence=verification_confidence,
                transcription=transcribed_text
            )

            logger.error(
                f"üö´ Voice verification FAILED for owner '{speaker_identified}'\n"
                f"   ‚îú‚îÄ Confidence: {verification_confidence:.2%} (threshold: {failure_diagnostics.get('threshold', 'unknown')})\n"
                f"   ‚îú‚îÄ Audio quality: {failure_diagnostics.get('audio_quality', 'unknown')}\n"
                f"   ‚îú‚îÄ Audio duration: {failure_diagnostics.get('audio_duration_ms', 0)}ms\n"
                f"   ‚îú‚îÄ Audio energy: {failure_diagnostics.get('audio_energy', 0):.6f}\n"
                f"   ‚îú‚îÄ Samples in DB: {failure_diagnostics.get('samples_in_db', 0)}\n"
                f"   ‚îú‚îÄ Embedding dimension: {failure_diagnostics.get('embedding_dimension', 'unknown')}\n"
                f"   ‚îú‚îÄ Primary failure reason: {failure_diagnostics.get('primary_reason', 'unknown')}\n"
                f"   ‚îú‚îÄ Suggested fix: {failure_diagnostics.get('suggested_fix', 'unknown')}\n"
                f"   ‚îî‚îÄ Architecture issue: {failure_diagnostics.get('architecture_issue', 'none detected')}"
            )

            # Record failed authentication with diagnostics
            await self._record_unlock_attempt(
                speaker_name=speaker_identified,
                transcribed_text=transcribed_text,
                success=False,
                rejection_reason="verification_failed",
                audio_data=audio_data,
                stt_confidence=stt_confidence,
                speaker_confidence=verification_confidence,
            )

            # Track in monitoring system
            try:
                import sys
                from pathlib import Path
                # Add parent directory to path to import from start_system
                sys.path.insert(0, str(Path(__file__).parent.parent.parent))
                from start_system import track_voice_verification_attempt
                track_voice_verification_attempt(False, verification_confidence, failure_diagnostics)
            except Exception as e:
                logger.debug(f"Failed to track verification in monitoring: {e}")

            return await self._create_failure_response(
                "verification_failed",
                f"Voice verification failed (confidence: {verification_confidence:.2%}). {failure_diagnostics.get('user_message', 'Please try again.')}",
                speaker_name=speaker_identified,
                diagnostics=failure_diagnostics
            )

        self.stats["owner_unlock_attempts"] += 1
        logger.info(f"‚úÖ Owner '{speaker_identified}' verified for unlock")

        # Track successful verification in monitoring system
        try:
            import sys
            from pathlib import Path
            sys.path.insert(0, str(Path(__file__).parent.parent.parent))
            from start_system import track_voice_verification_attempt
            track_voice_verification_attempt(True, verification_confidence, None)
        except Exception as e:
            logger.debug(f"Failed to track verification in monitoring: {e}")

        # Stage 7: Context Analysis (CAI)
        stage_context = metrics_logger.create_stage(
            "context_analysis",
            text=transcribed_text
        )

        context_analysis = await self._analyze_context(transcribed_text, context)

        stage_context.complete(
            success=True,
            algorithm_used="CAI (Context-Aware Intelligence)",
            metadata={'context_data': context_analysis}
        )
        stages.append(stage_context)

        # Stage 8: Scenario Analysis (SAI)
        stage_scenario = metrics_logger.create_stage(
            "scenario_analysis",
            speaker=speaker_identified
        )

        scenario_analysis = await self._analyze_scenario(
            transcribed_text, context, speaker_identified
        )

        stage_scenario.complete(
            success=True,
            algorithm_used="SAI (Scenario-Aware Intelligence)",
            metadata={'scenario_data': scenario_analysis}
        )
        stages.append(stage_scenario)

        # ü§ñ ML LEARNING: Record unlock attempt BEFORE performing unlock to get attempt_id
        # This allows password typing metrics to be linked to the unlock attempt
        attempt_id = await self._record_unlock_attempt(
            speaker_name=speaker_identified,
            transcribed_text=transcribed_text,
            success=True,  # Will be updated after unlock completes
            rejection_reason=None,
            audio_data=audio_data,
            stt_confidence=stt_confidence,
            speaker_confidence=verification_confidence,
            context_data=context_analysis,
            scenario_data=scenario_analysis,
        )

        # Stage 9: Screen Unlock Execution
        stage_unlock = metrics_logger.create_stage(
            "unlock_execution",
            speaker=speaker_identified
        )

        unlock_result = await self._perform_unlock(
            speaker_identified, context_analysis, scenario_analysis, attempt_id=attempt_id
        )

        stage_unlock.complete(
            success=unlock_result["success"],
            algorithm_used="macOS SecurePasswordTyper with ML metrics",
            metadata={
                'unlock_method': 'password_entry',
                'result_message': unlock_result.get("message"),
                'ml_attempt_id': attempt_id
            }
        )
        stages.append(stage_unlock)

        # Step 8: Update stats and speaker profile
        if unlock_result["success"]:
            self.stats["successful_unlocks"] += 1
            self.stats["last_unlock_time"] = datetime.now()

            logger.info(f"üîì Screen unlocked successfully by owner '{speaker_identified}'")

            # Update speaker profile with continuous learning
            await self._update_speaker_profile(
                speaker_identified, audio_data, transcribed_text, success=True
            )

        # Calculate total latency
        total_latency_ms = (datetime.now() - start_time).total_seconds() * 1000

        # Cleanup caffeinate
        try:
            caffeinate_process.terminate()
            logger.info("üîã Caffeinate terminated")
        except:
            pass

        # Get speaker verification threshold dynamically
        threshold = getattr(self.speaker_engine, 'threshold', 0.35) if self.speaker_engine else 0.35

        # Build detailed developer metrics (logged to JSON file)
        developer_metrics = {
            "biometrics": {
                "speaker_confidence": verification_confidence,
                "stt_confidence": stt_confidence,
                "threshold": threshold,
                "above_threshold": verification_confidence >= threshold,
                "confidence_margin": verification_confidence - threshold,
                "confidence_percentage": f"{verification_confidence * 100:.1f}%",
            },
            "performance": {
                "total_latency_ms": total_latency_ms,
                "transcription_time_ms": diagnostics.processing_time_ms if hasattr(self, 'diagnostics') else None,
            },
            "quality_indicators": {
                "audio_quality": "good" if stt_confidence > 0.7 else "fair" if stt_confidence > 0.5 else "poor",
                "voice_match_quality": "excellent" if verification_confidence > 0.6 else "good" if verification_confidence > 0.45 else "acceptable" if verification_confidence > threshold else "below_threshold",
                "overall_confidence": (stt_confidence + verification_confidence) / 2,
            }
        }

        # Get system information
        import platform
        import sys
        system_info = {
            "platform": platform.system(),
            "platform_version": platform.version(),
            "python_version": sys.version.split()[0],
            "stt_engine": diagnostics.stt_engine_used,
            "speaker_engine": "SpeechBrain" if self.speaker_engine else "None",
        }

        # Log advanced metrics with all stages to JSON file (async)
        try:
            await metrics_logger.log_unlock_attempt(
                success=unlock_result["success"],
                speaker_name=speaker_identified,
                transcribed_text=transcribed_text,
                stages=stages,
                biometrics=developer_metrics["biometrics"],
                performance=developer_metrics["performance"],
                quality_indicators=developer_metrics["quality_indicators"],
                system_info=system_info,
                error=None if unlock_result["success"] else unlock_result.get("message")
            )
        except Exception as e:
            logger.warning(f"Failed to log metrics: {e}")

        return {
            "success": unlock_result["success"],
            "speaker_name": speaker_identified,
            "transcribed_text": transcribed_text,
            "stt_confidence": stt_confidence,
            "speaker_confidence": verification_confidence,
            "verification_confidence": verification_confidence,
            "is_owner": True,
            "message": unlock_result.get("message", "Unlock successful"),
            "latency_ms": total_latency_ms,
            "context_analysis": context_analysis,
            "scenario_analysis": scenario_analysis,
            "timestamp": datetime.now().isoformat(),
            # Developer metrics (UI only, not announced)
            "dev_metrics": developer_metrics,
        }

    def _check_circuit_breaker(self, service_name: str) -> bool:
        """
        Check if circuit breaker allows operation.

        Returns:
            True if operation is allowed, False if circuit is open
        """
        import time

        current_time = time.time()

        # Check if circuit is open
        if self._circuit_breaker_failures[service_name] >= self.circuit_breaker_threshold:
            last_failure = self._circuit_breaker_last_failure[service_name]

            # Check if timeout has passed
            if current_time - last_failure < self.circuit_breaker_timeout:
                logger.warning(
                    f"üî¥ Circuit breaker OPEN for {service_name} "
                    f"({self._circuit_breaker_failures[service_name]} failures)"
                )
                return False
            else:
                # Reset circuit breaker after timeout
                logger.info(f"üü¢ Circuit breaker RESET for {service_name}")
                self._circuit_breaker_failures[service_name] = 0

        return True

    def _record_circuit_breaker_failure(self, service_name: str):
        """Record a failure for circuit breaker"""
        import time

        self._circuit_breaker_failures[service_name] += 1
        self._circuit_breaker_last_failure[service_name] = time.time()

        logger.debug(
            f"‚ö†Ô∏è Circuit breaker failure recorded for {service_name}: "
            f"{self._circuit_breaker_failures[service_name]}/{self.circuit_breaker_threshold}"
        )

    def _record_circuit_breaker_success(self, service_name: str):
        """Record a success - reset failure count"""
        if self._circuit_breaker_failures[service_name] > 0:
            logger.debug(f"‚úÖ Circuit breaker success for {service_name} - resetting failures")
            self._circuit_breaker_failures[service_name] = 0

    def _store_diagnostics(self, diagnostics: UnlockDiagnostics):
        """Store diagnostics in history for analysis"""
        self._diagnostics_history.append(diagnostics)

        # Keep only last N diagnostics
        if len(self._diagnostics_history) > self._max_diagnostics_history:
            self._diagnostics_history.pop(0)

    async def _transcribe_audio_with_retry(
        self, audio_data: bytes, diagnostics: UnlockDiagnostics, sample_rate: Optional[int] = None
    ):
        """
        Transcribe audio with retry logic and circuit breaker.

        Args:
            audio_data: Audio bytes to transcribe
            diagnostics: Diagnostics object to track attempts
            sample_rate: Optional sample rate from frontend (browser-reported)

        Returns:
            Transcription result or None if all retries failed
        """
        service_name = "stt_transcription"

        # Check circuit breaker
        if not self._check_circuit_breaker(service_name):
            diagnostics.error_messages.append(f"Circuit breaker open for {service_name}")
            return None

        for attempt in range(self.max_retries):
            diagnostics.retry_count = attempt + 1

            try:
                logger.info(f"üîÑ Transcription attempt {attempt + 1}/{self.max_retries}")

                result = await self._transcribe_audio(audio_data, sample_rate=sample_rate)

                if result:
                    # Success - record and return
                    self._record_circuit_breaker_success(service_name)
                    diagnostics.stt_engine_used = getattr(result, 'engine_used', 'unknown')
                    return result
                else:
                    logger.warning(f"‚ö†Ô∏è  Transcription attempt {attempt + 1} returned None")

            except Exception as e:
                error_msg = f"Transcription attempt {attempt + 1} failed: {str(e)}"
                logger.error(f"‚ùå {error_msg}")
                diagnostics.error_messages.append(error_msg)

            # Wait before retry (exponential backoff)
            if attempt < self.max_retries - 1:
                delay = self.retry_delay_seconds * (2 ** attempt)
                logger.info(f"‚è≥ Waiting {delay:.1f}s before retry...")
                await asyncio.sleep(delay)

        # All retries failed
        self._record_circuit_breaker_failure(service_name)
        logger.error(f"‚ùå All {self.max_retries} transcription attempts failed")
        return None

    async def _transcribe_audio(self, audio_data: bytes, sample_rate: Optional[int] = None):
        """
        Transcribe audio using Hybrid STT

        Args:
            audio_data: Audio bytes to transcribe
            sample_rate: Optional sample rate from frontend (browser-reported)
        """
        if not self.stt_router:
            logger.error("Hybrid STT not available")
            return None

        try:
            from voice.stt_config import RoutingStrategy

            # CRITICAL FIX: Convert base64 string to bytes before transcription
            if isinstance(audio_data, str):
                import base64
                try:
                    audio_data = base64.b64decode(audio_data)
                    logger.info(f"‚úÖ Decoded base64 audio: {len(audio_data)} bytes")
                except Exception as e:
                    logger.error(f"‚ùå Failed to decode base64 audio_data: {e}")
                    return None

            # Use ACCURACY strategy for unlock (security-critical)
            # **CRITICAL**: Use 'unlock' mode for ultra-fast 2-second window + VAD filtering
            result = await self.stt_router.transcribe(
                audio_data=audio_data,
                strategy=RoutingStrategy.ACCURACY,
                speaker_name=None,  # Auto-detect
                sample_rate=sample_rate,  # Pass sample rate for proper resampling
                mode='unlock',  # UNLOCK MODE: 2-second window, VAD filtering, maximum speed
            )

            return result

        except Exception as e:
            logger.error(f"Transcription failed: {e}")
            return None

    async def _verify_unlock_intent(
        self, transcribed_text: str, context: Optional[Dict[str, Any]]
    ) -> bool:
        """Verify that the transcribed text is an unlock command with fuzzy matching for STT errors"""
        text_lower = transcribed_text.lower()

        # Primary unlock phrases
        unlock_phrases = ["unlock", "open", "access", "let me in", "sign in", "log in"]

        # Check if any unlock phrase is present
        if any(phrase in text_lower for phrase in unlock_phrases):
            return True

        # Fuzzy matching for common Whisper STT transcription errors
        # "unlock my screen" often becomes "I'm like my screen" or similar
        fuzzy_patterns = [
            "like my screen",  # "unlock" ‚Üí "I'm like"
            "like the screen",
            "lock my screen",  # Sometimes "un" is dropped
            "lock the screen",
            "my screen",  # Core phrase
            "the screen",
        ]

        # If we see these patterns + context suggests unlock, accept it
        if any(pattern in text_lower for pattern in fuzzy_patterns):
            # Additional context: check if "screen" keyword is present
            if "screen" in text_lower:
                logger.info(f"üéØ Fuzzy match detected unlock intent from: '{transcribed_text}'")
                return True

        return False

    async def _apply_vad_for_speaker_verification(self, audio_data: bytes) -> bytes:
        """
        Apply VAD filtering to audio before speaker verification
        This dramatically speeds up speaker verification by removing silence
        and reducing audio to 2-second windows (unlock mode)
        """
        try:
            from voice.whisper_audio_fix import transcribe_with_whisper
            from voice.whisper_audio_fix import _whisper_handler
            import numpy as np
            import io
            import wave

            # Decode audio
            audio_bytes = _whisper_handler.decode_audio_data(audio_data)

            # Normalize to 16kHz float32
            normalized_audio = await _whisper_handler.normalize_audio(audio_bytes, sample_rate=16000)

            # Apply VAD + windowing (unlock mode = 2s)
            filtered_audio = await _whisper_handler._apply_vad_and_windowing(
                normalized_audio,
                mode='unlock'  # 2-second window, ultra-fast
            )

            # If VAD filtered everything, return minimal audio
            if len(filtered_audio) == 0:
                logger.warning("‚ö†Ô∏è VAD filtered all audio for speaker verification, using minimal audio")
                filtered_audio = np.zeros(16000, dtype=np.float32)  # 1 second of silence

            # Convert back to bytes (WAV format for speaker verification)
            with io.BytesIO() as wav_buffer: # create buffer to write to 
                with wave.open(wav_buffer, 'wb') as wav_file: # write to buffer instead of file 
                    wav_file.setnchannels(1) # mono WAV file (1 channel) 
                    wav_file.setsampwidth(2)  # 16-bit
                    wav_file.setframerate(16000) # 16kHz sample rate for speaker verification service
                    # Convert float32 to int16
                    audio_int16 = (filtered_audio * 32767).astype(np.int16) # convert to int16 
                    wav_file.writeframes(audio_int16.tobytes()) # write to buffer 

                wav_bytes = wav_buffer.getvalue() # get bytes from buffer 

            logger.info(f"‚úÖ VAD preprocessed audio for speaker verification: {len(audio_data)} ‚Üí {len(wav_bytes)} bytes")
            return wav_bytes # return filtered audio 

        except Exception as e:
            logger.error(f"Failed to apply VAD for speaker verification: {e}, using original audio")
            return audio_data

    async def _identify_speaker(self, audio_data: bytes) -> Tuple[Optional[str], float]:
        """Identify speaker from audio with VAD preprocessing for speed"""
        if not self.speaker_engine:
            return None, 0.0

        try:
            # Apply VAD filtering to speed up speaker verification (unlock mode = 2s max)
            filtered_audio = await self._apply_vad_for_speaker_verification(audio_data)

            # New SpeakerVerificationService
            if hasattr(self.speaker_engine, "verify_speaker"):
                result = await self.speaker_engine.verify_speaker(filtered_audio)
                return result.get("speaker_name"), result.get("confidence", 0.0)

            # Legacy speaker verification service - returns (speaker_name, confidence) 
            speaker_name, confidence = await self.speaker_engine.identify_speaker(filtered_audio) # Returns (speaker_name, confidence) 
            return speaker_name, confidence # Returns (speaker_name, confidence)
        except Exception as e:
            logger.error(f"Speaker identification failed: {e}")
            return None, 0.0 # Returns (speaker_name, confidence) 

    async def _get_speaker_confidence(self, audio_data: bytes, speaker_name: str) -> float:
        """Get confidence score for identified speaker with VAD preprocessing"""
        if not self.speaker_engine:
            return 0.0

        try:
            # Apply VAD filtering to speed up speaker verification
            filtered_audio = await self._apply_vad_for_speaker_verification(audio_data) # Returns filtered audio bytes 

            # New SpeakerVerificationService
            if hasattr(self.speaker_engine, "get_speaker_name"):
                result = await self.speaker_engine.verify_speaker(filtered_audio, speaker_name)
                return result.get("confidence", 0.0)

            # Legacy: Re-verify to get confidence
            is_match, confidence = await self.speaker_engine.verify_speaker(
                filtered_audio, speaker_name
            )
            return confidence
        except Exception as e:
            logger.error(f"Speaker confidence check failed: {e}")
            return 0.0

    async def _verify_owner(self, speaker_name: Optional[str]) -> bool:
        """Check if speaker is the device owner"""
        if not speaker_name:
            return False

        if not self.speaker_engine:
            # Fallback: check against cached owner profile
            if self.owner_profile:
                return speaker_name == self.owner_profile.get("speaker_name")
            return False

        # New SpeakerVerificationService - check is_owner from profiles
        if hasattr(self.speaker_engine, "speaker_profiles"):
            profile = self.speaker_engine.speaker_profiles.get(speaker_name)
            if profile:
                return profile.get("is_primary_user", False)

        # Legacy: use is_owner method
        if hasattr(self.speaker_engine, "is_owner"):
            return self.speaker_engine.is_owner(speaker_name)

        return False

    async def _verify_speaker_identity(
        self, audio_data: bytes, speaker_name: str
    ) -> Tuple[bool, float]:
        """Verify speaker identity with high threshold (anti-spoofing)"""
        if not self.speaker_engine:
            return False, 0.0

        try:
            # New SpeakerVerificationService - returns dict with adaptive thresholds
            if hasattr(self.speaker_engine, "get_speaker_name"):
                result = await self.speaker_engine.verify_speaker(audio_data, speaker_name)
                is_verified = result.get("verified", False)
                confidence = result.get("confidence", 0.0)

                # Trust the speaker verification service's adaptive threshold decision
                # (Uses 50% for legacy profiles, 75% for native profiles)
                return is_verified, confidence

            # Legacy: Use verify_speaker with high threshold (0.85)
            is_verified, confidence = await self.speaker_engine.verify_speaker(
                audio_data, speaker_name
            )

            return is_verified, confidence

        except Exception as e:
            logger.error(f"Speaker verification failed: {e}")
            return False, 0.0

    async def _analyze_verification_failure(
        self, audio_data: bytes, speaker_name: str, confidence: float, transcription: str
    ) -> Dict[str, Any]:
        """
        Comprehensive analysis of voice verification failure

        Diagnoses:
        - Audio quality issues
        - Database/profile issues
        - Embedding dimension mismatches
        - Sample count deficiencies
        - Environmental factors
        - System architecture flaws
        """
        diagnostics = {
            'primary_reason': 'unknown',
            'suggested_fix': 'Contact system administrator',
            'architecture_issue': 'none detected',
            'user_message': 'Please try again.',
            'threshold': 'unknown',
            'audio_quality': 'unknown',
            'audio_duration_ms': 0,
            'audio_energy': 0.0,
            'samples_in_db': 0,
            'embedding_dimension': 'unknown',
            'severity': 'low'
        }

        try:
            import numpy as np

            # 1. AUDIO QUALITY ANALYSIS
            if audio_data and len(audio_data) > 0:
                try:
                    # Parse audio (assuming int16 PCM)
                    audio_int16 = np.frombuffer(audio_data, dtype=np.int16)
                    audio_float32 = audio_int16.astype(np.float32) / 32768.0

                    # Calculate duration (assuming 16kHz sample rate)
                    duration_ms = (len(audio_int16) / 16000) * 1000
                    diagnostics['audio_duration_ms'] = int(duration_ms)

                    # Calculate energy
                    energy = np.mean(np.abs(audio_float32))
                    diagnostics['audio_energy'] = float(energy)

                    # Determine audio quality
                    if energy < 0.0001:
                        diagnostics['audio_quality'] = 'silent/corrupted'
                        diagnostics['primary_reason'] = 'Audio input is silent or corrupted'
                        diagnostics['suggested_fix'] = 'Check microphone connection and permissions'
                        diagnostics['architecture_issue'] = 'Audio pipeline may not be capturing input correctly'
                        diagnostics['user_message'] = 'Microphone not detecting audio. Check your audio settings.'
                        diagnostics['severity'] = 'critical'
                    elif energy < 0.001:
                        diagnostics['audio_quality'] = 'very_quiet'
                        diagnostics['primary_reason'] = 'Audio input too quiet'
                        diagnostics['suggested_fix'] = 'Speak louder or adjust microphone gain'
                        diagnostics['user_message'] = 'Please speak louder.'
                        diagnostics['severity'] = 'high'
                    elif duration_ms < 500:
                        diagnostics['audio_quality'] = 'too_short'
                        diagnostics['primary_reason'] = f'Audio too short ({duration_ms:.0f}ms, need 1000ms+)'
                        diagnostics['suggested_fix'] = 'Speak for longer duration'
                        diagnostics['user_message'] = 'Please speak the command more slowly.'
                        diagnostics['severity'] = 'high'
                    else:
                        diagnostics['audio_quality'] = 'acceptable'
                except Exception as e:
                    diagnostics['audio_quality'] = f'parse_error: {str(e)}'
                    logger.error(f"Audio parsing failed: {e}")
            else:
                diagnostics['audio_quality'] = 'no_data'
                diagnostics['primary_reason'] = 'No audio data received'
                diagnostics['suggested_fix'] = 'Verify audio recording pipeline'
                diagnostics['architecture_issue'] = 'Audio data not reaching verification service'
                diagnostics['severity'] = 'critical'

            # 2. DATABASE PROFILE ANALYSIS
            if self.speaker_engine and hasattr(self.speaker_engine, 'speaker_profiles'):
                profiles = self.speaker_engine.speaker_profiles

                # Handle case where speaker_name is "unknown" or not in profiles
                # This happens when verification didn't find a match
                target_profile = None
                target_name = speaker_name

                if speaker_name in profiles:
                    target_profile = profiles[speaker_name]
                elif speaker_name in ("unknown", "error", None, ""):
                    # Verification failed to identify speaker - use primary user profile for diagnostics
                    # This is NOT "profile not found" - it's "voice didn't match the profile"
                    for name, profile in profiles.items():
                        if profile.get('is_primary_user', False):
                            target_profile = profile
                            target_name = name
                            diagnostics['expected_speaker'] = name
                            break
                    # If no primary user, use the first profile
                    if not target_profile and profiles:
                        target_name, target_profile = next(iter(profiles.items()))
                        diagnostics['expected_speaker'] = target_name

                if target_profile:
                    # Get embedding info
                    embedding = target_profile.get('embedding')
                    if embedding is not None:
                        if hasattr(embedding, 'shape'):
                            diagnostics['embedding_dimension'] = int(embedding.shape[0])
                        elif hasattr(embedding, '__len__'):
                            diagnostics['embedding_dimension'] = len(embedding)

                    # Get sample count from profile or database
                    diagnostics['samples_in_db'] = target_profile.get('total_samples', 0)

                    if self.speaker_engine.learning_db and diagnostics['samples_in_db'] == 0:
                        try:
                            profile_data = await self.speaker_engine.learning_db.get_speaker_profile(target_name)
                            if profile_data:
                                diagnostics['samples_in_db'] = profile_data.get('total_samples', 0)
                        except Exception as e:
                            logger.debug(f"Could not get sample count: {e}")

                    # Check if insufficient samples
                    if diagnostics['samples_in_db'] < 10:
                        diagnostics['primary_reason'] = f'Insufficient voice samples ({diagnostics["samples_in_db"]}/30 recommended)'
                        diagnostics['suggested_fix'] = 'Re-enroll voice profile with more samples'
                        diagnostics['architecture_issue'] = 'Voice enrollment may not have captured enough samples'
                        diagnostics['user_message'] = 'Voice profile needs more training samples.'
                        diagnostics['severity'] = 'high'

                    # Get threshold
                    diagnostics['threshold'] = f"{target_profile.get('threshold', 0.40):.2%}"
                    if hasattr(self.speaker_engine, '_get_adaptive_threshold'):
                        try:
                            threshold = await self.speaker_engine._get_adaptive_threshold(target_name, target_profile)
                            diagnostics['threshold'] = f"{threshold:.2%}"
                        except:
                            pass

                elif len(profiles) == 0:
                    # Truly no profiles loaded
                    diagnostics['primary_reason'] = 'No voice profiles loaded in system'
                    diagnostics['suggested_fix'] = 'Enroll voice profile first'
                    diagnostics['architecture_issue'] = 'No speaker profiles registered in database'
                    diagnostics['user_message'] = 'Voice profile not found. Please enroll first.'
                    diagnostics['severity'] = 'critical'
                else:
                    # Profiles exist but speaker_name doesn't match any
                    available_profiles = list(profiles.keys())
                    diagnostics['primary_reason'] = f'Speaker "{speaker_name}" not in registered profiles: {available_profiles}'
                    diagnostics['suggested_fix'] = 'Voice may not match enrolled profile. Try re-enrolling.'
                    diagnostics['architecture_issue'] = 'Speaker name mismatch'
                    diagnostics['user_message'] = 'Voice not recognized. Try speaking more clearly.'
                    diagnostics['severity'] = 'high'

            # 3. CONFIDENCE ANALYSIS
            if diagnostics['audio_quality'] == 'acceptable' and diagnostics['samples_in_db'] >= 10:
                if confidence == 0.0:
                    # Exactly 0% means either silent audio or a processing error
                    diagnostics['primary_reason'] = 'Zero confidence - possible audio processing issue'
                    diagnostics['suggested_fix'] = 'Check microphone input and try speaking more clearly'
                    diagnostics['architecture_issue'] = 'Audio may not be reaching embedding extraction'
                    diagnostics['user_message'] = 'Could not process voice. Please try again.'
                    diagnostics['severity'] = 'critical'
                elif confidence < 0.05:
                    diagnostics['primary_reason'] = 'Voice does not match enrolled profile'
                    diagnostics['suggested_fix'] = 'Verify speaker identity or re-enroll'
                    diagnostics['architecture_issue'] = 'Possible embedding dimension mismatch or model version incompatibility'
                    diagnostics['user_message'] = 'Voice not recognized. Try re-enrolling your voice profile.'
                    diagnostics['severity'] = 'critical'
                elif confidence < 0.20:
                    diagnostics['primary_reason'] = f'Low confidence match ({confidence:.2%})'
                    diagnostics['suggested_fix'] = 'Improve audio quality, reduce background noise, or re-enroll'
                    diagnostics['user_message'] = 'Voice match uncertain. Speak in a quieter environment.'
                    diagnostics['severity'] = 'high'
                elif confidence < 0.40:
                    diagnostics['primary_reason'] = f'Moderate confidence ({confidence:.2%}) below threshold'
                    diagnostics['suggested_fix'] = 'System is learning your voice. Keep using it or re-enroll for better accuracy'
                    diagnostics['user_message'] = 'Almost there! System is still learning your voice.'
                    diagnostics['severity'] = 'medium'
                else:
                    diagnostics['primary_reason'] = f'Confidence ({confidence:.2%}) just below threshold'
                    diagnostics['suggested_fix'] = 'Try again with clearer audio'
                    diagnostics['user_message'] = 'Very close! Please try again.'
                    diagnostics['severity'] = 'low'

            # 4. SYSTEM ARCHITECTURE CHECKS
            if diagnostics['architecture_issue'] == 'none detected':
                # Check for known architectural issues
                if diagnostics.get('embedding_dimension') not in [192, 256, 512, 768]:
                    diagnostics['architecture_issue'] = f'Unusual embedding dimension: {diagnostics.get("embedding_dimension")}'

                if diagnostics['samples_in_db'] == 0 and confidence > 0:
                    diagnostics['architecture_issue'] = 'Profile exists but no samples in database - possible data corruption'

        except Exception as e:
            logger.error(f"Failure analysis error: {e}", exc_info=True)
            diagnostics['primary_reason'] = f'Diagnostic error: {str(e)}'

        return diagnostics

    async def _analyze_context(
        self, transcribed_text: str, context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Analyze context using CAI"""
        if not self.cai_handler:
            return {"available": False}

        try:
            # Use CAI to analyze context
            # This could check: screen state, time of day, location, etc.
            cai_result = {
                "available": True,
                "screen_state": context.get("screen_state", "locked") if context else "locked",
                "time_of_day": datetime.now().hour,
                "is_work_hours": 9 <= datetime.now().hour < 17,
                "context_score": 0.9,  # Placeholder
            }

            return cai_result

        except Exception as e:
            logger.error(f"Context analysis failed: {e}")
            return {"available": False, "error": str(e)}

    async def _analyze_scenario(
        self, transcribed_text: str, context: Optional[Dict[str, Any]], speaker_name: str
    ) -> Dict[str, Any]:
        """Analyze scenario using SAI"""
        if not self.sai_analyzer:
            return {"available": False}

        try:
            # Use SAI to detect scenario
            # This could detect: emergency unlock, routine unlock, suspicious activity, etc.
            scenario_result = {
                "available": True,
                "scenario_type": "routine_unlock",
                "risk_level": "low",
                "confidence": 0.95,
            }

            return scenario_result

        except Exception as e:
            logger.error(f"Scenario analysis failed: {e}")
            return {"available": False, "error": str(e)}

    async def _perform_unlock(
        self, speaker_name: str, context_analysis: Dict[str, Any], scenario_analysis: Dict[str, Any], attempt_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """Perform actual screen unlock with enhanced error handling and ML metrics collection"""
        try:
            # Get password from keychain
            import subprocess

            # Try multiple keychain service names for compatibility
            keychain_services = [
                ("com.jarvis.voiceunlock", "unlock_token"),  # Primary (enable_screen_unlock.sh format)
                ("jarvis_voice_unlock", "jarvis"),  # Alternative format
                ("JARVIS_Screen_Unlock", "jarvis_user"),  # Legacy format
            ]

            password = None
            service_used = None

            for service_name, account_name in keychain_services:
                result = subprocess.run(
                    [
                        "security",
                        "find-generic-password",
                        "-s",
                        service_name,
                        "-a",
                        account_name,
                        "-w",
                    ],
                    capture_output=True,
                    text=True,
                )

                if result.returncode == 0:
                    password = result.stdout.strip()
                    service_used = service_name
                    logger.debug(f"Password found in keychain (service: {service_name})")
                    break

            if not password:
                logger.error("Password not found in keychain")
                logger.error("Tried services: com.jarvis.voiceunlock, jarvis_voice_unlock, JARVIS_Screen_Unlock")
                logger.error("Run: ~/Documents/repos/JARVIS-AI-Agent/backend/voice_unlock/fix_keychain_password.sh")
                return {
                    "success": False,
                    "reason": "password_not_found",
                    "message": "Password not found in keychain. Run fix_keychain_password.sh to fix.",
                }

            # üñ•Ô∏è DISPLAY-AWARE SAI: Use situational awareness for display configuration
            # Automatically detects mirrored/TV displays and adapts typing strategy
            from voice_unlock.secure_password_typer import type_password_with_display_awareness

            unlock_success, typing_metrics, display_context = await type_password_with_display_awareness(
                password=password,
                submit=True,
                attempt_id=attempt_id  # Enable ML metrics collection
            )

            # Log display context for debugging
            if display_context:
                logger.info(f"üñ•Ô∏è [SAI] Display context: mode={display_context.get('display_mode')}, "
                           f"mirrored={display_context.get('is_mirrored')}, "
                           f"tv={display_context.get('is_tv_connected')}")

            # ü§ñ ML LEARNING: Update typing model with results (CRITICAL: Learn from failures too!)
            if self.ml_engine and typing_metrics:
                try:
                    # Extract metrics for ML learning
                    await self.ml_engine.typing_learner.update_from_typing_session(
                        success=unlock_success,  # ACTUAL unlock status
                        duration_ms=typing_metrics.get('total_duration_ms', 0),
                        failed_at_char=typing_metrics.get('failed_at_character'),
                        char_metrics=typing_metrics.get('character_metrics', [])
                    )
                    self.stats["ml_typing_updates"] += 1
                    status = "‚úÖ SUCCESS" if unlock_success else "‚ùå FAILURE"
                    logger.info(f"ü§ñ ML: Password typing model updated - {status} - learning from attempt")
                except Exception as e:
                    logger.error(f"ML typing learning update failed: {e}", exc_info=True)

            if unlock_success:
                logger.info(f"‚úÖ Screen unlocked by {speaker_name} (keychain: {service_used})")
            else:
                logger.error(f"‚ùå Unlock failed for {speaker_name} - password may be incorrect")

            return {
                "success": unlock_success,
                "message": (
                    f"Screen unlocked by {speaker_name}" if unlock_success else "Unlock failed - password may be incorrect"
                ),
            }

        except Exception as e:
            logger.error(f"Unlock failed: {e}", exc_info=True)
            return {"success": False, "reason": "unlock_error", "message": str(e)}

    async def _update_speaker_profile(
        self, speaker_name: str, audio_data: bytes, transcribed_text: str, success: bool
    ):
        """Update speaker profile with continuous learning"""
        if not self.speaker_engine or not self.learning_db:
            return

        try:
            # Extract embedding
            embedding = await self.speaker_engine._extract_embedding(audio_data)

            if embedding is None:
                return

            # Update profile in speaker engine (continuous learning)
            profile = self.speaker_engine.profiles.get(speaker_name)
            if profile:
                # Moving average of embeddings
                alpha = 0.05  # Slow learning rate for stability
                profile.embedding = (1 - alpha) * profile.embedding + alpha * embedding
                profile.sample_count += 1
                profile.updated_at = datetime.now()

                # Update in database
                await self.learning_db.update_speaker_embedding(
                    speaker_id=profile.speaker_id,
                    embedding=profile.embedding.tobytes(),
                    confidence=profile.confidence,
                    is_primary_user=profile.is_owner,
                )

                self.stats["learning_updates"] += 1
                logger.debug(
                    f"üìà Updated profile for {speaker_name} (sample #{profile.sample_count})"
                )

        except Exception as e:
            logger.error(f"Failed to update speaker profile: {e}")

    async def _record_unlock_attempt(
        self,
        speaker_name: Optional[str],
        transcribed_text: str,
        success: bool,
        rejection_reason: Optional[str],
        audio_data: bytes,
        stt_confidence: float,
        speaker_confidence: float,
        context_data: Optional[Dict[str, Any]] = None,
        scenario_data: Optional[Dict[str, Any]] = None,
        security_analysis: Optional[Dict[str, Any]] = None,
    ) -> Optional[int]:
        """
        Record unlock attempt to learning database with full security analysis

        Returns:
            Optional[int]: Attempt ID for ML metrics linkage
        """
        if not self.learning_db:
            return None

        try:
            # Record voice sample
            if speaker_name:
                await self.learning_db.record_voice_sample(
                    speaker_name=speaker_name,
                    audio_data=audio_data,
                    transcription=transcribed_text,
                    audio_duration_ms=len(audio_data) / 32,  # Estimate
                    quality_score=stt_confidence,
                )

            # Build comprehensive response including security analysis
            jarvis_response = "Unlock " + (
                "successful" if success else f"failed: {rejection_reason}"
            )
            if security_analysis:
                threat_level = security_analysis.get("threat_level", "unknown")
                scenario = security_analysis.get("scenario", "unknown")
                jarvis_response += f" [Threat: {threat_level}, Scenario: {scenario}]"

            # Record unlock attempt (custom table or use existing)
            interaction_id = await self.learning_db.record_interaction(
                user_query=transcribed_text,
                jarvis_response=jarvis_response,
                response_type="voice_unlock",
                confidence_score=speaker_confidence,
                success=success,
                metadata={
                    "speaker_name": speaker_name,
                    "rejection_reason": rejection_reason,
                    "security_analysis": security_analysis,
                    "context_data": context_data,
                    "scenario_data": scenario_data,
                },
            )

            logger.debug(f"üìù Recorded unlock attempt (ID: {interaction_id})")

            # If this is a high-threat event, log it separately for security monitoring
            if security_analysis and security_analysis.get("threat_level") == "high":
                logger.warning(
                    f"üö® HIGH THREAT: {speaker_name} - {security_analysis.get('scenario')} - Attempt #{security_analysis.get('historical_context', {}).get('recent_attempts_24h', 0)}"
                )

            return interaction_id

        except Exception as e:
            logger.error(f"Failed to record unlock attempt: {e}")
            return None

    async def _analyze_security_event(
        self,
        speaker_name: str,
        transcribed_text: str,
        context: Optional[Dict[str, Any]],
        speaker_confidence: float,
    ) -> Dict[str, Any]:
        """
        Analyze unauthorized unlock attempt using SAI (Situational Awareness Intelligence).
        Provides dynamic, intelligent analysis with zero hardcoding.
        """
        analysis = {
            "event_type": "unauthorized_unlock_attempt",
            "speaker_name": speaker_name,
            "confidence": speaker_confidence,
            "timestamp": datetime.now().isoformat(),
            "threat_level": "low",  # Will be dynamically determined
            "scenario": "unknown",
            "historical_context": {},
            "recommendations": [],
        }

        try:
            # Get historical data about this speaker
            if self.learning_db:
                # Check past attempts by this speaker
                past_attempts = await self._get_speaker_unlock_history(speaker_name)
                analysis["historical_context"] = {
                    "total_attempts": len(past_attempts),
                    "recent_attempts_24h": len(
                        [a for a in past_attempts if self._is_recent(a, hours=24)]
                    ),
                    "pattern": self._detect_attempt_pattern(past_attempts),
                }

                # Determine threat level based on patterns
                if analysis["historical_context"]["recent_attempts_24h"] > 5:
                    analysis["threat_level"] = "high"
                    analysis["scenario"] = "persistent_unauthorized_access"
                elif analysis["historical_context"]["recent_attempts_24h"] > 2:
                    analysis["threat_level"] = "medium"
                    analysis["scenario"] = "repeated_unauthorized_access"
                else:
                    analysis["threat_level"] = "low"
                    analysis["scenario"] = "single_unauthorized_access"

            # Use SAI to analyze scenario
            if self.sai_analyzer:
                try:
                    sai_analysis = await self._get_sai_scenario_analysis(
                        event_type="unauthorized_unlock",
                        speaker_name=speaker_name,
                        context=context,
                    )
                    analysis["sai_scenario"] = sai_analysis
                except Exception as e:
                    logger.debug(f"SAI analysis unavailable: {e}")

            # Determine if this is a known person (family member, friend, etc.)
            is_known_person = await self._is_known_person(speaker_name)
            analysis["is_known_person"] = is_known_person

            if is_known_person:
                analysis["relationship"] = "known_non_owner"
                analysis["scenario"] = "known_person_unauthorized_access"
            else:
                analysis["relationship"] = "unknown"

            # Generate recommendations
            if analysis["threat_level"] == "high":
                analysis["recommendations"] = [
                    "alert_owner",
                    "log_security_event",
                    "consider_additional_security",
                ]
            elif analysis["threat_level"] == "medium":
                analysis["recommendations"] = ["log_security_event", "monitor_future_attempts"]
            else:
                analysis["recommendations"] = ["log_attempt"]

        except Exception as e:
            logger.error(f"Security analysis failed: {e}")
            analysis["error"] = str(e)

        return analysis

    async def _generate_security_response(
        self,
        speaker_name: str,
        reason: str,
        analysis: Dict[str, Any],
        context: Optional[Dict[str, Any]],
    ) -> str:
        """
        Generate intelligent, dynamic security response.
        Uses SAI and historical data to create natural, contextual messages.
        ZERO hardcoding - fully dynamic and adaptive.
        """
        import random  # nosec B311 # UI message selection

        threat_level = analysis.get("threat_level", "low")
        scenario = analysis.get("scenario", "unknown")
        is_known_person = analysis.get("is_known_person", False)
        historical = analysis.get("historical_context", {})
        total_attempts = historical.get("total_attempts", 0)
        recent_attempts = historical.get("recent_attempts_24h", 0)

        # Dynamic response based on threat level and scenario
        # Handle None speaker_name throughout
        speaker_display = speaker_name if speaker_name and speaker_name != "None" else ""

        if threat_level == "high" and recent_attempts > 5:
            # Persistent unauthorized attempts - firm warning
            if speaker_display:
                responses = [
                    f"Access denied. {speaker_display}, this is your {recent_attempts}th unauthorized attempt in 24 hours. Only the device owner can unlock this system.",
                    f"I'm sorry {speaker_display}, but I cannot allow that. You've attempted unauthorized access {recent_attempts} times today. This system is secured for the owner only.",
                    f"{speaker_display}, I must inform you that I cannot grant access. This is your {recent_attempts}th attempt, and this device is owner-protected.",
                ]
            else:
                responses = [
                    f"Access denied. This is the {recent_attempts}th unauthorized attempt in 24 hours. Voice authentication failed.",
                    f"Multiple unauthorized access attempts detected. This system is secured for the owner only.",
                    f"Security alert: {recent_attempts} failed attempts recorded. Voice verification required.",
                ]
        elif threat_level == "medium" and recent_attempts > 2:
            # Multiple attempts - polite but firm
            responses = [
                f"I'm sorry {speaker_name}, but I cannot unlock this device. You've tried {recent_attempts} times recently. Only the device owner has voice unlock privileges.",
                f"Access denied, {speaker_name}. This is your {recent_attempts}th attempt. Voice unlock is restricted to the device owner.",
                f"{speaker_name}, I cannot grant access. You've attempted this {recent_attempts} times, but only the owner can unlock via voice.",
            ]
        elif is_known_person and total_attempts < 3:
            # Known person, first few attempts - friendly but clear
            responses = [
                f"I recognize you, {speaker_name}, but I'm afraid only the device owner can unlock via voice. Perhaps they can assist you?",
                f"Hello {speaker_name}. While I know you, voice unlock is reserved for the device owner only. You may need their assistance.",
                f"{speaker_name}, I cannot unlock the device for you. Voice authentication is owner-only. The owner can help you if needed.",
            ]
        elif scenario == "single_unauthorized_access":
            # First attempt by unknown person - polite explanation
            responses = [
                f"I'm sorry, but I don't recognize you as the device owner, {speaker_name}. Voice unlock is restricted to the owner only.",
                f"Access denied. {speaker_name}, only the device owner can unlock this system via voice. I cannot grant you access.",
                f"I cannot unlock this device for you, {speaker_name}. Voice unlock requires owner authentication, and you are not registered as the owner.",
                f"{speaker_name}, this device is secured with owner-only voice authentication. I cannot grant access to non-owner users.",
            ]
        else:
            # Default - clear and professional
            # Handle None speaker_name gracefully
            if speaker_name and speaker_name != "None":
                responses = [
                    f"Access denied, {speaker_name}. Only the device owner can unlock via voice authentication.",
                    f"I'm sorry {speaker_name}, but voice unlock is restricted to the device owner only.",
                    f"{speaker_name}, I cannot grant access. This system requires owner voice authentication.",
                ]
            else:
                # Unknown speaker (couldn't identify)
                responses = [
                    "Voice not recognized. Only the device owner can unlock via voice authentication.",
                    "I cannot verify your identity. Voice unlock is restricted to the registered device owner.",
                    "Access denied. Please speak clearly for voice verification, or use an alternative unlock method.",
                    "Voice authentication failed. This device is secured for the owner only.",
                ]

        # Select response dynamically
        message = random.choice(responses)  # nosec B311 # UI message selection

        # Add contextual information if available
        if scenario == "persistent_unauthorized_access":
            message += " This attempt has been logged for security purposes."

        return message

    async def _get_speaker_unlock_history(self, speaker_name: str) -> list:
        """Get past unlock attempts by this speaker from database"""
        try:
            if self.learning_db:
                # Query database for past attempts
                query = """
                    SELECT * FROM unlock_attempts
                    WHERE speaker_name = ?
                    ORDER BY timestamp DESC
                    LIMIT 100
                """
                results = await self.learning_db.execute_query(query, (speaker_name,))
                return results if results else []
        except Exception as e:
            logger.debug(f"Could not retrieve unlock history: {e}")
        return []

    def _is_recent(self, attempt: Dict[str, Any], hours: int = 24) -> bool:
        """Check if attempt is within recent time window"""
        try:
            from datetime import timedelta

            attempt_time = datetime.fromisoformat(attempt.get("timestamp", ""))
            return (datetime.now() - attempt_time) < timedelta(hours=hours)
        except:
            return False

    def _detect_attempt_pattern(self, attempts: list) -> str:
        """Detect pattern in unlock attempts"""
        if len(attempts) == 0:
            return "no_history"
        elif len(attempts) == 1:
            return "single_attempt"
        elif len(attempts) < 5:
            return "occasional_attempts"
        else:
            return "frequent_attempts"

    async def _is_known_person(self, speaker_name: str) -> bool:
        """Check if speaker is a known person (has voice profile but not owner)"""
        try:
            if self.speaker_engine and self.speaker_engine.profiles:
                return speaker_name in self.speaker_engine.profiles
        except:
            pass
        return False

    async def _get_sai_scenario_analysis(
        self, event_type: str, speaker_name: str, context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Get scenario analysis from SAI"""
        if not self.sai_analyzer:
            return {}

        try:
            # Use SAI to analyze the security scenario
            analysis = await self.sai_analyzer.analyze_scenario(
                event_type=event_type,
                speaker=speaker_name,
                context=context or {},
            )
            return analysis
        except Exception as e:
            logger.debug(f"SAI analysis failed: {e}")
            return {}

    async def _create_failure_response(
        self,
        reason: str,
        message: str,
        speaker_name: Optional[str] = None,
        security_analysis: Optional[Dict[str, Any]] = None,
        diagnostics: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Create standardized failure response with optional security analysis and diagnostics"""
        response = {
            "success": False,
            "reason": reason,
            "message": message,
            "speaker_name": speaker_name,
            "timestamp": datetime.now().isoformat(),
        }

        if security_analysis:
            response["security_analysis"] = security_analysis

        if diagnostics:
            response["diagnostics"] = diagnostics

        return response

    def get_stats(self) -> Dict[str, Any]:
        """Get service statistics"""
        return {
            **self.stats,
            "owner_profile_loaded": self.owner_profile is not None,
            "owner_name": self.owner_profile.get("speaker_name") if self.owner_profile else None,
            "password_loaded": self.owner_password_hash is not None,
            "components_initialized": {
                "hybrid_stt": self.stt_router is not None,
                "speaker_recognition": self.speaker_engine is not None,
                "learning_database": self.learning_db is not None,
                "cai": self.cai_handler is not None,
                "sai": self.sai_analyzer is not None,
            },
        }


# Global singleton
_intelligent_unlock_service: Optional[IntelligentVoiceUnlockService] = None


def get_intelligent_unlock_service() -> IntelligentVoiceUnlockService:
    """Get global intelligent unlock service instance"""
    global _intelligent_unlock_service
    if _intelligent_unlock_service is None:
        _intelligent_unlock_service = IntelligentVoiceUnlockService()
    return _intelligent_unlock_service
